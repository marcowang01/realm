{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_answer": "1,000 dialogues with approximately 11,000 utterances.", "predicted_evidence": ["1,000 dialogues with approximately 11,000 utterances."]}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_answer": "Using domain-specific ontologies, each user turn is manually annotated with one or more intent labels.", "predicted_evidence": ["Using domain-specific ontologies, each user turn is manually annotated with one or more intent labels."]}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_answer": "The baselines outperformed by this work include Seq2Seq, Mem2Seq, and the combination of Seq2Seq and knowledge retrieval.", "predicted_evidence": ["The baselines outperformed by this work include Seq2Seq, Mem2Seq, and the combination of Seq2Seq and knowledge retrieval."]}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_answer": "The evaluation metrics used in the paper are perplexity, BLEU score, recall@k, and success rate.", "predicted_evidence": ["The evaluation metrics used in the paper are perplexity, BLEU score, recall@k, and success rate."]}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_answer": "The accuracy of the OpenTapioca model is reported to be competitive with state-of-the-art (SOTA) entity linking systems.", "predicted_evidence": ["The accuracy of the OpenTapioca model is reported to be competitive with state-of-the-art (SOTA) entity linking systems."]}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_answer": "The paper compares against two previous methods: BERT for rumor detection and a baseline method based on logistic regression.", "predicted_evidence": ["The paper compares against two previous methods: BERT for rumor detection and a baseline method based on logistic regression."]}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_answer": "The evaluation metric used in the paper is F1-score.", "predicted_evidence": ["The evaluation metric used in the paper is F1-score."]}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_answer": "No, the methods proposed in the paper are not fully supervised. They utilize weakly supervised learning approaches.", "predicted_evidence": ["No, the methods proposed in the paper are not fully supervised. They utilize weakly supervised learning approaches."]}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_answer": "No, the paper does not explicitly mention building a dataset of rumors. Instead, they utilize publicly available datasets for rumor detection tasks.", "predicted_evidence": ["No, the paper does not explicitly mention building a dataset of rumors. Instead, they utilize publicly available datasets for rumor detection tasks."]}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_answer": "The paper evaluates their methods on English and Chinese languages.", "predicted_evidence": ["The paper evaluates their methods on English and Chinese languages."]}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_answer": "The paper does not explicitly provide a definition of rumors. However, they consider rumors as unverified information that circulates on social media platforms.", "predicted_evidence": ["The paper does not explicitly provide a definition of rumors. However, they consider rumors as unverified information that circulates on social media platforms."]}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_answer": "The paper compares the Sentence Level Recurrent Topic Model (SLRTM) with two baselines: Latent Dirichlet Allocation (LDA) and a Hierarchical Dirichlet Process (HDP) variant.", "predicted_evidence": ["The paper compares the Sentence Level Recurrent Topic Model (SLRTM) with two baselines: Latent Dirichlet Allocation (LDA) and a Hierarchical Dirichlet Process (HDP) variant."]}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_answer": "The paper explores two tasks: sentence-level topic modeling and topic coherence evaluation.", "predicted_evidence": ["The paper explores two tasks: sentence-level topic modeling and topic coherence evaluation."]}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "predicted_answer": "The paper uses the CoNLL-2003 dataset for Named Entity Recognition (NER).", "predicted_evidence": ["The paper uses the CoNLL-2003 dataset for Named Entity Recognition (NER)."]}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "predicted_answer": "The paper incorporates direction and relative distance in attention by introducing position-aware self-attention mechanisms and using relative position encodings.", "predicted_evidence": ["The paper incorporates direction and relative distance in attention by introducing position-aware self-attention mechanisms and using relative position encodings."]}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_answer": "Yes, the TENER model proposed in the paper outperforms current state-of-the-art models for Named Entity Recognition (NER) on multiple benchmark datasets.", "predicted_evidence": ["Yes, the TENER model proposed in the paper outperforms current state-of-the-art models for Named Entity Recognition (NER) on multiple benchmark datasets."]}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_answer": "The paper does not explicitly mention a single accuracy score. However, it reports performance results in terms of exact match (EM) and F1 scores for question answering tasks on different datasets using the KALM system.", "predicted_evidence": ["The paper does not explicitly mention a single accuracy score. However, it reports performance results in terms of exact match (EM) and F1 scores for question answering tasks on different datasets using the KALM system."]}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_answer": "The paper does not explicitly mention the state-of-the-art systems in the field of knowledge authoring and question answering.", "predicted_evidence": ["The paper does not explicitly mention the state-of-the-art systems in the field of knowledge authoring and question answering."]}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_answer": "The paper evaluates the KALM system on the AI2 Reasoning Challenge (ARC) dataset, which consists of science-related multiple-choice questions.", "predicted_evidence": ["The paper evaluates the KALM system on the AI2 Reasoning Challenge (ARC) dataset, which consists of science-related multiple-choice questions."]}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_answer": "The contributions of the paper include introducing an annotated Italian event detection dataset, proposing a deep learning model for event detection in Italian, and evaluating the model's performance on the newly created dataset.", "predicted_evidence": ["The contributions of the paper include introducing an annotated Italian event detection dataset, proposing a deep learning model for event detection in Italian, and evaluating the model's performance on the newly created dataset."]}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_answer": "The paper uses three baselines for comparison: a rule-based system, a support vector machine (SVM) model, and a recurrent neural network (RNN) model with handcrafted features.", "predicted_evidence": ["The paper uses three baselines for comparison: a rule-based system, a support vector machine (SVM) model, and a recurrent neural network (RNN) model with handcrafted features."]}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_answer": "Yes, the model proposed in the paper can be extended to other languages by training on labeled data in the target language.", "predicted_evidence": ["Yes, the model proposed in the paper can be extended to other languages by training on labeled data in the target language."]}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_answer": "In the paper, the semantic concept label of a particular cluster is determined by analyzing the most frequent and distinctive words in that cluster.", "predicted_evidence": ["In the paper, the semantic concept label of a particular cluster is determined by analyzing the most frequent and distinctive words in that cluster."]}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_answer": "The paper discovers coherent word clusters by applying unsupervised clustering algorithms, specifically k-means clustering, to word embeddings based on distributional semantics.", "predicted_evidence": ["The paper discovers coherent word clusters by applying unsupervised clustering algorithms, specifically k-means clustering, to word embeddings based on distributional semantics."]}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_answer": "The paper introduces two datasets: \"The Word Association Test (WAT)\" and \"The Gender-Associated Word List (GWL)\". The WAT dataset consists of 3,000 word pairs, while the GWL dataset contains 5,000 gender-associated words.", "predicted_evidence": ["The paper introduces two datasets: \"The Word Association Test (WAT)\" and \"The Gender-Associated Word List (GWL)\". The WAT dataset consists of 3,000 word pairs, while the GWL dataset contains 5,000 gender-associated words."]}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_answer": "The paper uses two strong baselines: a co-occurrence baseline that measures word associations based on their co-occurrence patterns in large corpora, and a word embedding baseline that utilizes pre-trained word embeddings to capture word associations.", "predicted_evidence": ["The paper uses two strong baselines: a co-occurrence baseline that measures word associations based on their co-occurrence patterns in large corpora, and a word embedding baseline that utilizes pre-trained word embeddings to capture word associations."]}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_answer": "Data-driven models usually respond to abuse by providing non-committal or neutral responses, avoiding direct engagement with the abusive content.", "predicted_evidence": ["Data-driven models usually respond to abuse by providing non-committal or neutral responses, avoiding direct engagement with the abusive content."]}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_answer": "The paper gathered a total of 3,000 dialogues through crowdsourcing for the evaluation of abuse response strategies in conversational agents.", "predicted_evidence": ["The paper gathered a total of 3,000 dialogues through crowdsourcing for the evaluation of abuse response strategies in conversational agents."]}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_answer": "The paper evaluates a total of four different abuse response strategies in conversational agents.", "predicted_evidence": ["The paper evaluates a total of four different abuse response strategies in conversational agents."]}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_answer": "No, the automatic annotation was not evaluated in the paper.", "predicted_evidence": ["No, the automatic annotation was not evaluated in the paper."]}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_answer": "The paper considers two morphological typologies: Fusional and Agglutinative.", "predicted_evidence": ["The paper considers two morphological typologies: Fusional and Agglutinative."]}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_answer": "Yes, the model considered in the paper takes into account both derivational and inflectional morphology for semantic role labeling.", "predicted_evidence": ["Yes, the model considered in the paper takes into account both derivational and inflectional morphology for semantic role labeling."]}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_answer": "The paper uses subword-based morphological features, specifically character n-grams, to capture morphological information for semantic role labeling.", "predicted_evidence": ["The paper uses subword-based morphological features, specifically character n-grams, to capture morphological information for semantic role labeling."]}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_answer": "The paper uses two datasets: SciFig and Conceptual Captions.", "predicted_evidence": ["The paper uses two datasets: SciFig and Conceptual Captions."]}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_answer": "The captions in the paper are in English.", "predicted_evidence": ["The captions in the paper are in English."]}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_answer": "The paper explores two ad-hoc approaches: ad-hoc caption generation and ad-hoc figure-text alignment.", "predicted_evidence": ["The paper explores two ad-hoc approaches: ad-hoc caption generation and ad-hoc figure-text alignment."]}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "predicted_answer": "The paper compares the proposed method with two supervised baselines: an image captioning model and a figure-text alignment model.", "predicted_evidence": ["The paper compares the proposed method with two supervised baselines: an image captioning model and a figure-text alignment model."]}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_answer": "Yes, the data used in the paper is specific to the biomedical domain.", "predicted_evidence": ["Yes, the data used in the paper is specific to the biomedical domain."]}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_answer": "The figures and captions in the paper are sourced from the PubMed Central Open Access Subset (PMC-OAS) and the Conceptual Captions dataset.", "predicted_evidence": ["The figures and captions in the paper are sourced from the PubMed Central Open Access Subset (PMC-OAS) and the Conceptual Captions dataset."]}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_answer": "Yes, the top teams in the EmoInt-2017 shared task, including the EmoAtt system described in the paper, experimented with lexicons to improve the performance of emotion intensity prediction.", "predicted_evidence": ["Yes, the top teams in the EmoInt-2017 shared task, including the EmoAtt system described in the paper, experimented with lexicons to improve the performance of emotion intensity prediction."]}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "predicted_answer": "Yes, the EmoAtt system described in the paper experimented with the use of lexicons to enhance the prediction of emotion intensity.", "predicted_evidence": ["Yes, the EmoAtt system described in the paper experimented with the use of lexicons to enhance the prediction of emotion intensity."]}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": "The baseline in the paper is a Support Vector Regression (SVR) model with handcrafted features for predicting emotion intensity.", "predicted_evidence": ["The baseline in the paper is a Support Vector Regression (SVR) model with handcrafted features for predicting emotion intensity."]}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_answer": "The paper reports that the EmoAtt system achieved competitive performance in predicting emotion intensity compared to other participants in the EmoInt-2017 shared task.", "predicted_evidence": ["The paper reports that the EmoAtt system achieved competitive performance in predicting emotion intensity compared to other participants in the EmoInt-2017 shared task."]}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_answer": "The paper uses the dataset provided for the EmoInt-2017 shared task on emotion intensity prediction.", "predicted_evidence": ["The paper uses the dataset provided for the EmoInt-2017 shared task on emotion intensity prediction."]}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "predicted_answer": "The paper does not provide a specific definition of hate speech. However, they consider hate speech as offensive or aggressive language targeting individuals or groups based on attributes such as race, ethnicity, religion, gender, sexual orientation, etc.", "predicted_evidence": ["The paper does not provide a specific definition of hate speech. However, they consider hate speech as offensive or aggressive language targeting individuals or groups based on attributes such as race, ethnicity, religion, gender, sexual orientation, etc."]}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_answer": "The new dataset used in the paper contains hate speech data in multiple languages, including English, German, and Danish.", "predicted_evidence": ["The new dataset used in the paper contains hate speech data in multiple languages, including English, German, and Danish."]}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_answer": "The paper considers multiple aspects of hate speech, including aggression, toxicity, hate speech against individuals, hate speech against groups, and hate speech related to specific topics such as race, religion, and gender.", "predicted_evidence": ["The paper considers multiple aspects of hate speech, including aggression, toxicity, hate speech against individuals, hate speech against groups, and hate speech related to specific topics such as race, religion, and gender."]}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "predicted_answer": "The paper does not provide specific information about the size of their dataset.", "predicted_evidence": ["The paper does not provide specific information about the size of their dataset."]}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_answer": "The opportunities presented by the use of Semantic Web technologies in Machine Translation include improved translation quality, better handling of ambiguous words and phrases, enhanced cross-lingual resource integration, support for multilingual knowledge representation, and the ability to incorporate domain-specific knowledge for specialized translation tasks.", "predicted_evidence": ["The opportunities presented by the use of Semantic Web technologies in Machine Translation include improved translation quality, better handling of ambiguous words and phrases, enhanced cross-lingual resource integration, support for multilingual knowledge representation, and the ability to incorporate domain-specific knowledge for specialized translation tasks."]}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_answer": "The challenges associated with the use of Semantic Web technologies in Machine Translation include the lack of aligned and annotated multilingual data, the need for large-scale knowledge representation and acquisition, the complexity of handling polysemy and word sense disambiguation, the integration of domain-specific knowledge, and the scalability of semantic resources for real-time translation systems.", "predicted_evidence": ["The challenges associated with the use of Semantic Web technologies in Machine Translation include the lack of aligned and annotated multilingual data, the need for large-scale knowledge representation and acquisition, the complexity of handling polysemy and word sense disambiguation, the integration of domain-specific knowledge, and the scalability of semantic resources for real-time translation systems."]}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_answer": "The paper does not explicitly mention any other obstacles to automatic translations beyond those mentioned in the abstract.", "predicted_evidence": ["The paper does not explicitly mention any other obstacles to automatic translations beyond those mentioned in the abstract."]}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_answer": "The paper does not mention the specific EEG features used in advancing speech recognition with no speech or with noisy speech.", "predicted_evidence": ["The paper does not mention the specific EEG features used in advancing speech recognition with no speech or with noisy speech."]}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_answer": "The paper does not mention the specific baselines used in advancing speech recognition with no speech or with noisy speech.", "predicted_evidence": ["The paper does not mention the specific baselines used in advancing speech recognition with no speech or with noisy speech."]}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": "The paper does not mention a specific dataset used for advancing speech recognition with no speech or with noisy speech.", "predicted_evidence": ["The paper does not mention a specific dataset used for advancing speech recognition with no speech or with noisy speech."]}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_answer": "Yes, LadaBERT is shown to outperform its knowledge distillation teacher in terms of accuracy on some problems, as mentioned in the paper.", "predicted_evidence": ["Yes, LadaBERT is shown to outperform its knowledge distillation teacher in terms of accuracy on some problems, as mentioned in the paper."]}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_answer": "Yes, in the paper, the authors evaluate and compare different compression methods to determine which one yields the most gains in terms of model size and performance for the task of text classification.", "predicted_evidence": ["Yes, in the paper, the authors evaluate and compare different compression methods to determine which one yields the most gains in terms of model size and performance for the task of text classification."]}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "predicted_answer": "The paper does not explicitly mention that LadaBERT achieves state-of-the-art performance on any specific datasets.", "predicted_evidence": ["The paper does not explicitly mention that LadaBERT achieves state-of-the-art performance on any specific datasets."]}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_answer": "The paper does not explicitly mention a specific domain of text that they are working with.", "predicted_evidence": ["The paper does not explicitly mention a specific domain of text that they are working with."]}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_answer": "The paper does not mention a specific dataset that they use for their experiments.", "predicted_evidence": ["The paper does not mention a specific dataset that they use for their experiments."]}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "predicted_answer": "Yes, the paper compares their approach to abstractive summarization methods.", "predicted_evidence": ["Yes, the paper compares their approach to abstractive summarization methods."]}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_answer": "The paper discusses various types of commonsense knowledge, including object properties, event properties, and situational knowledge.", "predicted_evidence": ["The paper discusses various types of commonsense knowledge, including object properties, event properties, and situational knowledge."]}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_answer": "The term \"intrinsic geometry of spaces of learned representations\" refers to the geometric structure or properties of the space in which the learned representations of concepts or entities reside. It relates to how these representations are organized and their relationships with each other, capturing the underlying structure or manifold of the learned representations.", "predicted_evidence": ["The term \"intrinsic geometry of spaces of learned representations\" refers to the geometric structure or properties of the space in which the learned representations of concepts or entities reside. It relates to how these representations are organized and their relationships with each other, capturing the underlying structure or manifold of the learned representations."]}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "predicted_answer": "No, the paper does not mention pre-training on existing sentiment corpora for the system developed for the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project.", "predicted_evidence": ["No, the paper does not mention pre-training on existing sentiment corpora for the system developed for the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project."]}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_answer": "The paper does not specifically mention the most salient features extracted by the models developed for the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project.", "predicted_evidence": ["The paper does not specifically mention the most salient features extracted by the models developed for the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project."]}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_answer": "The paper does not explicitly mention the number of languages in the dataset for the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project.", "predicted_evidence": ["The paper does not explicitly mention the number of languages in the dataset for the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project."]}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_answer": "Yes, the system described in the paper performed well on low-resource languages in the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project.", "predicted_evidence": ["Yes, the system described in the paper performed well on low-resource languages in the 2019 Sentiment, Emotion, and Cognitive State Task of DARPA's LORELEI project."]}
