{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_answer": "220 human-human dialogs.", "predicted_evidence": ["dialog is interleaved in the dialog. Datasets ::: AntiScam Dataset. To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value. Datasets ::: PersuasionForGood Dataset. The", "game BIBREF6, the multi-party game BIBREF7 and item splitting negotiation BIBREF8. Similar to the AntiScam dataset proposed in this paper, these datasets contain off-task content and can be used to train non-collaborative dialog systems. However, since they are not specifically collected and designed for non-collaborative tasks, it might be difficult to disentangle the on-task and off-task contents and measure the performance. Therefore, we propose the AntiScam dataset, which is designed to interleave the on-task and off-task contents in the conversation, and can serve as a benchmark dataset for similar non-collaborative tasks. To better understand user utterances and separate on-task and off-task content within a conversation, previous work has designed hierarchical annotation schemes for specific domains. BIBREF9 hardy2002multi followed the DAMSL schemeBIBREF10 and annotated a multilingual human-computer dialog corpus with a hierarchical dialog act annotation scheme. BIBREF11", "generation quality. Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn. Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system. Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance. Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score. Results and Analysis. Table TABREF19 presents the main experiment results on AntiScam dataset,", "against attackers trying to collect personal information. As non-collaborative tasks are still relatively new to the study of dialog systems, there are insufficiently many meaningful datasets for evaluation and we hope this provides a valuable example. We evaluate MISSA on the newly collected AntiScam dataset and an existing PersuasionForGood dataset. Both automatic and human evaluations suggest that MISSA outperforms multiple competitive baselines. In summary, our contributions include: (i) We design a hierarchical intent annotation scheme and a semantic slot annotation scheme to annotate the non-collaborative dialog dataset, we also propose a carefully-designed AntiScam dataset to facilitate the research of non-collaborative dialog systems. (ii) We propose a model that can be applied to all non-collaborative tasks, outperforming other baselines on two different non-collaborative tasks. (iii) We develop an anti-scam dialog system to occupy attacker's attention and elicit their", "(name, address and phone number) that the system obtained from attackers as the task success score. Results and Analysis. Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21. Compared to the first TransferTransfo baseline, MISSA outperforms the TransferTransfo baseline on the on-task contents. From Table TABREF19, we observe that MISSA maintains longer conversations (14.9 turns) compared with TransferTransfo (8.5 turns), which means MISSA is better at maintaining the attacker's engagement. MISSA also has a higher task success score (1.294) than TransferTransfo (1.025), which indicates that it", "and nonresponsive_statement) while social intents are common social actions (greeting, closing, apology, thanking,respond_to_thank, and hold). For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns. Datasets. We test our approach on two non-collaborative task datasets: the AntiScam dataset and the PersuasionForGood dataset BIBREF1. Both datasets are collected from the Amazon Mechanical Turk platform in the form of typing conversations and off-task dialog is interleaved in the dialog. Datasets ::: AntiScam Dataset. To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn", "recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value. Datasets ::: PersuasionForGood Dataset. The PersuasionForGood dataset BIBREF1 was collected from typing conversations on Amazon Mechanical Turk platform. Two workers were randomly paired, one was assigned the role of persuader, the other was persuadee. The goal of the persuader was to persuade the persuadee to donate a portion of task earning to a specific charity. The dataset consists of 1,017 dialogs, where 300 dialogs are annotated with dialog acts. The average conversation length is 10.43, the vocabulary size is 8,141. Since the original PersuasionForGood dataset is annotated with dialog acts, we select the on-task dialog acts as on-task intents shown in Table TABREF2, and categorize the other dialog acts into our pre-defined off-task intents. Model ::: Background. The TransferTransfo framework was proposed to build open domain", "domains or specific requirements, which makes our dialog system more controllable. Experiments. We evaluate MISSA on two non-collaborative task datasets. AntiScam aims to build a dialog system that occupies the attacker's attention and elicits the attacker's information while PersuasionForGood BIBREF1 aims to build a dialog system that persuades people to donate to a charity. We use $80\\%$ data for training, $10\\%$ data for validation, and $10\\%$ data for testing. More training details are presented in Appendix. Experiments ::: Baseline Models. We compare MISSA mainly with two baseline models: TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data. Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first"]}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_answer": "hierarchically", "predicted_evidence": ["are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. The advantage of this hierarchical annotation scheme is apparent when starting a new non-collaborative task: we only need to focus on designing the on-task categories and semantic slots which are the same as traditional task-oriented dialog systems. Consequently, we don't have to worry about the off-task annotation design since the off-task category is universal. In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation,", "for specific domains. BIBREF9 hardy2002multi followed the DAMSL schemeBIBREF10 and annotated a multilingual human-computer dialog corpus with a hierarchical dialog act annotation scheme. BIBREF11 gupta2018semantic used a hierarchical annotation scheme for semantic parsing. Inspired by these studies, our idea is to annotate the intent and semantic slot separately in non-collaborative tasks. We propose a hierarchical intent annotation scheme that can be adopted by all non-collaborative tasks. With this annotation scheme, MISSA is able to quickly build an end-to-end trainable dialog system for any non-collaborative task. Traditional task-oriented dialog systems BIBREF12 are usually composed of multiple independent modules, for example, natural language understanding, dialog state tracking BIBREF13, BIBREF14, dialog policy manager BIBREF15, and natural language generation BIBREF16. Conversational intent is adopted to capture the meaning of task content in these dialog systems BIBREF2,", "dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents. General intents are more closely related to the syntactic meaning of the sentence (open_question, yes_no_question, positive_answer, negative_answer, responsive_statement, and nonresponsive_statement) while social intents are common social actions (greeting, closing, apology, thanking,respond_to_thank, and hold). For specific tasks, we also design a semantic slot", "BIBREF4. Therefore, we need to design a system that handles both on-task and off-task information appropriately and in a way that leads back to the system's goal. To tackle the issue of incoherent system responses to off-task content, previous studies have built hybrid systems to interleave off-task and on-task content. BIBREF4 used a rule-based dialog manager for on-task content and a neural model for off-task content, and trained a reinforcement learning model to select between these two models based on the dialog context. However, such a method is difficult to train and struggles to generalize beyond the movie promotion task they considered. To tackle these problems, we propose a hierarchical intent annotation scheme that separates on-task and off-task information in order to provide detailed supervision. For on-task information, we directly use task-related intents for representation. Off-task information, on the other hand, is too general to categorize into specific intents, so", "detailed supervision. For on-task information, we directly use task-related intents for representation. Off-task information, on the other hand, is too general to categorize into specific intents, so we choose dialog acts that convey syntax information. These acts, such as \u201copen question\" are general to all tasks. Previous studies use template-based methods to maintain sentence coherence. However, rigid templates lead to limited diversity, causing the user losing engagement. On the other hand, language generation models can generate diverse responses but are bad at being coherent. We propose Multiple Intents and Semantic Slots Annotation Neural Network (MISSA) to combine the advantages of both template and generation models and takes advantage from the hierarchical annotation at the same time. MISSA follows the TransferTransfo framework BIBREF0 with three modifications: (i) We first concurrently predict user's, system's intents and semantic slots; (ii) We then perform conditional", "and nonresponsive_statement) while social intents are common social actions (greeting, closing, apology, thanking,respond_to_thank, and hold). For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns. Datasets. We test our approach on two non-collaborative task datasets: the AntiScam dataset and the PersuasionForGood dataset BIBREF1. Both datasets are collected from the Amazon Mechanical Turk platform in the form of typing conversations and off-task dialog is interleaved in the dialog. Datasets ::: AntiScam Dataset. To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn", "building a hybrid dialog system that combines a task-oriented model and a non-task-oriented model BIBREF4, BIBREF29. In these studies, task-oriented systems and non-task-oriented systems are designed separately and both systems generate candidate responses. A selector is then designed to choose an appropriate output from the candidate responses BIBREF4 and a connector to combine two response candidates BIBREF30, BIBREF31. Compared with these works, MISSA is end-to-end trainable and thus easier to train and update. Non-Collaborative Task Annotation Scheme. To decouple syntactic and semantic information in utterances and provide detailed supervision, we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific", "incorporate the proposed hierarchical intent and semantic slot annotation for non-collaborative tasks. (2) In dialog systems, multiple generated responses can be coherent under the current context. Generating diverse responses has proven to be an enduring challenge. To increase response diversity, we sample multiple generated responses and choose an appropriate one according to a set of pre-defined rules. Model ::: Intent and Semantic Slot Classifiers. We train MISSA in a multi-task fashion. In addition to the language model task and the next-utterance prediction task, we also use separate classifiers to predict the intents and semantic slots of both human utterances and system responses. The intent classifier and semantic slot classifier for human utterances capture the semantic and syntactic meaning of human utterances, providing information to select the appropriate response among response candidates while the classifiers for the system intents and semantic slots are designed to"]}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_answer": "Baselines: TransferTransfo and hybrid model.", "predicted_evidence": ["domains or specific requirements, which makes our dialog system more controllable. Experiments. We evaluate MISSA on two non-collaborative task datasets. AntiScam aims to build a dialog system that occupies the attacker's attention and elicits the attacker's information while PersuasionForGood BIBREF1 aims to build a dialog system that persuades people to donate to a charity. We use $80\\%$ data for training, $10\\%$ data for validation, and $10\\%$ data for testing. More training details are presented in Appendix. Experiments ::: Baseline Models. We compare MISSA mainly with two baseline models: TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data. Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first", "misunderstands that the attacker was talking about the order number, possibly because the token \u201corder\u201d appeared in the attacker's utterance. We suspect that the hybrid model's bad performance on the off-task content leads to its low coherence rating (2.76) and short dialog length (8.2). To explore the influence of the intent-based conditional response generation method and the designed response filter, we perform an ablation study. The results are shown in Table TABREF19. We find that MISSA has higher fluency score and coherence score than MISSA-con (4.18 vs 3.78 for fluency, and 3.75 vs 3.68 for coherence), which suggests that conditioning on the system intent to generate responses improves the quality of the generated sentences. Compared with MISSA-sel, MISSA achieves better performance on all the metrics. For example, the engagement score for MISSA is 3.69 while MISSA-sel only has 2.87. This is because the response filter removed all the incoherent responses, which makes the", "built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19. Fluency Fluency is used to explore different models' language generation quality. Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn. Engagement In the anti-scam scenario, one of", "Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result. Experiments ::: Human Evaluation Metrics. Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to", "user utterances in non-collaborative settings. Moreover, to validate MISSA's performance, we create a non-collaborate dialog dataset that focuses on deterring phone scammers. MISSA outperforms all baseline methods in terms of fluency, coherency, and user engagement on both the newly proposed anti-scam task and an existing persuasion task. However, MISSA still produces responses that are not consistent with their distant conversation history as GPT can only track a limited history span. In future work, we plan to address this issue by developing methods that can effectively track longer dialog context. Acknowledgements. This work was supported by DARPA ASED Program HR001117S0050. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes not withstanding any copyright notation therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed", "(name, address and phone number) that the system obtained from attackers as the task success score. Results and Analysis. Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21. Compared to the first TransferTransfo baseline, MISSA outperforms the TransferTransfo baseline on the on-task contents. From Table TABREF19, we observe that MISSA maintains longer conversations (14.9 turns) compared with TransferTransfo (8.5 turns), which means MISSA is better at maintaining the attacker's engagement. MISSA also has a higher task success score (1.294) than TransferTransfo (1.025), which indicates that it", "pre-defined intents. In order to reach a common state, the user and the system need to build rapport and trust which naturally involves off-task content. Previous work did not model off-task content BIBREF2, which may have led to less optimal results. For example, in the persuasion task BIBREF1, users would ask the system \u201cHow do you feel about war?\" An example of an on-task system response that the system could have made is \u201cDo you want to make a donation?\", which sticks to the task but neglects users' question. However, a better response to such an off-task question is \u201cWar is destructive and pitiless, but you can donate to help child victims of war.\" This response is better, as it has been found that users are more likely to end the conversation if the system neglects their questions BIBREF4. Therefore, we need to design a system that handles both on-task and off-task information appropriately and in a way that leads back to the system's goal. To tackle the issue of incoherent", "${\\mathcal {L}_{S_h}}$, ${\\mathcal {L}_{I_s}}$, and ${\\mathcal {L}_{S_s}}$ are losses of intent and slots classifiers, ${\\mathcal {L}_{nup}}$ is next-utterance classification loss. $\\lambda _{LM}$, $\\lambda _{I_h}$, $\\lambda _{S_h}$, $\\lambda _{I_s}$, $\\lambda _{S_s}$, and $\\lambda _{nup}$ are the hyper-parameters that control the relative importance of every loss. Model ::: Response Generation. MISSA can generate multiple sentences in a single system turn. Therefore, we perform system generation conditioned on predicted system intents. More specifically, during the training phase, in addition to inserting a special $<$sep$>$ token at the end of each sentence, we also insert the intent of the system response as special tokens at the head of each sentence in the system response. For example, in Figure FIGREF6, we insert a $<$pos_ans$>$ token at the head of $S_t^1$, which is the system response in green. We then use a cross entropy loss function to calculate the loss between the"]}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_answer": "Fluency, Coherence, Engagement, Dialog length, Task Success Score, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), Extended Response-Slot Prediction (ERSP).", "predicted_evidence": ["Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result. Experiments ::: Human Evaluation Metrics. Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to", "(name, address and phone number) that the system obtained from attackers as the task success score. Results and Analysis. Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21. Compared to the first TransferTransfo baseline, MISSA outperforms the TransferTransfo baseline on the on-task contents. From Table TABREF19, we observe that MISSA maintains longer conversations (14.9 turns) compared with TransferTransfo (8.5 turns), which means MISSA is better at maintaining the attacker's engagement. MISSA also has a higher task success score (1.294) than TransferTransfo (1.025), which indicates that it", "built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19. Fluency Fluency is used to explore different models' language generation quality. Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn. Engagement In the anti-scam scenario, one of", "generation quality. Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn. Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system. Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance. Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score. Results and Analysis. Table TABREF19 presents the main experiment results on AntiScam dataset,", "on all the metrics. For example, the engagement score for MISSA is 3.69 while MISSA-sel only has 2.87. This is because the response filter removed all the incoherent responses, which makes the attacker more willing to keep chatting. The ablation study shows both the conditional language generation mechanism and the response filter are essential to MISSA's good performance. We also apply our method to the PersuasionForGood dataset. As shown in Table TABREF23, MISSA and its variants outperform the TransferTransfo and the hybrid models on all evaluation metrics. Such good performance indicates MISSA can be easily applied to a different non-collaborative task and achieve good performance. Particularly, MISSA achieves the lowest perplexity, which confirms that using conditional response generation leads to high quality responses. Compared with the result on AntiScam dataset, MISSA-con performs the best in terms of RIP and ERIP. We suspect the underlying reason is that there are more", "BIBREF0 and train with undelexicalized data. Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline. In addition, we perform ablation studies on MISSA to show the effects of different components. MISSA-sel denotes MISSA without response filtering. MISSA-con denotes MISSA leaving out the intent token at the start of the response generation. Experiments ::: Automatic Evaluation Metrics. Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance. Response-Intent Prediction (RIP) $\\&$ Response-Slot Prediction (RSP)", "their responses' intents and slots. The intent predictor achieves a $84\\%$ accuracy and the semantic slot predictor achieves $77\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP). Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage,", "misunderstands that the attacker was talking about the order number, possibly because the token \u201corder\u201d appeared in the attacker's utterance. We suspect that the hybrid model's bad performance on the off-task content leads to its low coherence rating (2.76) and short dialog length (8.2). To explore the influence of the intent-based conditional response generation method and the designed response filter, we perform an ablation study. The results are shown in Table TABREF19. We find that MISSA has higher fluency score and coherence score than MISSA-con (4.18 vs 3.78 for fluency, and 3.75 vs 3.68 for coherence), which suggests that conditioning on the system intent to generate responses improves the quality of the generated sentences. Compared with MISSA-sel, MISSA achieves better performance on all the metrics. For example, the engagement score for MISSA is 3.69 while MISSA-sel only has 2.87. This is because the response filter removed all the incoherent responses, which makes the"]}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_answer": "Unanswerable", "predicted_evidence": ["similarity is chosen, we need to integrate it in the inference process. Most approaches build a graph of candidate entities, where edges indicate semantic relatedness: the difference between the heuristics lie in the way this graph is used for the matching decisions. BIBREF21 use an approximate algorithm to find the densest subgraph of the semantic graph. This determines choices of entities for each mention. In other approaches, the initial evidence given by the local compatibility score is propagated along the edges of the semantic graph BIBREF14 , BIBREF22 or aggregated at a global level with a Conditional Random Field BIBREF17 . OpenTapioca: an entity linking model for Wikidata. We propose a model that adapts previous approaches to Wikidata. Let $d$ be a document (a piece of text). A spot $s \\in d$ is a pair of start and end positions in $d$ . It defines a phrase $d[s]$ , and a set of candidate entities $E[s]$ : those are all Wikidata items for which $d[s]$ is a label or alias.", "come from the choice of knowledge graph and the quality of its data. We review the main differences between Wikidata and static knowledge bases extracted from Wikipedia, and analyze their implactions for entity linking. We illustrate these differences by building a simple entity linker, OpenTapioca, which only uses data from Wikidata, and show that it is competitive with other systems with access to larger data sources for some tasks. OpenTapioca can be trained easily from a Wikidata dump only, and can be efficiently kept up to date in real time as Wikidata evolves. We also propose tools to adapt existing entity linking datasets to Wikidata, and offer a new entity linking dataset, consisting of affiliation strings extracted from research articles. Particularities of Wikidata. Wikidata is a wiki itself, meaning that it can be edited by anyone, but differs from usual wikis by its data model: information about an entity can only be input as structured data, in a format that is similar to", "from Wikipedia pages), as is generally done when working with DBpedia or YAGO. By avoiding a complex mash-up of data coming from disparate sources, our entity linking system is also simpler and easier to reproduce. Finally, it is possible keep OpenTapioca in real-time synchronization with the live version of Wikidata, with a lag of a few seconds only. This means that users are able to fix or improve the knowledge graph, for instance by adding a missing alias on an item, and immediately see the benefits on their entity linking task. This constrasts with all other systems we are aware of, where the user either cannot directly intervene on the underlying data, or there is a significant delay in propagating these updates to the entity linking system. Related work. We review the dominant architecture of entity linking heuristics following BIBREF8 , and assess its applicability to Wikidata. Entities in the knowledge base are associated with a set (or probability distribution) of possible", "author affiliation strings extracted from research articles and exposed by the ISTEX text and data mining service. In this dataset, only 64 of the 2,624 Wikidata mentions do not have a corresponding DBpedia URI. We use the Wikidata JSON dump of 2018-02-24 for our experiments, indexed with Solr (Lucene). We restrict the index to humans, organizations and locations, by selecting only items whose type was a subclass of (P279) human (Q5), organization (Q43229) or geographical object (Q618123). Labels and aliases in all languages are added to a case-sensitive FST index. We trained our classifier and its hyper-parameters by five-fold cross-validation on the training sets of the ISTEX and RSS datasets. We used GERBIL BIBREF23 to evaluate OpenTapioca against other approaches. We report the InKB micro and macro F1 scores on test sets, with GERBIL's weak annotation match method. Conclusion. The surface forms curated by Wikidata editors are sufficient to reach honourable recall, without the need", "d[s])$ . Having no access to such a probability distribution, we choose to approximate this quantity by $\\frac{p(e)}{p(d[s])}$ , where $p(e)$ is the probability that $e$ is linked to, and $p(d[s])$ is the probability that $d[s]$ occurs in a text. In other words, we estimate the popularity of the entity and the commonness of the phrase separately. We estimate the popularity of an entity $e$ by a log-linear combination of its number of statements $n_e$ , site links $s_e$ and its PageRank $r(e)$ . The PageRank is computed on the entire Wikidata using statement values and qualifiers as edges. The probability $p(d[s])$ is estimated by a simple unigram language model that can be trained either on any large unannotated dataset. The local compatibility is therefore represented by a vector of features $F(e,w)$ and the local compatibility is computed as follows, where $\\lambda $ is a weights vector: $\nF(e,w) &= ( -\\log p(d[s]), \\log p(e) , n_e, s_e, 1 ) \\\\", "$  Topic similarity. The compatibility of the topic of a candidate entity with the rest of the document is traditionally estimated by similarity measures from information retrieval such as TFIDF BIBREF11 , BIBREF12 or keyword extraction BIBREF13 , BIBREF14 , BIBREF9 . Wikidata items only consist of structured data, except in their descriptions. This makes it difficult to compute topical information using the methods above. Vector-based representations of entities can be extracted from the knowledge graph alone BIBREF15 , BIBREF16 , but it is not clear how to compare them to topic representations for plain text, which would be computed differently. In more recent work, neural word embeddings were used to represent topical information for both text and entities BIBREF17 , BIBREF6 , BIBREF18 . This requires access to large amounts of text both to train the word vectors and to derive the entity vectors from them. These vectors have been shown to encode significant semantic information by", "by BIBREF20 considers the set of wiki links $|a|, |b|$ made from or to two entities $a$ , $b$ and computes their relatedness: $ \\text{rel}(a,b) = 1 - \\frac{\\log (\\max (|a|,|b|)) - \\log (|a| \\cap |b|)}{\\log (|K|) - \\log (\\min (|a|,|b|))}$  where $|K|$ is the number of entities in the knowledge base. When linking to Wikidata instead of Wikipedia, it is tempting to reuse these heuristics, replacing wikilinks by statements. However, Wikidata's linking structure is quite different from Wikipedia: statements are generally a lot sparser than links and they have a precise semantic meaning, as editors are restricted by the available properties when creating new statements. We propose in the next section a similarity measure that we find to perform well experimentally. Once a notion of semantic similarity is chosen, we need to integrate it in the inference process. Most approaches build a graph of candidate entities, where edges indicate semantic relatedness: the difference between the", "\\dots , F(e_n, d[s_n]))$ . After $k$ iterations in the Markov chain, this defines features $M_d^k F$ . Rather than relying on these features for a fixed number of steps $k$ , we record the features at each step, which defines the vector $(F, M_d \\cdot F, M_d^2 \\cdot F, \\dots , M_d^k \\cdot F)$  This alleviates the need for an $\\alpha $ parameter while keeping the number of features small. We train a linear support vector classifier on these features and this defines the final score of each candidate entity. For each spot, our system picks the highest-scoring candidate entity that the classifier predicts as a match, if any. Experimental setup. Most entity linking datasets are annotated against DBpedia or YAGO. Wikidata contains items which do not have any corresponding Wikipedia article (in any language), so these items do not have any DBpedia or YAGO URI either. Therefore, converting an entity linking dataset from DBpedia to Wikidata requires more effort than simply following"]}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_answer": "Liu et. al (2015) and Yang et. al (2012)", "predicted_evidence": ["This results in sub-documents whose length resembles those of social media messages. Novelty is computed using term weighted tf-idf dot products between the social media message and all news sub-documents. The inverse of the minimum similarity to the nearest neighbour equates to the degree of novelty. The second approach to compute novelty relies on kterm hashing (Wurzer et. al, 2015), a recent advance in novelty detection that improved the efficiency by an order of magnitude without sacrificing effectiveness. Kterm hashing computes novelty non-comparatively. Instead of measuring similarity between documents, a single representation of previously seen information is constructed. For each document, all possible kterms are formed and hashed onto a Bloom Filter. Novelty is computed by the fraction of unseen kterms. Kterm hashing has the interesting characteristic of forming a collective 'memory', able to span all trusted resources. We exhaustively form kterm for all news articles and", "The majority of current research focuses on improving the accuracy of classifiers through new features based on clustering (Cai et. al, 2014; Zhao et. al, 2015), sentiment analysis (Qazvinian et. al, 2011; Wu et. al, 2015) as well as propagation graphs (Kwon, et. al, 2013; Wang et. al, 2015). Recent research mainly focuses on further improving the quality of rumour detection while neglecting the increasing delay between the publication and detection of a rumour. The motivation for rumour detection lies in debunking them to prevent them from spreading and causing harm. Unfortunately, state-of-the-art systems operate in a retrospective manner, meaning they detect rumours long after they have spread. The most accurate systems rely on features based on propagation graphs and clustering techniques. These features can only detect rumours after the rumours have spread and already caused harm. Therefore, researchers like Liu et. al (2015), Wu et. al (2015), Zhao et. al (2015) and Zhou et. al", "need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds as we require a model of all other features to identify 'pseudo' rumours. In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent", "of documents processed become progressively slower, which is inapplicable when operating on data streams. Our experiments show that the proposed features perform effectively and their efficiency allows them to detect rumours instantly after their publication. Conclusion. We introduced two new categories of features which significantly improve instantaneous rumour detection performance. Novelty based features consider the increased presence of unconfirmed information within a message with respect to trusted sources as an indication of being a rumour. Pseudo feedback features consider messages that are similar to previously detected rumours as more likely to also be a rumour. Pseudo feedback and its variant, recursive pseudo feedback, allow harnessing repeated signals without the need of operating retrospectively. Our evaluation showed that novelty and pseudo feedback based features perform significantly more effective than other real-time and early detection baselines, when detecting", "early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours. Liu is claimed to perform in real-time while, requiring a cluster of 5 repeated messages to judge them for rumours. Note that although these", "IR. The concept builds upon the idea that documents, which reveal similar characteristics as previously detected rumours are also likely to be a rumour. During detection, feedback about which of the previous documents describes a rumour is not available. Therefore, we rely on 'pseudo' feedback and consider all documents whose rumour score exceeds a threshold as true rumours. The PF feature describes the maximum similarity between a new document and those documents previously considered as rumour. Similarities are measured by vector proximity in term space. Conceptually, PF passes on evidence to repeated signals by increasing the rumour score of future documents if they are similar to a recently detected rumour. Note that this allows harnessing information from repeated signals without the need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds", "Weibo API. Three human annotators judged these weibos based on unanimous decision making to ensure that they don't contain rumours. Since we operate in a streaming environment, all weibos are sorted based on their publication time-stamp. Table 3 shows a list of example for rumours found in our data set. We ordered the rumours and non-rumours chronologically and divided them in half, forming a training and test set. We ensured that each of the sets consists of 50% rumours and non-rumours. This is important when effectiveness is measured by accuracy. All training and optimization use the trainings set. Performance is then reported based on a single run on the test set. Rumour detection effectiveness. To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for", "retrospectively. Our evaluation showed that novelty and pseudo feedback based features perform significantly more effective than other real-time and early detection baselines, when detecting rumours instantly after their publication. This advantage vanishes when allowing an increased detection delay. We also showed that the proposed features can be computed efficiently enough to operate on the average Twitter and Sina Weibo stream while keeping time and space requirements constant."]}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_answer": "Evaluation metrics: accuracy and DET curves", "predicted_evidence": ["for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set. Evaluation metrics. We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages. Data set. Rumour", "2011; Liu et. al, 2015; Qazvinian et. al, 2011; Yang et. al, 2012; Zhao et. al, 2015). This includes features based on the presence or number of URLs, hash-tags and user-names, POS tags, punctuation characters as well as 8 different categories of sentiment and emotions. On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score. Pseudo Feedback. In addition to novelty based features we introduce another category of features - dubbed Pseudo-Feedback (PF) feature - to boost detection performance. The feature is conceptually related to pseudo relevance feedback found in retrieval and ranking tasks in IR. The concept builds upon the idea that documents, which reveal similar characteristics as previously detected rumours are also likely to be a rumour. During detection, feedback about which of the", "on social media evolved into a popular research field which also relies on assessing the credibility of messages and their sources. The most successful methods proposed focus on classification harnessing lexical, user-centric, propagation-based (Wu et. al, 2015) and cluster-based (Cai et. al, 2014; Liu et. al, 2015; Zhao et. al, 2015) features. Many of these context based features originate from a study by Castillo et. al (2011), which pioneered in engineering features for credibility assessment on Twitter (Liu et. al, 2015). They observed a significant correlation between the trustworthiness of a tweet with context-based characteristics including hashtags, punctuation characters and sentiment polarity. When assessing the credibility of a tweet, they also assessed the source of its information by constructing features based on provided URLs as well as user based features like the activeness of the user and social graph based features like the frequency of re-tweets. A comprehensive", "by constructing features based on provided URLs as well as user based features like the activeness of the user and social graph based features like the frequency of re-tweets. A comprehensive study by Castillo et. al (2011) of information credibility assessment widely influenced recent research on rumour detection, whose main focuses lies upon improving detection quality. While studying the trustworthiness of tweets during crises, Mendoza et. al (2010) found that the topology of a distrustful tweet's propagation pattern differs from those of news and normal tweets. These findings along with the fact that rumours tend to more likely be questioned by responses than news paved the way for future research examining propagation graphs and clustering methods (Cai et. al, 2014; Zhao et. al, 2015). The majority of current research focuses on improving the accuracy of classifiers through new features based on clustering (Cai et. al, 2014; Zhao et. al, 2015), sentiment analysis (Qazvinian et.", "Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours. Once we obtained the value for the PF feature, we compute its weight using the SVM. The combination of the weight for the PF feature with the weights for all other features, obtained in the initial trainings round, resembles the final model. Experiments. The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based", "metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages. Data set. Rumour detection on social media is a novel research field without official data sets. Since licences agreements forbid redistribution of data, no data sets from previous publications are available. We therefore followed previous researchers like Liu et. al (2015) and Yang et. al (2012) and created our own dataset. trusted resources: We randomly collected 200 news articles about broad topics commonly reported by news wires over our target time period. These range from news about celebrities and disasters to financial and political affairs as seen in table 1 . Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China. To ensure a fair evaluation, we collected the news articles before judging rumours, not knowing which rumours we would", "need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds as we require a model of all other features to identify 'pseudo' rumours. In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent", "This results in sub-documents whose length resembles those of social media messages. Novelty is computed using term weighted tf-idf dot products between the social media message and all news sub-documents. The inverse of the minimum similarity to the nearest neighbour equates to the degree of novelty. The second approach to compute novelty relies on kterm hashing (Wurzer et. al, 2015), a recent advance in novelty detection that improved the efficiency by an order of magnitude without sacrificing effectiveness. Kterm hashing computes novelty non-comparatively. Instead of measuring similarity between documents, a single representation of previously seen information is constructed. For each document, all possible kterms are formed and hashed onto a Bloom Filter. Novelty is computed by the fraction of unseen kterms. Kterm hashing has the interesting characteristic of forming a collective 'memory', able to span all trusted resources. We exhaustively form kterm for all news articles and"]}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_answer": "No", "predicted_evidence": ["need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds as we require a model of all other features to identify 'pseudo' rumours. In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent", "The optimal parameter setting for weight vector $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $1 and detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $2 are learned on a test to maximise prediction accuracy. Novelty-based Features. To increase instantaneous detection performance, we compensate for the absence of future information by consulting additional data sources. In particular, we make use of news wire articles, which are considered to be of high credibility. This is reasonable as according to Petrovic et. al (2013), in the majority of cases, news wires lead social media for reporting news. When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour. Note that this closely resembles the definition of what a rumour is. Novelty Feature Construction. High volume", "Our experiments reveal that novelty based features and pseudo feedback significantly increases detection performance for early rumour detection. The contributions of this paper include: Novelty based Features We introduced a new category of features for instant rumour detection that harnesses trusted resources. Unconfirmed (novel) information with respect to trusted resources is considered as an indication of rumours. Pseudo Feedback for Detection/Classification Pseudo feedback increases detection accuracy by harnessing repeated signals, without the need of retrospective operation. Related Work. Before rumour detection, scientists already studied the related problem of information credibility evaluation (Castillo et. al. 2011; Richardson et. al, 2003). Recently, automated rumour detection on social media evolved into a popular research field which also relies on assessing the credibility of messages and their sources. The most successful methods proposed focus on classification", "performance at an optimal threshold. Figure 1 compares the effectiveness of the three algorithms for the full range of rumour scores for instantaneous detection. Different applications require a different balance between miss and false alarm. But the DET curve shows that Liu\u2019s method would be preferable over Yang for any application. Similarly, the plot reveals that our approach dominates both baselines throughout all threshold settings and for the high-recall region in particular. When increasing the detection delay to 12 and 24 hours, all three algorithms reach comparable performance with no statistically significant difference, as seen in table 4. For our approach, none of the features are computed retrospectively, which explains why the performance does not change when increasing the detection delay. The additional time allows Liu and Yang to collect repeated signals, which improves their detection accuracy. After 24 hours Liu performs the highest due to its retrospectively", "for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set. Evaluation metrics. We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages. Data set. Rumour", "IR. The concept builds upon the idea that documents, which reveal similar characteristics as previously detected rumours are also likely to be a rumour. During detection, feedback about which of the previous documents describes a rumour is not available. Therefore, we rely on 'pseudo' feedback and consider all documents whose rumour score exceeds a threshold as true rumours. The PF feature describes the maximum similarity between a new document and those documents previously considered as rumour. Similarities are measured by vector proximity in term space. Conceptually, PF passes on evidence to repeated signals by increasing the rumour score of future documents if they are similar to a recently detected rumour. Note that this allows harnessing information from repeated signals without the need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds", "of documents processed become progressively slower, which is inapplicable when operating on data streams. Our experiments show that the proposed features perform effectively and their efficiency allows them to detect rumours instantly after their publication. Conclusion. We introduced two new categories of features which significantly improve instantaneous rumour detection performance. Novelty based features consider the increased presence of unconfirmed information within a message with respect to trusted sources as an indication of being a rumour. Pseudo feedback features consider messages that are similar to previously detected rumours as more likely to also be a rumour. Pseudo feedback and its variant, recursive pseudo feedback, allow harnessing repeated signals without the need of operating retrospectively. Our evaluation showed that novelty and pseudo feedback based features perform significantly more effective than other real-time and early detection baselines, when detecting", "The majority of current research focuses on improving the accuracy of classifiers through new features based on clustering (Cai et. al, 2014; Zhao et. al, 2015), sentiment analysis (Qazvinian et. al, 2011; Wu et. al, 2015) as well as propagation graphs (Kwon, et. al, 2013; Wang et. al, 2015). Recent research mainly focuses on further improving the quality of rumour detection while neglecting the increasing delay between the publication and detection of a rumour. The motivation for rumour detection lies in debunking them to prevent them from spreading and causing harm. Unfortunately, state-of-the-art systems operate in a retrospective manner, meaning they detect rumours long after they have spread. The most accurate systems rely on features based on propagation graphs and clustering techniques. These features can only detect rumours after the rumours have spread and already caused harm. Therefore, researchers like Liu et. al (2015), Wu et. al (2015), Zhao et. al (2015) and Zhou et. al"]}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_answer": "Yes", "predicted_evidence": ["metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages. Data set. Rumour detection on social media is a novel research field without official data sets. Since licences agreements forbid redistribution of data, no data sets from previous publications are available. We therefore followed previous researchers like Liu et. al (2015) and Yang et. al (2012) and created our own dataset. trusted resources: We randomly collected 200 news articles about broad topics commonly reported by news wires over our target time period. These range from news about celebrities and disasters to financial and political affairs as seen in table 1 . Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China. To ensure a fair evaluation, we collected the news articles before judging rumours, not knowing which rumours we would", "in increased latency between publication and detection. The approach with the lowest latency banks on the 'wisdom of the crowd' (Liu et. al, 2015). In addition to traditional context and user based features they also rely on clustering micro-blogs by their topicality to identify conflicting claims, which indicate increased likelihood of rumours. Although they claim to operate in real-time, they require a cluster of at least 5 messages to detect a rumour. In contrast, we introduce new features to detect rumours as early as possible - preferably instantly, allowing them to be debunked before they spread and cause harm. Rumour Detection. Rumour detection is a challenging task, as it requires determining the truth of information (Zhao et. al, 2015). The Cambridge dictionary, defines a rumour as information of doubtful or unconfirmed truth. We rely on classification using an SVM, which is the state-of-the-art approach for novelty detection. Numerous features have been proposed for rumour", "for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set. Evaluation metrics. We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages. Data set. Rumour", "detection on-the-fly, without looking into the future. We provide an effective and highly scalable approach to detect rumours instantly after they were posted with zero delay. We introduce a new features category called novelty based features. Novelty based features compensate the absence of repeated information by consulting additional data sources - news wire articles. We hypothesize that information not confirmed by official news is an indication of rumours. Additionally we introduce pseudo feedback for classification. In a nutshell, documents that are similar to previously detected rumours are considered to be more likely to also be a rumour. The proposed features can be computed in constant time and space allowing us to process high-volume streams in real-time (Muthukrishnan, 2005). Our experiments reveal that novelty based features and pseudo feedback significantly increases detection performance for early rumour detection. The contributions of this paper include: Novelty based", "The optimal parameter setting for weight vector $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $1 and detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $2 are learned on a test to maximise prediction accuracy. Novelty-based Features. To increase instantaneous detection performance, we compensate for the absence of future information by consulting additional data sources. In particular, we make use of news wire articles, which are considered to be of high credibility. This is reasonable as according to Petrovic et. al (2013), in the majority of cases, news wires lead social media for reporting news. When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour. Note that this closely resembles the definition of what a rumour is. Novelty Feature Construction. High volume", "Weibo API. Three human annotators judged these weibos based on unanimous decision making to ensure that they don't contain rumours. Since we operate in a streaming environment, all weibos are sorted based on their publication time-stamp. Table 3 shows a list of example for rumours found in our data set. We ordered the rumours and non-rumours chronologically and divided them in half, forming a training and test set. We ensured that each of the sets consists of 50% rumours and non-rumours. This is important when effectiveness is measured by accuracy. All training and optimization use the trainings set. Performance is then reported based on a single run on the test set. Rumour detection effectiveness. To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for", "as information of doubtful or unconfirmed truth. We rely on classification using an SVM, which is the state-of-the-art approach for novelty detection. Numerous features have been proposed for rumour detection on social media, many of which originate from an original study on information credibility by Castillo et. al (2011). Unfortunately, the currently most successful features rely on information based on graph propagation and clustering, which can only be computed retrospectively. This renders them close to useless when detecting rumours early on. We introduce two new classes of features, one based on novelty, the other on pseudo feedback. Both feature categories improve detection accuracy early on, when information is limited. Problem Statement. We frame the Real-time Rumour Detection task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass", "task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass over the data. More formally, we denote by $d_t$ the document that arrives from stream $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $ at time $t$ . Upon arrival of document $d_t$ we compute its corresponding feature vector $f_{d,t}$ . Given $f_{d,t}$ and the previously obtained weigh vector $w$ we compute the rumour score $RS_{d,t} = w^T \\times f_{d,t}$ . The rumour prediction is based on a fixed thresholding strategy with respect to $\\theta $ . We predict that message $d_t$ is likely to become a rumour if its rumour score exceeds the detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $0 . The optimal parameter setting for weight vector $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $1 and detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $2 are learned on a test to maximise prediction"]}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_answer": "Unanswerable", "predicted_evidence": ["for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set. Evaluation metrics. We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages. Data set. Rumour", "This results in sub-documents whose length resembles those of social media messages. Novelty is computed using term weighted tf-idf dot products between the social media message and all news sub-documents. The inverse of the minimum similarity to the nearest neighbour equates to the degree of novelty. The second approach to compute novelty relies on kterm hashing (Wurzer et. al, 2015), a recent advance in novelty detection that improved the efficiency by an order of magnitude without sacrificing effectiveness. Kterm hashing computes novelty non-comparatively. Instead of measuring similarity between documents, a single representation of previously seen information is constructed. For each document, all possible kterms are formed and hashed onto a Bloom Filter. Novelty is computed by the fraction of unseen kterms. Kterm hashing has the interesting characteristic of forming a collective 'memory', able to span all trusted resources. We exhaustively form kterm for all news articles and", "need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds as we require a model of all other features to identify 'pseudo' rumours. In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent", "on social media evolved into a popular research field which also relies on assessing the credibility of messages and their sources. The most successful methods proposed focus on classification harnessing lexical, user-centric, propagation-based (Wu et. al, 2015) and cluster-based (Cai et. al, 2014; Liu et. al, 2015; Zhao et. al, 2015) features. Many of these context based features originate from a study by Castillo et. al (2011), which pioneered in engineering features for credibility assessment on Twitter (Liu et. al, 2015). They observed a significant correlation between the trustworthiness of a tweet with context-based characteristics including hashtags, punctuation characters and sentiment polarity. When assessing the credibility of a tweet, they also assessed the source of its information by constructing features based on provided URLs as well as user based features like the activeness of the user and social graph based features like the frequency of re-tweets. A comprehensive", "Weibo API. Three human annotators judged these weibos based on unanimous decision making to ensure that they don't contain rumours. Since we operate in a streaming environment, all weibos are sorted based on their publication time-stamp. Table 3 shows a list of example for rumours found in our data set. We ordered the rumours and non-rumours chronologically and divided them in half, forming a training and test set. We ensured that each of the sets consists of 50% rumours and non-rumours. This is important when effectiveness is measured by accuracy. All training and optimization use the trainings set. Performance is then reported based on a single run on the test set. Rumour detection effectiveness. To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for", "Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours. Once we obtained the value for the PF feature, we compute its weight using the SVM. The combination of the weight for the PF feature with the weights for all other features, obtained in the initial trainings round, resembles the final model. Experiments. The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based", "performance at an optimal threshold. Figure 1 compares the effectiveness of the three algorithms for the full range of rumour scores for instantaneous detection. Different applications require a different balance between miss and false alarm. But the DET curve shows that Liu\u2019s method would be preferable over Yang for any application. Similarly, the plot reveals that our approach dominates both baselines throughout all threshold settings and for the high-recall region in particular. When increasing the detection delay to 12 and 24 hours, all three algorithms reach comparable performance with no statistically significant difference, as seen in table 4. For our approach, none of the features are computed retrospectively, which explains why the performance does not change when increasing the detection delay. The additional time allows Liu and Yang to collect repeated signals, which improves their detection accuracy. After 24 hours Liu performs the highest due to its retrospectively", "2011; Liu et. al, 2015; Qazvinian et. al, 2011; Yang et. al, 2012; Zhao et. al, 2015). This includes features based on the presence or number of URLs, hash-tags and user-names, POS tags, punctuation characters as well as 8 different categories of sentiment and emotions. On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score. Pseudo Feedback. In addition to novelty based features we introduce another category of features - dubbed Pseudo-Feedback (PF) feature - to boost detection performance. The feature is conceptually related to pseudo relevance feedback found in retrieval and ranking tasks in IR. The concept builds upon the idea that documents, which reveal similar characteristics as previously detected rumours are also likely to be a rumour. During detection, feedback about which of the"]}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_answer": "Information of doubtful or unconfirmed truth.", "predicted_evidence": ["in increased latency between publication and detection. The approach with the lowest latency banks on the 'wisdom of the crowd' (Liu et. al, 2015). In addition to traditional context and user based features they also rely on clustering micro-blogs by their topicality to identify conflicting claims, which indicate increased likelihood of rumours. Although they claim to operate in real-time, they require a cluster of at least 5 messages to detect a rumour. In contrast, we introduce new features to detect rumours as early as possible - preferably instantly, allowing them to be debunked before they spread and cause harm. Rumour Detection. Rumour detection is a challenging task, as it requires determining the truth of information (Zhao et. al, 2015). The Cambridge dictionary, defines a rumour as information of doubtful or unconfirmed truth. We rely on classification using an SVM, which is the state-of-the-art approach for novelty detection. Numerous features have been proposed for rumour", "detection on-the-fly, without looking into the future. We provide an effective and highly scalable approach to detect rumours instantly after they were posted with zero delay. We introduce a new features category called novelty based features. Novelty based features compensate the absence of repeated information by consulting additional data sources - news wire articles. We hypothesize that information not confirmed by official news is an indication of rumours. Additionally we introduce pseudo feedback for classification. In a nutshell, documents that are similar to previously detected rumours are considered to be more likely to also be a rumour. The proposed features can be computed in constant time and space allowing us to process high-volume streams in real-time (Muthukrishnan, 2005). Our experiments reveal that novelty based features and pseudo feedback significantly increases detection performance for early rumour detection. The contributions of this paper include: Novelty based", "as information of doubtful or unconfirmed truth. We rely on classification using an SVM, which is the state-of-the-art approach for novelty detection. Numerous features have been proposed for rumour detection on social media, many of which originate from an original study on information credibility by Castillo et. al (2011). Unfortunately, the currently most successful features rely on information based on graph propagation and clustering, which can only be computed retrospectively. This renders them close to useless when detecting rumours early on. We introduce two new classes of features, one based on novelty, the other on pseudo feedback. Both feature categories improve detection accuracy early on, when information is limited. Problem Statement. We frame the Real-time Rumour Detection task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass", "IR. The concept builds upon the idea that documents, which reveal similar characteristics as previously detected rumours are also likely to be a rumour. During detection, feedback about which of the previous documents describes a rumour is not available. Therefore, we rely on 'pseudo' feedback and consider all documents whose rumour score exceeds a threshold as true rumours. The PF feature describes the maximum similarity between a new document and those documents previously considered as rumour. Similarities are measured by vector proximity in term space. Conceptually, PF passes on evidence to repeated signals by increasing the rumour score of future documents if they are similar to a recently detected rumour. Note that this allows harnessing information from repeated signals without the need of operating retrospectively. Training Pseudo Feedback Features The trainings routine differs from the standard procedure, because the computation of the PF feature requires two training rounds", "Our experiments reveal that novelty based features and pseudo feedback significantly increases detection performance for early rumour detection. The contributions of this paper include: Novelty based Features We introduced a new category of features for instant rumour detection that harnesses trusted resources. Unconfirmed (novel) information with respect to trusted resources is considered as an indication of rumours. Pseudo Feedback for Detection/Classification Pseudo feedback increases detection accuracy by harnessing repeated signals, without the need of retrospective operation. Related Work. Before rumour detection, scientists already studied the related problem of information credibility evaluation (Castillo et. al. 2011; Richardson et. al, 2003). Recently, automated rumour detection on social media evolved into a popular research field which also relies on assessing the credibility of messages and their sources. The most successful methods proposed focus on classification", "of documents processed become progressively slower, which is inapplicable when operating on data streams. Our experiments show that the proposed features perform effectively and their efficiency allows them to detect rumours instantly after their publication. Conclusion. We introduced two new categories of features which significantly improve instantaneous rumour detection performance. Novelty based features consider the increased presence of unconfirmed information within a message with respect to trusted sources as an indication of being a rumour. Pseudo feedback features consider messages that are similar to previously detected rumours as more likely to also be a rumour. Pseudo feedback and its variant, recursive pseudo feedback, allow harnessing repeated signals without the need of operating retrospectively. Our evaluation showed that novelty and pseudo feedback based features perform significantly more effective than other real-time and early detection baselines, when detecting", "caused panic and influenced public opinion. The cases in Germany and Austria in 2016, show how misleading and false information about crimes committed by refugees negatively influenced the opinion of citizens. Detecting these rumours allows debunking them to prevent them from further spreading and causing harm. The further a rumour has spread, the more likely it is to be debunked by users or traditional media (Liu et. al, 2015). However, by then rumours might have already caused harm. This highlights the importance and necessity of recognizing rumours as early as possible - preferably instantaneously. Rumour detection on social media is challenging due to the short texts, creative lexical variations and high volume of the streams. The task becomes even harder if we attempt to perform rumour detection on-the-fly, without looking into the future. We provide an effective and highly scalable approach to detect rumours instantly after they were posted with zero delay. We introduce a new", "Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours. Once we obtained the value for the PF feature, we compute its weight using the SVM. The combination of the weight for the PF feature with the weights for all other features, obtained in the initial trainings round, resembles the final model. Experiments. The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based"]}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_answer": "Memory Neural Networks (MemNNs) in babidataset, several state-of-the-art topic models on two tasks: generative document evaluation and document classification.", "predicted_evidence": ["set as INLINEFORM8 , and we ran the E-Step of the algorithm for only one iteration for efficiently consideration, which leads to the final convergence after about 6 epochs for both datasets. Gradient clipping with a clip value of 20 was used during the optimization of LSTM weights. Asynchronous stochastic gradient descent BIBREF32 with Adagrad was used to perform multi-thread parallel training. We measure the performances of different topic models according to the perplexity per word on the test set, defined as INLINEFORM0 , where INLINEFORM1 is the number of words in document INLINEFORM2 . The experimental results are summarized in Table TABREF33 . Based on the table, we have the following discussions: Our proposed SLRTM consistently outperforms the baseline models by significant margins, showing its outstanding ability in modelling the generative process of documents. In fact, as tested in our further verifications, the perplexity of SLRTM is close to that of standard LSTM language", "GMNTM. In this experiment, we fed the document vectors (e.g., the INLINEFORM0 values in SLRTM) learnt by different topic models to supervised classifiers, to compare their representation power. For 20Newsgroup, we used the multi-class logistic regression classifier and used accuracy as the evaluation criterion. For Wiki10+, since multiple labels (tags) might be associated with each document, we used logistic regression for each label and the classification result is measured by Micro- INLINEFORM1 score BIBREF33 . For both datasets, we use INLINEFORM2 of the original training set for validation, and the remaining for training. All the classification results are shown in Table TABREF37 . From the table, we can see that SLRTM is the best model under each setting on both datasets. We can further find that the embedding based methods (Doc-NADE, GMNTM and SLRTM) generate better document representations than other models, demonstrating the representative power of neural networks based on", "for LSTM BIBREF17 . Experiments. We report our experimental results in this section. Our experiments include two parts: (1) quantitative experiments, including a generative document evaluation task and a document classification task, on two datasets; (2) qualitative inspection, including the examination of the sentences generated under each topic, in order to test whether SLRTM performs well in the topic2sentence task. Quantitative Results. We compare SLRTM with several state-of-the-art topic models on two tasks: generative document evaluation and document classification. The former task is to investigate the generation capability of the models, while the latter is to show the representation ability of the models. We base our experiments on two benchmark datasets: 20Newsgroup, which contains 18,845 emails categorized into 20 different topical groups such as religion, politics, and sports. The dataset is originally partitioned into 11,314 training documents and 7,531 test documents.", "interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory. We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM's advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations. Related Work. One of the most representative topic models is Latent Dirichlet Allocation BIBREF2 , in which every word in a document has its topic drawn from document level topic weights. Several variants of LDA have been developed such as hierarchical topic models BIBREF22 and supervised topic models BIBREF3 . With the recent development of deep learning, there are also neural network", "showing its outstanding ability in modelling the generative process of documents. In fact, as tested in our further verifications, the perplexity of SLRTM is close to that of standard LSTM language model, with a small gap of about 100 (higher perplexity) on both datasets which we conjecture is due to the margin between the lower bound in equation ( EQREF16 ) and true data likelihood for SLRTM. Models that consider sequential property within sentences (i.e., GMNTM and SLRTM) are generally better than other models, which verifies the importance of words' sequential information. Furthermore, LSTM-RNN is much better in modelling such a sequential dependency than standard feed-forward networks with fixed words window as input, as verified by the lower perplexity of SLRTM compared with GMNTM. In this experiment, we fed the document vectors (e.g., the INLINEFORM0 values in SLRTM) learnt by different topic models to supervised classifiers, to compare their representation power. For", "the descriptions in their papers by our own. For SLRTM, we implemented it in C++ using Eigen and Intel MKL. For the sake of fairness, similar to BIBREF12 , we set the word embedding size, topic embedding size, and LSTM hidden layer size to be 128, 128, and 600 respectively. In the experiment, we tested the performances of SLRTM and the baselines with respect to different number of topics INLINEFORM0 , i.e., INLINEFORM1 . In initialization (values of INLINEFORM2 and INLINEFORM3 ), the LSTM weight matrices were initialized as orthogonal matrices, the word/topic embeddings were randomly sampled from the uniform distribution INLINEFORM4 and are fined-tuned through the training process, INLINEFORM5 and INLINEFORM6 were both set to INLINEFORM7 . The mini-batch size in Algorithm SECREF15 was set as INLINEFORM8 , and we ran the E-Step of the algorithm for only one iteration for efficiently consideration, which leads to the final convergence after about 6 epochs for both datasets. Gradient", "find that the embedding based methods (Doc-NADE, GMNTM and SLRTM) generate better document representations than other models, demonstrating the representative power of neural networks based on distributed representations. In addition, when the training data is larger (i.e., with more sentences per document as Wiki10+), GMNTM generates worse topical information than Doc-NADE while our SLRTM outperforms Doc-NADE, showing that with sufficient data, SLRTM is more effective in topic modeling since topic coherence is further constrained for each sentence. Qualitative Results. In this subsection, we demonstrate the capability of SLRTM in generating reasonable and understandable sentences given particular topics. In the experiment, we trained a larger SLRTM with 128 topics on a randomly sampled INLINEFORM0 Wikipedia documents in the year of 2010 with average 275 words per document. The dictionary is composed of roughly INLINEFORM1 most frequent words including common punctuation marks, with", "minibatch INLINEFORM3 in INLINEFORM4 INLINEFORM5 E-Step: INLINEFORM6 INLINEFORM7 , i.e., every topic index: Obtain INLINEFORM8 by LSTM forward pass. INLINEFORM9 DISPLAYFORM0   convergence Collect variational parameters INLINEFORM0 . M-Step: Compute the gradient INLINEFORM1 by LSTM backward pass. Use INLINEFORM2 to obtain INLINEFORM3 by stochastic gradient descent methods such as Adagrad BIBREF30 . In Algorithm SECREF15 , INLINEFORM4 is the digamma function. Equation ( EQREF18 ) guarantees the estimate of INLINEFORM5 is unbiased. In equation (), INLINEFORM6 is set as INLINEFORM7 , where INLINEFORM8 , to make sure INLINEFORM9 will converge BIBREF4 . Due to space limit, we omit the derivation details for the updating equations in Algorithm SECREF15 , as well as the forward/backward pass details for LSTM BIBREF17 . Experiments. We report our experimental results in this section. Our experiments include two parts: (1) quantitative experiments, including a generative document evaluation"]}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_answer": "generative document evaluation task and a document classification task", "predicted_evidence": ["for LSTM BIBREF17 . Experiments. We report our experimental results in this section. Our experiments include two parts: (1) quantitative experiments, including a generative document evaluation task and a document classification task, on two datasets; (2) qualitative inspection, including the examination of the sentences generated under each topic, in order to test whether SLRTM performs well in the topic2sentence task. Quantitative Results. We compare SLRTM with several state-of-the-art topic models on two tasks: generative document evaluation and document classification. The former task is to investigate the generation capability of the models, while the latter is to show the representation ability of the models. We base our experiments on two benchmark datasets: 20Newsgroup, which contains 18,845 emails categorized into 20 different topical groups such as religion, politics, and sports. The dataset is originally partitioned into 11,314 training documents and 7,531 test documents.", "length is reached (set as 25) or an EOS token is met. Such an EOS is also appended after every training sentence. The generating results are shown in Table TABREF40 . In the table, the sentences generated by random sampling and beam search are shown in the second and the third columns respectively. In the fourth column, we show the most representative words for each topics generated by SLRTM. For this purpose, we constrained the maximum sentence length to 1 in beam search, and removed stop words that are frequently used to start a sentence such as the, he, and there. From the table we have the following observations: Most of the sentences generated by both mechanisms are natural and semantically correlated with particular topics that are summarized in the first column of the table. The random sampling mechanism usually produces diverse sentences, whereas some grammar errors may happen (e.g., the last sampled sentence for Topic 4; re-ranking the randomly sampled words by a standalone", "interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory. We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM's advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations. Related Work. One of the most representative topic models is Latent Dirichlet Allocation BIBREF2 , in which every word in a document has its topic drawn from document level topic weights. Several variants of LDA have been developed such as hierarchical topic models BIBREF22 and supervised topic models BIBREF3 . With the recent development of deep learning, there are also neural network", "sampling mechanism usually produces diverse sentences, whereas some grammar errors may happen (e.g., the last sampled sentence for Topic 4; re-ranking the randomly sampled words by a standalone language model might further improve the correctness of the sentence). In contrast, sentences outputted by beam search are safer in matching grammar rules, but are not diverse enough. This is consistent with the observations in BIBREF21 . In addition to topic2sentece, SLRTM maintains the capability of generating words for topics (shown in the last column of the table), similar to conventional topic models. Conclusion. In this paper, we proposed a novel topic model called Sentence Level Recurrent Topic Model (SLRTM), which models the sequential dependency of words and topic coherence within a sentence using Recurrent Neural Networks, and shows superior performance in both predictive document modeling and document classification. In addition, it makes topic2sentence possible, which can benefit", "Introduction. Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 have been proven to be effective in modeling textual documents. In these models, a word token in a document is assumed to be generated by a hidden mixture model, where the hidden variables are the topic indexes for each word and the topic assignments for words are related to document level topic weights. Due to the effectiveness and efficiency in modeling the document generation process, topic models are widely adopted in quite a lot of real world tasks such as sentiment classification BIBREF5 , social network analysis BIBREF6 , BIBREF5 , and recommendation systems BIBREF7 . Most topic models take the bag-of-words assumption, in which every document is treated as an unordered set of words and the word tokens in such a document are sampled independently with each other. The bag-of-words assumption brings computational convenience, however, it", "set of its historical words in the sentence and the second is the sentence topic, which we regard as a pseudo word and has its own distributed representations. We use Recurrent Neural Network (RNN) BIBREF16 , such as Long Short Term Memory (LSTM) BIBREF17 or Gated Recurrent Unit (GRU) network BIBREF18 , to model such a long term dependency. With the proposed SLRTM, we can not only model the document generation process more accurately, but also construct new natural sentences that are coherent with a given topic (we call it topic2sentence, similar to image2sentece BIBREF19 ). Topic2sentence has its huge potential for many real world tasks. For example, it can serve as the basis of personalized short text conversation system BIBREF20 , BIBREF21 , in which once we detect that the user is interested in certain topics, we can let these topics speak for themselves using SLRTM to improve the user satisfactory. We have conducted experiments to compare SLRTM with several strong topic model", "variants of LDA have been developed such as hierarchical topic models BIBREF22 and supervised topic models BIBREF3 . With the recent development of deep learning, there are also neural network based topic models such as BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , which use distributed representations of words to improve topic semantics. Most of the aforementioned works take the bag-of-words assumption, which might be too simple according to our discussions in the introduction. That is, it ignores both sequential dependency of words and topic coherence of words. There are some efforts trying to address the limitations of the bag-of-words assumption. For example, in BIBREF27 , both semantic (i.e., related with topics) and syntactic properties of words were modeled. After that, a hidden Markov transition model for topics was proposed BIBREF9 , in which all the words in a sentence were regarded as having the same topic. Such a one sentence, one topic assumption was also used by some other", "and Gated Recurrent Unit BIBREF18 . These new RNN models characterize long range dependencies for a sequence, and has been widely adopted in sequence modeling tasks such as machine translation BIBREF18 and short text conversation BIBREF20 . In particular, for language modeling tasks, it has been shown that RNN (and its variants such as LSTM) is much more effective than simple feedforward neural networks with fixed window size BIBREF16 given that it can model dependencies with nearly arbitrary length. Sentence Level Recurrent Topic Model. In this section, we describe the proposed Sentence Level Recurrent Topic Model (SLRTM). First of all, we list three important design factors in SLRTM as below. With the three points in mind, let us introduce the detailed generative process of SLRTM, as well as the stochastic variational inference and learning algorithm for SLRTM in the following subsections. The generative process. Suppose we have INLINEFORM0 topics, INLINEFORM1 words contained in"]}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_answer": "Yes", "predicted_evidence": ["model. The superior performance of the adapted Transformer in four datasets ranging from small datasets to big datasets depicts that the adapted Transformer is more robust to the number of training examples than the vanilla Transformer. As the last line of Table TABREF29 depicts, the scaled attention will deteriorate the performance. Experiment ::: Results on English NER datasets. The comparison between different NER models on English NER datasets is shown in Table TABREF32. The poor performance of the Transformer in the NER datasets was also reported by BIBREF16. Although performance of the Transformer is higher than BIBREF16, it still lags behind the BiLSTM-based models BIBREF5. Nonetheless, the performance is massively enhanced by incorporating the relative positional encoding and unscaled attention into the Transformer. The adaptation not only makes the Transformer achieve superior performance than BiLSTM based models, but also unveil the new state-of-the-art performance in two", "model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features. Related Work ::: Neural Architecture for NER. BIBREF3 utilized the Multi-Layer Perceptron (MLP) and CNN to avoid using task-specific features to tackle different sequence labeling tasks, such as Chunking, Part-of-Speech (POS) and NER. In BIBREF4, BiLSTM-CRF was introduced to solve sequence labeling questions. Since then, the BiLSTM has been extensively used in the field of NER BIBREF7, BIBREF21, BIBREF22, BIBREF5. Despite BiLSTM's great success in the NER task, it has to compute token representations one by one, which massively hinders full", "Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation. As shown in Table TABREF29, the vanilla Transformer does not perform well and is worse than the BiLSTM and CNN based models. However, when relative positional encoding combined, the performance was enhanced greatly, resulting in better results than the BiLSTM and CNN in all datasets. The number of training examples of the Weibo dataset is tiny, therefore the performance of the Transformer is abysmal, which is as expected since the Transformer is data-hungry. Nevertheless, when enhanced with the relative positional encoding and unscaled attention, it can achieve even better performance than the BiLSTM-based model. The superior performance of the adapted Transformer in four datasets ranging from small datasets to big datasets depicts that the adapted Transformer is more robust to the number of training", "deviation are reported. We used random-search to find the optimal hyper-parameters, hyper-parameters and their ranges are displayed in the supplemental material. We use SGD and 0.9 momentum to optimize the model. We run 100 epochs and each batch has 16 samples. During the optimization, we use the triangle learning rate BIBREF39 where the learning rate rises to the pre-set learning rate at the first 1% steps and decreases to 0 in the left 99% steps. The model achieves the highest development performance was used to evaluate the test set. The hyper-parameter search range and other settings can be found in the supplementary material. Codes are available at https://github.com/fastnlp/TENER. Experiment ::: Results on Chinese NER Datasets. We first present our results in the four Chinese NER datasets. Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation. As shown", "encoders when different word-level encoders being used. Moreover, no matter what character encoder being used or none being used, the AdaTrans word-level encoder gets the best performance. This implies that when the number of training examples increases, the AdaTrans character-level and word-level encoder can better realize their ability. Experiment ::: Convergent Speed Comparison. We compare the convergent speed of BiLSTM, ID-CNN, Transformer, and TENER in the development set of the OntoNotes 5.0. The curves are shown in Fig FIGREF37. TENER converges as fast as the BiLSTM model and outperforms the vanilla Transformer. Conclusion. In this paper, we propose TENER, a model adopting Transformer Encoder with specific customizations for the NER task. Transformer Encoder has a powerful ability to capture the long-range context. In order to make the Transformer more suitable to the NER task, we introduce the direction-aware, distance-aware and un-scaled attention. Experiments in two English", "encoder (AdaTrans for short) ) and different word-level encoders (BiLSTM, ID-CNN and AdaTrans) to implement the NER task. Results on CoNLL2003 and OntoNotes 5.0 are presented in Table TABREF34 and Table TABREF34, respectively. The ID-CNN encoder is from BIBREF23, and we re-implement their model in PyTorch. For different combinations, we use random search to find its best hyper-parameters. Hyper-parameters for character encoders were fixed. The details can be found in the supplementary material. For the results on CoNLL2003 dataset which is depicted in Table TABREF34, the AdaTrans performs as good as the BiLSTM in different character encoder scenario averagely. In addition, from Table TABREF34, we can find the pattern that the AdaTrans character encoder outpaces the BiLSTM and CNN character encoders when different word-level encoders being used. Moreover, no matter what character encoder being used or none being used, the AdaTrans word-level encoder gets the best performance. This", "to capture the long-range context. In order to make the Transformer more suitable to the NER task, we introduce the direction-aware, distance-aware and un-scaled attention. Experiments in two English NER tasks and four Chinese NER tasks show that the performance can be massively increased. Under the same pre-trained embeddings and external knowledge, our proposed modification outperforms previous models in the six datasets. Meanwhile, we also found the adapted Transformer is suitable for being used as the English character encoder, because it has the potentiality to extract intricate patterns from characters. Experiments in two English NER datasets show that the adapted Transformer character encoder performs better than BiLSTM and CNN character encoders. Supplemental Material ::: Character Encoder. We exploit four kinds of character encoders. For all character encoders, the randomly initialized character embeddings are 30d. The hidden size of BiLSTM used in the character encoder is", "attention into the Transformer. The adaptation not only makes the Transformer achieve superior performance than BiLSTM based models, but also unveil the new state-of-the-art performance in two NER datasets when only the Glove 100d embedding and CNN character embedding are used. The same deterioration of performance was observed when using the scaled attention. Besides, if ELMo was used BIBREF28, the performance of TENER can be further boosted as depicted in Table TABREF33. Experiment ::: Analysis of Different Character Encoders. The character-level encoder has been widely used in the English NER task to alleviate the data sparsity and OOV problem in word representation. In this section, we cross different character-level encoders (BiLSTM, CNN, Transformer encoder and our adapted Transformer encoder (AdaTrans for short) ) and different word-level encoders (BiLSTM, ID-CNN and AdaTrans) to implement the NER task. Results on CoNLL2003 and OntoNotes 5.0 are presented in Table TABREF34 and"]}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_answer": "95.6%", "predicted_evidence": ["exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems. For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 . Future Work Beyond The Thesis. This section discusses the future work beyond the thesis: (1) enhancing KALM to author rules, and (2) supporting time reasoning. Authoring Rules from CNL. There are two research problems with rules. The first problem is the standardization of rules parses that express the same", "BIBREF6 is developed for question answering. KALM-QA shares the same components with KALM for syntactic parsing, frame-based parsing and role-filler disambiguation. Different from KALM, KALM-QA translates the questions to unique logical representation for queries (ULRQ), which are used to query the authored knowledge base. Evaluations. This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems. For KALM-QA, we evaluate it", "based on FrameNet BIBREF7 and BabelNet BIBREF8 where FrameNet is used to capture the meaning of the sentence and BabelNet BIBREF8 is used to disambiguate the meaning of the extracted entities from the sentence. Experiment results show that KALM achieves superior accuracy in knowledge authoring and question answering as compared to the state-of-the-art systems. The rest parts are organized as follows: Section SECREF2 discusses the related works, Section SECREF3 presents the KALM architecture, Section SECREF4 presents KALM-QA, the question answering part of KALM, Section SECREF5 shows the evaluation results, Section SECREF6 shows the future work beyond the thesis, and Section SECREF7 concludes the paper. Related Works. As is described in Section SECREF1 , CNL systems were proposed as the technology for knowledge representation and reasoning. Related works also include knowledge extraction tools, e.g., OpenIE BIBREF9 , SEMEFOR BIBREF10 , SLING BIBREF11 , and Standford KBP system BIBREF12", "causality, triggering) or using a reserved argument to represent time in each fluent. Conclusions. This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems.", "for knowledge representation and reasoning. Related works also include knowledge extraction tools, e.g., OpenIE BIBREF9 , SEMEFOR BIBREF10 , SLING BIBREF11 , and Standford KBP system BIBREF12 . These knowledge extraction tools are designed to extract semantic relations from English sentences that capture the meaning. The limitations of these tools are two-fold: first, they lack sufficient accuracy to extract the correct semantic relations and entities while KRR is very sensitive to incorrect data; second, these systems are not able to map the semantic relations to logical forms and therefore not capable of doing KRR. Other related works include the question answering frameworks, e.g., Memory Network BIBREF13 , Variational Reasoning Network BIBREF14 , ATHENA BIBREF15 , PowerAqua BIBREF16 . The first two belong to end-to-end learning approaches based on machine learning models. The last two systems have implemented semantic parsers which translate natural language sentences into", ". The first two belong to end-to-end learning approaches based on machine learning models. The last two systems have implemented semantic parsers which translate natural language sentences into intermediate query languages and then query the knowledge base to get the answers. For the machine learning based approaches, the results are not explainable. Besides, their accuracy is not high enough to provide correct answers. For ATHENA and PowerAqua, these systems perform question answering based on a priori knowledge bases. Therefore, they do not support knowledge authoring while KALM is able to support both knowledge authoring and question answering. The KALM Architecture. Figure FIGREF1 shows the architecture of KALM which translates a CNL sentence to the corresponding logical representations, unique logical representations (ULR). Attempto Parsing Engine. The input sentences are CNL sentences based on ACE grammar. KALM starts with parsing the input sentence using ACE Parser and", "corresponding frame role a BabelNet Synset ID. A complex algorithm BIBREF5 was proposed to measure the semantic similarity between a candidate BabelNet synset that contains the role-filler word and the frame-role synset. The algorithm also has optimizations that improve the efficiency of the algorithm e.g., priority-based search, caching, and so on. In addition to disambiguating the meaning of the role-fillers, this module is also used to prune the extracted frame instances where the role-filler word and the frame role are semantically incompatible. Constructing ULR. The extracted frame instances are translated into the corresponding logical representations, unique logical representation (ULR). Examples can be found in reference BIBREF5 . KALM-QA for Question Answering. Based on KALM, KALM-QA BIBREF6 is developed for question answering. KALM-QA shares the same components with KALM for syntactic parsing, frame-based parsing and role-filler disambiguation. Different from KALM, KALM-QA", "Introduction. Knowledge representation and reasoning (KRR) is the process of representing the domain knowledge in formal languages (e.g., SPARQL, Prolog) such that it can be used by expert systems to execute querying and reasoning services. KRR have been applied in many fields including financial regulations, medical diagnosis, laws, and so on. One major obstacle in KRR is the creation of large-scale knowledge bases with high quality. For one thing, this requires the knowledge engineers (KEs) not only to have the background knowledge in a certain domain but have enough skills in knowledge representation as well. Unfortunately, qualified KEs are also in short supply. Therefore, it would be useful to build a tool that allows the domain experts without any background in logic to construct and query the knowledge base simply from text. Controlled natural languages (CNLs) BIBREF0 were developed as a technology that achieves this goal. CNLs are designed based on natural languages (NLs) but"]}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_answer": "OpenIE, SEMEFOR, SLING, and Stanford KBP system", "predicted_evidence": ["based on FrameNet BIBREF7 and BabelNet BIBREF8 where FrameNet is used to capture the meaning of the sentence and BabelNet BIBREF8 is used to disambiguate the meaning of the extracted entities from the sentence. Experiment results show that KALM achieves superior accuracy in knowledge authoring and question answering as compared to the state-of-the-art systems. The rest parts are organized as follows: Section SECREF2 discusses the related works, Section SECREF3 presents the KALM architecture, Section SECREF4 presents KALM-QA, the question answering part of KALM, Section SECREF5 shows the evaluation results, Section SECREF6 shows the future work beyond the thesis, and Section SECREF7 concludes the paper. Related Works. As is described in Section SECREF1 , CNL systems were proposed as the technology for knowledge representation and reasoning. Related works also include knowledge extraction tools, e.g., OpenIE BIBREF9 , SEMEFOR BIBREF10 , SLING BIBREF11 , and Standford KBP system BIBREF12", "causality, triggering) or using a reserved argument to represent time in each fluent. Conclusions. This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems.", ". The first two belong to end-to-end learning approaches based on machine learning models. The last two systems have implemented semantic parsers which translate natural language sentences into intermediate query languages and then query the knowledge base to get the answers. For the machine learning based approaches, the results are not explainable. Besides, their accuracy is not high enough to provide correct answers. For ATHENA and PowerAqua, these systems perform question answering based on a priori knowledge bases. Therefore, they do not support knowledge authoring while KALM is able to support both knowledge authoring and question answering. The KALM Architecture. Figure FIGREF1 shows the architecture of KALM which translates a CNL sentence to the corresponding logical representations, unique logical representations (ULR). Attempto Parsing Engine. The input sentences are CNL sentences based on ACE grammar. KALM starts with parsing the input sentence using ACE Parser and", "enough background knowledge to preserve semantic equivalences of sentences that represent the same meaning but are expressed via different linguistic structures. For instance, the sentences Mary buys a car and Mary makes a purchase of a car are translated into different logical representations by the current CNL parsers. As a result, if the user ask a question who is a buyer of a car, these systems will fail to find the answer. In this thesis proposal, I will present KALM BIBREF5 , BIBREF6 , a system for knowledge authoring and question answering. KALM is superior to the current CNL systems in that KALM has a complex frame-semantic parser which can standardize the semantics of the sentences that express the same meaning via different linguistic structures. The frame-semantic parser is built based on FrameNet BIBREF7 and BabelNet BIBREF8 where FrameNet is used to capture the meaning of the sentence and BabelNet BIBREF8 is used to disambiguate the meaning of the extracted entities from", "exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems. For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 . Future Work Beyond The Thesis. This section discusses the future work beyond the thesis: (1) enhancing KALM to author rules, and (2) supporting time reasoning. Authoring Rules from CNL. There are two research problems with rules. The first problem is the standardization of rules parses that express the same", "Introduction. Knowledge representation and reasoning (KRR) is the process of representing the domain knowledge in formal languages (e.g., SPARQL, Prolog) such that it can be used by expert systems to execute querying and reasoning services. KRR have been applied in many fields including financial regulations, medical diagnosis, laws, and so on. One major obstacle in KRR is the creation of large-scale knowledge bases with high quality. For one thing, this requires the knowledge engineers (KEs) not only to have the background knowledge in a certain domain but have enough skills in knowledge representation as well. Unfortunately, qualified KEs are also in short supply. Therefore, it would be useful to build a tool that allows the domain experts without any background in logic to construct and query the knowledge base simply from text. Controlled natural languages (CNLs) BIBREF0 were developed as a technology that achieves this goal. CNLs are designed based on natural languages (NLs) but", "query the knowledge base simply from text. Controlled natural languages (CNLs) BIBREF0 were developed as a technology that achieves this goal. CNLs are designed based on natural languages (NLs) but with restricted syntax and interpretation rules that determine the unique meaning of the sentence. Representative CNLs include Attempto Controlled English BIBREF1 and PENG BIBREF2 . Each CNL is developed with a language parser which translates the English sentences into an intermediate structure, discourse representation structure (DRS) BIBREF3 . Based on the DRS structure, the language parsers further translate the DRS into the corresponding logical representations, e.g., Answer Set Programming (ASP) BIBREF4 programs. One main issue with the aforementioned CNLs is that the systems do not provide enough background knowledge to preserve semantic equivalences of sentences that represent the same meaning but are expressed via different linguistic structures. For instance, the sentences Mary", "author rules, and (2) supporting time reasoning. Authoring Rules from CNL. There are two research problems with rules. The first problem is the standardization of rules parses that express the same information but via different syntactic forms or using different expressions. Suppose the knowledge base contains sentences like: (1) if a person buys a car then the person owns the car, (2) every person who is a purchaser of a car is an owner of the car, (3) if a car is bought by a person then the person possesses the car. All the above sentences represent rules and express exactly the same meaning. However, KALM's current syntactic parser will represent them in different DRSs and therefore not being able to map them into the same logical form. The second problem involves the recognition and representation of different types of rules in logic. For instance, defeasible rules are very common in text. However, this type of rules cannot be handled by first order logic. We believe defeasible"]}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_answer": "Unanswerable", "predicted_evidence": ["exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems. For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 . Future Work Beyond The Thesis. This section discusses the future work beyond the thesis: (1) enhancing KALM to author rules, and (2) supporting time reasoning. Authoring Rules from CNL. There are two research problems with rules. The first problem is the standardization of rules parses that express the same", "Introduction. Knowledge representation and reasoning (KRR) is the process of representing the domain knowledge in formal languages (e.g., SPARQL, Prolog) such that it can be used by expert systems to execute querying and reasoning services. KRR have been applied in many fields including financial regulations, medical diagnosis, laws, and so on. One major obstacle in KRR is the creation of large-scale knowledge bases with high quality. For one thing, this requires the knowledge engineers (KEs) not only to have the background knowledge in a certain domain but have enough skills in knowledge representation as well. Unfortunately, qualified KEs are also in short supply. Therefore, it would be useful to build a tool that allows the domain experts without any background in logic to construct and query the knowledge base simply from text. Controlled natural languages (CNLs) BIBREF0 were developed as a technology that achieves this goal. CNLs are designed based on natural languages (NLs) but", "BIBREF6 is developed for question answering. KALM-QA shares the same components with KALM for syntactic parsing, frame-based parsing and role-filler disambiguation. Different from KALM, KALM-QA translates the questions to unique logical representation for queries (ULRQ), which are used to query the authored knowledge base. Evaluations. This section provides a summary of the evaluation of KALM and KALM-QA, where KALM is evaluated for knowledge authoring and KALM-QA is evaluated for question answering. We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems. For KALM-QA, we evaluate it", "for knowledge representation and reasoning. Related works also include knowledge extraction tools, e.g., OpenIE BIBREF9 , SEMEFOR BIBREF10 , SLING BIBREF11 , and Standford KBP system BIBREF12 . These knowledge extraction tools are designed to extract semantic relations from English sentences that capture the meaning. The limitations of these tools are two-fold: first, they lack sufficient accuracy to extract the correct semantic relations and entities while KRR is very sensitive to incorrect data; second, these systems are not able to map the semantic relations to logical forms and therefore not capable of doing KRR. Other related works include the question answering frameworks, e.g., Memory Network BIBREF13 , Variational Reasoning Network BIBREF14 , ATHENA BIBREF15 , PowerAqua BIBREF16 . The first two belong to end-to-end learning approaches based on machine learning models. The last two systems have implemented semantic parsers which translate natural language sentences into", "based on FrameNet BIBREF7 and BabelNet BIBREF8 where FrameNet is used to capture the meaning of the sentence and BabelNet BIBREF8 is used to disambiguate the meaning of the extracted entities from the sentence. Experiment results show that KALM achieves superior accuracy in knowledge authoring and question answering as compared to the state-of-the-art systems. The rest parts are organized as follows: Section SECREF2 discusses the related works, Section SECREF3 presents the KALM architecture, Section SECREF4 presents KALM-QA, the question answering part of KALM, Section SECREF5 shows the evaluation results, Section SECREF6 shows the future work beyond the thesis, and Section SECREF7 concludes the paper. Related Works. As is described in Section SECREF1 , CNL systems were proposed as the technology for knowledge representation and reasoning. Related works also include knowledge extraction tools, e.g., OpenIE BIBREF9 , SEMEFOR BIBREF10 , SLING BIBREF11 , and Standford KBP system BIBREF12", "role(Seller,[bn:00053479n],[]),  role(Goods,[bn:00006126n,bn:00021045n],[]),  role(Recipient,[bn:00066495n],[]),  role(Money,[bn:00017803n],[currency])]).  In each role-term, the first argument is the name of the role and the second is a list of role meanings represented via BabelNet synset IDs BIBREF8 . The third argument of a role-term is a list of constraints on that role. For instance, the sentence Mary buys a car implies the Commerce_Buy frame where Mary is the Buyer and car is the Goods. To extract a frame instance from a given CNL sentence, KALM uses logical valence patterns (lvps) which are learned via structural learning. An example of the lvp is shown below:   lvp(buy,v,Commerce_Buy, [  pattern(Buyer,verb->subject,required),  pattern(Goods,verb->object,required),  pattern(Recipient,verb->pp(for)->dep,optnl),  pattern(Money,verb->pp(for)->dep,optnl),  pattern(Seller,verb->pp(from)->dep,optnl)]).  The first three arguments of an lvp-fact identify the lexical unit, its part of", "causality, triggering) or using a reserved argument to represent time in each fluent. Conclusions. This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems.", "representation of different types of rules in logic. For instance, defeasible rules are very common in text. However, this type of rules cannot be handled by first order logic. We believe defeasible logic BIBREF19 is a good fit. Time Reasoning. Time-related information is a crucial part of human knowledge, but semantic parsing that takes the time into account is rather hard. However, we can develop a CNL that would incorporate enough time related idioms to be useful in a number of domains of discourse (e.g., tax law). Time can then be added to DRSs and incorporated into our frame based approach down to the very level of the logical facts into which sentences will be translated. This time information can be represented either via special time-aware relations among events (e.g., before, after, causality, triggering) or using a reserved argument to represent time in each fluent. Conclusions. This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition,"]}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_answer": "The document describes multiple papers, so there is no specific contribution mentioned in the documents. Unanswerable.", "predicted_evidence": ["articles. For our experiments, we focused only on the Main Task. In addition to the training and test data, we have created also a Main Task development set by excluding from the training data all the articles that composed the test data of the Italian dataset at the SemEval 2010 TempEval-2 campaign BIBREF6 . The new partition of the corpus results in the following distribution of the $<$ EVENT $>$ tag: i) 17,528 events in the training data, of which 1,207 are multi-token mentions; ii.) 301 events in the development set, of which 13 are multi-token mentions; and finally, iii.) 3,798 events in the Main task test, of which 271 are multi-token mentions. Tables 1 and 1 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. Not surprisingly, verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases. Such a distribution reflects both a kind of \u201cnatural\u201d distribution of the realization of events in an", "solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic summarization and question answering (Q.A.). Event detection and classification has seen a growing interest in the NLP community thanks to the availability of annotated corpora BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 and evaluation campaigns BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . In the context of the 2014 EVALITA Workshop, the EVENTI evaluation exercise BIBREF11 was organized to promote research in Italian Temporal Processing, of which event detection and classification is a core subtask. Since the EVENTI campaign, there has been a lack of further research, especially in the application of deep learning models to this task in Italian. The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural", "any correct overlap between the system output and the reference gold data. The classification aspect is evaluated using the F1-attribute score BIBREF7 , that captures how well a system identify both the entity (extent) and attribute (i.e. class) together. We approached the task in a single-step by detecting and classifying event mentions at once rather than in the standard two step approach, i.e. detection first and classification on top of the detected elements. The task is formulated as a seq2seq problem, by converting the original annotation format into an BIO scheme (Beginning, Inside, Outside), with the resulting alphabet being B-class_label, I-class_label and O. Example \"System and Experiments\" below illustrates a simplified version of the problem for a short sentence:  input problem solution  Marco (B-STATE $|$ I-STATE $|$ ... $|$ O) O  pensa (B-STATE $|$ I-STATE $|$ ... $|$ O) B-ISTATE  di (B-STATE $|$ I-STATE $|$ ... $|$ O) O  andare (B-STATE $|$ I-STATE $|$ ... $|$ O)", "last layer BIBREF14 . BIBREF14 demonstrated that word embeddings, among other hyper-parameters, have a major impact on the performance of the network, regardless of the specific task. On the basis of these experimental observations, we decided to investigate the impact of different Italian word embeddings for the Subtask B Main Task of the EVENTI exercise. We thus selected 5 word embeddings for Italian to initialize the network, differentiating one with respect to each other either for the representation model used (word2vec vs. GloVe; CBOW vs. skip-gram), dimensionality (300 vs. 100), or corpora used for their generation (Italian Wikipedia vs. crawled web document vs. large textual corpora or archives): As for the other parameters, the network maintains the optimized configurations used for the event detection task for English BIBREF14 : two LSTM layers of 100 units each, Nadam optimizer, variational dropout (0.5, 0.5), with gradient normalization ( $\\tau $ = 1), and batch size of 8.", "and classification tasks, respectively. The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of", "Annotation Guidelines BIBREF12 . The TimeML event classes distinguishes with respect to other classifications, such as ACE BIBREF1 or FrameNet BIBREF13 , because they expresses relationships the target event participates in (such as factual, evidential, reported, intensional) rather than semantic categories denoting the meaning of the event. This means that the EVENT classes are assigned by taking into account both the semantic and the syntactic context of occurrence of the target event. Readers are referred to the EVENTI Annotation Guidelines for more details. Dataset. The EVENTI corpus consists of three datasets: the Main Task training data, the Main task test data, and the Pilot task test data. The Main Task data are on contemporary news articles, while the Pilot Task on historical news articles. For our experiments, we focused only on the Main Task. In addition to the training and test data, we have created also a Main Task development set by excluding from the training data all", "are the largest annotated category, followed by nouns, adjectives, and prepositional phrases. Such a distribution reflects both a kind of \u201cnatural\u201d distribution of the realization of events in an Indo-european language, and, at the same time, specific annotation choices. For instance, adjectives have been annotated only when in a predicative position and when introduced by a copula or a copular construction. As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I_STATE and I_ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION). System and Experiments. We adapted a publicly available Bi-LSTM network with a CRF classifier as last layer BIBREF14 . BIBREF14 demonstrated that word embeddings, among other hyper-parameters, have a major impact on the performance of the network, regardless of the specific task. On the basis of", "Introduction. Current societies are exposed to a continuous flow of information that results in a large production of data (e.g. news articles, micro-blogs, social media posts, among others), at different moments in time. In addition to this, the consumption of information has dramatically changed: more and more people directly access information through social media platforms (e.g. Facebook and Twitter), and are less and less exposed to a diversity of perspectives and opinions. The combination of these factors may easily result in information overload and impenetrable \u201cfilter bubbles\u201d. Events, i.e. things that happen or hold as true in the world, are the basic components of such data stream. Being able to correctly identify and classify them plays a major role to develop robust solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic"]}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_answer": "Unanswerable", "predicted_evidence": ["and classification tasks, respectively. The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of", "problem solution  Marco (B-STATE $|$ I-STATE $|$ ... $|$ O) O  pensa (B-STATE $|$ I-STATE $|$ ... $|$ O) B-ISTATE  di (B-STATE $|$ I-STATE $|$ ... $|$ O) O  andare (B-STATE $|$ I-STATE $|$ ... $|$ O) B-OCCUR  a (B-STATE $|$ I-STATE $|$ ... $|$ O) O  casa (B-STATE $|$ I-STATE $|$ ... $|$ O) O  . (B-STATE $|$ I-STATE $|$ ... $|$ O) O Results and Discussion. Results for the experiments are illustrated in Table 2 . We also report the results of the best system that participated at EVENTI Subtask B, FBK-HLT BIBREF23 . FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Figure 1 plots charts comparing F1 scores of the network initialized with each of the five embeddings against the FBK-HLT system for the event detection and classification tasks, respectively. The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the", "embeddings). Although FBK-HLT suffers in the classification subtask, it qualifies as a highly competitive system for the detection subtask. By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK-100, ILC-ItWack, Berardi2015_Glove) , almost equals one (Berardi2015_w2v) , and it is outperformed only by one (Fastext-It) . In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack. The results also indicates that word embeddings have a major contribution on Recall, supporting observations that distributed representations have better generalization capabilities than discrete feature vectors. This is further supported by the fact that these results are obtained using a single step approach, where the", "solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic summarization and question answering (Q.A.). Event detection and classification has seen a growing interest in the NLP community thanks to the availability of annotated corpora BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 and evaluation campaigns BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . In the context of the 2014 EVALITA Workshop, the EVENTI evaluation exercise BIBREF11 was organized to promote research in Italian Temporal Processing, of which event detection and classification is a core subtask. Since the EVENTI campaign, there has been a lack of further research, especially in the application of deep learning models to this task in Italian. The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural", "STATE. OCCURRENCE, not surprisingly, is the class that tends to be assigned more often by both systems, being also the most frequent. However, if FBK-HLT largely overgeneralizes OCCURRENCE (59.53% of all class errors), this corresponds to only one third of the errors (37.70%) in the Bi-LSTM-CRF network. Other notable differences concern I_ACTION (27.82% of errors for the Bi-LSTM-CRF vs. 17.28% for FBK-HLT), STATE (8.79% for the Bi-LSTM-CRF vs. 15.22% for FBK-HLT) and REPORTING (7.89% for the Bi-LSTM-CRF vs. 2.33% for FBK-HLT) classes. Conclusion and Future Work. This paper has investigated the application of different word embeddings for the initialization of a state-of-the-art Bi-LSTM-CRF network to solve the event detection and classification task in Italian, according to the EVENTI exercise. We obtained new state-of-the-art results using the Fastext-It embeddings, and improved the F1-class score of 6.5 points in strict evaluation mode. As for the event detection subtask, we observe", "We obtained new state-of-the-art results using the Fastext-It embeddings, and improved the F1-class score of 6.5 points in strict evaluation mode. As for the event detection subtask, we observe a limited improvement (+1.3 points in strict F1), mainly due to gains in Recall. Such results are extremely positive as the task has been modeled in a single step approach, i.e. detection and classification at once, for the first time in Italian. Further support that embeddings have a major impact in the performance of neural architectures is provided, as the variations in performance of the Bi-LSMT-CRF models show. This is due to a combination of factors such as dimensionality, (raw) data, and the method used for generating the embeddings. Future work should focus on the development of embeddings that move away from the basic word level, integrating extra layers of linguistic analysis (e.g. syntactic dependencies) BIBREF24 , that have proven to be very powerful for the same task in English.", "Introduction. Current societies are exposed to a continuous flow of information that results in a large production of data (e.g. news articles, micro-blogs, social media posts, among others), at different moments in time. In addition to this, the consumption of information has dramatically changed: more and more people directly access information through social media platforms (e.g. Facebook and Twitter), and are less and less exposed to a diversity of perspectives and opinions. The combination of these factors may easily result in information overload and impenetrable \u201cfilter bubbles\u201d. Events, i.e. things that happen or hold as true in the world, are the basic components of such data stream. Being able to correctly identify and classify them plays a major role to develop robust solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic", "last layer BIBREF14 . BIBREF14 demonstrated that word embeddings, among other hyper-parameters, have a major impact on the performance of the network, regardless of the specific task. On the basis of these experimental observations, we decided to investigate the impact of different Italian word embeddings for the Subtask B Main Task of the EVENTI exercise. We thus selected 5 word embeddings for Italian to initialize the network, differentiating one with respect to each other either for the representation model used (word2vec vs. GloVe; CBOW vs. skip-gram), dimensionality (300 vs. 100), or corpora used for their generation (Italian Wikipedia vs. crawled web document vs. large textual corpora or archives): As for the other parameters, the network maintains the optimized configurations used for the event detection task for English BIBREF14 : two LSTM layers of 100 units each, Nadam optimizer, variational dropout (0.5, 0.5), with gradient normalization ( $\\tau $ = 1), and batch size of 8."]}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_answer": "Yes (implied, as they mention the language independence of the character-based approach)", "predicted_evidence": ["the event detection task for English BIBREF14 : two LSTM layers of 100 units each, Nadam optimizer, variational dropout (0.5, 0.5), with gradient normalization ( $\\tau $ = 1), and batch size of 8. Character-level embeddings, learned using a Convolutional Neural Network (CNN) BIBREF22 , are concatenated with the word embedding vector to feed into the LSTM network. Final layer of the network is a CRF classifier. Evaluation is conducted using the EVENTI evaluation framework. Standard Precision, Recall, and F1 apply for the event detection. Given that the extent of an event tag may be composed by more than one tokens, systems are evaluated both for strict match, i.e. one point only if all tokens which compose an $<$ EVENT $>$ tag are correctly identified, and relaxed match, i.e. one point for any correct overlap between the system output and the reference gold data. The classification aspect is evaluated using the F1-attribute score BIBREF7 , that captures how well a system identify both", "solutions to deal with the current stream of data (e.g. the storyline framework BIBREF0 ), as well to improve the performance of many Natural Language Processing (NLP) applications such as automatic summarization and question answering (Q.A.). Event detection and classification has seen a growing interest in the NLP community thanks to the availability of annotated corpora BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 and evaluation campaigns BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . In the context of the 2014 EVALITA Workshop, the EVENTI evaluation exercise BIBREF11 was organized to promote research in Italian Temporal Processing, of which event detection and classification is a core subtask. Since the EVENTI campaign, there has been a lack of further research, especially in the application of deep learning models to this task in Italian. The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural", "in the application of deep learning models to this task in Italian. The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier. The pre-trained models and scripts running the system (or re-train it) are publicly available. . Task Description. We follow the formulation of the task as specified in the EVENTI exercise: determine the extent and the class of event mentions in a text, according to the It-TimeML $<$ EVENT $>$ tag definition (Subtask B in EVENTI). In EVENTI, the tag $<$ EVENT $>$ is applied to every linguistic expression denoting a situation that happens or occurs, or a state in which something obtains or holds true, regardless of the specific parts-of-speech that may", "Annotation Guidelines BIBREF12 . The TimeML event classes distinguishes with respect to other classifications, such as ACE BIBREF1 or FrameNet BIBREF13 , because they expresses relationships the target event participates in (such as factual, evidential, reported, intensional) rather than semantic categories denoting the meaning of the event. This means that the EVENT classes are assigned by taking into account both the semantic and the syntactic context of occurrence of the target event. Readers are referred to the EVENTI Annotation Guidelines for more details. Dataset. The EVENTI corpus consists of three datasets: the Main Task training data, the Main task test data, and the Pilot task test data. The Main Task data are on contemporary news articles, while the Pilot Task on historical news articles. For our experiments, we focused only on the Main Task. In addition to the training and test data, we have created also a Main Task development set by excluding from the training data all", "representations have better generalization capabilities than discrete feature vectors. This is further supported by the fact that these results are obtained using a single step approach, where the network has to deal with a total of 15 possible different labels. We further compared the outputs of the best model, i.e. Fastext-It, against FBK-HLT. As for the event detection subtask, we have adopted an event-based analysis rather than a token based one, as this will provide better insights on errors concerning multi-token events and event parts-of-speech (see Table 1 for reference). By analyzing the True Positives, we observe that the Fastext-It model has better performances than FBK-HLT with nouns (77.78% vs. 65.64%, respectively) and prepositional phrases (28.00% vs. 16.00%, respectively). Performances are very close for verbs (88.04% vs. 88.49%, respectively) and adjectives (80.50% vs. 79.66%, respectively). These results, especially those for prepositional phrases, indicates that the", "problem solution  Marco (B-STATE $|$ I-STATE $|$ ... $|$ O) O  pensa (B-STATE $|$ I-STATE $|$ ... $|$ O) B-ISTATE  di (B-STATE $|$ I-STATE $|$ ... $|$ O) O  andare (B-STATE $|$ I-STATE $|$ ... $|$ O) B-OCCUR  a (B-STATE $|$ I-STATE $|$ ... $|$ O) O  casa (B-STATE $|$ I-STATE $|$ ... $|$ O) O  . (B-STATE $|$ I-STATE $|$ ... $|$ O) O Results and Discussion. Results for the experiments are illustrated in Table 2 . We also report the results of the best system that participated at EVENTI Subtask B, FBK-HLT BIBREF23 . FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Figure 1 plots charts comparing F1 scores of the network initialized with each of the five embeddings against the FBK-HLT system for the event detection and classification tasks, respectively. The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the", "$>$ is applied to every linguistic expression denoting a situation that happens or occurs, or a state in which something obtains or holds true, regardless of the specific parts-of-speech that may realize it. EVENTI distinguishes between single token and multi-tokens events, where the latter are restricted to specific cases of eventive multi-word expressions in lexicographic dictionaries (e.g. \u201cfare le valigie\u201d [to pack]), verbal periphrases (e.g. \u201c(essere) in grado di\u201d [(to be) able to]; \u201cc'\u00e8\u201d [there is]), and named events (e.g. \u201cla strage di Beslan\u201d [Beslan school siege]). Each event is further assigned to one of 7 possible classes, namely: OCCURRENCE, ASPECTUAL, PERCEPTION, REPORTING, I(NTESIONAL)_STATE, I(NTENSIONAL)_ACTION, and STATE. These classes are derived from the English TimeML Annotation Guidelines BIBREF12 . The TimeML event classes distinguishes with respect to other classifications, such as ACE BIBREF1 or FrameNet BIBREF13 , because they expresses relationships the", "STATE. OCCURRENCE, not surprisingly, is the class that tends to be assigned more often by both systems, being also the most frequent. However, if FBK-HLT largely overgeneralizes OCCURRENCE (59.53% of all class errors), this corresponds to only one third of the errors (37.70%) in the Bi-LSTM-CRF network. Other notable differences concern I_ACTION (27.82% of errors for the Bi-LSTM-CRF vs. 17.28% for FBK-HLT), STATE (8.79% for the Bi-LSTM-CRF vs. 15.22% for FBK-HLT) and REPORTING (7.89% for the Bi-LSTM-CRF vs. 2.33% for FBK-HLT) classes. Conclusion and Future Work. This paper has investigated the application of different word embeddings for the initialization of a state-of-the-art Bi-LSTM-CRF network to solve the event detection and classification task in Italian, according to the EVENTI exercise. We obtained new state-of-the-art results using the Fastext-It embeddings, and improved the F1-class score of 6.5 points in strict evaluation mode. As for the event detection subtask, we observe"]}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_answer": "Using a three-step process: sense disambiguation, candidate label generation, and candidate label ranking.", "predicted_evidence": ["cluster labeled cognition. Thus, cluster labeling proves to be very effective at pulling out the patterns that we believed we saw at the word-level, but could not formally validate. The clusters we mentioned so far all lean heavily toward one gender association or the other, but some clusters are interesting precisely because they do not lean heavily \u2013 this allows us to see where semantic groupings do not align exactly with gender association. For example, in the celebrity domain, there is a cluster labeled lover that has a mix of female-associated words (\u201cboyfriend,\u201d \u201cbeau,\u201d \u201chubby\u201d) and male-associated words (\u201cwife,\u201d \u201cgirlfriend\u201d). Jointly leveraging cluster labels and gender associations allows us to see that in the semantic context of having a lover, women are typically associated with male figures and men with female figures, which reflects heteronormativity in society. Human Evaluations. To test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator", "may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors. To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps: Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets. Candidate label generation: In this step, we generate $L$, the set", "of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets. Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$. Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance. In steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well. Clustering & Cluster Labeling ::: Findings. Table TABREF11 displays a sample of our results \u2013 we find that the clusters are coherent in context and the labels seem reasonable. In the next section, we discuss human evaluations that we conducted to more rigorously evaluate the output, but", "it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above the random baseline of 20%. To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming", "permeates through language in the first place. We also build on methods to cluster words in word embedding space and automatically label clusters. Clustering word embeddings has proven useful for discovering salient patterns in text corpora BIBREF25, BIBREF26. Once clusters are derived, we would like them to be interpretable. Much research simply considers the top-n words from each cluster, but this method can be subjective and time-consuming to interpret. Thus, there are efforts to design methods of automatic cluster labeling BIBREF27. We take a similar approach to BIBREF28, who leverage word embeddings and WordNet during labeling, and we extend their method with additional techniques and evaluations. Data Collection. Our first dataset contains articles from celebrity magazines People, UsWeekly, and E!News. We labeled each article for whether it was reporting on men, women, or neither/unknown. To do this, we first extracted the article's topic tags. Some of these tags referred to", "both domains align with prior studies and real world trends, which validates that our methods can capture meaningful patterns and innovatively provide evidence at the large-scale. This analysis also hints that it can be helpful to abstract from words to topics to recognize higher-level patterns of gender associations, which motivates our next section on clustering. Clustering & Cluster Labeling. With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters. Clustering & Cluster Labeling ::: Methods. First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors. To automatically label the clusters, we combined the grounded knowledge of", "\u2013 we find that the clusters are coherent in context and the labels seem reasonable. In the next section, we discuss human evaluations that we conducted to more rigorously evaluate the output, but first we discuss the value of these methods toward analysis. At the word-level, we hypothesized that in the celebrity domain, women were more associated with appearance and men with creating content. Now, we can validate those hypotheses against labeled clusters \u2013 indeed, there is a cluster labeled clothing that is 100% female (i.e. 100% words are female-associated), and a 80% male cluster labeled movie. Likewise, in the professor domain, we had guessed that women are associated with communication and men with knowledge, and there is a 100% female cluster labeled communication and a 89% male cluster labeled cognition. Thus, cluster labeling proves to be very effective at pulling out the patterns that we believed we saw at the word-level, but could not formally validate. The clusters we", "and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks. Conclusion. We have presented two substantial datasets and a novel integration of methods to automatically infer gender associations in language. We have demonstrated that in both datasets, there are clear differences in how people talk about women and men. Furthermore, we have shown that clustering and cluster labeling are effective at identifying higher-level patterns of gender associations, and that our methods outperform strong baselines in human evaluations. In future work, we hope to use our findings to improve performance on tasks such as abusive language detection. We also hope to delve into finer-grained analyses, exploring how"]}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_answer": "using k-means clustering and automatic labeling with a combination of WordNet and domain-specific word embeddings.", "predicted_evidence": ["may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors. To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps: Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets. Candidate label generation: In this step, we generate $L$, the set", "permeates through language in the first place. We also build on methods to cluster words in word embedding space and automatically label clusters. Clustering word embeddings has proven useful for discovering salient patterns in text corpora BIBREF25, BIBREF26. Once clusters are derived, we would like them to be interpretable. Much research simply considers the top-n words from each cluster, but this method can be subjective and time-consuming to interpret. Thus, there are efforts to design methods of automatic cluster labeling BIBREF27. We take a similar approach to BIBREF28, who leverage word embeddings and WordNet during labeling, and we extend their method with additional techniques and evaluations. Data Collection. Our first dataset contains articles from celebrity magazines People, UsWeekly, and E!News. We labeled each article for whether it was reporting on men, women, or neither/unknown. To do this, we first extracted the article's topic tags. Some of these tags referred to", "male figures and men with female figures, which reflects heteronormativity in society. Human Evaluations. To test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator with five words \u2013 four drawn from one cluster and one drawn randomly from the domain vocabulary \u2013 and we ask them to pick out the intruder. The intuition is that if the cluster is coherent, then an observer should be able to identify the out-of-cluster word as the intruder. For both domains, we report results on all clusters and on the top 8, ranked by ascending normalized sum of squared errors, which can be seen as a prediction of coherence. In the celebrity domain, annotators identified the out-of-cluster word 73% of the time in the top-8 and 53% overall. In the professor domain, annotators identified it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above", "both domains align with prior studies and real world trends, which validates that our methods can capture meaningful patterns and innovatively provide evidence at the large-scale. This analysis also hints that it can be helpful to abstract from words to topics to recognize higher-level patterns of gender associations, which motivates our next section on clustering. Clustering & Cluster Labeling. With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters. Clustering & Cluster Labeling ::: Methods. First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors. To automatically label the clusters, we combined the grounded knowledge of", "of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets. Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$. Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance. In steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well. Clustering & Cluster Labeling ::: Findings. Table TABREF11 displays a sample of our results \u2013 we find that the clusters are coherent in context and the labels seem reasonable. In the next section, we discuss human evaluations that we conducted to more rigorously evaluate the output, but", "cluster labeled cognition. Thus, cluster labeling proves to be very effective at pulling out the patterns that we believed we saw at the word-level, but could not formally validate. The clusters we mentioned so far all lean heavily toward one gender association or the other, but some clusters are interesting precisely because they do not lean heavily \u2013 this allows us to see where semantic groupings do not align exactly with gender association. For example, in the celebrity domain, there is a cluster labeled lover that has a mix of female-associated words (\u201cboyfriend,\u201d \u201cbeau,\u201d \u201chubby\u201d) and male-associated words (\u201cwife,\u201d \u201cgirlfriend\u201d). Jointly leveraging cluster labels and gender associations allows us to see that in the semantic context of having a lover, women are typically associated with male figures and men with female figures, which reflects heteronormativity in society. Human Evaluations. To test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator", "\u2013 we find that the clusters are coherent in context and the labels seem reasonable. In the next section, we discuss human evaluations that we conducted to more rigorously evaluate the output, but first we discuss the value of these methods toward analysis. At the word-level, we hypothesized that in the celebrity domain, women were more associated with appearance and men with creating content. Now, we can validate those hypotheses against labeled clusters \u2013 indeed, there is a cluster labeled clothing that is 100% female (i.e. 100% words are female-associated), and a 80% male cluster labeled movie. Likewise, in the professor domain, we had guessed that women are associated with communication and men with knowledge, and there is a 100% female cluster labeled communication and a 89% male cluster labeled cognition. Thus, cluster labeling proves to be very effective at pulling out the patterns that we believed we saw at the word-level, but could not formally validate. The clusters we", "it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above the random baseline of 20%. To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming"]}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_answer": "Over 300K sentences.", "predicted_evidence": ["of 80 tags that we manually annotated, we found that comparing pronoun counts predicted gender with perfect accuracy. Finally, if an article tagged at least one woman and did not tag any men, we labeled the article as Female; in the opposite case, we labeled it as Male. Our second dataset contains reviews from RateMyProfessors (RMP), an online platform where students can review their professors. We included all 5,604 U.S. schools on RMP, and collected all reviews for CS professors at those schools. We labeled each review with the gender of the professor whom it was about, which we determined by comparing the count of male versus female pronouns over all reviews for that professor. This method was again effective, because the reviews are expressly written about a certain professor, so the pronouns typically resolve to that professor. In addition to extracting the text of the articles or reviews, for each dataset we also collected various useful metadata. For the celebrity dataset, we", "pronouns typically resolve to that professor. In addition to extracting the text of the articles or reviews, for each dataset we also collected various useful metadata. For the celebrity dataset, we recorded each article's timestamp and the name of the author, if available. Storing author names creates the potential to examine the relationship between the gender of the author and the gender of the subject, such as asking if there are differences between how women write about men and how men write about men. In this work, we did not yet pursue this direction because we wanted to begin with a simpler question of how gender is discussed: regardless of the gender of the authors, what is the content being put forth and consumed? Furthermore, we were unable to extract author gender in the professor dataset since the RMP reviews are anonymous. However, in future work, we may explore the influence of author gender in the celebrity dataset. For the professor dataset, we captured metadata such", "of data points, these patterns, if they do exist, become undeniable. Thus, in this work, we introduce new datasets and methods so that we can study subtle gender associations in language at the large-scale. Our contributions include: Two datasets for studying language and gender, each consisting of over 300K sentences. Methods to infer gender-associated words and labeled clusters in any domain. Novel findings that demonstrate in both domains that people do talk about women and men in different ways. Each contribution brings us closer to modeling how gender associations appear in everyday language. In the remainder of the paper, we present related work, our data collection, methods and findings, and human evaluations of our system. Related Work. The study of gender and language has a rich history in social science. Its roots are often attributed to Robin Lakoff, who argued that language is fundamental to gender inequality, \u201creflected in both the ways women are expected to speak, and", "dataset since the RMP reviews are anonymous. However, in future work, we may explore the influence of author gender in the celebrity dataset. For the professor dataset, we captured metadata such as each review's rating, which indicates how the student feels about the professor on a scale of AWFUL to AWESOME. This additional variable in our data creates the option in future work to factor in sentiment; for example, we could study whether there are differences in language used when criticizing a female versus a male professor. Inferring Word-Level Associations. Our first goal was to discover words that are significantly associated with men or women in a given domain. We employed an approach used by BIBREF10 in their work to analyze differences in how men and women write on Twitter. Inferring Word-Level Associations ::: Methods. First, to operationalize, we say that term $i$ is associated with gender $j$ if, when discussing individuals of gender $j$, $i$ is used with unusual frequency \u2013", "Word-Level Associations ::: Methods. First, to operationalize, we say that term $i$ is associated with gender $j$ if, when discussing individuals of gender $j$, $i$ is used with unusual frequency \u2013 which we can check with statistical hypothesis tests. Let $f_i$ represent the likelihood of $i$ appearing when discussing women or men. $f_i$ is unknown, but we can model the distribution of all possible $f_i$ using the corpus of texts that we have from the domain. We construct a gender-balanced version of the corpus by randomly undersampling the more prevalent gender until the proportions of each gender are equal. Assuming a non-informative prior distribution on $f_i$, the posterior distribution is Beta($k_i$, $N - k_i$), where $k_i$ is the count of $i$ in the gender-balanced corpus and $N$ is the total count of words in that corpus. As BIBREF10 discuss, \u201cthe distribution of the gender-specific counts can be described by an integral over all possible $f_i$. This integral defines the", "and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks. Conclusion. We have presented two substantial datasets and a novel integration of methods to automatically infer gender associations in language. We have demonstrated that in both datasets, there are clear differences in how people talk about women and men. Furthermore, we have shown that clustering and cluster labeling are effective at identifying higher-level patterns of gender associations, and that our methods outperform strong baselines in human evaluations. In future work, we hope to use our findings to improve performance on tasks such as abusive language detection. We also hope to delve into finer-grained analyses, exploring how", "male figures and men with female figures, which reflects heteronormativity in society. Human Evaluations. To test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator with five words \u2013 four drawn from one cluster and one drawn randomly from the domain vocabulary \u2013 and we ask them to pick out the intruder. The intuition is that if the cluster is coherent, then an observer should be able to identify the out-of-cluster word as the intruder. For both domains, we report results on all clusters and on the top 8, ranked by ascending normalized sum of squared errors, which can be seen as a prediction of coherence. In the celebrity domain, annotators identified the out-of-cluster word 73% of the time in the top-8 and 53% overall. In the professor domain, annotators identified it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above", "it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above the random baseline of 20%. To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming"]}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_answer": "the centroid of the cluster as a strong baseline label.", "predicted_evidence": ["it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above the random baseline of 20%. To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming", "male figures and men with female figures, which reflects heteronormativity in society. Human Evaluations. To test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator with five words \u2013 four drawn from one cluster and one drawn randomly from the domain vocabulary \u2013 and we ask them to pick out the intruder. The intuition is that if the cluster is coherent, then an observer should be able to identify the out-of-cluster word as the intruder. For both domains, we report results on all clusters and on the top 8, ranked by ascending normalized sum of squared errors, which can be seen as a prediction of coherence. In the celebrity domain, annotators identified the out-of-cluster word 73% of the time in the top-8 and 53% overall. In the professor domain, annotators identified it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above", "and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks. Conclusion. We have presented two substantial datasets and a novel integration of methods to automatically infer gender associations in language. We have demonstrated that in both datasets, there are clear differences in how people talk about women and men. Furthermore, we have shown that clustering and cluster labeling are effective at identifying higher-level patterns of gender associations, and that our methods outperform strong baselines in human evaluations. In future work, we hope to use our findings to improve performance on tasks such as abusive language detection. We also hope to delve into finer-grained analyses, exploring how", "potential to discover subtleties in the data. Modeling gender associations in language could also be instrumental to other NLP tasks. Abusive language is often founded in sexism BIBREF0, BIBREF1, so models of gender associations could help to improve detection in those cases. Gender bias also manifests in NLP pipelines: prior research has found that word embeddings preserve gender biases BIBREF19, BIBREF20, BIBREF21, and some have developed methods to reduce this bias BIBREF22, BIBREF23. Yet, the problem is far from solved; for example, BIBREF24 showed that it is still possible to recover gender bias from \u201cde-biased\u201d embeddings. These findings further motivate our research, since before we can fully reduce gender bias in embeddings, we need to develop a deeper understanding of how gender permeates through language in the first place. We also build on methods to cluster words in word embedding space and automatically label clusters. Clustering word embeddings has proven useful for", "Introduction. It is well-established that gender bias exists in language \u2013 for example, we see evidence of this given the prevalence of sexism in abusive language datasets BIBREF0, BIBREF1. However, these are extreme cases of gender norms in language, and only encompass a small proportion of speakers or texts. Less studied in NLP is how gender norms manifest in everyday language \u2013 do people talk about women and men in different ways? These types of differences are far subtler than abusive language, but they can provide valuable insight into the roots of more extreme acts of discrimination. Subtle differences are difficult to observe because each case on its own could be attributed to circumstance, a passing comment or an accidental word. However, at the level of hundreds of thousands of data points, these patterns, if they do exist, become undeniable. Thus, in this work, we introduce new datasets and methods so that we can study subtle gender associations in language at the", "of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets. Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$. Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance. In steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well. Clustering & Cluster Labeling ::: Findings. Table TABREF11 displays a sample of our results \u2013 we find that the clusters are coherent in context and the labels seem reasonable. In the next section, we discuss human evaluations that we conducted to more rigorously evaluate the output, but", "$N$ is the total count of words in that corpus. As BIBREF10 discuss, \u201cthe distribution of the gender-specific counts can be described by an integral over all possible $f_i$. This integral defines the Beta-Binomial distribution BIBREF29, and has a closed form solution.\u201d We say that term $i$ is significantly associated with gender $j$ if the cumulative distribution at $k_{ij}$ (the count of $i$ in the $j$ portion of the gender-balanced corpus) is $p \\le 0.05$. As in the original work, we apply the Bonferroni correction BIBREF30 for multiple comparisons because we are computing statistical tests for thousands of hypotheses. Inferring Word-Level Associations ::: Findings. We applied this method to discover gender-associated words in both domains. In Table TABREF9, we present a sample of the most gender-associated nouns from the celebrity domain. Several themes emerge: for example, female celebrities seem to be more associated with appearance (\u201cgown,\u201d \u201cphoto,\u201d \u201chair,\u201d \u201clook\u201d), while male", "Word-Level Associations ::: Methods. First, to operationalize, we say that term $i$ is associated with gender $j$ if, when discussing individuals of gender $j$, $i$ is used with unusual frequency \u2013 which we can check with statistical hypothesis tests. Let $f_i$ represent the likelihood of $i$ appearing when discussing women or men. $f_i$ is unknown, but we can model the distribution of all possible $f_i$ using the corpus of texts that we have from the domain. We construct a gender-balanced version of the corpus by randomly undersampling the more prevalent gender until the proportions of each gender are equal. Assuming a non-informative prior distribution on $f_i$, the posterior distribution is Beta($k_i$, $N - k_i$), where $k_i$ is the count of $i$ in the gender-balanced corpus and $N$ is the total count of words in that corpus. As BIBREF10 discuss, \u201cthe distribution of the gender-specific counts can be described by an integral over all possible $f_i$. This integral defines the"]}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_answer": "Unanswerable", "predicted_evidence": ["employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse. The results of this current paper suggest that the strategy should be adaptive to user type/ age, as well as to the severity of abuse. Conclusion. This paper presents the first user study on perceived appropriateness of system responses after verbal abuse. We put strategies used by state-of-the-art systems to the test in a large-scale, crowd-sourced evaluation. The full annotated corpus contains 2441 system replies, categorised into 14 response types, which were evaluated by 472 raters - resulting in 7.7 ratings per reply. Our results show that: (1) The user's age has an significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate. (2) Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands. (3)", "Introduction. Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them. Data Collection. We first gather abusive utterances from 600K conversations with US-based customers. We search for relevant utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11%", "we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way. BIBREF29 report that a pilot using a similar setup let to unnatural interactions, which limits the conclusions we can draw about the effectiveness of abuse mitigation strategies. Our next step therefore is to employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse. The results of this current paper suggest", "refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users. Related and Future Work. Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response", "BIBREF20. We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study. Following this methodology, we collected a total of 2441 system replies in July-August 2018 - 3.5 times more data than Amanda:EthicsNLP2018 - which 2 expert annotators manually annotated according to the categories in Table TABREF14 ($\\kappa =0.66$). Human Evaluation. In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as \u201cacceptable behaviour in a work environment\u201d and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker", "Requests and Demands, e.g. \u201cWill you have sex with me?\u201d, \u201cTalk dirty to me.\u201d We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018. [leftmargin=5mm, noitemsep] 4 Commercial: Amazon Alexa, Apple Siri, Google Home, Microsoft's Cortana. 4 Non-commercial rule-based: E.L.I.Z.A. BIBREF8, Parry BIBREF9, A.L.I.C.E. BIBREF10, Alley BIBREF11. 4 Data-driven approaches: Cleverbot BIBREF12; NeuralConvo BIBREF13, a re-implementation of BIBREF14; an implementation of BIBREF15's Information Retrieval approach; a vanilla Seq2Seq model trained on clean Reddit data BIBREF1. Negative Baselines: We also compile responses by adult chatbots: Sophia69 BIBREF16, Laurel Sweet BIBREF17, Captain Howdy BIBREF18, Annabelle Lee BIBREF19, Dr Love BIBREF20. We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study.", "utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11% BIBREF2 and 30% BIBREF6. Since we are not allowed to directly quote from our corpus in order to protect customer rights, we summarise the data to a total of 109 \u201cprototypical\" utterances - substantially extending the previous dataset of 35 utterances from Amanda:EthicsNLP2018 - and categorise these utterances based on the Linguistic Society's definition of sexual harassment BIBREF7: [noitemsep] Gender and Sexuality, e.g. \u201cAre you gay?\u201d, \u201cHow do you have sex?\u201d Sexualised Comments, e.g. \u201cI love watching porn.\u201d, \u201cI'm horny.\u201d Sexualised Insults, e.g. \u201cStupid bitch.\u201d, \u201cWhore\u201d Sexual Requests and Demands, e.g. \u201cWill you have sex with me?\u201d, \u201cTalk dirty to me.\u201d We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018.", "(C). Avoidance (2f) is considered most appropriate in the context of Sexualised Demands. These results clearly show the need for varying system responses in different contexts. However, the corpus study from Amanda:EthicsNLP2018 shows that current state-of-the-art systems do not adapt their responses sufficiently. Results ::: Systems. Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings,"]}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_answer": "9960 HITs from 472 crowd workers.", "predicted_evidence": ["(1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker is to label following the methodology described in BIBREF21, where each utterance is rated relatively to a reference on a user-defined scale. Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table", "refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users. Related and Future Work. Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response", "BIBREF20. We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study. Following this methodology, we collected a total of 2441 system replies in July-August 2018 - 3.5 times more data than Amanda:EthicsNLP2018 - which 2 expert annotators manually annotated according to the categories in Table TABREF14 ($\\kappa =0.66$). Human Evaluation. In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as \u201cacceptable behaviour in a work environment\u201d and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker", "responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women. Most raters (62.6%) are under the age of 44, with similar proportions across age groups for men and women. This is in-line with our target population: 57% of users of smart speakers are male and the majority are under 44 BIBREF22. Results. The ranks and mean scores of response categories can be seen in Table TABREF29. Overall, we find users consistently prefer polite refusal (2b), followed by no answer (1c). Chastising (2d) and \u201cdon't know\" (1e) rank together at position 3, while flirting (3c) and retaliation (2e) rank lowest. The rest of the response categories are similarly ranked, with no", "employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse. The results of this current paper suggest that the strategy should be adaptive to user type/ age, as well as to the severity of abuse. Conclusion. This paper presents the first user study on perceived appropriateness of system responses after verbal abuse. We put strategies used by state-of-the-art systems to the test in a large-scale, crowd-sourced evaluation. The full annotated corpus contains 2441 system replies, categorised into 14 response types, which were evaluated by 472 raters - resulting in 7.7 ratings per reply. Our results show that: (1) The user's age has an significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate. (2) Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands. (3)", "Requests and Demands, e.g. \u201cWill you have sex with me?\u201d, \u201cTalk dirty to me.\u201d We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018. [leftmargin=5mm, noitemsep] 4 Commercial: Amazon Alexa, Apple Siri, Google Home, Microsoft's Cortana. 4 Non-commercial rule-based: E.L.I.Z.A. BIBREF8, Parry BIBREF9, A.L.I.C.E. BIBREF10, Alley BIBREF11. 4 Data-driven approaches: Cleverbot BIBREF12; NeuralConvo BIBREF13, a re-implementation of BIBREF14; an implementation of BIBREF15's Information Retrieval approach; a vanilla Seq2Seq model trained on clean Reddit data BIBREF1. Negative Baselines: We also compile responses by adult chatbots: Sophia69 BIBREF16, Laurel Sweet BIBREF17, Captain Howdy BIBREF18, Annabelle Lee BIBREF19, Dr Love BIBREF20. We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study.", "(C). Avoidance (2f) is considered most appropriate in the context of Sexualised Demands. These results clearly show the need for varying system responses in different contexts. However, the corpus study from Amanda:EthicsNLP2018 shows that current state-of-the-art systems do not adapt their responses sufficiently. Results ::: Systems. Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings,", "Introduction. Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them. Data Collection. We first gather abusive utterances from 600K conversations with US-based customers. We search for relevant utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11%"]}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_answer": "14 (categorized)", "predicted_evidence": ["employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse. The results of this current paper suggest that the strategy should be adaptive to user type/ age, as well as to the severity of abuse. Conclusion. This paper presents the first user study on perceived appropriateness of system responses after verbal abuse. We put strategies used by state-of-the-art systems to the test in a large-scale, crowd-sourced evaluation. The full annotated corpus contains 2441 system replies, categorised into 14 response types, which were evaluated by 472 raters - resulting in 7.7 ratings per reply. Our results show that: (1) The user's age has an significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate. (2) Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands. (3)", "(C). Avoidance (2f) is considered most appropriate in the context of Sexualised Demands. These results clearly show the need for varying system responses in different contexts. However, the corpus study from Amanda:EthicsNLP2018 shows that current state-of-the-art systems do not adapt their responses sufficiently. Results ::: Systems. Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings,", "we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way. BIBREF29 report that a pilot using a similar setup let to unnatural interactions, which limits the conclusions we can draw about the effectiveness of abuse mitigation strategies. Our next step therefore is to employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse. The results of this current paper suggest", "refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users. Related and Future Work. Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response", "refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the \u201cdeflection\u201d strategy, such as \u201cWhy do you ask?\u201d. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. In turn, the \u201cclean\u201d seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively", "BIBREF20. We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study. Following this methodology, we collected a total of 2441 system replies in July-August 2018 - 3.5 times more data than Amanda:EthicsNLP2018 - which 2 expert annotators manually annotated according to the categories in Table TABREF14 ($\\kappa =0.66$). Human Evaluation. In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as \u201cacceptable behaviour in a work environment\u201d and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker", "Introduction. Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them. Data Collection. We first gather abusive utterances from 600K conversations with US-based customers. We search for relevant utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11%", "Requests and Demands, e.g. \u201cWill you have sex with me?\u201d, \u201cTalk dirty to me.\u201d We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018. [leftmargin=5mm, noitemsep] 4 Commercial: Amazon Alexa, Apple Siri, Google Home, Microsoft's Cortana. 4 Non-commercial rule-based: E.L.I.Z.A. BIBREF8, Parry BIBREF9, A.L.I.C.E. BIBREF10, Alley BIBREF11. 4 Data-driven approaches: Cleverbot BIBREF12; NeuralConvo BIBREF13, a re-implementation of BIBREF14; an implementation of BIBREF15's Information Retrieval approach; a vanilla Seq2Seq model trained on clean Reddit data BIBREF1. Negative Baselines: We also compile responses by adult chatbots: Sophia69 BIBREF16, Laurel Sweet BIBREF17, Captain Howdy BIBREF18, Annabelle Lee BIBREF19, Dr Love BIBREF20. We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study."]}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_answer": "Yes.", "predicted_evidence": ["to use CoNNL-2002 because our primary focus was on the NER task and experiments. CoNNL is one of the best practice formats for NER datasets. All relevant tools support CoNNL, including WebAnno for manual annotation. Nevertheless, it is possible, of course, to re-insert the annotated information back into the XML documents. Description of the Dataset ::: Annotation of Named Entities. The dataset consists of 66,723 sentences with 2,157,048 tokens (incl. punctuation), see Table . The sizes of the seven court-specific datasets varies between 5,858 and 12,791 sentences, and 177,835 to 404,041 tokens. The distribution of annotations on a per-token basis corresponds to approx. 19\u201323 %. The Federal Patent Court (BPatG) dataset contains the lowest number of annotated entities (10.41 %). The dataset includes two different versions of annotations, one with a set of 19 fine-grained semantic classes and another one with a set of 7 coarse-grained classes (Table ). There are 53,632 annotated", "than half of city and street, about 55 %, have also been modified. Landscape and organization are affected as well, with 40 % and 15 % of the occurrences edited accordingly. However, anonymisation is typically not applied to judge, country, institution and court (1\u20135 %). The dataset was originally annotated by the first author. To evaluate and potentially improve the quality of the annotations, part of the dataset was annotated by a second linguist (using the annotation guidelines specifically prepared for its construction). We selected a small part that could be annotated in approx. two weeks. For the sentence extraction we paid special attention to the anonymised mentions of person, location or organization entities, because these are usually explained at their first mention. The resulting sample consisted of 2005 sentences with a broad variety of different entities (3 % of all sentences from each federal court). The agreement between the two annotators was measured using Kappa on a", "to their categories (i. e., `Schoch, in: Schoch/Schneider/Bier, VwGO \u00a7 123 Rn. 35', `Bekanntmachung des BMG gem\u00e4\u00df \u00a7\u00a7 295 und 301 SGB V zur Anwendung des OPS vom 21.10.2010'). The second annotator had difficulties annotating the class law, not all instances were identified (`\u00a7 272 Abs. 1a und 1b HGB', `\u00a7 3c Abs. 2 Satz 1 EStG'), others only partially (`\u00a7 716 in Verbindung mit' in `\u00a7 716 in Verbindung mit \u00a7\u00a7 321 , 711 ZPO'). Some titles of contract were not recognised and annotated (`BAT', `TV-L', `TV\u00dc-L\u00e4nder' etc.). This evaluation has revealed deficiencies in the annotation guidelines, especially regarding court decision and legal literature as well as non-entities. It would also be helpful for the identification and classification to list well-known sources of law, court decision, legal literature etc. Description of the Dataset ::: Annotation of Time Expressions. All court decisions were annotated automatically for time expressions using a customised version of HeidelTime BIBREF19,", "dataset includes two different versions of annotations, one with a set of 19 fine-grained semantic classes and another one with a set of 7 coarse-grained classes (Table ). There are 53,632 annotated entities in total, the majority of which (74.34 %) are legal entities, the others are person, location and organization (25.66 %). Overall, the most frequent entities are law GS (34.53 %) and court decision RS (23.46 %). The other legal classes (ordinance VO, European legal norm EUN, regulation VS, contract VT, and legal literature LIT) are much less frequent (1\u20136 % each). Even less frequent (less than 1 %) are lawyer AN, street STR, landscape LDS, and brand MRK. The classes person, lawyer and company are heavily affected by the anonymisation process (80 %, 95 % and 70 % respectively). More than half of city and street, about 55 %, have also been modified. Landscape and organization are affected as well, with 40 % and 15 % of the occurrences edited accordingly. However, anonymisation is", "sample consisted of 2005 sentences with a broad variety of different entities (3 % of all sentences from each federal court). The agreement between the two annotators was measured using Kappa on a token basis. All class labels were taken into account in accordance with the IOB2 scheme BIBREF18. The inter-annotator agreement is 0.89, i. e., there is mostly very good agreement between the two annotators. Differences were in the identification of court decision and legal literature. Some unusual references of court decision (consisting only of decision type, court, date, file number) were not annotated such as `Urteil des Landgerichts Darmstadt vom 16. April 2014 \u2013 7 S 8/13 \u2013'. Apart from missing legal literature annotations, author names and law designations were annotated according to their categories (i. e., `Schoch, in: Schoch/Schneider/Bier, VwGO \u00a7 123 Rn. 35', `Bekanntmachung des BMG gem\u00e4\u00df \u00a7\u00a7 295 und 301 SGB V zur Anwendung des OPS vom 21.10.2010'). The second annotator had", "gazetteers and lookup). For BiLSTM, we used models with pre-trained word embeddings BIBREF22: BiLSTM-CRF BIBREF23, BiLSTM-CRF+ with character embeddings from BiLSTM BIBREF24, and BiLSTM-CNN-CRF with character embeddings from CNN BIBREF25. To evaluate the performance we used stratified 10-fold cross-validation. As expected, BiLSTMs perform best (see Table ). The F$_1$ score for the fine-grained classification reaches 95.46 and 95.95 for the coarse-grained one. CRFs reach up to 93.23 F$_1$ for the fine-grained classes and 93.22 F$_1$ for the coarse-grained ones. Both models perform best for judge, court and law. Conclusions and Future Work. We describe a dataset that consists of German legal documents. For the annotation, we specified a typology of characteristic semantic categories that are relevant for court decisions (i. e., court, institution, law, court decision, and legal literature) with corresponding annotation guidelines. A functional service based on the work described in this", "1, 4 ...' Description of the Dataset. The dataset, which also includes annotation guidelines, is freely available under a CC-BY 4.0 license. The named entity annotations adhere to the CoNLL-2002 format BIBREF14, while time expressions were annotated using TimeML BIBREF15. Description of the Dataset ::: Original Source Documents. Legal documents are a rather heterogeneous class, which also manifests in their linguistic properties, including the use of named entities and references. Their type and frequency varies significantly, depending on the text type. Texts belonging to specific text type, which are to be selected for inclusion in a corpus must contain enough different named entities and references and they need to be freely available. When comparing legal documents such as laws, court decisions or administrative regulations, decisions are the best option. In laws and administrative regulations, the frequencies of person, location and organization are not high enough for NER", "contract, statute etc. are used. Unfortunately, the authors do not provide any details regarding the questions how the entities were categorised or if there is any correlations between the different levels. They work with Wikipedia articles and decisions of the European Court of Human Rights. glaser2017named use GermaNER BIBREF9 and DBpedia Spotlight BIBREF10, BIBREF11 for the recognition of person, location and organization entities. References are identified based on the rules described by landthaler2016unveiling. The authors created an evaluation dataset of 20 court decisions. Annotation of the Dataset. In the following, we describe the rationale behind the annotation of the dataset including the definition of the various semantic classes and the annotation guidelines. Annotation of the Dataset ::: Named Entities vs. Legal Entities ::: Named Entity. An entity is an object or set of objects in the real world and can be referenced in a text with a proper name, noun or pronoun"]}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_answer": "Turkish, Finnish, Czech, German, Spanish, and Catalan.", "predicted_evidence": ["We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are. We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as: Method. Formally, we generate a label sequence $\\vec{l}$ for each sentence and predicate pair: $(s,p)$ . Each $l_t\\in \\vec{l}$ is chosen from $\\mathcal {L}=\\lbrace  \\mathit {roles \\cup nonrole}\\rbrace $ , where $roles$ are language-specific semantic roles (mostly consistent with PropBank) and $nonrole$ is a symbol to present tokens that are not arguments. Given $\\theta", "We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions. Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores", "However, we wonder how the models behave when given a larger network. To answer this question, we trained char3 and oracle models with more layers for two fusional languages (Spanish, Catalan), and two agglutinative languages (Finnish, Turkish). The results given in Table 6 clearly shows that model complexity provides relatively more benefit to morphological models. This indicates that morphological signals help to extract more complex linguistic features that have semantic clues. Predicted Morphological Tags. Although models with access to gold morphological tags achieve better F1 scores than character models, they can be less useful a in real-life scenario since they require gold tags at test time. To predict the performance of morphology-level models in such a scenario, we train the same models with the same parameters with predicted morphological features. Predicted tags were only available for German, Spanish, Catalan and Czech. Our results given in Fig. 5 , show that (except for", "2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values. Similarity between models. One way to infer similarity is to measure diversity. Consider a set of baseline models that are not diverse, i.e., making similar errors with similar inputs. In such a case, combination of these models would not be able to overcome the biases of the learners, hence the combination would not achieve a better result. In order to test if character and morphological models are similar, we combine them and measure the performance of the ensemble. Suppose that a prediction $p_{i}$ is generated for each token by a model $m_i$ , $i \\in n$ , then the final prediction is calculated from these predictions by:  $$p_{final} = f(p_0, p_1,..,p_n|\\phi )$$   (Eq. 36)  where $f$ is the combining function with parameter $\\phi $ . The simplest global", "us gain more useful insights on character level models. Results and Analysis. Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set. The biggest improvement over the word baseline is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently outperformed characters by a small margin. Same pattern is observed on the results of the development set. IOW has the values between 0% to 38% while IOC values range between 2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic", "by character-morphology ensembles is higher (shown with green) than ensembles between characters and character trigrams (shown with red), whereas the opposite is true for the second set of languages. It can be interpreted as character level models being more similar to the morphology level models for the first cluster, i.e., languages with high OOV%, and characters and morphology being more diverse for the second cluster. Limitations and Strengths. To expand our understanding and reveal the limitations and strengths of the models, we analyze their ability to handle long range dependencies, their relation with training data and model size; and measure their performances on out of domain data. Long Range Dependencies. Long range dependency is considered as an important linguistic issue that is hard to solve. Therefore the ability to handle it is a strong performance indicator. To gain insights on this issue, we measure how models perform as the distance between the predicate and the", "the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Character-level models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks BIBREF0 , BIBREF1 , BIBREF2 . However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of long-range dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully understanding the character-level models. To achieve this, we perform", "models with the same parameters with predicted morphological features. Predicted tags were only available for German, Spanish, Catalan and Czech. Our results given in Fig. 5 , show that (except for Czech), predicted morphological tags are not as useful as characters alone. Conclusion. Character-level neural models are becoming the defacto standard for NLP problems due to their accessibility and ability to handle unseen data. In this work, we investigated how they compare to models with access to gold morphological analysis, on a sentence-level semantic task. We evaluated their quality on semantic role labeling in a number of agglutinative and fusional languages. Our results lead to the following conclusions: Acknowledgements. G\u00f6zde G\u00fcl \u015eahin was a PhD student at Istanbul Technical University and a visiting research student at University of Edinburgh during this study. She was funded by T\u00fcbitak (The Scientific and Technological Research Council of Turkey) 2214-A scholarship during her"]}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_answer": "Yes", "predicted_evidence": ["We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are. We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as: Method. Formally, we generate a label sequence $\\vec{l}$ for each sentence and predicate pair: $(s,p)$ . Each $l_t\\in \\vec{l}$ is chosen from $\\mathcal {L}=\\lbrace  \\mathit {roles \\cup nonrole}\\rbrace $ , where $roles$ are language-specific semantic roles (mostly consistent with PropBank) and $nonrole$ is a symbol to present tokens that are not arguments. Given $\\theta", "$ 19:PBArgM_mod meaning the node is an argument of $17_{th}$ and $19_{th}$ tokens with ArgM-mod (temporary modifier) semantic role. They have been converted into CoNLL-09 tabular format, where each predicate's arguments are given in a specific column. Words are splitted from derivational boundaries in the original dataset, where each inflectional group is represented as a separate token. We first merge boundaries of the same word, i.e, tokens of the word, then we use our own $\\rho $ function to split words into subwords. We lowercase all tokens beforehand and place special start and end of the token characters. For all experiments, we initialized weight parameters orthogonally and used one layer bi-LSTMs both for subword composition and argument labeling with hidden size of 200. Subword embedding size is chosen as 200. We used gradient clipping and early stopping to prevent overfitting. Stochastic gradient descent is used as the optimizer. The initial learning rate is set to 1 and", "2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values. Similarity between models. One way to infer similarity is to measure diversity. Consider a set of baseline models that are not diverse, i.e., making similar errors with similar inputs. In such a case, combination of these models would not be able to overcome the biases of the learners, hence the combination would not achieve a better result. In order to test if character and morphological models are similar, we combine them and measure the performance of the ensemble. Suppose that a prediction $p_{i}$ is generated for each token by a model $m_i$ , $i \\in n$ , then the final prediction is calculated from these predictions by:  $$p_{final} = f(p_0, p_1,..,p_n|\\phi )$$   (Eq. 36)  where $f$ is the combining function with parameter $\\phi $ . The simplest global", "us gain more useful insights on character level models. Results and Analysis. Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set. The biggest improvement over the word baseline is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently outperformed characters by a small margin. Same pattern is observed on the results of the development set. IOW has the values between 0% to 38% while IOC values range between 2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic", "(came)\u201d and \u201cSendika+l\u0131-lar (union members) meclis+e (to council) geldi (came)\u201d. Here the stems k\u00f6y (village) and sendika (union) function similarly in semantic terms with respect to the verb come (as the origin of the agents of the verb), where \u015fehir (town) and meclis (council) both function as the end point. These semantic similarities are determined by the common word parts shown in bold. However ortographic similarity does not always correspond to semantic similarity. For instance the ortographically similar words knight and night have large semantic differences. Therefore, for a successful semantic application, the model should be able to capture both the regularities, i.e, morphological tags and the irregularities, i.e, lemmas of the word. Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high", "can be achieved by altering the input representation, the learning algorithm, training data or the hyperparameters. To ensure that the only factor contributing to the diversity of the learners is the input representation, all parameters, training data and model settings are left unchanged. Our results are given in Table 4 . IOB shows the improvement over the best of the baseline models in the ensemble. Averaging and stacking methods gave similar results, meaning that there is no immediate nonlinear relations between units. We observe two language clusters: (1) Czech and agglutinative languages (2) Spanish, Catalan, German and English. The common property of that separate clusters are (1) high OOV% and (2) relatively low OOV%. Amongst the first set, we observe that the improvement gained by character-morphology ensembles is higher (shown with green) than ensembles between characters and character trigrams (shown with red), whereas the opposite is true for the second set of languages.", "by character-morphology ensembles is higher (shown with green) than ensembles between characters and character trigrams (shown with red), whereas the opposite is true for the second set of languages. It can be interpreted as character level models being more similar to the morphology level models for the first cluster, i.e., languages with high OOV%, and characters and morphology being more diverse for the second cluster. Limitations and Strengths. To expand our understanding and reveal the limitations and strengths of the models, we analyze their ability to handle long range dependencies, their relation with training data and model size; and measure their performances on out of domain data. Long Range Dependencies. Long range dependency is considered as an important linguistic issue that is hard to solve. Therefore the ability to handle it is a strong performance indicator. To gain insights on this issue, we measure how models perform as the distance between the predicate and the", "We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions. Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores"]}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_answer": "gold morphological features", "predicted_evidence": ["However, we wonder how the models behave when given a larger network. To answer this question, we trained char3 and oracle models with more layers for two fusional languages (Spanish, Catalan), and two agglutinative languages (Finnish, Turkish). The results given in Table 6 clearly shows that model complexity provides relatively more benefit to morphological models. This indicates that morphological signals help to extract more complex linguistic features that have semantic clues. Predicted Morphological Tags. Although models with access to gold morphological tags achieve better F1 scores than character models, they can be less useful a in real-life scenario since they require gold tags at test time. To predict the performance of morphology-level models in such a scenario, we train the same models with the same parameters with predicted morphological features. Predicted tags were only available for German, Spanish, Catalan and Czech. Our results given in Fig. 5 , show that (except for", "We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions. Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores", "(came)\u201d and \u201cSendika+l\u0131-lar (union members) meclis+e (to council) geldi (came)\u201d. Here the stems k\u00f6y (village) and sendika (union) function similarly in semantic terms with respect to the verb come (as the origin of the agents of the verb), where \u015fehir (town) and meclis (council) both function as the end point. These semantic similarities are determined by the common word parts shown in bold. However ortographic similarity does not always correspond to semantic similarity. For instance the ortographically similar words knight and night have large semantic differences. Therefore, for a successful semantic application, the model should be able to capture both the regularities, i.e, morphological tags and the irregularities, i.e, lemmas of the word. Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high", "We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are. We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as: Method. Formally, we generate a label sequence $\\vec{l}$ for each sentence and predicate pair: $(s,p)$ . Each $l_t\\in \\vec{l}$ is chosen from $\\mathcal {L}=\\lbrace  \\mathit {roles \\cup nonrole}\\rbrace $ , where $roles$ are language-specific semantic roles (mostly consistent with PropBank) and $nonrole$ is a symbol to present tokens that are not arguments. Given $\\theta", "the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Character-level models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks BIBREF0 , BIBREF1 , BIBREF2 . However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of long-range dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully understanding the character-level models. To achieve this, we perform", "models with the same parameters with predicted morphological features. Predicted tags were only available for German, Spanish, Catalan and Czech. Our results given in Fig. 5 , show that (except for Czech), predicted morphological tags are not as useful as characters alone. Conclusion. Character-level neural models are becoming the defacto standard for NLP problems due to their accessibility and ability to handle unseen data. In this work, we investigated how they compare to models with access to gold morphological analysis, on a sentence-level semantic task. We evaluated their quality on semantic role labeling in a number of agglutinative and fusional languages. Our results lead to the following conclusions: Acknowledgements. G\u00f6zde G\u00fcl \u015eahin was a PhD student at Istanbul Technical University and a visiting research student at University of Edinburgh during this study. She was funded by T\u00fcbitak (The Scientific and Technological Research Council of Turkey) 2214-A scholarship during her", "2%-10% dependending on the properties of the language and the dataset. We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values. Similarity between models. One way to infer similarity is to measure diversity. Consider a set of baseline models that are not diverse, i.e., making similar errors with similar inputs. In such a case, combination of these models would not be able to overcome the biases of the learners, hence the combination would not achieve a better result. In order to test if character and morphological models are similar, we combine them and measure the performance of the ensemble. Suppose that a prediction $p_{i}$ is generated for each token by a model $m_i$ , $i \\in n$ , then the final prediction is calculated from these predictions by:  $$p_{final} = f(p_0, p_1,..,p_n|\\phi )$$   (Eq. 36)  where $f$ is the combining function with parameter $\\phi $ . The simplest global", "by character-morphology ensembles is higher (shown with green) than ensembles between characters and character trigrams (shown with red), whereas the opposite is true for the second set of languages. It can be interpreted as character level models being more similar to the morphology level models for the first cluster, i.e., languages with high OOV%, and characters and morphology being more diverse for the second cluster. Limitations and Strengths. To expand our understanding and reveal the limitations and strengths of the models, we analyze their ability to handle long range dependencies, their relation with training data and model size; and measure their performances on out of domain data. Long Range Dependencies. Long range dependency is considered as an important linguistic issue that is hard to solve. Therefore the ability to handle it is a strong performance indicator. To gain insights on this issue, we measure how models perform as the distance between the predicate and the"]}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_answer": "SemScholar, SciGraph, TQA, Flickr30K, COCO", "predicted_evidence": ["figures and captions and multi-modal machine comprehension for question answering given a context of text, figures and images. Results and Discussion ::: Datasets. We have used the following datasets for training and evaluation: The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22. We randomly selected 500K papers to train the FCC task on their figures and captions and another 500K to train Vecsigrafo on the text of their titles and abstracts. Springer Nature's SciGraph contains 7M scientific publications organized in 22 scientific fields or categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers", "Introduction. Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented. In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific", "problem of image-sentence matching through a bidirectional retrieval task where images are sought given a text query and vice versa. While table TABREF20 focuses on natural images datasets (Flickr30K and COCO), table TABREF21 shows results on scientific datasets (SciGraph and SemScholar) rich in scientific figures and diagrams. The selected baselines (Embedding network, 2WayNet, VSE++ and DSVE-loc) report results obtained on the Flickr30K and COCO datasets, which we also include in table TABREF20. Performance is measured in recall at k ($Rk$), with k={1,5,10}. From the baselines, we successfully reproduced DSVE-loc, using the code made available by the authors, and trained it on SciGraph and SemScholar. We trained the FCC task on all the datasets, both in a totally unsupervised way and with pre-trained semantic embeddings (indicated with subscript $vec$), and executed the bidirectional retrieval task using the resulting text and visual features. We also experimented with pre-trained", "embeddings, slightly outperforms \"Ours $FCC_7$\", suggesting a larger margin to learn from the task-specific corpus. Note that both $FCC_6$ and $FCC_7$ were trained on SemScholar. Results and Discussion ::: Textbook Question Answering (TQA) for Multi-Modal Machine Comprehension. We leverage the TQA dataset and the baselines in BIBREF23 to evaluate the features learnt by the FCC task in a multi-modal machine comprehension scenario. We study how our model, which was not originally trained for this task, performs against state of the art models specifically trained for diagram question answering and textual reading comprehension in a very challenging dataset. We also study how pre-trained semantic embeddings impact in the TQA task: first, by enriching the visual features learnt in the FCC task as shown in section SECREF6 and then by using pre-trained semantic embeddings to enrich word representations in the TQA corpus. We focus on multiple-choice questions, 73% of the dataset. Table", "categories. Since SciGraph does not provide a link to the PDF of the publication, we selected the intersection with SemScholar, producing a smaller corpus of 80K papers (in addition to the 1M papers from SemScholar mentioned above) and 82K figures that we used for training certain FCC configurations and supervised baselines (section SECREF14). The Textbook Question Answering corpus BIBREF23 includes 1,076 lessons and 26,260 multi-modal test questions from middle school science curricula. Its complexity and scope make it a challenging textual and visual question answering dataset. Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information. Flickr30K and COCO, as image-sentence matching benchmarks. Results and Discussion ::: Figure-Caption Correspondence. We evaluate our method in the task it was trained to", "task as shown in section SECREF6 and then by using pre-trained semantic embeddings to enrich word representations in the TQA corpus. We focus on multiple-choice questions, 73% of the dataset. Table TABREF24 shows the performance of our model against the results reported in BIBREF23 for five TQA baselines: random, BiDAF (focused on text machine comprehension), text only ($TQA_1$, based on MemoryNet), text+image ($TQA_2$, VQA), and text+diagrams ($TQA_3$, DSDP-NET). We successfully reproduced the $TQA_1$ and $TQA_2$ architectures and adapted the latter. Then, we replaced the visual features in $TQA_2$ with those learnt by the FCC visual subnetwork both in a completely unsupervised way ($FCC_6$ in table TABREF15) and with pre-trained semantic embeddings ($FCC_7$), resulting in $TQA_4$ and $TQA_5$, respectively. While $TQA_{1-5}$ used no pre-trained embeddings at all, $TQA_{6-10}$ were trained including pre-trained Vecsigrafo semantic embeddings. Unlike FCC, where we used concatenation to", "with the most significant activation over the whole dataset and select the figures that activate them most. To this purpose, we prioritize those with higher maximum activation against the average activation. Figure FIGREF27 shows a selection of 6 visual features with the 4 figures that activate each feature more significantly and their activation heatmaps. Only figures are used as input, no text. As can be seen, the vision subnetwork has automatically learnt, without explicit supervision, to recognize different types of diagrams, charts and content, such as (from left to right) whisker plots, western blots (a technique used to identify proteins in a tissue sample), multi-image comparison diagrams, multi-modal data visualization charts (e.g. western plots vs. bar charts), line plots, and text within the figures. Furthermore, as shown by the heatmaps, our model discriminates the key elements associated to the figures that most activate each feature: the actual whiskers, the blots, the", "visualization purposes, we picked the figures corresponding to the captions in SciGraph that most activate such features (figure FIGREF28). No visual information is used. Several distinct patterns emerge from the text. The text feature in the first column seems to focus on genetics and histochemistry, including terms like western blots or immunostaining and variations like immunoblot-s/ted/ting. Interestingly, it also seems to have learnt some type of is-a relations (western blot is a type of immunoblot). The second feature focuses on variations of the term radiograph, e.g. radiograph-y/s. The third feature specializes in text related to curve plots involving several statistic analysis, e.g. Real-time PCR, one-way ANOVA or Gaussian distribution. Sometimes (fourth figure from top) the caption does not mention the plot directly, but focuses on the analysis instead, e.g. \"the data presented here are mean values of duplicate experiments\", indicating transfer of knowledge from the visual"]}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_answer": "Unanswerable", "predicted_evidence": ["captions, calculated on the embeddings layer of the language subnetwork. The upper one corresponds to the fourth column left-right and third figure top-down in figure FIGREF28. Its caption reads: \"The Aliev-Panfilov model with $\\alpha =0.01$...The phase portrait depicts trajectories for distinct initial values $\\varphi _0$ and $r_0$...\". Below, (first column, fourth figure in figure FIGREF28): \"Relative protein levels of ubiquitin-protein conjugates in M. quadriceps...A representative immunoblot specific to ubiquitin...\". Consistently with our analysis, activation focuses on the most relevant tokens for each text feature: \"Aliev-Panfilov model\" and \"immunoblot\", respectively. Conclusions. There is a wealth of knowledge in scientific literature and only a fraction of it is text. However, understanding scientific figures is a challenging task for machines, which is beyond their ability to process natural images. In this paper, we provide empirical evidence of this and show that", "which are then stitched together. In contrast, FCC co-trains text and visual features from figures and their captions and supports the enrichment of such features with lexical and semantic knowledge transferred from a KG during the training of the FCC task. Figure-Caption Correspondence. The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like. We leverage this observation to learn a figure-caption correspondence task. In essence, FCC is a binary classification task that receives a figure and a caption and determines whether", "and text features from scratch in a completely unsupervised manner, just by observing the correspondence of figures and captions. Next, we extend it to enable the transfer of additional pre-trained information. Here, we focus on adding pre-trained embeddings on the language branch, and then back-propagate to the visual features during FCC training. Adding pre-trained visual features is also possible and indeed we also evaluate its impact in the FCC task in section SECREF14. Let $V$ be a vocabulary of words from a collection of documents $D$. Also, let $L$ be their lemmas, i.e. base forms without morphological or conjugational variations, and $C$ the concepts (or senses) in a KG. Each word $w_k$ in $V$, e.g. made, has one lemma $l_k$ (make) and may be linked to one or more concepts $c_k$ in $C$ (create or produce something). For each word $w_k$, the FCC task learns a d-D embedding $\\vec{w}_k$, which can be combined with pre-trained word ($\\vec{w^{\\prime }}_k$), lemma ($\\vec{l}_k$) and", "that takes the resulting features from the visual and text blocks and uses them to evaluate figure-caption correspondence. The vision subnetwork follows a VGG-style BIBREF13 design, with 3x3 convolutional filters, 2x2 max-pooling layers with stride 2 and no padding. It contains 4 blocks of conv+conv+pool layers, where inside each block the two convolutional layers have the same number of filters, while consecutive blocks have doubling number of filters (64, 128, 256, 512). The input layer receives 224x224x3 images. The final layer produces a 512-D vector after 28x28 max-pooling. Each convolutional layer is followed by batch normalization BIBREF14 and ReLU layers. Based on BIBREF15, the language subnetwork has 3 convolutional blocks, each with 512 filters and a 5-element window size with ReLU activation. Each convolutional layer is followed by a 5-max pooling layer, except for the final layer, which produces a 512-D vector after 35-max pooling. The language subnetwork has a 300-D", "and even less in SemScholar, where the combination of FCC and Vecsigrafo (\"Oursvec\") obtains the best results across the board. This and the extremely poor performance of the best image-sentence matching baseline (DSVE-loc) in the scientific datasets shows evidence that dealing with scientific figures is considerably more complex than natural images. Indeed, the best results in figure-caption correspondence (\"Oursvec\" in SemScholar) are still far from the SoA in image-sentence matching (DSVE-loc in COCO). Results and Discussion ::: Caption and Figure Classification. We evaluate the language and visual representations emerging from FCC in the context of two classification tasks that aim to identify the scientific field an arbitrary text fragment (a caption) or a figure belong to, according to the SciGraph taxonomy. The latter is a particularly hard task due to the whimsical nature of the figures that appear in our corpus: figure and diagram layout is arbitrary; charts, e.g. bar and pie", "confirming that the visual and language branches learn from each other and also that figure-caption correspondence is an effective source of free supervision. Adding pre-trained knowledge at the input layer of the language subnetwork provides an additional boost, particularly with lemma and concept embeddings from Vecsigrafo ($FCC_5$). Vecsigrafo clearly outperformed HolE ($FCC_3$), which was also beaten by pre-trained fastText BIBREF24 word embeddings ($FCC_2$) trained on SemScholar. Since graph-based KG embedding approaches like HolE only generate embeddings of the artifacts explicitly contained in the KG, this may indicate that Sensigrafo, the KG used in this task, provides a partial coverage of the scientific domain, as could be expected since we are using an off-the-shelf version. Deeper inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora", "and quantitative analysis of the learnt text and visual features through transfer learning tasks. A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar. We present the FCC task in section SECREF3, including the network architecture, training protocol, and how adding pre-trained word and semantic embeddings can enrich the resulting text and visual features. In section SECREF4, we first introduce our datasets and evaluate the performance of our method in the task it was trained to solve, the correspondence between scientific figures and captions. Then, we relate our work to the state of the art in image-sentence matching and evaluate our approach in two challenging transfer learning tasks: caption and figure classification and multi-modal machine comprehension. In section SECREF5 we perform a qualitative study that illustrates how the FCC task leads to detailed textual and visual discrimination. Finally, in section SECREF6 we conclude the paper", "publication there is only one caption that corresponds with one figure and vice versa. The FCC task presented herein is a form of co-training BIBREF4 where there are two views of the data and each view provides complementary information. Similar two-branch neural architectures focus on image-sentence BIBREF5, BIBREF6 and audio-video BIBREF7 matching. Others like BIBREF8 learn common embeddings from images and text. However, in such cases one or both networks are typically pre-trained. Focused on geometry, BIBREF9 maximize the agreement between text and visual data. In BIBREF10, the authors apply machine vision and natural language processing to extract data from figures and their associated text in bio-curation tasks. In BIBREF11, they parse diagram components and connectors as a Diagram Parse Graph (DPG), semantically interpret the DPG and use the model to answer diagram questions. While we rely on the correspondence between figures and captions, they train a specific classifier for"]}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_answer": "Unanswerable", "predicted_evidence": ["The latter extends the Swivel algorithm BIBREF20 to jointly learn word, lemma and concept embeddings on a corpus disambiguated against the KG, outperforming the previous state of the art in word and word-sense embeddings by co-training word, lemma and concept embeddings as opposed to training each individually. In contrast to Vecsigrafo, which requires both a text corpus and a KG, HolE follows a graph-based approach where embeddings are learnt exclusively from the KG. As section SECREF14 will show, this gives Vecsigrafo a certain advantage in the FCC task. Following up with the work presented in BIBREF16, our experiments focus on Sensigrafo, the KG underlying Expert System's Cogito NLP proprietary platform. Similar to WordNet, on which Vecsigrafo has also been successfully trained, Sensigrafo is a general-purpose KG with lexical and semantic information that contains over 300K concepts, 400K lemmas and 80 types of relations rendering 3M links. We use Cogito to disambiguate the text", "figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The supervised pre-training baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture as shown in section SECREF6, and trains the FCC task on the fully connected layers. While direct combination provides a notion of the agreement between the two branches, supervised pre-training is the most similar supervised approach to our method. Table TABREF15 shows the results of the FCC task and the supervised baselines. $FCC_k$ denotes the corpus and word representation used to train the FCC task. Acc$_{vgg}$ shows the accuracy after replacing our visual branch with pre-trained VGG16 features learnt on ImageNet. This provides an estimate of how specific of the scientific domain scientific figures and therefore the resulting visual", "respectively. While $TQA_{1-5}$ used no pre-trained embeddings at all, $TQA_{6-10}$ were trained including pre-trained Vecsigrafo semantic embeddings. Unlike FCC, where we used concatenation to combine pre-trained lemma and concept embeddings with the word embeddings learnt by the task, element-wise addition worked best in the case of TQA. Following the recommendations in BIBREF23, we pre-processed the TQA corpus to i) consider knowledge from previous lessons in the textbook in addition to the lesson of the question at hand and ii) address challenges like long question contexts with a large lexicon. In both text and diagram MC, applying the Pareto principle to reduce the maximum token sequence length in the text of each question, their answers and context improved accuracy considerably. This optimization allowed reducing the amount of text to consider for each question, improving the signal to noise ratio. Finally, we obtained the most relevant paragraphs for each question through", "confirming that the visual and language branches learn from each other and also that figure-caption correspondence is an effective source of free supervision. Adding pre-trained knowledge at the input layer of the language subnetwork provides an additional boost, particularly with lemma and concept embeddings from Vecsigrafo ($FCC_5$). Vecsigrafo clearly outperformed HolE ($FCC_3$), which was also beaten by pre-trained fastText BIBREF24 word embeddings ($FCC_2$) trained on SemScholar. Since graph-based KG embedding approaches like HolE only generate embeddings of the artifacts explicitly contained in the KG, this may indicate that Sensigrafo, the KG used in this task, provides a partial coverage of the scientific domain, as could be expected since we are using an off-the-shelf version. Deeper inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora", "problem of image-sentence matching through a bidirectional retrieval task where images are sought given a text query and vice versa. While table TABREF20 focuses on natural images datasets (Flickr30K and COCO), table TABREF21 shows results on scientific datasets (SciGraph and SemScholar) rich in scientific figures and diagrams. The selected baselines (Embedding network, 2WayNet, VSE++ and DSVE-loc) report results obtained on the Flickr30K and COCO datasets, which we also include in table TABREF20. Performance is measured in recall at k ($Rk$), with k={1,5,10}. From the baselines, we successfully reproduced DSVE-loc, using the code made available by the authors, and trained it on SciGraph and SemScholar. We trained the FCC task on all the datasets, both in a totally unsupervised way and with pre-trained semantic embeddings (indicated with subscript $vec$), and executed the bidirectional retrieval task using the resulting text and visual features. We also experimented with pre-trained", "Introduction. Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented. In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific", "understanding scientific figures is a challenging task for machines, which is beyond their ability to process natural images. In this paper, we provide empirical evidence of this and show that co-training text and visual features from a large corpus of scientific figures and their captions in a correspondence task (FCC) is an effective, flexible and elegant unsupervised means towards overcoming such complexity. We show how such features can be significantly improved by enriching them with additional knowledge sources and, particularly, structured KGs. We prove the benefits of our approach against supervised baselines and in different transfer learning tasks, including text and visual classification and multi-modal machine comprehension applied to question answering, with results generally beyond the state of the art. In the future, it will be interesting to further the study of the interplay between the semantic concepts explicitly represented in different KGs, contextualized", "This optimization allowed reducing the amount of text to consider for each question, improving the signal to noise ratio. Finally, we obtained the most relevant paragraphs for each question through tf-idf and trained the models using 10-fold cross validation, Adam, learning rate $10^{-2}$ and batch size 128. In text MC we also used 0.5 dropout and recurrent dropout in the LSTM layers. Fitting multi-modal sources into a single memory, the use of visual FCC features clearly outperforms all the TQA baselines in diagram MC. Enhancing word representation with pre-trained semantic embeddings during training of the TQA task provides an additional boost that results in the highest accuracies for both text MC and diagram MC. These are significantly good results since, according to the TQA authors BIBREF23, most diagram questions in the TQA corpus would normally require a specific rich diagram parse, which we did not aim to provide. Qualitative Analysis. We inspect the features learnt by our"]}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_answer": "Yes", "predicted_evidence": ["Parse Graph (DPG), semantically interpret the DPG and use the model to answer diagram questions. While we rely on the correspondence between figures and captions, they train a specific classifier for each component and connector type and yet another model to ground the semantics of the DPG in each domain, like food webs or water cycles. Knowledge fusion approaches like BIBREF12 investigate the potential of complementing KG embeddings with text and natural images by integrating information across the three modalities in a single latent representation. They assume pre-trained entity representations exist in each individual modality, e.g. the visual features encoding the image of a ball, the word embeddings associated to the token \"ball\", and the KG embeddings related to the ball entity, which are then stitched together. In contrast, FCC co-trains text and visual features from figures and their captions and supports the enrichment of such features with lexical and semantic knowledge", "inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora it is trained on, Wikipedia or SemScholar, raising lemma coverage to 42% and 47%, respectively. Although the size of Wikipedia is almost triple of our SemScholar corpus, training Vecsigrafo on the latter resulted in better FCC accuracy ($FCC_4$ vs. $FCC_5$), suggesting that domain relevance is more significant than sheer volume, in line with our previous findings in BIBREF25. Training FCC on SemScholar, much larger than SciGraph, further improves accuracy, as shown in $FCC_6$ and $FCC_7$. Results and Discussion ::: Image-Sentence Matching. We put our FCC task in the context of the more general problem of image-sentence matching through a bidirectional retrieval task where images are sought given a text query and vice versa. While table TABREF20 focuses on natural images datasets (Flickr30K", "and even less in SemScholar, where the combination of FCC and Vecsigrafo (\"Oursvec\") obtains the best results across the board. This and the extremely poor performance of the best image-sentence matching baseline (DSVE-loc) in the scientific datasets shows evidence that dealing with scientific figures is considerably more complex than natural images. Indeed, the best results in figure-caption correspondence (\"Oursvec\" in SemScholar) are still far from the SoA in image-sentence matching (DSVE-loc in COCO). Results and Discussion ::: Caption and Figure Classification. We evaluate the language and visual representations emerging from FCC in the context of two classification tasks that aim to identify the scientific field an arbitrary text fragment (a caption) or a figure belong to, according to the SciGraph taxonomy. The latter is a particularly hard task due to the whimsical nature of the figures that appear in our corpus: figure and diagram layout is arbitrary; charts, e.g. bar and pie", "problem of image-sentence matching through a bidirectional retrieval task where images are sought given a text query and vice versa. While table TABREF20 focuses on natural images datasets (Flickr30K and COCO), table TABREF21 shows results on scientific datasets (SciGraph and SemScholar) rich in scientific figures and diagrams. The selected baselines (Embedding network, 2WayNet, VSE++ and DSVE-loc) report results obtained on the Flickr30K and COCO datasets, which we also include in table TABREF20. Performance is measured in recall at k ($Rk$), with k={1,5,10}. From the baselines, we successfully reproduced DSVE-loc, using the code made available by the authors, and trained it on SciGraph and SemScholar. We trained the FCC task on all the datasets, both in a totally unsupervised way and with pre-trained semantic embeddings (indicated with subscript $vec$), and executed the bidirectional retrieval task using the resulting text and visual features. We also experimented with pre-trained", "the captions of the figures that most activate it and used Cogito to disambiguate the Sensigrafo concepts that appear in them. Then, we estimated how important each concept is to each feature by calculating its tf-idf. Finally, we averaged the resulting values to obtain a consolidated semantic specificity score per feature. The scores of the features in figure FIGREF27 range between 0.42 and 0.65, which is consistently higher than average (0.4). This seems to indicate a correlation between activation and the semantic specificity of each visual feature. For example, the heatmaps of the figures related to the feature with the lowest tf-idf (left-most column) highlights a particular visual pattern, i.e. the whiskers, that may spread over many, possibly unrelated domains. On the other hand, the feature with the highest score (second column) focuses on a type of diagrams, western blots, almost exclusive of protein and genetic studies. Others, like the feature illustrated by the figures in", "confirming that the visual and language branches learn from each other and also that figure-caption correspondence is an effective source of free supervision. Adding pre-trained knowledge at the input layer of the language subnetwork provides an additional boost, particularly with lemma and concept embeddings from Vecsigrafo ($FCC_5$). Vecsigrafo clearly outperformed HolE ($FCC_3$), which was also beaten by pre-trained fastText BIBREF24 word embeddings ($FCC_2$) trained on SemScholar. Since graph-based KG embedding approaches like HolE only generate embeddings of the artifacts explicitly contained in the KG, this may indicate that Sensigrafo, the KG used in this task, provides a partial coverage of the scientific domain, as could be expected since we are using an off-the-shelf version. Deeper inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora", "is a general-purpose KG with lexical and semantic information that contains over 300K concepts, 400K lemmas and 80 types of relations rendering 3M links. We use Cogito to disambiguate the text corpora prior to training Vecsigrafo. All the semantic (lemma and concept) embeddings produced with HolE or Vecsigrafo are 100-D. Results and Discussion. In this section, first we evaluate the actual FCC task against two supervised baselines. Then, we situate our work in the more general image-sentence matching problem, showing empirical evidence of the additional complexity associated to the scientific domain and the figure-caption case compared to natural images. Next, we test the visual and text features learnt in the FCC task in two different transfer learning settings: classification of scientific figures and captions and multi-modal machine comprehension for question answering given a context of text, figures and images. Results and Discussion ::: Datasets. We have used the following", "ImageNet (\"Oursvgg\" and \"Oursvgg-vec\"). Interestingly, the situation reverts with the scientific datasets. While the recall of DSVE-loc drops dramatically in SciGraph, and even more in SemScholar, our approach shows the opposite behavior in both figure and caption retrieval. Using visual features enriched with pre-trained semantic embeddings from Vecsigrafo during training of the FCC task further improves recall in the bidirectional retrieval task. Compared to natural images, the additional complexity of scientific figures and their caption texts, which in addition are considerably longer (see table TABREF19), seems to have a clear impact in this regard. Unlike in Flickr30K and COCO, replacing the FCC visual features with pre-trained ones from ImageNet brings us little benefit in SciGraph and even less in SemScholar, where the combination of FCC and Vecsigrafo (\"Oursvec\") obtains the best results across the board. This and the extremely poor performance of the best image-sentence"]}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_answer": "SN SciGraph and AI2 Semantic Scholar.", "predicted_evidence": ["and even less in SemScholar, where the combination of FCC and Vecsigrafo (\"Oursvec\") obtains the best results across the board. This and the extremely poor performance of the best image-sentence matching baseline (DSVE-loc) in the scientific datasets shows evidence that dealing with scientific figures is considerably more complex than natural images. Indeed, the best results in figure-caption correspondence (\"Oursvec\" in SemScholar) are still far from the SoA in image-sentence matching (DSVE-loc in COCO). Results and Discussion ::: Caption and Figure Classification. We evaluate the language and visual representations emerging from FCC in the context of two classification tasks that aim to identify the scientific field an arbitrary text fragment (a caption) or a figure belong to, according to the SciGraph taxonomy. The latter is a particularly hard task due to the whimsical nature of the figures that appear in our corpus: figure and diagram layout is arbitrary; charts, e.g. bar and pie", "which are then stitched together. In contrast, FCC co-trains text and visual features from figures and their captions and supports the enrichment of such features with lexical and semantic knowledge transferred from a KG during the training of the FCC task. Figure-Caption Correspondence. The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like. We leverage this observation to learn a figure-caption correspondence task. In essence, FCC is a binary classification task that receives a figure and a caption and determines whether", "that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific publication. To this purpose, we explore how multi-modal scientific knowledge can be learnt from the correspondence between figures and captions. The main contributions of this paper are the following: An unsupervised Figure-Caption Correspondence task (FCC) that jointly learns text and visual features useful to address a range of tasks involving scientific text and figures. A method to enrich such features with semantic knowledge transferred from structured knowledge graphs (KG). A study of the complexity of figure-caption correspondence compared to classical image-sentence matching. A qualitative and quantitative analysis of the learnt text and visual features through transfer learning tasks. A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar. We", "captions, calculated on the embeddings layer of the language subnetwork. The upper one corresponds to the fourth column left-right and third figure top-down in figure FIGREF28. Its caption reads: \"The Aliev-Panfilov model with $\\alpha =0.01$...The phase portrait depicts trajectories for distinct initial values $\\varphi _0$ and $r_0$...\". Below, (first column, fourth figure in figure FIGREF28): \"Relative protein levels of ubiquitin-protein conjugates in M. quadriceps...A representative immunoblot specific to ubiquitin...\". Consistently with our analysis, activation focuses on the most relevant tokens for each text feature: \"Aliev-Panfilov model\" and \"immunoblot\", respectively. Conclusions. There is a wealth of knowledge in scientific literature and only a fraction of it is text. However, understanding scientific figures is a challenging task for machines, which is beyond their ability to process natural images. In this paper, we provide empirical evidence of this and show that", "Introduction. Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented. In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific", "ImageNet (\"Oursvgg\" and \"Oursvgg-vec\"). Interestingly, the situation reverts with the scientific datasets. While the recall of DSVE-loc drops dramatically in SciGraph, and even more in SemScholar, our approach shows the opposite behavior in both figure and caption retrieval. Using visual features enriched with pre-trained semantic embeddings from Vecsigrafo during training of the FCC task further improves recall in the bidirectional retrieval task. Compared to natural images, the additional complexity of scientific figures and their caption texts, which in addition are considerably longer (see table TABREF19), seems to have a clear impact in this regard. Unlike in Flickr30K and COCO, replacing the FCC visual features with pre-trained ones from ImageNet brings us little benefit in SciGraph and even less in SemScholar, where the combination of FCC and Vecsigrafo (\"Oursvec\") obtains the best results across the board. This and the extremely poor performance of the best image-sentence", "general-purpose information. Flickr30K and COCO, as image-sentence matching benchmarks. Results and Discussion ::: Figure-Caption Correspondence. We evaluate our method in the task it was trained to solve: determining whether a figure and a caption correspond. We also compare the performance of the FCC task against two supervised baselines, training them on a classification task against the SciGraph taxonomy. For such baselines we first train the vision and language networks independently and then combine them. The feature extraction parts of both networks are the same as described in section SECREF6. On top of them, we attach a fully connected layer with 128 neurons and ReLU activation and a softmax layer, with as many neurons as target classes. The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The", "looks like. We leverage this observation to learn a figure-caption correspondence task. In essence, FCC is a binary classification task that receives a figure and a caption and determines whether they correspond or not. For training, the positive pairs are actual figures and their captions from a collection of scientific publications. Negative pairs are extracted from combinations of figures and any other randomly selected captions. The network is then made to learn text and visual features from scratch, without additional labelled data. Figure-Caption Correspondence ::: FCC Architecture and Model. We propose a 2-branch neural architecture (figure FIGREF7) that has three main parts: the vision and language subnetworks, respectively extracting visual and text features, and a fusion subnetwork that takes the resulting features from the visual and text blocks and uses them to evaluate figure-caption correspondence. The vision subnetwork follows a VGG-style BIBREF13 design, with 3x3"]}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_answer": "Yes", "predicted_evidence": ["sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet. Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular", "of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features. While the structure of our introduced model allows us to easily include more linguistic features that could potentially improve our predictive power, such as lexicons, since our focus is to study sentence representation for emotion intensity, we do not experiment adding any additional sources of information as input. In this paper we also only report results for LSTMs, which outperformed regular RNNs as well as GRUs and a batch normalized version of the LSTM in on preliminary experiments. The hidden size of the attentional component is set to match the size of the augmented hidden vectors on each case.", "portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of words missing in GloVe. In this sense, we think the usage of additional features, such as the ones derived from emotion or sentiment lexicons could indeed boost our model capabilities. This is proposed for future work. On the other hand, our model also offers us very interesting insights on how the learning is performed, since we can inspect the attention weights that the neural network is assigning to each specific token when predicting the emotion intensity. By visualizing these weights we can have a clear notion about the parts of the sentence that the model considers are more important. As Figure FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset,", "Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings. To validate the usefulness of our binary features, we performed an ablation experiment and trained our best models for each corpus without them. Table TABREF15 summarizes our results in terms of Pearson correlation on the development portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of", "Introduction. Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect\u2014emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated. While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount", "the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one. The task is specially challenging since tweets contain informal language, spelling errors and text referring to external content. Given the 140 character limit of tweets, it is also possible to find some phenomena such as the intensive usage of emoticons and of other special Twitter features, such as hashtags and usernames mentions \u2014used to call or notify other users. In this paper we describe our system designed for the WASSA-2017 Shared Task on Emotion Intensity, which we tackle based on the premise of representation learning without the usage of external information, such as lexicons. In particular, we use a Bi-LSTM model with intra-sentence attention on top of word embeddings to generate a tweet representation that is suitable for emotion intensity. Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external", "sequence INLINEFORM0 and returns a list of hidden vectors that capture the context each input token INLINEFORM1 . To improve the capabilities of the RNN to capture short-term temporal dependencies BIBREF4 , we define the following: DISPLAYFORM0  Where INLINEFORM0 can be regarded as a context window of ordered word embedding vectors around position INLINEFORM1 , with a total size of INLINEFORM2 . To further complement the context-aware token representations, we concatenate each hidden vector to a vector of binary features INLINEFORM3 , extracted from each tweet token, defining an augmented hidden state INLINEFORM4 . Finally, we combine our INLINEFORM5 augmented hidden states, compressing them into a single vector, using a global intra-sentence attentional component in a fashion similar to vinyalsgrammar2015. Formally, DISPLAYFORM0  Where INLINEFORM0 is the vector that compresses the input sentence INLINEFORM1 , focusing on the relevant parts to estimate emotion intensity. We input this", "FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset, where positive words are generally identified. However, we also see some examples where the lack of semantic information about the input words, specially for hashtags or user mentions, makes the model unable to identify some of these the most salient words to predict emotion intensity. Several pre-processing techniques can be implemented to alleviate this problem, which we intend to explore in the future. Anger Dataset. For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100. However on ly the first of these values was significant, with a p-value of INLINEFORM0 . Regarding the hidden size of the RNN, we could not find statistical"]}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "predicted_answer": "Yes", "predicted_evidence": ["Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings. To validate the usefulness of our binary features, we performed an ablation experiment and trained our best models for each corpus without them. Table TABREF15 summarizes our results in terms of Pearson correlation on the development portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of", "of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features. While the structure of our introduced model allows us to easily include more linguistic features that could potentially improve our predictive power, such as lexicons, since our focus is to study sentence representation for emotion intensity, we do not experiment adding any additional sources of information as input. In this paper we also only report results for LSTMs, which outperformed regular RNNs as well as GRUs and a batch normalized version of the LSTM in on preliminary experiments. The hidden size of the attentional component is set to match the size of the augmented hidden vectors on each case.", "sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet. Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular", "portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of words missing in GloVe. In this sense, we think the usage of additional features, such as the ones derived from emotion or sentiment lexicons could indeed boost our model capabilities. This is proposed for future work. On the other hand, our model also offers us very interesting insights on how the learning is performed, since we can inspect the attention weights that the neural network is assigning to each specific token when predicting the emotion intensity. By visualizing these weights we can have a clear notion about the parts of the sentence that the model considers are more important. As Figure FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset,", "scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5. For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data. We used exponential decay of ratio INLINEFORM0 and early stopping on the validation when there was no improvement after 1000 steps. Our code is available for download on GitHub . Results and Discussion. In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any", "Formally, DISPLAYFORM0  Where INLINEFORM0 is the vector that compresses the input sentence INLINEFORM1 , focusing on the relevant parts to estimate emotion intensity. We input this compressed sentence representation into a feed-forward neural network, INLINEFORM2 , where INLINEFORM3 is the final predicted emotion intensity. As a loss function we use the mini-batch negative Pearson correlation with the gold-standard. Experimental Setup. To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 . We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of", "Introduction. Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect\u2014emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated. While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount", "FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset, where positive words are generally identified. However, we also see some examples where the lack of semantic information about the input words, specially for hashtags or user mentions, makes the model unable to identify some of these the most salient words to predict emotion intensity. Several pre-processing techniques can be implemented to alleviate this problem, which we intend to explore in the future. Anger Dataset. For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100. However on ly the first of these values was significant, with a p-value of INLINEFORM0 . Regarding the hidden size of the RNN, we could not find statistical"]}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": "Weka", "predicted_evidence": ["Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings. To validate the usefulness of our binary features, we performed an ablation experiment and trained our best models for each corpus without them. Table TABREF15 summarizes our results in terms of Pearson correlation on the development portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of", "scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5. For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data. We used exponential decay of ratio INLINEFORM0 and early stopping on the validation when there was no improvement after 1000 steps. Our code is available for download on GitHub . Results and Discussion. In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any", "hidden units, the performance difference was statistically not significant. Fear Dataset. On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 ( INLINEFORM2 ) and 0.108 ( INLINEFORM3 ) over sizes 50 and 200. Sadness Dataset. Finally, on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25. Results were statistically equivalent for size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200.", "as GRUs and a batch normalized version of the LSTM in on preliminary experiments. The hidden size of the attentional component is set to match the size of the augmented hidden vectors on each case. Given this setting, we explored different hyper-parameter configurations, including context window sizes of 1, 3 and 5 as well as RNN hidden state sizes of 100, 200 and 300. We experimented with unidirectional and bidirectional versions of the RNNs. To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of INLINEFORM0 and INLINEFORM1 . We also added a weighed L2 regularization term to our loss function. We experimented with different values for weight INLINEFORM2 , with a minimum value of 0.01 and a maximum of 0.2. To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a", "and of 0.021 for embeddings of size 100. However on ly the first of these values was significant, with a p-value of INLINEFORM0 . Regarding the hidden size of the RNN, we could not find statistical difference across the tested sizes. Dropout also had inconsistent effects, but was generally useful. Joy Dataset. In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25. Regarding the hidden size of the RNN, we observed that 100 hidden units offered better performance in our experiments, with an average absolute gain of 0.052 ( INLINEFORM2 ) over 50 hidden units. Compared to the models with 200 hidden units, the performance difference was statistically not significant. Fear Dataset. On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average", "sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet. Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular", "size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200. Conclusions. In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the Weka baseline for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.", "Introduction. Twitter is a huge micro-blogging service with more than 500 million tweets per day from different locations in the world and in different languages. This large, continuous, and dynamically updated content is considered a valuable resource for researchers. In particular, many of these messages contain emotional charge, conveying affect\u2014emotions, feelings and attitudes, which can be studied to understand the expression of emotion in text, as well as the social phenomena associated. While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount"]}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_answer": "Answer with content missing: table TABREF13 shows that their intra-sentence attention RNN outperformed the Weka baseline on the development dataset.", "predicted_evidence": ["size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200. Conclusions. In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the Weka baseline for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.", "some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount of sadness. While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many for a sentence or passage. However, it is often useful for applications to know the degree to which an emotion is expressed in text. To this end, the WASSA-2017 Shared Task on Emotion Intensity BIBREF0 represents the first task where systems have to automatically determine the intensity of emotions in tweets. Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one. The task is specially challenging since tweets contain informal language, spelling errors and", "the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one. The task is specially challenging since tweets contain informal language, spelling errors and text referring to external content. Given the 140 character limit of tweets, it is also possible to find some phenomena such as the intensive usage of emoticons and of other special Twitter features, such as hashtags and usernames mentions \u2014used to call or notify other users. In this paper we describe our system designed for the WASSA-2017 Shared Task on Emotion Intensity, which we tackle based on the premise of representation learning without the usage of external information, such as lexicons. In particular, we use a Bi-LSTM model with intra-sentence attention on top of word embeddings to generate a tweet representation that is suitable for emotion intensity. Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external", "to generate a tweet representation that is suitable for emotion intensity. Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external information sources. Proposed Approach. Our work is related to deep learning techniques for emotion recognition in images BIBREF1 and videos BIBREF2 , as well as and emotion classification BIBREF3 . Our work is also related to liuattention-based2016, who introduced an attention RNN for slot filling in Natural Language Understanding. Since in the task the input-output alignment is explicit, they investigated how the alignment can be best utilized in encoder-decoder models concluding that the attention mechanisms are helpful. EmoAtt is based on a bidirectional RNN that receives an embedded input sequence INLINEFORM0 and returns a list of hidden vectors that capture the context each input token INLINEFORM1 . To improve the capabilities of the RNN to capture short-term temporal dependencies", "of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features. While the structure of our introduced model allows us to easily include more linguistic features that could potentially improve our predictive power, such as lexicons, since our focus is to study sentence representation for emotion intensity, we do not experiment adding any additional sources of information as input. In this paper we also only report results for LSTMs, which outperformed regular RNNs as well as GRUs and a batch normalized version of the LSTM in on preliminary experiments. The hidden size of the attentional component is set to match the size of the augmented hidden vectors on each case.", "Formally, DISPLAYFORM0  Where INLINEFORM0 is the vector that compresses the input sentence INLINEFORM1 , focusing on the relevant parts to estimate emotion intensity. We input this compressed sentence representation into a feed-forward neural network, INLINEFORM2 , where INLINEFORM3 is the final predicted emotion intensity. As a loss function we use the mini-batch negative Pearson correlation with the gold-standard. Experimental Setup. To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 . We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of", "scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5. For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data. We used exponential decay of ratio INLINEFORM0 and early stopping on the validation when there was no improvement after 1000 steps. Our code is available for download on GitHub . Results and Discussion. In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any", "portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of words missing in GloVe. In this sense, we think the usage of additional features, such as the ones derived from emotion or sentiment lexicons could indeed boost our model capabilities. This is proposed for future work. On the other hand, our model also offers us very interesting insights on how the learning is performed, since we can inspect the attention weights that the neural network is assigning to each specific token when predicting the emotion intensity. By visualizing these weights we can have a clear notion about the parts of the sentence that the model considers are more important. As Figure FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset,"]}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "predicted_answer": "No clear answer provided.", "predicted_evidence": ["immigrants as target groups, (b) individual or generic hate and, (c) aggressive or non-aggressive hate speech. Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To", "normal as labels; BIBREF13 label their data as hateful, offensive (but not hateful), and neither, while BIBREF16 present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. BIBREF15 label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal. On the other hand, BIBREF20 have chosen to detect ideologies of hate speech counting 40 different hate ideologies among 13 extremist hate groups. The detection of hate speech targets is yet another challenging aspect of the annotation. BIBREF21 report the bias that exists in the current datasets towards identity words, such as women, which may later cause false predictions. They propose to debias gender identity word embeddings with additional data for training and tuning their binary classifier. We address this false positive bias problem", "the general opinion, yet it promotes the dehumanization of people who are typically from minority groups BIBREF0, BIBREF1 and can incite hate crime BIBREF2. Moreover, although people of various linguistic backgrounds are exposed to hate speech BIBREF3, BIBREF2, English is still at the center of existing work on toxic language analysis. Recently, some research studies have been conducted on languages such as German BIBREF4, Arabic BIBREF5, and Italian BIBREF6. However, such studies usually use monolingual corpora and do not contrast, or examine the correlations between online hate speech in different languages. On the other hand, tasks involving more than one language such as the hatEval task, which covers English and Spanish, include only separate classification tasks, namely (a) women and immigrants as target groups, (b) individual or generic hate and, (c) aggressive or non-aggressive hate speech. Treating hate speech classification as a binary task may not be enough to inspect the", "Related Work. There is little consensus on the difference between profanity and hate speech and, how to define the latter BIBREF17. As shown in Figure FIGREF11, slurs are not an unequivocal indicator of hate speech and can be part of a non-aggressive conversation, while some of the most offensive comments may come in the form of subtle metaphors or sarcasm BIBREF18. Consequently, there is no existing human annotated vocabulary that explicitly reveals the presence of hate speech, which makes the available hate speech corpora sparse and noisy BIBREF19. Given the subjectivity and the complexity of such data, annotation schemes have rarely been made fine-grained. Table TABREF10 compares different labelsets that exist in the literature. For instance, BIBREF12 use racist, sexist, and normal as labels; BIBREF13 label their data as hateful, offensive (but not hateful), and neither, while BIBREF16 present an English dataset that records the target category based on which hate speech", "which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech. We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task. Since in natural language processing, there is a peculiar interest in multitask learning, where different tasks can be used to help each other BIBREF7, BIBREF8, BIBREF9, we use a unified model to handle the annotated data in all three languages and five tasks. We adopt", "Introduction. With the expanding amount of text data generated on different social media platforms, current filters are insufficient to prevent the spread of hate speech. Most internet users involved in a study conducted by the Pew Research Center report having been subjected to offensive name calling online or witnessed someone being physically threatened or harassed online. Additionally, Amnesty International within Element AI have lately reported that many women politicians and journalists are assaulted every 30 seconds on Twitter. This is despite the Twitter policy condemning the promotion of violence against people on the basis of race, ethnicity, national origin, sexual orientation, gender identity, religious affiliation, age, disability, or serious disease. Hate speech may not represent the general opinion, yet it promotes the dehumanization of people who are typically from minority groups BIBREF0, BIBREF1 and can incite hate crime BIBREF2. Moreover, although people of various", "there are several Arabic dialects spoken in North Africa and the Middle East in use on Twitter. Therefore, we searched for derogatory terms adapted to different circumstances, and acquired an Arabic corpus that combines tweets written in MSA and Arabic dialects. For instance, the tweet shown in Figure FIGREF5 contains a dialectal slur that means \u201cmaiden.\u201d Dataset ::: Annotation Process. We rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech. Given the subjectivity and difficulty of the task, we reminded the annotators not to let their personal opinions about the topics being discussed in the tweets influence their annotation decisions. Our annotation guidelines explained the fact that offensive comments and hate do not necessarily come in the form of profanity. Since different degrees of discrimination work on the dehumanization of individuals or groups of people in distinct ways, we chose not to annotate the tweets", "later cause false predictions. They propose to debias gender identity word embeddings with additional data for training and tuning their binary classifier. We address this false positive bias problem and the common ambiguity of target detection by asking the annotators to label target attributes such as origin, gender, or religious affiliation within 16 named target groups such as refugees, or immigrants. Furthermore, BIBREF22 have reproduced the experiment of BIBREF12 in order to study how hate speech affects the popularity of a tweet, but discovered that some tweets have been deleted. For replication purposes, we provide the community with anonymized tweet texts rather than IDs. Non-English hate speech datasets include Italian, German, Dutch, and Arabic corpora. BIBREF6 present a dataset of Italian tweets, in which the annotations capture the degree of intensity of offensive and aggressive tweets, in addition to whether the tweets are ironic and contain stereotypes or not. BIBREF2"]}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_answer": "English, French, and Arabic", "predicted_evidence": ["learning architectures tackle challenges that include sharing the label space and the question of private and shared space for loosely related tasks BIBREF8, for which techniques may involve a massive space of potential parameter sharing architectures. Dataset. In this section, we present our data collection methodology and annotation process. Dataset ::: Data Collection. Considering the cultural differences and commonly debated topics in the main geographic regions where English, French, and Arabic are spoken, searching for equivalent terms in the three languages led to different results at first. Therefore, after looking for 1,000 tweets per 15 more or less equivalent phrases in the three languages, we revised our search words three times by questioning the results, adding phrases, and taking off unlikely ones in each of the languages. In fact, we started our data collection by searching for common slurs and demeaning expressions such as \u201cgo back to where you come from\u201d. Then, we", "attributes and groups along with other tasks, to set logic rules and develop better multilingual and multitask settings. Conclusion. In this paper, we presented a multilingual hate speech dataset of English, French, and Arabic tweets. We analyzed in details the difficulties related to the collection and annotation of this dataset. We performed multilingual and multitask learning on our corpora and showed that deep learning models perform better than traditional BOW-based models in most of the multilabel classification tasks. Multilingual multitask learning also helped tasks where each label had less annotated data associated with it. Better tuned deep learning settings in our multilingual and multitask models would be expected to outperform the existing state-of-the-art embeddings and algorithms applied to our data. The different annotation labels and comparable corpora would help us perform transfer learning and investigate how multimodal information on the tweets, additional", "Similarly, we use such embeddings to take advantage of the multilinguality and comparability of our corpora during the classification. Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types. Additionally, to the best of our knowledge, this is the first work that examines how annotators react to hate speech comments. To fully exploit the collected annotations, we tested multitask learning on our dataset. Multitask learning BIBREF7 allows neural networks to share parameters with one another and, thus, learn from related tasks. It has been used in different NLP tasks such as parsing BIBREF9, dependency parsing BIBREF26, neural machine translation BIBREF27, sentiment analysis BIBREF28, and other tasks. Multitask learning architectures tackle challenges that include sharing the label space and the question of private and shared space for loosely related tasks BIBREF8, for which techniques may involve a", "be due to the difference in label distributions across languages. Yet, multilingual training of the target group classification task improves in all languages. Since the target group classification task involves 16 labels, the amount of data annotated for each label is lower than in other tasks. Hence, when aggregating annotated data in different languages, the size of the training data also increases, due to the relative regularity of identification words of different groups in all three languages in comparison to other tasks. Experiments ::: Results and Analysis ::: MTML. MTML settings do not lead to a big improvement which may be due to the class imbalance, multilabel tasks, and the difference in the nature of the tasks. In order to inspect which tasks hurt or help one another, we trained multilingual models for pairwise tasks such as (group, target), (hostility, annotator's sentiment), (hostility, target), (hostility, group), (annotator's sentiment, target) and (annotator's", "multitask learning, where different tasks can be used to help each other BIBREF7, BIBREF8, BIBREF9, we use a unified model to handle the annotated data in all three languages and five tasks. We adopt BIBREF8 as a learning algorithm adapted to loosely related tasks such as our five annotated aspects and, use the Babylon cross-lingual embeddings BIBREF10 to align the three languages. We compare the multilingual multitask learning settings with monolingual multitask, multilingual single-task, and monolingual single-task learning settings respectively. Then, we report the performance results of the different settings and discuss how each task affects the remaining ones. We release our dataset and code to the community to extend research work on multilingual hate speech detection and classification. Related Work. There is little consensus on the difference between profanity and hate speech and, how to define the latter BIBREF17. As shown in Figure FIGREF11, slurs are not an unequivocal", "of Italian tweets, in which the annotations capture the degree of intensity of offensive and aggressive tweets, in addition to whether the tweets are ironic and contain stereotypes or not. BIBREF2 have collected more than 500 German tweets against refugees, and annotated them as hateful and not hateful. BIBREF23 detect bullies and victims among youngsters in Dutch comments on AskFM, and classify cyberbullying comments as insults or threats. Moreover, BIBREF5 provide a corpus of Arabic sectarian speech. Another predominant phenomenon in hate speech corpora is code switching. BIBREF24 present a dataset of code mixed Hindi-English tweets, while BIBREF25 report the presence of Hindi tokens in English data and use multilingual word embeddings to deal with this issue when detecting toxicity. Similarly, we use such embeddings to take advantage of the multilinguality and comparability of our corpora during the classification. Our dataset is the first trilingual dataset comprising English,", "models performed poorly due to the size of the tweets. We chose to use Sluice networks BIBREF8 since they are suitable for loosely related tasks such as the annotated aspects of our corpora. We test different models, namely single task single language (STSL), single task multilingual (STML), and multitask multilingual models (MTML) on our dataset. In multilingual settings, we tested Babylon multilingual word embeddings BIBREF10 and MUSE BIBREF30 on the different tasks. We use Babylon embeddings since they appear to outperform MUSE on our data. Sluice networks BIBREF8 learn the weights of the neural networks sharing parameters (sluices) jointly with the rest of the model and share an embedding layer, Babylon embeddings in our case, that associates the elements of an input sequence. We use a standard 1-layer BiLSTM partitioned into two subspaces, a shared subspace and a private one, forced to be orthogonal through a regularization penalty term in the loss function in order to enable the", "target individuals or fall into the \u201cother\u201d category. In the latter case, they may target people with different political views such as liberals or conservatives in English and French, or specific ethnic groups such as Kurdish people in Arabic. English tweets tend to have more tweets targeting people with special needs, due to common language-specific demeaning terms used in conversations where people insult one another. Arabic tweets contain more hateful comments towards women for the same reason. On the other hand, the French corpus contains more tweets that are offensive towards African people, due to hateful comments generated by debates about immigrants. Dataset ::: Final Dataset ::: Sentiment of the annotator. We claim that the choice of a suitable emotion representation model is key to this sub-task, given the subjective nature and social ground of the annotator's sentiment analysis. After collecting the annotation results of the pilot dataset regarding how people feel about"]}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_answer": "(a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments.", "predicted_evidence": ["the general opinion, yet it promotes the dehumanization of people who are typically from minority groups BIBREF0, BIBREF1 and can incite hate crime BIBREF2. Moreover, although people of various linguistic backgrounds are exposed to hate speech BIBREF3, BIBREF2, English is still at the center of existing work on toxic language analysis. Recently, some research studies have been conducted on languages such as German BIBREF4, Arabic BIBREF5, and Italian BIBREF6. However, such studies usually use monolingual corpora and do not contrast, or examine the correlations between online hate speech in different languages. On the other hand, tasks involving more than one language such as the hatEval task, which covers English and Spanish, include only separate classification tasks, namely (a) women and immigrants as target groups, (b) individual or generic hate and, (c) aggressive or non-aggressive hate speech. Treating hate speech classification as a binary task may not be enough to inspect the", "immigrants as target groups, (b) individual or generic hate and, (c) aggressive or non-aggressive hate speech. Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To", "trained multilingual models for pairwise tasks such as (group, target), (hostility, annotator's sentiment), (hostility, target), (hostility, group), (annotator's sentiment, target) and (annotator's sentiment, group). We noticed that when trained jointly, the target attribute slightly improves the performance of the tweet's hostility type classification by 0.03,0.05 and 0.01 better than the best reported scores in English, French, and Arabic, respectively. When target groups and attributes are trained jointly, the macro F-score of the target group classification in Arabic improves by 0.25 and when we train the tweet's hostility type within the annotator's sentiment, we improve the macro F-score of Arabic by 0.02. We believe that we can take advantage of the correlations between target attributes and groups along with other tasks, to set logic rules and develop better multilingual and multitask settings. Conclusion. In this paper, we presented a multilingual hate speech dataset of", "Introduction. In the age of social media, offensive content online has become prevalent in recent years. There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types BIBREF0. BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in", "Introduction. Hate speech represents written or oral communication that in any way discredits a person or a group based on characteristics such as race, color, ethnicity, gender, sexual orientation, nationality, or religion BIBREF0. Hate speech targets disadvantaged social groups and harms them both directly and indirectly BIBREF1. Social networks like Twitter and Facebook, where hate speech frequently occurs, receive many critics for not doing enough to deal with it. As the connection between hate speech and the actual hate crimes is high BIBREF2, the importance of detecting and managing hate speech is not questionable. Early identification of users who promote such kind of communication can prevent an escalation from speech to action. However, automatic hate speech detection is difficult, especially when the text does not contain explicit hate speech keywords. Lexical detection methods tend to have low precision because, during classification, they do not take into account the", "which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech. We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task. Since in natural language processing, there is a peculiar interest in multitask learning, where different tasks can be used to help each other BIBREF7, BIBREF8, BIBREF9, we use a unified model to handle the annotated data in all three languages and five tasks. We adopt", "structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 . Other supervised approaches to hate speech classification have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which they are really identifying hate speech BIBREF5 , BIBREF8 . Neural language models show promise in the task but existing work has used training data has a similarly broad definition of hate speech BIBREF9 . Non-linguistic features like the gender or ethnicity of the author can help improve hate speech classification but this information is often unavailable or unreliable on social media BIBREF8 . Data. We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user,", "for hate speech identification with multiple classifiers BIBREF12. A logistic regression classifier trained with paragraph2vec word representations of comments from Yahoo Finance BIBREF13. The latest approaches in detecting hate speech include a dataset of Twitter posts, labelled as hateful, offensive or clean, used to train a logistic regression classifier with part-of-speech and word n-grams and a sentiment lexicon BIBREF0 and a linear SVM trained on character 4-grams, with an extra RBF SVM meta-classifier that boosts accuracy in hateful language detection BIBREF14. Both attempts tried to distinguish offensive language and hate speech, with the hate class being the hardest to classify.  Related Work ::: Non-English Datasets. Research on other languages includes datasets such as: A Dutch corpus of posts from the social networking site Ask.fm for the detection of cyberbullying BIBREF15, a German Twitter corpus exploring the issue of hate speech targeted to refugees BIBREF16, another"]}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_answer": "Empowerment of MT systems", "predicted_evidence": ["NMT has been achieving impressive results and is now the state-of-the-art in MT approaches. However, NMT is still a statistical approach sharing some semantic drawbacks from other well-defined SMT approaches BIBREF5 . One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words. According to our survey BIBREF6 , the obvious opportunity of using SWT for MT has already been studied by a number of approaches, especially", "web page which contains some idioms, this property may be used to gather appropriate translations of idioms from English to Russian using a given RDF KB. Therefore, an MT system can be adapted to a user by using specific data about him in RDF along with given KBs. Recently, Moussallem et al BIBREF16 have released a multilingual linked idioms dataset as a first part of supporting the investigation of this suggestion. The dataset contains idioms in 5 languages and are represented by knowledge graphs which facilitates the retrieval and inference of translations among the idioms. Translating KBs. According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to", "infer the right meaning of ambiguous sentences or words. According to our survey BIBREF6 , the obvious opportunity of using SWT for MT has already been studied by a number of approaches, especially w.r.t. the issue of ambiguity. In this paper, we present the challenges and opportunities in the use of SWT in MT for translating texts. Related Works. The idea of using a structured KB in MT systems started in the 90s with the work of Knight and Luk BIBREF7 . Still, only a few researchers have designed different strategies for benefiting of structured knowledge in MT architectures BIBREF8 . Recently, the idea of using KG into MT systems has gained renewed attention. Du et al. BIBREF9 created an approach to address the problem of OOV words by using BabelNet BIBREF10 . Their approach applies different methods of using BabelNet. In summary, they create additional training data and apply a post-editing technique, which replaces the OOV words while querying BabelNet. Shi et al. BIBREF11 have", "approaches BIBREF0 . Suggestions and Possible Directions using SW. Based on the surveyed works on our research BIBREF6 , SWT have mostly been applied at the semantic analysis step, rather than at the other stages of the translation process, due to their ability to deal with concepts behind the words and provide knowledge about them. As SWT have developed, they have increasingly been able to resolve some of the open challenges of MT. They may be applied in different ways according to each MT approach. Disambiguation. Human language is very ambiguous. Most words have multiple interpretations depending on the context in which they are mentioned. In the MT field, WSD techniques are concerned with finding the respective meaning and correct translation to these ambiguous words in target languages. This ambiguity problem was identified early in MT development. In 1960 Bar-Hillel BIBREF1 stated that an MT system is not able to find the right meaning without a specific knowledge. Although the", "the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the output translation in the target language as a post-editing technique. Although applying one of these techniques has increased the quality of a translation, both techniques are tedious to implement when they have to translate common words instead of named entities, then be applied several times to achieve a successful translation. The real benefit of SW comes from its capacity to provide unseen knowledge about emergent data, which appears every day. Therefore, we suggest performing the topic-modelling technique over the source text to provide a necessary context before translation. Instead of applying the topic-modeling over the entire text, we would follow the principle of communication (i.e from 3 to 5 sentences for describing an idea and define a context for each piece of text. Thus,", "Introduction. Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 . However, translation is a difficult task due to the complexity of natural languages and their structure BIBREF0 . In addition, manual translation does not scale to the magnitude of the Web. One remedy for this problem is MT. The main goal of MT is to enable people to assess content in languages other than the languages in which they are fluent BIBREF1 . From a formal point of view, this means that the goal of MT is to transfer the semantics of text from an input language to an output language BIBREF2 . Although MT systems are", "methods of using BabelNet. In summary, they create additional training data and apply a post-editing technique, which replaces the OOV words while querying BabelNet. Shi et al. BIBREF11 have recently built a semantic embedding model reliant upon a specific KB to be used in NMT systems. The model relies on semantic embeddings to encode the key information contained in words to translate the meaning of sentences correctly. The work consists of mapping a source sentence to triples, which are then used to extract the intrinsic meaning of words to generate a target sentence. This mapping results in a semantic embedding model containing KB triples, which are responsible for gathering the key information of each word in the sentences. Open MT Challenges. The most problematic unresolved MT challenges, from our point of view, which are still experienced by the aforementioned MT approaches are the following: Additionally, there are five MT open challenges posed by Lopez and Post BIBREF12 which", "contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms. Therefore, we suggest investigating an MT approach mainly based on SWT using NN for translating KBs. Once the KBs are translated,"]}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_answer": "lexical and syntactic ambiguity, disambiguation, and disambiguation process", "predicted_evidence": ["methods of using BabelNet. In summary, they create additional training data and apply a post-editing technique, which replaces the OOV words while querying BabelNet. Shi et al. BIBREF11 have recently built a semantic embedding model reliant upon a specific KB to be used in NMT systems. The model relies on semantic embeddings to encode the key information contained in words to translate the meaning of sentences correctly. The work consists of mapping a source sentence to triples, which are then used to extract the intrinsic meaning of words to generate a target sentence. This mapping results in a semantic embedding model containing KB triples, which are responsible for gathering the key information of each word in the sentences. Open MT Challenges. The most problematic unresolved MT challenges, from our point of view, which are still experienced by the aforementioned MT approaches are the following: Additionally, there are five MT open challenges posed by Lopez and Post BIBREF12 which", "from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech. The challenges above are clearly not independent, which means that addressing one of them can have an impact on the others. Since NMT has shown impressive results on reordering, the main problem turns out to be the disambiguation process (both syntactically and semantically) in SMT approaches BIBREF0 . Suggestions and Possible Directions using SW. Based on the surveyed works on our research BIBREF6 , SWT have mostly been applied at the semantic analysis step, rather than at the", "are fluent BIBREF1 . From a formal point of view, this means that the goal of MT is to transfer the semantics of text from an input language to an output language BIBREF2 . Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popovi\u0107 BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for", "approaches BIBREF0 . Suggestions and Possible Directions using SW. Based on the surveyed works on our research BIBREF6 , SWT have mostly been applied at the semantic analysis step, rather than at the other stages of the translation process, due to their ability to deal with concepts behind the words and provide knowledge about them. As SWT have developed, they have increasingly been able to resolve some of the open challenges of MT. They may be applied in different ways according to each MT approach. Disambiguation. Human language is very ambiguous. Most words have multiple interpretations depending on the context in which they are mentioned. In the MT field, WSD techniques are concerned with finding the respective meaning and correct translation to these ambiguous words in target languages. This ambiguity problem was identified early in MT development. In 1960 Bar-Hillel BIBREF1 stated that an MT system is not able to find the right meaning without a specific knowledge. Although the", "Introduction. Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 . However, translation is a difficult task due to the complexity of natural languages and their structure BIBREF0 . In addition, manual translation does not scale to the magnitude of the Web. One remedy for this problem is MT. The main goal of MT is to enable people to assess content in languages other than the languages in which they are fluent BIBREF1 . From a formal point of view, this means that the goal of MT is to transfer the semantics of text from an input language to an output language BIBREF2 . Although MT systems are", "This ambiguity problem was identified early in MT development. In 1960 Bar-Hillel BIBREF1 stated that an MT system is not able to find the right meaning without a specific knowledge. Although the ambiguity problem has been lessened significantly since the contribution of Carpuat and subsequent works BIBREF13 , this problem still remains a challenge. As seen in Moussallem et al. BIBREF6 , MT systems still try to resolve this problem by using domain specific language models to prefer domain specific expressions, but when translating a highly ambiguous sentence or a short text which covers multiple domains, the languages models are not enough. SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the", "NMT has been achieving impressive results and is now the state-of-the-art in MT approaches. However, NMT is still a statistical approach sharing some semantic drawbacks from other well-defined SMT approaches BIBREF5 . One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words. According to our survey BIBREF6 , the obvious opportunity of using SWT for MT has already been studied by a number of approaches, especially", "web page which contains some idioms, this property may be used to gather appropriate translations of idioms from English to Russian using a given RDF KB. Therefore, an MT system can be adapted to a user by using specific data about him in RDF along with given KBs. Recently, Moussallem et al BIBREF16 have released a multilingual linked idioms dataset as a first part of supporting the investigation of this suggestion. The dataset contains idioms in 5 languages and are represented by knowledge graphs which facilitates the retrieval and inference of translations among the idioms. Translating KBs. According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to"]}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_answer": "Excessive focus on English and European languages, limitations of SMT approaches for translating across domains, difficult translations among morphologically rich languages, non-standard language problem from social networks, syntactic disambiguation problem.", "predicted_evidence": ["This ambiguity problem was identified early in MT development. In 1960 Bar-Hillel BIBREF1 stated that an MT system is not able to find the right meaning without a specific knowledge. Although the ambiguity problem has been lessened significantly since the contribution of Carpuat and subsequent works BIBREF13 , this problem still remains a challenge. As seen in Moussallem et al. BIBREF6 , MT systems still try to resolve this problem by using domain specific language models to prefer domain specific expressions, but when translating a highly ambiguous sentence or a short text which covers multiple domains, the languages models are not enough. SW has already shown its capability for semantic disambiguation of polysemous and homonymous words. However, SWT were applied in two ways to support the semantic disambiguation in MT. First, the ambiguous words were recognized in the source text before carrying out the translation, applying a pre-editing technique. Second, SWT were applied to the", "et al. BIBREF15 have shown promising results by applying a multilingual entity linking algorithm along with knowledge graph embeddings into the translation phase of a neural machine translation model for improving the translation of entities in texts. Their approach achieved significant and consistent improvements of +3 BLEU, METEOR and CHRF3 on average on the newstest datasets between 2014 and 2018 for WMT English-German translation task. Non-standard speech. The non-standard language problem is a rather important one in the MT field. Many people use the colloquial form to speak and write to each other on social networks. Thus, when MT systems are applied on this context, the input text frequently contains slang, MWE, and unreasonable abbreviations such as \u201cIdr = I don't remember.\u201d and \u201ccya = see you\u201d. Additionally, idioms contribute to this problem, decreasing the translation quality. Idioms often have an entirely different meaning than their separated word meanings. Consequently,", "are fluent BIBREF1 . From a formal point of view, this means that the goal of MT is to transfer the semantics of text from an input language to an output language BIBREF2 . Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popovi\u0107 BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for", "Named Entities are a common and difficult problem in both MT (see Koehn BIBREF0 ) and SW fields. The SW achieved important advances in NERD using structured data and semantic annotations, e.g., by adding an rdf:type statement which identifies whether a certain kiwi is a fruit BIBREF14 . In MT systems, however, this problem is directly related to the ambiguity problem and therefore has to be resolved in that wider context. Although MT systems include good recognition methods, they still need improvement. When an MT system does not recognize an entity, the translation output often has poor quality, immediately deteriorating the target text readability. Therefore, we suggest recognizing such entities before the translation process and first linking them to a reference knowledge base. Afterwards, the type of entities would be agglutinated along with their labels and their translations from a reference knowledge base. For instance, in NMT, the idea is to include in the training set for the", "Introduction. Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 . However, translation is a difficult task due to the complexity of natural languages and their structure BIBREF0 . In addition, manual translation does not scale to the magnitude of the Web. One remedy for this problem is MT. The main goal of MT is to enable people to assess content in languages other than the languages in which they are fluent BIBREF1 . From a formal point of view, this means that the goal of MT is to transfer the semantics of text from an input language to an output language BIBREF2 . Although MT systems are", "from our point of view, which are still experienced by the aforementioned MT approaches are the following: Additionally, there are five MT open challenges posed by Lopez and Post BIBREF12 which we describe more generically below. (1) Excessive focus on English and European languages as one of the involved languages in MT approaches and poor research on low-resource language pairs such as African and/or South American languages. (2) The limitations of SMT approaches for translating across domains. Most MT systems exhibit good performance on law and the legislative domains due to the large amount of data provided by the European Union. In contrast, translations performed on sports and life-hacks commonly fail, because of the lack of training data. (3) How to translate the huge amount of data from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same", "use of two or more SW resources simultaneously has not yet been investigated. On the other hand, there is also a syntactic disambiguation problem which as yet lacks good solutions. For instance, the English language contains irregular verbs like \u201cset\u201d or \u201cput\u201d. Depending on the structure of a sentence, it is not possible to recognize their verbal tense, e.g., present or past tense. Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue. For", "from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech. The challenges above are clearly not independent, which means that addressing one of them can have an impact on the others. Since NMT has shown impressive results on reordering, the main problem turns out to be the disambiguation process (both syntactically and semantically) in SMT approaches BIBREF0 . Suggestions and Possible Directions using SW. Based on the surveyed works on our research BIBREF6 , SWT have mostly been applied at the semantic analysis step, rather than at the"]}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_answer": "root mean square, zero crossing rate, moving window average, kurtosis, power spectral entropy", "predicted_evidence": ["signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel. We used spectral entropy because it captures the spectral ( frequency domain) and signal complexity information of EEG. It is also a widely used feature in EEG signal analysis BIBREF18 . Similarly zero crossing rate was chosen as it is a commonly used feature both for speech recognition and bio signal analysis. Remaining features were chosen to capture time domain statistical information. We performed lot of experiments to identify this set of features. Initially we used only spectral entropy and zero crossing rate but we noticed that the", "which might be another reason for giving higher WER. Conclusion and Future work. In this paper we demonstrated the feasibility of using EEG features, concatenation of EEG and acoustic features for performing noisy continuous speech recognition. To our best knowledge this is the first time a continuous noisy speech recognition is demonstrated using only EEG features. For both attention and CTC model we observed that as the vocabulary size increase, concatenating acoustic features with EEG features will help in reducing the test time error rates. We further plan to publish our speech EEG data base used in this work to help advancement of research in this area. For future work, we plan to build a much larger speech EEG data base and also perform experiments with data collected from subjects with speaking disabilities. We will also investigate whether it is possible to improve the attention model results by tuning hyper parameters to improve the model's ability to condition on the", "as explained in the previous section, we used non linear methods to do feature dimension reduction in order to obtain set of EEG features which are better representation of acoustic features. We reduced the 155 EEG features to a dimension of 30 by applying Kernel Principle Component Analysis (KPCA) BIBREF19 .We plotted cumulative explained variance versus number of components to identify the right feature dimension as shown in Figure 2. We used KPCA with polynomial kernel of degree 3 BIBREF0 . We further computed delta, delta and delta of those 30 EEG features, thus the final feature dimension of EEG was 90 (30 times 3) for both the data sets. When we used the EEG features for ASR without dimension reduction, the ASR performance went down by 40 %. The non linear dimension reduction of EEG features significantly improved the performance of ASR. Results. The attention model was predicting a word and CTC model was predicting a character at every time step, hence we used word error rate", "by the human auditory cortex BIBREF1 , BIBREF2 we used very noisy speech data for this work and demonstrated lower word error rate (WER) for smaller corpus using EEG features, concatenation of EEG features and acoustic features. In BIBREF3 authors decode imagined speech from EEG using synthetic EEG data and connectionist temporal classification (CTC) network but in our work we use real EEG data, use EEG data recorded along with acoustics. In BIBREF4 authors perform envisioned speech recognition using random forest classifier but in our case we use end to end state of art models and perform recognition for noisy speech. In BIBREF5 authors demonstrate speech recognition using electrocorticography (ECoG) signals, which are invasive in nature but in our work we use non invasive EEG signals. This work is mainly motivated by the results explained in BIBREF0 , BIBREF6 , BIBREF7 , BIBREF3 . In BIBREF6 the authors used classification approach for identifying phonological categories in imagined", "in Figure 1. We used EEGLab BIBREF17 to obtain the EEG sensor location mapping. It is based on standard 10-20 EEG sensor placement method for 32 electrodes. For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively. For data set B, we used data from first 6 subjects for training the model, remaining two subjects data for validation and test set respectively. EEG and Speech feature extraction details. EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square,", "speaking disabilities and disorders. The research work presented in this paper tries to address this issue by investigating speech recognition using only EEG signals with no acoustic input and also by combining EEG features along with traditional acoustic features to perform recognition. We believe the former will help with speech restoration for people who can not speak at all and the latter will help people who are having speaking disabilities like broken or discontinued speech etc to use voice activated technologies with better user experience there by helping in improving technology accessibility. ASR performance is degraded in presence of noisy speech and in real life situations most of the speech is noisy. Inspired from the unique robustness to environmental artifacts exhibited by the human auditory cortex BIBREF1 , BIBREF2 we used very noisy speech data for this work and demonstrated lower word error rate (WER) for smaller corpus using EEG features, concatenation of EEG", "on the most noisy data set B. We observed that as vocabulary size increase, error rates were slightly lower than the error rates from Table iv@ where we used EEG features from all 31 sensors with dimension reduction. Table iii@ shows the results for attention model when trained with EEG features from sensors T7 and T8 only on data set B. We observed that error rates were higher in this case compared to the error rates reported in table ii@. Figures 4 shows the visualization of the attention weights when the attention model was trained and tested using only EEG features for Data set B. The plots shows the EEG feature importance ( attention) distribution across time steps for predicting first sentence and it indicates that attention model was not able to attend properly to EEG features, which might be another reason for giving higher WER. Conclusion and Future work. In this paper we demonstrated the feasibility of using EEG features, concatenation of EEG and acoustic features for", "Introduction. Electroencephalography (EEG) is a non invasive way of measuring electrical activity of human brain. In BIBREF0 we demonstrated deep learning based automatic speech recognition (ASR) using EEG signals for a limited English vocabulary of four words and five vowels. In this paper we extend our work for a much larger English vocabulary and we use state-of-art end-to-end continuous speech recognition models to perform recognition. In our prior work we predicted isolated words and vowels. ASR systems forms the front end or back end in many cutting edge voice activated technologies like Amazon Alexa, Apple Siri, Windows Cortana, Samsung Bixby etc. Unfortunately these systems are trained to recognize text only from acoustic features. This limits technology accessibility to people with speaking disabilities and disorders. The research work presented in this paper tries to address this issue by investigating speech recognition using only EEG signals with no acoustic input and also"]}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_answer": "BiDAF BIBREF10", "predicted_evidence": ["For data set A, the 10 subjects were asked to speak the first 30 sentences from the USC-TIMIT database BIBREF16 and their simultaneous speech and EEG signals were recorded. This data was recorded in presence of background noise of 40 dB (noise generated by room air conditioner fan). We then asked each subject to repeat the same experiment two more times, thus we had 30 speech EEG recording examples for each sentence. For data set B, the 8 subjects were asked to repeat the same previous experiment but this time we used background music played from our lab computer to generate a background noise of 65 dB. Here we had 24 speech EEG recording examples for each sentence. We used Brain Vision EEG recording hardware. Our EEG cap had 32 wet EEG electrodes including one electrode as ground as shown in Figure 1. We used EEGLab BIBREF17 to obtain the EEG sensor location mapping. It is based on standard 10-20 EEG sensor placement method for 32 electrodes. For data set A, we used data from first 8", "hidden state of the decoder GRU at time step INLINEFORM7 . The way of computing value for INLINEFORM0 depends on the type of attention used. In this work, we used bahdanau's additive style attention BIBREF13 , which defines INLINEFORM1 as INLINEFORM2 ) where INLINEFORM3 and INLINEFORM4 are learnable parameters during training of the model. Design of Experiments for building the database. We built two types of simultaneous speech EEG recording databases for this work. For database A five female and five male subjects took part in the experiment. For database B five male and three female subjects took part in the experiment. Except two subjects, rest all were native English speakers for both the databases. All subjects were UT Austin undergraduate,graduate students in their early twenties. For data set A, the 10 subjects were asked to speak the first 30 sentences from the USC-TIMIT database BIBREF16 and their simultaneous speech and EEG signals were recorded. This data was recorded in", "each time step INLINEFORM11 be INLINEFORM12 and let INLINEFORM13 value of INLINEFORM14 be denoted by INLINEFORM15 . The probability that model outputs INLINEFORM16 on input INLINEFORM17 is given by INLINEFORM18 . During the training phase, we would like to maximize the conditional probability INLINEFORM19 , and thereby define the loss function as INLINEFORM20 . In case when the length of INLINEFORM0 is less than INLINEFORM1 , we extend the target vector INLINEFORM2 by repeating a few of its values and by introducing blank token ( INLINEFORM3 ) to create a target vector of length INLINEFORM4 . Let the possible extensions of INLINEFORM5 be denoted by INLINEFORM6 . For example, when INLINEFORM7 and INLINEFORM8 , the possible extensions are INLINEFORM9 , INLINEFORM10 , INLINEFORM11 , INLINEFORM12 , INLINEFORM13 , INLINEFORM14 and INLINEFORM15 . We then define INLINEFORM16 as INLINEFORM17 . In our work we used character based CTC ASR model. CTC assumes the conditional independence", "signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel. We used spectral entropy because it captures the spectral ( frequency domain) and signal complexity information of EEG. It is also a widely used feature in EEG signal analysis BIBREF18 . Similarly zero crossing rate was chosen as it is a commonly used feature both for speech recognition and bio signal analysis. Remaining features were chosen to capture time domain statistical information. We performed lot of experiments to identify this set of features. Initially we used only spectral entropy and zero crossing rate but we noticed that the", "any training example, label pair ( INLINEFORM0 , INLINEFORM1 ). Let the number of times steps of encoder GRU for that example be INLINEFORM2 . The GRU encoder will transform the input features ( INLINEFORM3 ) into hidden output feature vectors ( INLINEFORM4 ). Let INLINEFORM5 word label in INLINEFORM6 (sentence) be INLINEFORM7 , then to predict INLINEFORM8 at decoder time step INLINEFORM9 , context vector INLINEFORM10 is computed and fed into the decoder GRU. INLINEFORM11 is computed as INLINEFORM12 , where INLINEFORM13 is the attention weight vector satisfying the property INLINEFORM14 .  INLINEFORM0 can be intuitively seen as a measure of how much attention INLINEFORM1 must pay to INLINEFORM2 , INLINEFORM3 . INLINEFORM4 is mathematically defined as INLINEFORM5 , where INLINEFORM6 is hidden state of the decoder GRU at time step INLINEFORM7 . The way of computing value for INLINEFORM0 depends on the type of attention used. In this work, we used bahdanau's additive style attention", "with speaking disabilities. We will also investigate whether it is possible to improve the attention model results by tuning hyper parameters to improve the model's ability to condition on the input,improve CTC model results by training with more number of examples and by using external language model during inference time. Acknowledgement. We would like to thank Kerry Loader and Rezwanul Kabir from Dell, Austin, TX for donating us the GPU to train the models used in this work.", "with adam optimizer BIBREF11 and during inference time we used CTC beam search decoder. We now explain the loss function used in our CTC model. Consider training data set INLINEFORM0 with training examples INLINEFORM1 and the corresponding label set INLINEFORM2 with target vectors INLINEFORM3 . Consider any training example, label pair ( INLINEFORM4 , INLINEFORM5 ). Let the number of time steps of the RNN encoder for ( INLINEFORM6 , INLINEFORM7 ) is INLINEFORM8 . In case of character based CTC model, the RNN predicts a character at every time step. Whereas in word based CTC model, the RNN predicts a word at every time step. For the sake of simplicity, let us assume that length of target vector INLINEFORM9 is equal to INLINEFORM10 . Let the probability vector output by the RNN at each time step INLINEFORM11 be INLINEFORM12 and let INLINEFORM13 value of INLINEFORM14 be denoted by INLINEFORM15 . The probability that model outputs INLINEFORM16 on input INLINEFORM17 is given by", "in Figure 1. We used EEGLab BIBREF17 to obtain the EEG sensor location mapping. It is based on standard 10-20 EEG sensor placement method for 32 electrodes. For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively. For data set B, we used data from first 6 subjects for training the model, remaining two subjects data for validation and test set respectively. EEG and Speech feature extraction details. EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square,"]}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": "Not provided in the documents.", "predicted_evidence": ["speaking disabilities and disorders. The research work presented in this paper tries to address this issue by investigating speech recognition using only EEG signals with no acoustic input and also by combining EEG features along with traditional acoustic features to perform recognition. We believe the former will help with speech restoration for people who can not speak at all and the latter will help people who are having speaking disabilities like broken or discontinued speech etc to use voice activated technologies with better user experience there by helping in improving technology accessibility. ASR performance is degraded in presence of noisy speech and in real life situations most of the speech is noisy. Inspired from the unique robustness to environmental artifacts exhibited by the human auditory cortex BIBREF1 , BIBREF2 we used very noisy speech data for this work and demonstrated lower word error rate (WER) for smaller corpus using EEG features, concatenation of EEG", "work is mainly motivated by the results explained in BIBREF0 , BIBREF6 , BIBREF7 , BIBREF3 . In BIBREF6 the authors used classification approach for identifying phonological categories in imagined and silent speech but in our work we used continuous speech recognition state of art models and our models were predicting words, characters at each time step. Similarly in BIBREF7 neural network based classification approach was used for predicting phonemes. Major contribution of this paper is the demonstration of end to end continuous noisy speech recognition using only EEG features and this paper further validates the concepts introduced in BIBREF0 for a much larger English corpus. Automatic Speech Recognition System Models. An end-to-end ASR model maps input feature vectors to an output sequence of vectors of posterior probabilities of tokens without using separate acoustic model, pronunciation model and language model. In this work we implemented two different types of state of art end", "by the human auditory cortex BIBREF1 , BIBREF2 we used very noisy speech data for this work and demonstrated lower word error rate (WER) for smaller corpus using EEG features, concatenation of EEG features and acoustic features. In BIBREF3 authors decode imagined speech from EEG using synthetic EEG data and connectionist temporal classification (CTC) network but in our work we use real EEG data, use EEG data recorded along with acoustics. In BIBREF4 authors perform envisioned speech recognition using random forest classifier but in our case we use end to end state of art models and perform recognition for noisy speech. In BIBREF5 authors demonstrate speech recognition using electrocorticography (ECoG) signals, which are invasive in nature but in our work we use non invasive EEG signals. This work is mainly motivated by the results explained in BIBREF0 , BIBREF6 , BIBREF7 , BIBREF3 . In BIBREF6 the authors used classification approach for identifying phonological categories in imagined", "Introduction. Electroencephalography (EEG) is a non invasive way of measuring electrical activity of human brain. In BIBREF0 we demonstrated deep learning based automatic speech recognition (ASR) using EEG signals for a limited English vocabulary of four words and five vowels. In this paper we extend our work for a much larger English vocabulary and we use state-of-art end-to-end continuous speech recognition models to perform recognition. In our prior work we predicted isolated words and vowels. ASR systems forms the front end or back end in many cutting edge voice activated technologies like Amazon Alexa, Apple Siri, Windows Cortana, Samsung Bixby etc. Unfortunately these systems are trained to recognize text only from acoustic features. This limits technology accessibility to people with speaking disabilities and disorders. The research work presented in this paper tries to address this issue by investigating speech recognition using only EEG signals with no acoustic input and also", "which might be another reason for giving higher WER. Conclusion and Future work. In this paper we demonstrated the feasibility of using EEG features, concatenation of EEG and acoustic features for performing noisy continuous speech recognition. To our best knowledge this is the first time a continuous noisy speech recognition is demonstrated using only EEG features. For both attention and CTC model we observed that as the vocabulary size increase, concatenating acoustic features with EEG features will help in reducing the test time error rates. We further plan to publish our speech EEG data base used in this work to help advancement of research in this area. For future work, we plan to build a much larger speech EEG data base and also perform experiments with data collected from subjects with speaking disabilities. We will also investigate whether it is possible to improve the attention model results by tuning hyper parameters to improve the model's ability to condition on the", "of vectors of posterior probabilities of tokens without using separate acoustic model, pronunciation model and language model. In this work we implemented two different types of state of art end to end ASR models used for the task of continuous speech recognition and the input feature vectors can be EEG features or concatenation of acoustic and EEG features. We used Google's tensorflow and keras deep learning libraries for building our ASR models. Connectionist Temporal Classification (CTC). The main ideas behind CTC based ASR were first introduced in the following papers BIBREF8 , BIBREF9 . In our work we used a single layer gated recurrent unit (GRU) BIBREF10 with 128 hidden units as encoder for the CTC network. The decoder consists of a combination of a dense layer ( fully connected layer) and a softmax activation. Output at every time step of the GRU layer is fed into the decoder network. The number of time steps of the GRU encoder is equal to product of the sampling frequency of", "and a softmax activation. Output at every time step of the GRU layer is fed into the decoder network. The number of time steps of the GRU encoder is equal to product of the sampling frequency of the input features and the length of the input sequence. Since different speakers have different rate of speech, we used dynamic recurrent neural network (RNN) cell. There is no fixed value for time steps of the encoder. Usually the number of time steps of the encoder (T) is greater than the length of output tokens for a continuous speech recognition problem. A RNN based CTC network tries to make length of output tokens equal to T by allowing the repetition of output prediction unit tokens and by introducing a special token called blank token BIBREF8 across all the frames. We used CTC loss function with adam optimizer BIBREF11 and during inference time we used CTC beam search decoder. We now explain the loss function used in our CTC model. Consider training data set INLINEFORM0 with training", "capture time domain statistical information. We performed lot of experiments to identify this set of features. Initially we used only spectral entropy and zero crossing rate but we noticed that the performance of the ASR system significantly went up by 20 % when we added the remaining additional features. The recorded speech signal was sampled at 16KHz frequency. We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. The MFCC features were also sampled at 100Hz same as the sampling frequency of EEG features to avoid seq2seq problem. EEG Feature Dimension Reduction Algorithm Details. After extracting EEG and acoustic features as explained in the previous section, we used non linear methods to do feature dimension reduction in order to obtain set of EEG features which are better representation of acoustic features. We"]}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_answer": "Yes", "predicted_evidence": ["of a teacher. For example, DistilBERT BIBREF2 is re-trained on the same corpus as pre-training a vanilla BERT from scratch; and TinyBERT BIBREF3 utilizes expensive data augmentation to fit the distillation target. The costs of these model compression methods are as large as pre-training and unaffordable for low-resource settings. Therefore, it is straight-forward to ask, can we design a lightweight method to generate adaptive models with comparable accuracy using significantly less time and resource consumption? In this paper, we propose LadaBERT (Lightweight adaptation of BERT through hybrid model compression) to tackle the raised questions. Specifically, LadaBERT is based on an iterative hybrid model compression framework consisting of weighting pruning, matrix factorization and knowledge distillation. Initially, the architecture and weights of student model are inherited from the BERT teacher. In each iteration, the student model is first compressed by a small ratio based on weight", "In this paper, we propose LadaBERT to address this problem. LadaBERT is a lightweight model compression pipeline that generates adaptive BERT model efficiently based on a given task and specific constraint. It is based on a hybrid solution, which conducts matrix factorization, weight pruning and knowledge distillation in a reinforce manner. The experimental results verify that EAdaBERT is able to achieve comparable performance with other state-of-the-art solutions using much less training data and time budget. Therefore, LadaBERT can be easily plugged into various applications with competitive performances and little training overheads. In the future, we would like to apply LadaBERT to large-scale industrial applications, such as search relevance and query recommendation.", "model compression for shipping its performance gain into latency-critical or low-resource scenarios. Most existing works focus on knowledge distillation. For instance, BERT-PKD BIBREF33 is a patient knowledge distillation approach that compresses the original BERT model into a lightweight shallow network. Different from traditional knowledge distillation methods, BERT-PKD enables an exploitation of rich information in the teacher's hidden layers by utilizing a layer-wise distillation constraint. DistillBERT BIBREF2 pre-trains a smaller general-purpose language model on the same corpus as vanilla BERT. Distilled BiLSTM BIBREF34 adopts a single-layer BiLSTM as the student model and achieves comparable results with ELMo BIBREF35 through much fewer parameters and less inference time. TinyBERT BIBREF3 reports the best-ever performance on BERT model compression, which exploits a novel attention-based distillation schema that encourages the linguistic knowledge in teacher to be well", "distillation. Initially, the architecture and weights of student model are inherited from the BERT teacher. In each iteration, the student model is first compressed by a small ratio based on weight pruning and matrix factorization, and is then fine-tuned under the guidance of teacher model through knowledge distillation. Because weight pruning and matrix factorization help to generate better initial and intermediate status in the knowledge distillation iterations, the accuracy and efficiency of model compression can be greatly improved. We conduct extensive experiments on five public datasets of natural language understanding. As an example, the performance comparison of LadaBERT and state-of-the-art models on MNLI-m dataset is illustrated in Figure FIGREF1. We can see that LadaBERT outperforms other BERT-oriented model compression baselines at various model compression ratios. Especially, LadaBERT-1 outperforms BERT-PKD significantly under $2.5\\times $ compression ratio, and", "3 of 12 layers in the pre-trained BERT-Base model. It turns out that the discarded layers have huge impact on the model performance, which is hard to be recovered by knowledge distillation. On the other hand, LadaBERT generates the student model by iterative pruning on the pre-trained teacher. In this way, the original knowledge in the teacher model can be preserved to the largest extent, and the benefit of which is complementary to knowledge distillation. LadaBERT-3 has a comparable size as TinyBERT with a $7.5 \\times $ compression ratio. As shown in the results, TinyBERT does not work well without expensive data augmentation and general distillation, hindering its application to low-resource settings. The reason is that the student model of TinyBERT is distilled from scratch, so it requires much more data to mimic the teacher's behaviors. Instead, LadaBERT has better initial and intermediate status calculated by hybrid model compression, which is much more light-weighted and", "it requires much more data to mimic the teacher's behaviors. Instead, LadaBERT has better initial and intermediate status calculated by hybrid model compression, which is much more light-weighted and achieves competitive performances with much faster learning speed (learning curve comparison is shown in Section SECREF41). Moreover, LadaBERT-3 also outperforms BERT-SMALL on most of the datasets, which is pre-trained from scratch by the official BERT pipeline on a $7.5 \\times $ smaller architecture. This indicates that LadaBERT can quickly adapt to a smaller model size and achieve competitive performance without expansive re-training on a large corpus. Moreover, Distilled-BiLSTM performs well on SST-2 dataset with more than $10 \\times $ compression ratio, perhaps owing to its advantage of generalization on small datasets. Nevertheless, the performance of LadaBERT-4 is competitive on larger datasets such as MNLI and QQP. This is impressive as LadaBERT is much more efficient without", "TABREF38. Experiments ::: Performance Comparison. The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table TABREF40, where the models are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance of LadaBERT demonstrates the superiority of hybrid combination of SVD-based matrix factorization, weight pruning and knowledge distillation. With model size of $2.5\\times $ reduction, LadaBERT-1 performs significantly better than BERT-PKD, boosting the performance by relative 8.9, 8.1, 6.1, 3.8 and 5.8 percentages on MNLI-m, MNLI-mm, SST-2, QQP and QNLI datasets respectively. Recall that BERT-PKD initializes the student model by selecting 3 of 12 layers in the pre-trained BERT-Base model. It turns out that the discarded layers have huge impact on the model performance, which is hard to be recovered by knowledge distillation. On the", "{X}$ is the training dataset. $f^{(s)}({x})$ and $f^{(t)}({x})$ represent intermediate outputs or weight matrices for the student model and teacher model correspondingly. $L(\\cdot )$ represents for a loss function which can be carefully defined for different types of knowledge distillation. We follow the recent technique proposed by TinyBERT BIBREF3, which applies knowledge distillation constraints upon embedding, self-attention, hidden representation and prediction levels. Concretely, there are four types of knowledge distillation constraints as follows: Embedding-layer distillation is performed upon the embedding layer. $f({x}) \\in \\mathbb {R}^{n \\times d}$ represents for the word embedding output for input $x$, where $n$ is the input word length and $d$ is the dimension of word embedding. Mean Squared Error (MSE) is adopted as the loss function $L(\\cdot )$. Attention-layer distillation is performed upon the self-attention sub-layer. $f({x}) = \\lbrace a_{ij}\\rbrace \\in \\mathbb"]}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_answer": "Yes", "predicted_evidence": ["compression is denoted by: where ${M_A}$ and ${M_B}$ are binary masks derived by the weight pruning algorithm with compression ratio $P_{weight}$. The compression ratio of this hybrid approach can be calculated by: In LadaBERT, the hybrid compression produce is applied to each layer of the pre-trained BERT model. Given an overall model compression target $P$, the following constraint should be satisfied: where $|\\theta |$ is the total number of model parameters and $P$ is the target compression ratio; $|\\theta _{embd}|$ denotes the parameter number of embedding layer, which has a relative compression ratio of $P_embd$, and $|\\theta _{encd}|$ denotes the number of parameters of all layers in BERT encoder, which have a compression ratio of $P_{hybrid}$. The classification layer (often MLP layer with Softmax activation) has a small parameter size ($|\\theta _{cls}|$), so it is not modified in the model compression procedure. In the experiments, these fine-grained compression ratios can be", "but the memory consumption and computational cost expand greatly with the growing complexity of models. Therefore, model compression has become an indispensable technique for practice, especially in low-resource settings. In this section, we review the current progresses of model compression techniques briefly, which can be divided into four categories, namely weight pruning, matrix factorization, weight quantization and knowledge distillation. We also present hybrid approaches and the applications of model compression to pre-trained BERT models. Related Work ::: Weight pruning. Numerous researches have shown that removing a large portion of connections or neurons does not cause significant performance drop in deep neural network models BIBREF4, BIBREF5, BIBREF6, BIBREF7. For example, Han et al. BIBREF4 proposed a method to reduce the storage and computation of neural networks by removing unimportant connections, resulting in sparse networks without affecting the model accuracy. Li et", "weight pruning on the decomposed matrices. First, we apply SVD-based matrix factorization to reduce 60% of total parameters. Then, weight pruning is applied on the decomposed matrices by 50%, resulting in only 20% parameters while the error distribution changes slightly. As a result, it has smaller mean and deviation than pure matrix factorization. In addition, a smoother distribution is more appropriate for the knowledge distillation procedure to fine-tune the weights, so it is advantageous than pure weight pruning. Conclusion. Model compression is a common way to deal with latency-critical or memory-intensive scenarios. Existing model compression methods for BERT need to be re-trained on a large corpus to reserve its original performance, which is inapplicable in low-resource settings. In this paper, we propose LadaBERT to address this problem. LadaBERT is a lightweight model compression pipeline that generates adaptive BERT model efficiently based on a given task and specific", "outperforms other BERT-oriented model compression baselines at various model compression ratios. Especially, LadaBERT-1 outperforms BERT-PKD significantly under $2.5\\times $ compression ratio, and LadaBERT-3 outperforms TinyBERT under $7.5\\times $ compression ratio while the training speed is accelerated by an order of magnitude. The rest of this paper is organized as follows. First, we summarizes the related works of model compression and their applications to BERT in Section SECREF2. Then, the methodology of LadaBERT is introduced in Section SECREF3, and experimental results are presented in Section SECREF4. At last, we conclude this work and discuss future works in Section SECREF5. Related Work. Deep Neural Networks (DNNs) have achieved great success in many areas in recent years, but the memory consumption and computational cost expand greatly with the growing complexity of models. Therefore, model compression has become an indispensable technique for practice, especially in", "which enabled extra parallelism for training large-scale data. In addition, knowledge distillation is also useful for aggregating model ensembles into a single model by treating the ensemble model as a teacher. Related Work ::: Hybrid approach. To improve the performance of model compression, there are many attempts to conduct hybrid model compression method that combines more than one category of algorithms. Han et al. BIBREF27 combined quantization, hamming coding and weight pruning to conduct model compression on image classification tasks. Yu et al. BIBREF28 proposed a unified framework for low-rank and sparse decomposition of weight matrices with feature map reconstructions. Polino et al. BIBREF29 advocated a combination of distillation and quantization techniques and proposed two hybrid models, i.e., quantified distillation and differentiable quantization to address this problem. Li et al., BIBREF30 compressed DNN-based acoustic model through knowledge distillation and pruning.", "two matrices are initialized by SVD and will be further tuned during training. Given a rank $r \\le min(m, n)$, the compression ratio of matrix factorization is defined as: Therefore, for a target model compression ratio $P_{svd}$, the desired rank $r$ can be calculated by: Lightweight Adaptation of BERT ::: Overview ::: Weight pruning. Weight pruning BIBREF4 is an unstructured compression method that induces desirable sparsity for a neural network model. For a neural network $f({x; \\theta })$ with parameters $\\theta $, weight pruning finds a binary mask ${M} \\in \\lbrace 0, 1\\rbrace ^{|\\theta |}$ subject to a given sparsity ratio, $P_{weight}$. The neural network after pruning will be $f({x; M \\cdot \\theta })$, where the non-zero parameter size is $||{M}||_1 = P_{weight}\\cdot |\\theta |$, where $|\\theta |$ is the number of parameters in $\\theta $. For example, when $P_m = 0.3$, there are 70% zeros and 30% ones in the mask ${m}$. We adopt a simple pruning strategy in our implementation:", "distillation. Initially, the architecture and weights of student model are inherited from the BERT teacher. In each iteration, the student model is first compressed by a small ratio based on weight pruning and matrix factorization, and is then fine-tuned under the guidance of teacher model through knowledge distillation. Because weight pruning and matrix factorization help to generate better initial and intermediate status in the knowledge distillation iterations, the accuracy and efficiency of model compression can be greatly improved. We conduct extensive experiments on five public datasets of natural language understanding. As an example, the performance comparison of LadaBERT and state-of-the-art models on MNLI-m dataset is illustrated in Figure FIGREF1. We can see that LadaBERT outperforms other BERT-oriented model compression baselines at various model compression ratios. Especially, LadaBERT-1 outperforms BERT-PKD significantly under $2.5\\times $ compression ratio, and", "the pre-trained checkpoint of base-bert-uncased as the initial model for compression, which contains 12 layers, 12 heads, 110M parameters, and 768 hidden units per layer. Hyper-parameter selection is conducted on the validation data for each dataset. After training, the prediction results are submitted to the GLUE-benchmark evaluation platform to get the evaluation performance on test data. For a comprehensive evaluation, we experiment with four settings of LadaBERT, namely LadaBERT-1, -2, -3 and -4, which reduce the model parameters of BERT-Base by 2.5, 5, 7.5 and 10 times respectively. In our experiment, we take the batch size as 32, learning rate as 2e-5. The optimizer is BertAdam with default setting. Fine-grained compression ratios are optimized by random search and shown in Table TABREF38. Experiments ::: Performance Comparison. The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table TABREF40, where the models are ranked by parameter sizes for"]}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_answer": "Unanswerable.", "predicted_evidence": ["at the sentence level using a single-layer convolutional neural network (CNN) with a max-over-time pooling operation BIBREF16 , BIBREF17 , BIBREF18 . Next, we build representations for documents using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the acquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below. We opted for a convolutional neural network model for representing sentences for two reasons. Firstly, single-layer CNNs can be trained effectively (without any long-term dependencies in the model) and secondly, they have been successfully used for sentence-level classification tasks such as sentiment analysis BIBREF19 . Let $d$ denote the dimension of word embeddings, and $s$ a document sentence consisting of a sequence of $n$ words $(w_1,", "dataset using Rouge. nn-se represents our neural sentence extraction model, nn-we our word extraction model, and nn-abs the neural abstractive baseline. The table also includes results for the lead baseline, the logistic regression classifier (lreg), and three previously published systems (ilp, tgraph, and urank). The nn-se outperforms the lead and lreg baselines with a significant margin, while performing slightly better than the ilp model. This is an encouraging result since our model has only access to embedding features obtained from raw text. In comparison, lreg uses a set of manually selected features, while the ilp system takes advantage of syntactic information and extracts summaries subject to well-engineered linguistic constraints, which are not available to our models. Overall, our sentence extraction model achieves performance comparable to the state of the art without sophisticated constraint optimization (ilp, tgraph) or sentence ranking mechanisms (urank). We visualize", "Acknowledgments. We would like to thank three anonymous reviewers and members of the ILCC at the School of Informatics for their valuable feedback. The support of the European Research Council under award number 681760 \u201cTranslating Multiple Modalities into Text\u201d is gratefully acknowledged. Appendix. In addition to the DUC 2002 and 500 DailyMail samples, we additionally report results on the entire DailyMail test set (Table 3 ). Since there is no established evaluation standard for this task, we experimented with three different ROUGE limits: 75 bytes, 275 bytes and full length.", "representations from which the decoder generates the target sequence. An attention mechanism BIBREF11 is often used to locate the region of focus during decoding. We develop a general framework for single-document summarization which can be used to extract sentences or words. Our model includes a neural network-based hierarchical document reader or encoder and an attention-based content extractor. The role of the reader is to derive the meaning representation of a document based on its sentences and their constituent words. Our models adopt a variant of neural attention to extract sentences or words. Contrary to previous work where attention is an intermediate step used to blend hidden units of an encoder to a vector propagating additional information to the decoder, our model applies attention directly to select sentences or words of the input document as the output summary. Similar neural attention architectures have been previously used for geometry reasoning BIBREF12 , under the", "reflects the intuition that documents are generated compositionally from words, sentences, paragraphs, or even larger units. We therefore employ a representation framework which reflects the same architecture, with global information being discovered and local information being preserved. Such a representation yields minimum information loss and is flexible allowing us to apply neural attention for selecting salient sentences and words within a larger context. In the following, we first describe the document reader, and then present the details of our sentence and word extractors. Document Reader. The role of the reader is to derive the meaning representation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-over-time pooling operation BIBREF16 , BIBREF17 , BIBREF18 . Next, we build representations for documents", "This is not shown in the equation.}$$   (Eq. 25)  $$a_j^t = \\mathbf {z}^\\mathtt {T} \\tanh (\\mathbf {W}_e \\mathbf {\\bar{h}}_t + \\mathbf {W}_r \\mathbf {h}_j), h_j \\in D$$   (Eq. 26)  In the above equations, $\\mathbf {w}_i$ corresponds to the vector of the $i$ -th word in the input document, whereas $\\mathbf {z}$ , $\\mathbf {W}_e$ , $\\mathbf {W}_r$ , $\\mathbf {v}$ , $\\mathbf {W}_{e^{\\prime }}$ , and $\\mathbf {W}_{r^{\\prime }}$ are model weights. The model architecture is shown in Figure 3 . The word extractor can be viewed as a conditional language model with a vocabulary constraint. In practice, it is not powerful enough to enforce grammaticality due to the lexical diversity and sparsity of the document highlights. A possible enhancement would be to pair the extractor with a neural language model, which can be pre-trained on a large amount of unlabeled documents and then jointly tuned with the extractor during decoding BIBREF23 . A simpler alternative which we adopt is to use $n$ -gram", "| d )$ . Word Extractor. Compared to sentence extraction which is a purely sequence labeling task, word extraction is closer to a generation task where relevant content must be selected and then rendered fluently and grammatically. A small extension to the structure of the sequential labeling model makes it suitable for generation: instead of predicting a label for the next sentence at each time step, the model directly outputs the next word in the summary. The model uses a hierarchical attention architecture: at time step $t$ , the decoder softly attends each document sentence and subsequently attends each word in the document and computes the probability of the next word to be included in the summary $p(w^{\\prime }_t = w_i|\nd, w^{\\prime }_1, \\cdots , w^{\\prime }_{t-1})$ with a softmax classifier:  $$\\bar{\\mathbf {h}}_{t} = \\text{LSTM} ( \\mathbf {w^{\\prime }}_{t-1},\n\\mathbf {\\bar{h}}_{t-1})\\footnote {We empirically found that feeding", "and extractive summarization which exhibits none. We formulate word extraction as a language generation task with an output vocabulary restricted to the original document. In our supervised setting, the training goal is to maximize the likelihood of the generated sentences, which can be further decomposed by enforcing conditional dependencies among their constituent words:  $$\\hspace*{-5.69046pt}\\log p(\\mathbf {y}_s |D;"]}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "predicted_answer": "Yes", "predicted_evidence": ["Introduction. The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document. Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length BIBREF0 , the words in the title, the presence of proper nouns, content features such as word frequency BIBREF1 , and event features such as action nouns BIBREF2 . Sentences are typically assigned a score indicating the strength of presence of these features. Several methods have been used in order to select the summary sentences ranging from binary classifiers BIBREF3 , to hidden Markov models", "Variance (ANOVA) which revealed a reliable effect of system type. Specifically, post-hoc Tukey tests showed that nn-se and ilp are significantly ( $p < 0.01$ ) better than lead, nn-we, and nn-abs but do not differ significantly from each other or the human goldstandard. Conclusions. In this work we presented a data-driven summarization framework based on an encoder-extractor architecture. We developed two classes of models based on sentence and word extraction. Our models can be trained on large scale datasets and learn informativeness features based on continuous representations without recourse to linguistic annotations. Two important ideas behind our work are the creation of hierarchical neural structures that reflect the nature of the summarization task and generation by extraction. The later effectively enables us to sidestep the difficulties of generating under a large vocabulary, essentially covering the entire dataset, with many low-frequency words and named entities.", "given the input document $D$ and model parameters $\\theta $ :  $$\\log p(\\mathbf {y}_L |D; \\theta ) = \\sum \\limits _{i=1}^{m} \\log p(y_L^i |D; \\theta )$$   (Eq. 5)  Although extractive methods yield naturally grammatical summaries and require relatively little linguistic analysis, the selected sentences make for long summaries containing much redundant information. For this reason, we also develop a model based on word extraction which seeks to find a subset of words in $D$ and their optimal ordering so as to form a summary $\\mathbf {y}_s = (w^{\\prime }_1, \\cdots , w^{\\prime }_k), w^{\\prime }_i \\in D$ . Compared to sentence extraction which is a sequence labeling problem, this task occupies the middle ground between full abstractive summarization which can exhibit a wide range of rewrite operations and extractive summarization which exhibits none. We formulate word extraction as a language generation task with an output vocabulary restricted to the original document. In our supervised", "that nn-we consistently outperforms the purely abstractive model. As nn-we generates summaries by picking words from the original document, decoding is easier for this model compared to nn-abs which deals with an open vocabulary. The extraction-based generation approach is more robust for proper nouns and rare words, which pose a serious problem to open vocabulary models. An example of the generated summaries for nn-we is shown at the lower half of Figure 4 . Table 1 (lower half) shows system results on the 500 DailyMail news articles (test set). In general, we observe similar trends to DUC 2002, with nn-se performing the best in terms of all rouge metrics. Note that scores here are generally lower compared to DUC 2002. This is due to the fact that the gold standard summaries (aka highlights) tend to be more laconic and as a result involve a substantial amount of paraphrasing. More experimental results on this dataset are provided in the appendix. The results of our human evaluation", "score indicating the strength of presence of these features. Several methods have been used in order to select the summary sentences ranging from binary classifiers BIBREF3 , to hidden Markov models BIBREF4 , graph-based algorithms BIBREF5 , BIBREF6 , and integer linear programming BIBREF7 . In this work we propose a data-driven approach to summarization based on neural networks and continuous sentence features. There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation BIBREF8 , question answering BIBREF9 , and sentence compression BIBREF10 . Central to these approaches is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism BIBREF11 is often used to locate the region of focus during decoding. We develop a general", "tend to be more laconic and as a result involve a substantial amount of paraphrasing. More experimental results on this dataset are provided in the appendix. The results of our human evaluation study are shown in Table 2 . Specifically, we show, proportionally, how often our participants ranked each system 1st, 2nd, and so on. Perhaps unsurprisingly, the human-written descriptions were considered best and ranked 1st 27% of the time, however closely followed by our nn-se model which was ranked 1st 22% of the time. The ilp system was mostly ranked in 2nd place (38% of the time). The rest of the systems occupied lower ranks. We further converted the ranks to ratings on a scale of 1 to 6 (assigning ratings 6 $\\dots $ 1 to rank placements 1 $\\dots $ 6). This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable effect of system type. Specifically, post-hoc Tukey tests showed that nn-se and ilp are significantly ( $p < 0.01$ ) better than lead, nn-we, and nn-abs but", "our sentence extraction model achieves performance comparable to the state of the art without sophisticated constraint optimization (ilp, tgraph) or sentence ranking mechanisms (urank). We visualize the sentence weights of the nn-se model in the top half of Figure 4 . As can be seen, the model is able to locate text portions which contribute most to the overall meaning of the document. Rouge scores for the word extraction model are less promising. This is somewhat expected given that Rouge is $n$ -gram based and not very well suited to measuring summaries which contain a significant amount of paraphrasing and may deviate from the reference even though they express similar meaning. However, a meaningful comparison can be carried out between nn-we and nn-abs which are similar in spirit. We observe that nn-we consistently outperforms the purely abstractive model. As nn-we generates summaries by picking words from the original document, decoding is easier for this model compared to nn-abs", "and organizes them into sentences. A few recent studies BIBREF14 , BIBREF15 perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task. rush2015neural propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article. In contrast, our model summarizes documents rather than individual sentences, producing multi-sentential discourse. A major architectural difference is that our decoder selects output symbols from the document of interest rather than the entire vocabulary. This effectively helps us sidestep the difficulty of searching for the next output symbol under a large vocabulary, with low-frequency words and named entities whose representations can be"]}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_answer": "Hierarchical and Is-A relations.", "predicted_evidence": ["Introduction. A core problem in artificial intelligence is to capture, in machine-usable form, the collection of information that an ordinary person would have, known as commonsense knowledge. For example, a machine should know that a room may have a door, and that when a person enters a room, it is generally through a door. This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines. More than just curating a static collection of facts, we would like commonsense knowledge to be represented in a way that lends itself to machine reasoning and inference of missing information. We concern ourselves in this paper with the problem of learning commonsense knowledge representations. In machine learning settings, knowledge is usually represented as a hypergraph of triplets such as Freebase BIBREF1 , WordNet BIBREF2 , and ConceptNet BIBREF3 . In these", "previous models. We focus on the order-embedding model BIBREF0 which was proposed for general hierarchical prediction including multimodal problems such as image captioning. While the original work included results on ontology prediction on WordNet, we focus exclusively on the model's application to commonsense knowledge, with its unique characteristics including complex ordering structure, compositional, multi-word entities, and the wealth of commonsense knowledge to be found in large-scale unstructured text data. We propose two extensions to the order embedding model. The first augments hierarchical supervision from existing ontologies with non-hierarchical knowledge in the form of raw text. We find incorporating unstructured text brings accuracy from 92.0 to 93.0 on a commonsense dataset containing Is-A relations from ConceptNet and Microsoft Concept Graph (MCG), with larger relative gains from smaller amounts of labeled data. The second extension uses the complex partial-order", "Is-A and entailment, there is no mechanism to ensure that its predictions are internally consistent. For example, if we know that a dog is a mammal, and a pit bull is a dog, we would like the model to also predict that a pit bull is a mammal. These transitive entailment relations describe ontologies of hierarchical data, a key component of commonsense knowledge which we focus on in this work. Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models. We focus on the order-embedding model BIBREF0 which was proposed for general hierarchical prediction including multimodal problems such as image captioning. While the original work", "knowledge representations. In machine learning settings, knowledge is usually represented as a hypergraph of triplets such as Freebase BIBREF1 , WordNet BIBREF2 , and ConceptNet BIBREF3 . In these knowledge graphs, nodes represent entities or terms $t$ , and hyperedges are relations $R$ between these entities or terms, with each fact in the knowledge graph represented as a triplet $<t_1, R, t_2>$ . Researchers have developed many models for knowledge representation and learning in this setting BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , under the umbrella of knowledge graph completion. However, none of these naturally lend themselves to traditional methods of logical reasoning such as transitivity and negation. While a knowledge graph completion model can represent relations such as Is-A and entailment, there is no mechanism to ensure that its predictions are internally consistent. For example, if we know that a dog is a mammal, and a pit bull is a dog, we would like the model", "consistent prediction of ontologies. Data. In this work, we use the ConceptNet BIBREF3 , WordNet BIBREF2 , and Microsoft Concept Graph (MCG) BIBREF11 , BIBREF12 knowledge bases for our ontology prediction experiments. WordNet is a knowledge base (KB) of single words and relations between them such as hypernymy and meronymy. For our task, we use the hypernym relations only. ConceptNet is a KB of triples consisting of a left term $t_1$ , a relation $R$ , and a right term $t_2$ . The relations come from a fixed set of size 34. But unlike WordNet, terms in ConceptNet can be phrases. We focus on the Is-A relation in this work. MCG also consists of hierarchical relations between multi-word phrases, ranging from extremely general to specific. Examples from each dataset are shown in Table 1 . For experiments involving unstructured text, we use the WaCkypedia corpus BIBREF13 . Models. We introduce two variants of order embeddings. The first incorporates non-hierarchical unstructured text data", "L_{\\text{Order}} = \\sum _{x,y}\\max (0, m+d(x,y)-d(x^{\\prime }, y^{\\prime }))\n$  Joint Text and Order Embedding. We aim to augment our ontology prediction embedding model with more general commonsense knowledge mined from raw text. A standard method for learning word representations is word2vec BIBREF14 , which predicts current word embeddings using a context of surrounding word embeddings. We incorporate a modification of the CBOW model in this work, which uses the average embedding from a window around the current word as a context vector $v_2$ to predict the current word vector $v_1$ : $\nv_2 = \\frac{1}{window}\\sum _{k \\in \\lbrace -window/2,...,window/2\\rbrace \\setminus \\lbrace t\\rbrace }v_{t+k}", "can add many additional training examples to our data by enforcing that the vector join and meet operations satisfy the joins and meets found in the training lattice/DAG. If $w_c$ and $w_p$ are the nearest common child and parent for a pair $w_1, w_2$ , the loss for join and meet learning can be written as the following: $", "containing Is-A relations from ConceptNet and Microsoft Concept Graph (MCG), with larger relative gains from smaller amounts of labeled data. The second extension uses the complex partial-order structure of real-world ontologies to find long-distance triplet constraints among embeddings which are poorly enforced by the standard pairwise training method. By adding our additional triplet constraints to the baseline order-embedding model, we find performance improves from 90.6 to 91.3 accuracy on the WordNet ontology dataset. We find that order embeddings' ease of extension, both by incorporating non-ordered data, and additional training constraints derived from the structure of the problem, makes it a promising avenue for the development of further algorithms for automatic learning and jointly consistent prediction of ontologies. Data. In this work, we use the ConceptNet BIBREF3 , WordNet BIBREF2 , and Microsoft Concept Graph (MCG) BIBREF11 , BIBREF12 knowledge bases for our ontology"]}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_answer": "Globally consistent structured prediction", "predicted_evidence": ["Is-A and entailment, there is no mechanism to ensure that its predictions are internally consistent. For example, if we know that a dog is a mammal, and a pit bull is a dog, we would like the model to also predict that a pit bull is a mammal. These transitive entailment relations describe ontologies of hierarchical data, a key component of commonsense knowledge which we focus on in this work. Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models. We focus on the order-embedding model BIBREF0 which was proposed for general hierarchical prediction including multimodal problems such as image captioning. While the original work", "Order embeddings map words to a partially-ordered space, which we can think of as a directed acyclic graph (DAG). A simple way to add more training examples is to take the transitive closure of this graph. For example, if we have $<$ dog IsA mammal $>$ , $<$ mammal IsA animal $>$ , we can produce the training example $<$ dog IsA animal $>$ . We observe that even more training examples can be created by treating our partial-order structure as a lattice. A lattice is a partial order equipped with two additional operations, join and meet. The join and meet of a pair P are respectively the supremum (least upper bound) of P, denoted $\\vee $ , and the infimum (greatest lower bound), denoted $\\wedge $ . In our case, the vector join and meet would be the pointwise max and min of two embeddings. We can add many additional training examples to our data by enforcing that the vector join and meet operations satisfy the joins and meets found in the training lattice/DAG. If $w_c$ and $w_p$ are the", "x \\preceq y \\text{ if and only if } \\bigwedge _{i=1}^{N}x_{i}\\ge y_i\n$  where $x$ is the subcategory and $y$ is the supercategory. This means the general concept embedding should be smaller than the specific concept embedding in every coordinate of the embeddings. An illustration of this geometry can be found in Figure 1. We can define a surrogate energy for this ordering function as $d(x, y) = \\left\\Vert  \\max (0,y-x) \\right\\Vert ^2$ . The learning objective for order embeddings becomes the following, where $m$ is a margin parameter, $x$ and $y$ are the hierarchically supervised pairs, and $x^{\\prime }$ and $y^{\\prime }$ are negatively sampled concepts: $\nL_{\\text{Order}} = \\sum _{x,y}\\max (0, m+d(x,y)-d(x^{\\prime }, y^{\\prime }))", "Introduction. A core problem in artificial intelligence is to capture, in machine-usable form, the collection of information that an ordinary person would have, known as commonsense knowledge. For example, a machine should know that a room may have a door, and that when a person enters a room, it is generally through a door. This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines. More than just curating a static collection of facts, we would like commonsense knowledge to be represented in a way that lends itself to machine reasoning and inference of missing information. We concern ourselves in this paper with the problem of learning commonsense knowledge representations. In machine learning settings, knowledge is usually represented as a hypergraph of triplets such as Freebase BIBREF1 , WordNet BIBREF2 , and ConceptNet BIBREF3 . In these", "knowledge representations. In machine learning settings, knowledge is usually represented as a hypergraph of triplets such as Freebase BIBREF1 , WordNet BIBREF2 , and ConceptNet BIBREF3 . In these knowledge graphs, nodes represent entities or terms $t$ , and hyperedges are relations $R$ between these entities or terms, with each fact in the knowledge graph represented as a triplet $<t_1, R, t_2>$ . Researchers have developed many models for knowledge representation and learning in this setting BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , under the umbrella of knowledge graph completion. However, none of these naturally lend themselves to traditional methods of logical reasoning such as transitivity and negation. While a knowledge graph completion model can represent relations such as Is-A and entailment, there is no mechanism to ensure that its predictions are internally consistent. For example, if we know that a dog is a mammal, and a pit bull is a dog, we would like the model", "$  We perform two sets of experiments on the combined ConceptNet and MCG Is-A relations, using different amounts of training and testing data. The first data set, called Data1, uses 119,159 training examples, 1,089 dev examples, and 1,089 test examples. The second dataset, Data2, evenly splits the data in 47,662 examples for each set. Our baselines for this model are a standard order embedding model, and a bilinear classifier BIBREF6 trained to predict Is-A, both with and without additional unstructured text augmenting the model in the same way as the joint order embedding model. We see in Table 2 that while adding extra text data helps all models, the best performance is consistently achieved by a combination of order embeddings and unstructured text. Long-Range Join and Meet Constraints. Order embeddings map words to a partially-ordered space, which we can think of as a directed acyclic graph (DAG). A simple way to add more training examples is to take the transitive closure of this", "can add many additional training examples to our data by enforcing that the vector join and meet operations satisfy the joins and meets found in the training lattice/DAG. If $w_c$ and $w_p$ are the nearest common child and parent for a pair $w_1, w_2$ , the loss for join and meet learning can be written as the following: $", "containing Is-A relations from ConceptNet and Microsoft Concept Graph (MCG), with larger relative gains from smaller amounts of labeled data. The second extension uses the complex partial-order structure of real-world ontologies to find long-distance triplet constraints among embeddings which are poorly enforced by the standard pairwise training method. By adding our additional triplet constraints to the baseline order-embedding model, we find performance improves from 90.6 to 91.3 accuracy on the WordNet ontology dataset. We find that order embeddings' ease of extension, both by incorporating non-ordered data, and additional training constraints derived from the structure of the problem, makes it a promising avenue for the development of further algorithms for automatic learning and jointly consistent prediction of ontologies. Data. In this work, we use the ConceptNet BIBREF3 , WordNet BIBREF2 , and Microsoft Concept Graph (MCG) BIBREF11 , BIBREF12 knowledge bases for our ontology"]}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_answer": "word unigrams and bigrams", "predicted_evidence": ["of simpler but more robust models and the utilization of transfer learning or data augmentation techniques. The Sentiment, Emotion, and Cognitive State (SEC) evaluation task was a recent addition to the LORELEI program introduced in 2019, which aims to leverage sentiment information from the incoming documents. This in turn may be used in identifying severity of the crisis in different geographic locations for efficient distribution of the available resources. In this work, we describe our systems for targeted sentiment detection for the SEC task. Our systems are designed to identify authored expressions of sentiment and emotion towards a HADR crisis. To this end, our models are based on a combination of state-of-the-art sentiment classifiers and simple rule-based systems. We evaluate our systems as part of the NIST LoREHLT 2019 SEC pilot task. Previous Work. Social media has received a lot of attention as a way to understand what people communicate during disasters BIBREF16 ,", "of these organizations recognize the value of the information found online\u2014specially during the on-set of a crisis\u2014they are in need of automatic tools that locate actionable and tactical information BIBREF7 , BIBREF0 . Opinion mining and sentiment analysis techniques offer a viable way of addressing these needs, with complementary insights to what keyword searches or topic and event extraction might offer BIBREF8 . Studies have shown that sentiment analysis of social media during crises can be useful to support response coordination BIBREF9 or provide information about which audiences might be affected by emerging risk events BIBREF10 . For example, identifying tweets labeled as \u201cfear\u201d might support responders on assessing mental health effects among the affected population BIBREF11 . Given the critical and global nature of the HADR events, tools must process information quickly, from a variety of sources and languages, making it easily accessible to first responders and decision", "it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information. Our results show that state-of-the-art sentiment classifiers can be leveraged out-of-the-box for a reasonable performance on English data. By identifying possible differences coming from the information sources, as well as by exploiting the information communicated as the situation unfolds, we showed significant performance gains on both English and Spanish.", "our systems as part of the NIST LoREHLT 2019 SEC pilot task. Previous Work. Social media has received a lot of attention as a way to understand what people communicate during disasters BIBREF16 , BIBREF11 . These communications typically center around collective sense-making BIBREF17 , supportive actions BIBREF18 , BIBREF19 , and social sharing of emotions and empathetic concerns for affected individuals BIBREF20 . To organize and make sense of the sentiment information found in social media, particularly those messages sent during the disaster, several works propose the use of machine learning models (e.g., Support Vector Machines, Naive Bayes, and Neural Networks) trained on a multitude of linguistic features. These features include bag of words, part-of-speech tags, n-grams, and word embeddings; as well as previously validated sentiment lexica such as Linguistic Inquiry and Word Count (LIWC) BIBREF22 , AFINN BIBREF23 , and SentiWordNet BIBREF24 . Most of the work is centered around", "segment. To identify sentiment targeted towards an entity, we use the recently released Target-Based Sentiment Analysis (TBSA) model from BIBREF45 . In TBSA, two stacked LSTM cells are trained to predict both sentiment and target boundary tags (e.g., predicting S-POS to indicate the start of the target towards which the author is expressing positive sentiment, I-POS and E-POS to indicate intermediate and end of the target). In our submission, since input text documents can be arbitrarily long, we only consider sentences which include a known and relevant entity; these segments are then fed to the TBSA model to predict targeted sentiment. If the target predicted by this model matched with any of the known entities, the system would output the polarity and the target. In this model we limit our focus on the task of correctly identifying those segments with sentiment towards a SF. That is, given a pair of SF and segment, we train models to identify if this segment contains any sentiment", "the Status of the situation (e.g., entities involved in resolution, time and urgency). An example of a SF can be found in table 1 . A list of situation frames and documents serve as input for our sentiment analysis systems. Data. Training data provided for the task included documents were collected from social media, SMS, news articles, and news wires. This consisted of 76 documents in English and 47 in Spanish. The data are relevant to the HADR domain but are not grounded in a common HADR incident. Each document is annotated for situation frames and associated sentiment by 2 trained annotators from the Linguistic Data Consortium (LDC). Sentiment annotations were done at a segment (sentence) level, and included Situation Frame, Polarity (positive / negative), Sentiment Score, Emotion, Source and Target. Sentiment labels were annotated between the values of -3 (very negative) and +3 (very positive) with 0.5 increments excluding 0. Additionally, the presence or absence of three specific", "$  Method. We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation. Machine Translation. Automatic translations from Spanish to English were obtained from Microsoft Bing using their publicly available API. For the pilot evaluation, we translated all of the Spanish documents into English, and included them as additional training data. At this time we do not translate English to Spanish, but plan to explore this thread in future work. Linguistic Features. We extract word unigrams and bigrams. These features were then transformed using term frequencies (TF) and Inverse document-frequency (IDF). Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between", "BIBREF27 . However, certain aspects of these communication channels are still apt for sentiment analysis, such as column pieces BIBREF28 or political news BIBREF27 , BIBREF29 . In the context of leveraging the information found online for HADR emergencies, approaches for languages other than English have been limited. Most of which are done by manually constructing resources for a particular language (e.g., in tweets BIBREF30 , BIBREF31 , BIBREF32 and in disaster-related news coverage BIBREF33 ), or by applying cross-language text categorization to build language-specific models BIBREF31 , BIBREF34 . In this work, we develop systems that identify positive and negative sentiments expressed in social media posts, news articles and blogs in the context of a humanitarian emergency. Our systems work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the"]}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_answer": "At least two (English and Spanish)", "predicted_evidence": ["$  Method. We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation. Machine Translation. Automatic translations from Spanish to English were obtained from Microsoft Bing using their publicly available API. For the pilot evaluation, we translated all of the Spanish documents into English, and included them as additional training data. At this time we do not translate English to Spanish, but plan to explore this thread in future work. Linguistic Features. We extract word unigrams and bigrams. These features were then transformed using term frequencies (TF) and Inverse document-frequency (IDF). Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between", "Given the critical and global nature of the HADR events, tools must process information quickly, from a variety of sources and languages, making it easily accessible to first responders and decision makers for damage assessment and to launch relief efforts accordingly BIBREF12 , BIBREF13 . However, research efforts in these tasks are primarily focused on high resource languages such as English, even though such crises may happen anywhere in the world. The LORELEI program provides a framework for developing and testing systems for real-time humanitarian crises response in the context of low-resource languages. The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational", "BIBREF27 . However, certain aspects of these communication channels are still apt for sentiment analysis, such as column pieces BIBREF28 or political news BIBREF27 , BIBREF29 . In the context of leveraging the information found online for HADR emergencies, approaches for languages other than English have been limited. Most of which are done by manually constructing resources for a particular language (e.g., in tweets BIBREF30 , BIBREF31 , BIBREF32 and in disaster-related news coverage BIBREF33 ), or by applying cross-language text categorization to build language-specific models BIBREF31 , BIBREF34 . In this work, we develop systems that identify positive and negative sentiments expressed in social media posts, news articles and blogs in the context of a humanitarian emergency. Our systems work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the", "For the classifiers, we explored the use of Support Vector Machines and Random Forests. Model performance was estimated through 10-fold cross validation on the train set. Hyper-parameters, such as of regularization, were selected based on the performance on grid-search using an 10-fold inner-cross validation loop. After choosing the parameters, models were re-trained on all the available data. We consider some of the most popular baseline models in the literature: (i) minority class baseline (due to the heavily imbalanced dataset), (ii) Support Vector Machines trained on TF-IDF bi-gram language model, (iii) and Support Vector Machines trained on word2vec representations. These models were trained using English documents only. Two types of targeted sentiment are evaluated for the task: those expressed towards either a situation frame or those towards an entity. To identify sentiment expressed towards an SF, we use the pretrained model described in BIBREF44 , in which a multiplicative", "work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the need to manually construct lexica resources. Problem Definition. This section describes the SEC task in the LORELEI program along with the dataset, evaluation conditions and metrics. The Sentiment, Emotion and Cognitive State (SEC) Task. Given a dataset of text documents and manually annotated situation frames, the task is to automatically detect sentiment polarity relevant to existing frames and identify the source and target for each sentiment instance. The source is defined as a person or a group of people expressing the sentiment, and can be either a PER/ORG/GPE (person, organization or geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the", "time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames \u2014 data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames. While the task of identifying Situation Frames is similar to existing tasks in literature (e.g., slot filling), it is defined by the very limited availability of data BIBREF15 . This lack of data requires the use of simpler but more robust models and the utilization of transfer learning or data augmentation techniques. The Sentiment, Emotion, and Cognitive State (SEC) evaluation task was a recent addition to", "it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information. Our results show that state-of-the-art sentiment classifiers can be leveraged out-of-the-box for a reasonable performance on English data. By identifying possible differences coming from the information sources, as well as by exploiting the information communicated as the situation unfolds, we showed significant performance gains on both English and Spanish.", "geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the frame or an entity in the document. Situation awareness information is encoded into situation frames in the LORELEI program BIBREF35 . Situation Frames (SF) are similar in nature to those used in Natural Language Understanding (NLU) systems: in essence they are data structures that record information corresponding to a single incident at a single location BIBREF15 . A SF frame includes a situation Type taken from a fixed inventory of 11 categories (e.g., medical need, shelter, infrastructure), Location where the situation exists (if a location is mentioned) and additional variables highlighting the Status of the situation (e.g., entities involved in resolution, time and urgency). An example of a SF can be found in table 1 . A list of situation frames and documents serve as input for our"]}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_answer": "Yes", "predicted_evidence": ["Given the critical and global nature of the HADR events, tools must process information quickly, from a variety of sources and languages, making it easily accessible to first responders and decision makers for damage assessment and to launch relief efforts accordingly BIBREF12 , BIBREF13 . However, research efforts in these tasks are primarily focused on high resource languages such as English, even though such crises may happen anywhere in the world. The LORELEI program provides a framework for developing and testing systems for real-time humanitarian crises response in the context of low-resource languages. The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational", "time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames \u2014 data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames. While the task of identifying Situation Frames is similar to existing tasks in literature (e.g., slot filling), it is defined by the very limited availability of data BIBREF15 . This lack of data requires the use of simpler but more robust models and the utilization of transfer learning or data augmentation techniques. The Sentiment, Emotion, and Cognitive State (SEC) evaluation task was a recent addition to", "$  Method. We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation. Machine Translation. Automatic translations from Spanish to English were obtained from Microsoft Bing using their publicly available API. For the pilot evaluation, we translated all of the Spanish documents into English, and included them as additional training data. At this time we do not translate English to Spanish, but plan to explore this thread in future work. Linguistic Features. We extract word unigrams and bigrams. These features were then transformed using term frequencies (TF) and Inverse document-frequency (IDF). Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between", "For the classifiers, we explored the use of Support Vector Machines and Random Forests. Model performance was estimated through 10-fold cross validation on the train set. Hyper-parameters, such as of regularization, were selected based on the performance on grid-search using an 10-fold inner-cross validation loop. After choosing the parameters, models were re-trained on all the available data. We consider some of the most popular baseline models in the literature: (i) minority class baseline (due to the heavily imbalanced dataset), (ii) Support Vector Machines trained on TF-IDF bi-gram language model, (iii) and Support Vector Machines trained on word2vec representations. These models were trained using English documents only. Two types of targeted sentiment are evaluated for the task: those expressed towards either a situation frame or those towards an entity. To identify sentiment expressed towards an SF, we use the pretrained model described in BIBREF44 , in which a multiplicative", "geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the frame or an entity in the document. Situation awareness information is encoded into situation frames in the LORELEI program BIBREF35 . Situation Frames (SF) are similar in nature to those used in Natural Language Understanding (NLU) systems: in essence they are data structures that record information corresponding to a single incident at a single location BIBREF15 . A SF frame includes a situation Type taken from a fixed inventory of 11 categories (e.g., medical need, shelter, infrastructure), Location where the situation exists (if a location is mentioned) and additional variables highlighting the Status of the situation (e.g., entities involved in resolution, time and urgency). An example of a SF can be found in table 1 . A list of situation frames and documents serve as input for our", "at the time of submission only partial score had been made public. As previously mentioned, the pre-trained state-of-the-art models (model I) were directly applied to the evaluation data without any adaptation. These performed reasonably well for the English data. Among the submissions of the SEC Task pilot, our systems outperformed the other competitors for both languages. Conclusion. Understanding the expressed sentiment from an affected population during the on-set of a crisis is a particularly difficult task, especially in low-resource scenarios. There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment.", "BIBREF27 . However, certain aspects of these communication channels are still apt for sentiment analysis, such as column pieces BIBREF28 or political news BIBREF27 , BIBREF29 . In the context of leveraging the information found online for HADR emergencies, approaches for languages other than English have been limited. Most of which are done by manually constructing resources for a particular language (e.g., in tweets BIBREF30 , BIBREF31 , BIBREF32 and in disaster-related news coverage BIBREF33 ), or by applying cross-language text categorization to build language-specific models BIBREF31 , BIBREF34 . In this work, we develop systems that identify positive and negative sentiments expressed in social media posts, news articles and blogs in the context of a humanitarian emergency. Our systems work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the", "work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the need to manually construct lexica resources. Problem Definition. This section describes the SEC task in the LORELEI program along with the dataset, evaluation conditions and metrics. The Sentiment, Emotion and Cognitive State (SEC) Task. Given a dataset of text documents and manually annotated situation frames, the task is to automatically detect sentiment polarity relevant to existing frames and identify the source and target for each sentiment instance. The source is defined as a person or a group of people expressing the sentiment, and can be either a PER/ORG/GPE (person, organization or geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the"]}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "predicted_answer": "Nodes and graphs", "predicted_evidence": ["particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14. This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic", "about the structure of diagrams and other forms of multimodal discourse may be broadly conceptualised as multimodal discourse parsing. Recent examples of work in this area include alikhanietal2019 and ottoetal2019, who model discourse relations between natural language and photographic images, drawing on linguistic theories of coherence and text\u2013image relations, respectively. In most cases, however, predicting a single discourse relation covers only a part of the discourse structure. sachanetal2019 note that there is a need for comprehensive theories and models of multimodal communication, as they can be used to rethink tasks that have been previously considered only from the perspective of natural language processing. Unlike many other areas, the study of diagrammatic representations is particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares", "Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer. Both AI2D and AI2D-RST represent the multimodal structure of diagrams using graphs. This enables learning their representations using graph neural networks, which are gaining currency as a graph is a natural choice for representing many types of data BIBREF15. This article reports on two experiments that evaluate the capability of AI2D and AI2D-RST to represent the multimodal structure of diagrams using graphs, focusing particularly on spatial layout, the hierarchical organisation of diagram elements and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced", "expressed using diagrammatic elements. Whether computational modelling of diagrammatic structures, or more generally, multimodal discourse parsing, benefits from pulling apart different types of multimodal structure remains an open question, which we pursued by developing an alternative annotation schema for AI2D, named AI2D-RST, which is introduced below. Data ::: Expert Annotations from AI2D-RST. AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:", "The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10. I have previously argued that describing different types of multimodal structures in diagrammatic representations requires different types of graphs BIBREF18. To exemplify, many forms of multimodal discourse are assumed to possess a hierarchical structure, whose representation requires a tree graph. Diagrams, however, use arrows and lines to draw connections between elements that are not necessarily part of the same subtree, and for this reason representing connectivity requires a cyclic graph. AI2D DPGs, in turn, conflate the description of semantic relations and connections expressed using diagrammatic elements. Whether computational modelling of diagrammatic structures, or more generally, multimodal discourse parsing, benefits from pulling apart different types of", "between five annotators on random samples from the AI2D-RST corpus using Fleiss' $\\kappa $ BIBREF23. The results show high agreement on grouping ($N = 256, \\kappa = 0.84$), diagram types ($N = 119, \\kappa = 0.78$), connectivity ($N = 239, \\kappa = 0.88$) and discourse structure ($N = 227, \\kappa = 0.73$). It should be noted, however, that these measures may be affected by implicit knowledge that tends to develop among expert annotators who work towards the same task BIBREF24. Graph-based Representations. Both AI2D and AI2D-RST use graphs to represent the multimodal structure of diagrams. This section explicates how the graphs and their node and edge types differ across the two multimodal resources. Graph-based Representations ::: Nodes ::: Node Types. AI2D and AI2D-RST share most node types that represent different diagram elements, namely text, graphics, arrows and the image constant, which is a node that stands for the entire diagram. In AI2D, generic diagram elements such as titles", "groups of diagram elements and their hierarchical organisation, provides a robust foundation for describing the spatial organisation of diagrammatic representations. This kind of generic schema can be expanded beyond diagrams to other modes of expression that make use of the spatial extent, such as entire page layouts. A description of how the layout space is used can be incorporated into any effort to model discourse relations that may hold between the groups or their parts. The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very", "processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation. The need to parse this discourse structure shifts the focus towards the field of natural language processing. Understanding and making inferences about the structure of diagrams and other forms of multimodal discourse may be broadly conceptualised as multimodal discourse parsing. Recent examples of work in this area include alikhanietal2019"]}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "predicted_answer": "Unanswerable", "predicted_evidence": ["use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema. Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24. In light of the results for graph classification, one should note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes,", "particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14. This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic", "and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4 Data ::: Crowd-sourced Annotations from AI2D. The Allen Institute for Artificial Intelligence Diagrams dataset (AI2D) contains 4903 English-language diagrams, which represent topics in primary school natural sciences, such as food webs, human physiology and life cycles, amounting to a total of 17 classes BIBREF10. The dataset was originally developed to support research on diagram understanding and visual question answering", "Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer. Both AI2D and AI2D-RST represent the multimodal structure of diagrams using graphs. This enables learning their representations using graph neural networks, which are gaining currency as a graph is a natural choice for representing many types of data BIBREF15. This article reports on two experiments that evaluate the capability of AI2D and AI2D-RST to represent the multimodal structure of diagrams using graphs, focusing particularly on spatial layout, the hierarchical organisation of diagram elements and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced", "expressed using diagrammatic elements. Whether computational modelling of diagrammatic structures, or more generally, multimodal discourse parsing, benefits from pulling apart different types of multimodal structure remains an open question, which we pursued by developing an alternative annotation schema for AI2D, named AI2D-RST, which is introduced below. Data ::: Expert Annotations from AI2D-RST. AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:", "especially if both AI2D and AI2D-RST graphs were enriched with features from state of the art semantic representations for natural language and graphic elements. Conclusion. In this article, I compared graph-based representations of diagrams representing primary school science topics from two datasets that contain the same diagrams, which have been annotated by either crowd-sourced workers or trained experts. The comparison involved two tasks, graph and node classification, using four different architectures for graph neural networks, which were compared to baselines from dummy, random forest and support vector machine classifiers. The results showed that graph neural networks can learn to accurately identify diagram elements from their size, shape and position in layout. These node representations could then be used as features for graph classification. Identifying diagrams, either in terms of what they represent (semantic content) or how (abstract diagram type), proved more", "webs, human physiology and life cycles, amounting to a total of 17 classes BIBREF10. The dataset was originally developed to support research on diagram understanding and visual question answering BIBREF16, but has also been used to study the contextual interpretation of diagrammatic elements, such as arrows and lines BIBREF17. The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon", "the performance of graph-based representations in AI2D and AI2D-RST for classifying entire diagrams. Here the aim is to evaluate to what extent graph neural networks can learn about the generic structure of primary school science diagrams from the graph-based representations in AI2D and AI2D-RST. Correctly identifying what the diagram attempts to communicate and how carries implications for tasks such as visual question answering, as the type of a diagram constrains the interpretation of key diagrammatic elements, such as the meaning of lines and arrows BIBREF1, BIBREF17. To enable a fair comparison, the target classes are derived from both AI2D and AI2D-RST. Whereas AI2D includes 17 classes that represent the semantic content of diagrams, as exemplified by categories such as `parts of the Earth', `volcano', and `food chains and webs', AI2D-RST classifies diagrams into abstract diagram types, such as cycles, networks, cross-sections and cut-outs. More specifically, AI2D-RST provides"]}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "predicted_answer": "The two resources are compared based on their capability to represent the multimodal structure of diagrams using graphs.", "predicted_evidence": ["use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema. Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24. In light of the results for graph classification, one should note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes,", "particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14. This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic", "and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4 Data ::: Crowd-sourced Annotations from AI2D. The Allen Institute for Artificial Intelligence Diagrams dataset (AI2D) contains 4903 English-language diagrams, which represent topics in primary school natural sciences, such as food webs, human physiology and life cycles, amounting to a total of 17 classes BIBREF10. The dataset was originally developed to support research on diagram understanding and visual question answering", "Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer. Both AI2D and AI2D-RST represent the multimodal structure of diagrams using graphs. This enables learning their representations using graph neural networks, which are gaining currency as a graph is a natural choice for representing many types of data BIBREF15. This article reports on two experiments that evaluate the capability of AI2D and AI2D-RST to represent the multimodal structure of diagrams using graphs, focusing particularly on spatial layout, the hierarchical organisation of diagram elements and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced", "expressed using diagrammatic elements. Whether computational modelling of diagrammatic structures, or more generally, multimodal discourse parsing, benefits from pulling apart different types of multimodal structure remains an open question, which we pursued by developing an alternative annotation schema for AI2D, named AI2D-RST, which is introduced below. Data ::: Expert Annotations from AI2D-RST. AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:", "groups of diagram elements and their hierarchical organisation, provides a robust foundation for describing the spatial organisation of diagrammatic representations. This kind of generic schema can be expanded beyond diagrams to other modes of expression that make use of the spatial extent, such as entire page layouts. A description of how the layout space is used can be incorporated into any effort to model discourse relations that may hold between the groups or their parts. The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very", "note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes, which ensures that features are propagated to neighbouring nodes, the crowd-sourced AI2D graphs contain both isolated nodes and subgraphs. To what extent these disconnections affect the performance for AI2D warrant a separate study. Additionally, more advanced techniques than mere averaging, such as pooling, should be explored in future work. Finally, there are many aspects of diagrammatic representation that were not explored in this study. To begin with, a comparison of representations for discourse structures using the question-answering set accompanying AI2D would be particularly interesting, especially if both AI2D and AI2D-RST graphs were enriched with features from state of the art semantic representations for natural language and graphic elements. Conclusion. In this article, I", "webs, human physiology and life cycles, amounting to a total of 17 classes BIBREF10. The dataset was originally developed to support research on diagram understanding and visual question answering BIBREF16, but has also been used to study the contextual interpretation of diagrammatic elements, such as arrows and lines BIBREF17. The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon"]}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "predicted_answer": "Unanswerable", "predicted_evidence": ["use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema. Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24. In light of the results for graph classification, one should note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes,", "and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4 Data ::: Crowd-sourced Annotations from AI2D. The Allen Institute for Artificial Intelligence Diagrams dataset (AI2D) contains 4903 English-language diagrams, which represent topics in primary school natural sciences, such as food webs, human physiology and life cycles, amounting to a total of 17 classes BIBREF10. The dataset was originally developed to support research on diagram understanding and visual question answering", "particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14. This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic", "groups of diagram elements and their hierarchical organisation, provides a robust foundation for describing the spatial organisation of diagrammatic representations. This kind of generic schema can be expanded beyond diagrams to other modes of expression that make use of the spatial extent, such as entire page layouts. A description of how the layout space is used can be incorporated into any effort to model discourse relations that may hold between the groups or their parts. The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very", "Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer. Both AI2D and AI2D-RST represent the multimodal structure of diagrams using graphs. This enables learning their representations using graph neural networks, which are gaining currency as a graph is a natural choice for representing many types of data BIBREF15. This article reports on two experiments that evaluate the capability of AI2D and AI2D-RST to represent the multimodal structure of diagrams using graphs, focusing particularly on spatial layout, the hierarchical organisation of diagram elements and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced", "note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes, which ensures that features are propagated to neighbouring nodes, the crowd-sourced AI2D graphs contain both isolated nodes and subgraphs. To what extent these disconnections affect the performance for AI2D warrant a separate study. Additionally, more advanced techniques than mere averaging, such as pooling, should be explored in future work. Finally, there are many aspects of diagrammatic representation that were not explored in this study. To begin with, a comparison of representations for discourse structures using the question-answering set accompanying AI2D would be particularly interesting, especially if both AI2D and AI2D-RST graphs were enriched with features from state of the art semantic representations for natural language and graphic elements. Conclusion. In this article, I", "between five annotators on random samples from the AI2D-RST corpus using Fleiss' $\\kappa $ BIBREF23. The results show high agreement on grouping ($N = 256, \\kappa = 0.84$), diagram types ($N = 119, \\kappa = 0.78$), connectivity ($N = 239, \\kappa = 0.88$) and discourse structure ($N = 227, \\kappa = 0.73$). It should be noted, however, that these measures may be affected by implicit knowledge that tends to develop among expert annotators who work towards the same task BIBREF24. Graph-based Representations. Both AI2D and AI2D-RST use graphs to represent the multimodal structure of diagrams. This section explicates how the graphs and their node and edge types differ across the two multimodal resources. Graph-based Representations ::: Nodes ::: Node Types. AI2D and AI2D-RST share most node types that represent different diagram elements, namely text, graphics, arrows and the image constant, which is a node that stands for the entire diagram. In AI2D, generic diagram elements such as titles", "expressed using diagrammatic elements. Whether computational modelling of diagrammatic structures, or more generally, multimodal discourse parsing, benefits from pulling apart different types of multimodal structure remains an open question, which we pursued by developing an alternative annotation schema for AI2D, named AI2D-RST, which is introduced below. Data ::: Expert Annotations from AI2D-RST. AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:"]}
{"question_id": "fff1ed2435ba622d884ecde377ff2de127167638", "predicted_answer": "Experts (from AI2D-RST dataset).", "predicted_evidence": ["Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer. Both AI2D and AI2D-RST represent the multimodal structure of diagrams using graphs. This enables learning their representations using graph neural networks, which are gaining currency as a graph is a natural choice for representing many types of data BIBREF15. This article reports on two experiments that evaluate the capability of AI2D and AI2D-RST to represent the multimodal structure of diagrams using graphs, focusing particularly on spatial layout, the hierarchical organisation of diagram elements and their connections expressed using arrows and lines. Data. This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced", "particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14. This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic", "expressed using diagrammatic elements. Whether computational modelling of diagrammatic structures, or more generally, multimodal discourse parsing, benefits from pulling apart different types of multimodal structure remains an open question, which we pursued by developing an alternative annotation schema for AI2D, named AI2D-RST, which is introduced below. Data ::: Expert Annotations from AI2D-RST. AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:", "implementation in the Tune BIBREF32 and hyperopt BIBREF33 libraries. For each dataset, architecture and task, I evaluated a total of 100 hyperparameter combinations for a maximum of 100 epochs, using 850 diagrams for training and 150 for validation. The objective metric to be maximised was macro F1 score. Tables TABREF20 and TABREF21 give the hyperparameters and spaces searched for node and graph classification. Following shcuretal2018, I shuffled the training and validation splits for each run to prevent overfitting and used the same training procedure throughout. I used the Adam optimiser BIBREF34 for both hyperparameter search and training. To address the issue of class imbalance present in both tasks, class weights were calculated by dividing the total number of samples by the product of the number of unique classes and the number of samples for each class, as implemented in scikit-learn BIBREF35. These weights were passed to the loss function during hyperparameter search and", "use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema. Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24. In light of the results for graph classification, one should note that node features are averaged before classification regardless of their connections in the graph. Whereas the expert-annotated grouping graph in AI2D-RST has been pruned from isolated nodes,", "BIBREF27 with 2 heads GraphSAGE (SAGE) BIBREF28 with LSTM aggregation I implemented all graph neural networks using Deep Graph Library 0.4 BIBREF29 on the PyTorch 1.3 backend BIBREF30. For GCN, GAT and SAGE, each network consists of two of the aforementioned layers with a Rectified Linear Unit (ReLU) activation, followed by a dense layer and a final softmax function for predicting class membership probabilities. For SGC, the network consists of a single SGC layer without an activation function. The implementations for each network are available in the repository associated with this article. Experiments ::: Hyperparameters and Training. I used the Tree of Parzen Estimators (TPE) algorithm BIBREF31 to tune model hyperparameters separately for each dataset, architecture and task using the implementation in the Tune BIBREF32 and hyperopt BIBREF33 libraries. For each dataset, architecture and task, I evaluated a total of 100 hyperparameter combinations for a maximum of 100 epochs, using", "of the number of unique classes and the number of samples for each class, as implemented in scikit-learn BIBREF35. These weights were passed to the loss function during hyperparameter search and training. After hyperparameter optimisation, I trained each model with the best hyperparameter combination for 20 runs, using 850 diagrams for training, 75 for validation and 75 for testing, shuffling the splits for each run while monitoring performance on the evaluation set and stopping training early if the macro F1 score failed to improve over 15 epochs for graph classification or over 25 epochs for node classification. I then evaluated the model on the testing set and recorded the result. Experiments ::: Tasks ::: Node Classification. The purpose of the node classification task is to evaluate how well algorithms learn to classify the parts of a diagram using the graph-based representations in AI2D and AI2D-RST and node features representing the position, size and shape of the element, as", "machine classifiers trained on 850 and tested on 150 diagrams. Both AI2D and AI2D-RST include five node types, of which four are the same: the difference is that whereas AI2D includes arrowheads, AI2D-RST includes nodes for groups of diagram elements, as outlined in Section SECREF9 The results seem to reflect the fact that image constants and grouping nodes have their features set to zero, and RF and SVM cannot leverage features incoming from their neighbouring nodes to learn node representations. This is likely to affect the result for AI2D-RST, which includes 7300 grouping nodes that are used to create a hierarchy of diagram elements. Table TABREF22 shows the results for node classification using various graph neural network architectures. Because the results are not entirely comparable due to different node types present in the two resources, it is more reasonable to compare architectures. SAGE, GCN and GAT clearly outperform SGC in classifying nodes from both resources, as does"]}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "predicted_answer": "RNNs, HAN, CNN", "predicted_evidence": ["preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural models to illustrate the overall increase in robustness by neural approaches. Interestingly, the SVM and HAN model show some unexpected improvement with regard to Precision when applied to unseen timeframes. For both models, this increase in Precision is offset by a greater loss in Recall, which seems to indicate both models `memorize` the controversial topics in a given timeframe instead of the controversial language. Overall, the neural approaches seem to compare favorably in terms of cross-temporal stability. Robustness of the model across topics. To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the", "detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks. Methods. A proven approach in modelling text with neural networks is to use Recurrent Neural Networks (RNNs) which enjoy weight sharing capabilities to model words irrespective of their sequence location. A specific type, the Hierarchical Attention Network (HAN) proposed by BIBREF10 makes use of attention to build document representations in a hierarchical manner. It uses bi-directional Gated Recurrent Units (GRUs) BIBREF12 to selectively update representations of both words and sentences. This allows the network to both capture the hierarchy from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference. Recently, Convolutional Neural Networks (CNNs) have enjoyed increasing success", "are taken to express semantic relatedness, despite having different surface forms. By using embeddings, neural architectures are also able to leverage features learned on other texts (e.g. pretrained word embeddings) and create higher level representations of input (e.g. convolutional feature maps or hidden-states). These properties suggest that neural approaches are better able to generalize to unseen examples that poorly match the training set. We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question. RQ: Can we increase robustness of controversy detection using neural methods? Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to", "10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics. Robustness of the model across domains. Most work on controversy has looked into using existing knowledge bases as a source of controversy information BIBREF6 , BIBREF1 . In this paper, we focus on text-based classification", "data and tested on the 2018 Wikipedia data. Table 3 shows the results for each of the text-based detection models. Within year, the hierarchical attention model (HAN) outperforms all other models on Recall, F1 and AUC, losing Precision to the CNN and SVM models. However, our main interest is the robustness when a model is trained on a different year (2009) than the test set (2018). These between year experiments show a superior score for the HAN model compared to the non-neural models on Recall, and show significant improvements on F1 (p < 0.05) and AUC (p < 0.05), losing only to the SVM model on Precision (non significantly). In terms of robustness, we can also take the percentage change between the within year and between year experiment into account (were smaller absolute changes are preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural", "size. The resulting confidence intervals based on percentiles provide a measure of significance. Baseline models. To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique", "the language model trained on the DBPedia dataset outperforms other models on Recall but shows no significant difference compared to the CNN model. Notably, the neural approaches show comparable results to the TILE-Clique model in terms of F1, demonstrating a balanced performance in terms of Precision and Recall. Furthermore, the CNN model shows a significant improvement compared to the other non neural baselines in terms of the AUC value (p < 0.05). Robustness of the model across time. Controversy is expected to change over time. Some issues become controversial, others cease to be so. To investigate robustness of controversy detection models with respect to changes over time, we evaluate model performance in two variants: trained and tested on 2018, or trained on the 2009 Wikipedia data and tested on the 2018 Wikipedia data. Table 3 shows the results for each of the text-based detection models. Within year, the hierarchical attention model (HAN) outperforms all other models on", "apart. Thirdly, we show that CNN and HAN models outperform the SVM and LM baselines on Precision, F1 and AUC when tested on held-out-topics. Fourthly, we show that neural methods are better able to generalize from Wikipedia pages to unseen general web pages in terms of Precision, F1 and AUC. Lastly, neural methods seem better in line with human annotators with regard to certainty and disagreement."]}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "predicted_answer": "Cross-domain", "predicted_evidence": ["combine lexical and explicit modelling approaches by looking at lexical similarities between a given text and a set of texts in a domain that provides explicit features BIBREF1 , BIBREF6 , BIBREF7 . Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific", "Introduction & Prior work. Controversy detection is an increasingly important task. Controversial content can signal the need for moderation on social platforms, either to prevent conflict between users or limit the spread of misinformation. More generally, controversies provide insight into societies BIBREF0 . Often, the controversial content is outside the direct control of a platform on which it is shared, mentioned or discussed. This raises the requirement of generally applicable methods to gauge controversial content on the web for moderation purposes. Unfortunately, what is controversial changes, and may lie more in the way topics are discussed rather than what is discussed, making it difficult to detect controversies in a robust fashion. We take the task of controversy detection and evaluate robustness of different methodologies with respect to the varying nature of controversies. Prior work on detecting controversies has taken three kinds of approaches: 1) lexical approaches,", "(non)controversial documents, correlate more strongly with the certainty of human annotators and are susceptible to errors in similar conditions as when annotators disagree. Conclusion. Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time. We analysed the performance of language model, SVM, CNN and HAN models on different tasks. First, we have demonstrated that neural methods perform as state-of-the-art tools in controversy detection on the ClueWeb09 BIBREF0 based testset, even beating matching models. Second, we investigated temporal stability, and demonstrated neural -and especially CNN- robustness in terms of Recall, F1 and AUC performance and stability with train and test sets that are 9 years apart. Thirdly, we show that CNN and HAN models outperform the SVM and LM baselines on Precision, F1 and AUC when tested on held-out-topics. Fourthly, we show that neural methods are better able to", "apart. Thirdly, we show that CNN and HAN models outperform the SVM and LM baselines on Precision, F1 and AUC when tested on held-out-topics. Fourthly, we show that neural methods are better able to generalize from Wikipedia pages to unseen general web pages in terms of Precision, F1 and AUC. Lastly, neural methods seem better in line with human annotators with regard to certainty and disagreement.", "expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance. Certainty is the distance of human annotations to the midpoint of the four-point controversy scale, i.e. a score between 0 and 2.5 that expresses how sure annotators are of document (non)controversy. Here, the HAN shows errors most strongly negatively correlated to the certainty of annotators. Finally, annotators disagree on the controversy of some documents, expressed as the standard deviation of their controversy annotations. Again, the HAN model seems preferable, as it's errors are most strongly correlated to annotator disagreement. Overall, the neural methods have less biased performance in relation to (non)controversial documents, correlate more strongly with the certainty of human annotators and are susceptible to errors in similar conditions as when annotators disagree. Conclusion. Controversy", "10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics. Robustness of the model across domains. Most work on controversy has looked into using existing knowledge bases as a source of controversy information BIBREF6 , BIBREF1 . In this paper, we focus on text-based classification", "also outperform both language models on AUC significantly (p < 0.05). Precision and Recall are more mixed, with the CNN and SVM outperforming the HAN on Precision and the language model -again- performing best in terms of Recall. Together, the neural methods seem to work best on three out of the four metrics. Human agreement. Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance. Certainty is", "preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural models to illustrate the overall increase in robustness by neural approaches. Interestingly, the SVM and HAN model show some unexpected improvement with regard to Precision when applied to unseen timeframes. For both models, this increase in Precision is offset by a greater loss in Recall, which seems to indicate both models `memorize` the controversial topics in a given timeframe instead of the controversial language. Overall, the neural approaches seem to compare favorably in terms of cross-temporal stability. Robustness of the model across topics. To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the"]}
{"question_id": "6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "predicted_answer": "Unanswerable", "predicted_evidence": ["detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks. Methods. A proven approach in modelling text with neural networks is to use Recurrent Neural Networks (RNNs) which enjoy weight sharing capabilities to model words irrespective of their sequence location. A specific type, the Hierarchical Attention Network (HAN) proposed by BIBREF10 makes use of attention to build document representations in a hierarchical manner. It uses bi-directional Gated Recurrent Units (GRUs) BIBREF12 to selectively update representations of both words and sentences. This allows the network to both capture the hierarchy from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference. Recently, Convolutional Neural Networks (CNNs) have enjoyed increasing success", "the language model trained on the DBPedia dataset outperforms other models on Recall but shows no significant difference compared to the CNN model. Notably, the neural approaches show comparable results to the TILE-Clique model in terms of F1, demonstrating a balanced performance in terms of Precision and Recall. Furthermore, the CNN model shows a significant improvement compared to the other non neural baselines in terms of the AUC value (p < 0.05). Robustness of the model across time. Controversy is expected to change over time. Some issues become controversial, others cease to be so. To investigate robustness of controversy detection models with respect to changes over time, we evaluate model performance in two variants: trained and tested on 2018, or trained on the 2009 Wikipedia data and tested on the 2018 Wikipedia data. Table 3 shows the results for each of the text-based detection models. Within year, the hierarchical attention model (HAN) outperforms all other models on", "expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance. Certainty is the distance of human annotations to the midpoint of the four-point controversy scale, i.e. a score between 0 and 2.5 that expresses how sure annotators are of document (non)controversy. Here, the HAN shows errors most strongly negatively correlated to the certainty of annotators. Finally, annotators disagree on the controversy of some documents, expressed as the standard deviation of their controversy annotations. Again, the HAN model seems preferable, as it's errors are most strongly correlated to annotator disagreement. Overall, the neural methods have less biased performance in relation to (non)controversial documents, correlate more strongly with the certainty of human annotators and are susceptible to errors in similar conditions as when annotators disagree. Conclusion. Controversy", "(non)controversial documents, correlate more strongly with the certainty of human annotators and are susceptible to errors in similar conditions as when annotators disagree. Conclusion. Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time. We analysed the performance of language model, SVM, CNN and HAN models on different tasks. First, we have demonstrated that neural methods perform as state-of-the-art tools in controversy detection on the ClueWeb09 BIBREF0 based testset, even beating matching models. Second, we investigated temporal stability, and demonstrated neural -and especially CNN- robustness in terms of Recall, F1 and AUC performance and stability with train and test sets that are 9 years apart. Thirdly, we show that CNN and HAN models outperform the SVM and LM baselines on Precision, F1 and AUC when tested on held-out-topics. Fourthly, we show that neural methods are better able to", "preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural models to illustrate the overall increase in robustness by neural approaches. Interestingly, the SVM and HAN model show some unexpected improvement with regard to Precision when applied to unseen timeframes. For both models, this increase in Precision is offset by a greater loss in Recall, which seems to indicate both models `memorize` the controversial topics in a given timeframe instead of the controversial language. Overall, the neural approaches seem to compare favorably in terms of cross-temporal stability. Robustness of the model across topics. To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the", "are taken to express semantic relatedness, despite having different surface forms. By using embeddings, neural architectures are also able to leverage features learned on other texts (e.g. pretrained word embeddings) and create higher level representations of input (e.g. convolutional feature maps or hidden-states). These properties suggest that neural approaches are better able to generalize to unseen examples that poorly match the training set. We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question. RQ: Can we increase robustness of controversy detection using neural methods? Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to", "from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference. Recently, Convolutional Neural Networks (CNNs) have enjoyed increasing success in text classification. One such network introduced by BIBREF11 looks at patterns in words within a window, such as \"Scientology [...] brainwashes people\". The occurrences of these patterns are then summarized to their 'strongest' observation (max-pooling) and used for classification. Since pooling is applied after each convolution, the output size of each convolutional operation itself is irrelevant. Therefore, filters of different sizes can be used, each capturing patterns in different sized word windows. We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The", "data and tested on the 2018 Wikipedia data. Table 3 shows the results for each of the text-based detection models. Within year, the hierarchical attention model (HAN) outperforms all other models on Recall, F1 and AUC, losing Precision to the CNN and SVM models. However, our main interest is the robustness when a model is trained on a different year (2009) than the test set (2018). These between year experiments show a superior score for the HAN model compared to the non-neural models on Recall, and show significant improvements on F1 (p < 0.05) and AUC (p < 0.05), losing only to the SVM model on Precision (non significantly). In terms of robustness, we can also take the percentage change between the within year and between year experiment into account (were smaller absolute changes are preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural"]}
{"question_id": "60e6296ca2a697892bd67558a21a83ef01a38177", "predicted_answer": "Yes", "predicted_evidence": ["trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific features and reducing sensitivity to an exact lexical representation is paramount to robust controversy detection. To this end, we focus only on fulltext features and suggest to leverage the semantic representations of word embeddings to reduce the vocabulary-gap for unseen topics and exact lexical representations. The majority of NLP-task related neural architectures rely on word embeddings, popularized by Mikolov et al BIBREF9 to represent texts. In essence these embeddings are latent-vector representations that aim to capture the underlying meaning of words. Distances between such latent-vectors are taken to express semantic relatedness, despite having different surface forms. By using embeddings, neural architectures are also able to leverage features learned on other texts (e.g.", "evaluate robustness of different methodologies with respect to the varying nature of controversies. Prior work on detecting controversies has taken three kinds of approaches: 1) lexical approaches, which seek to detect controversies through signal terms, either through bag-of-word classifiers, lexicons, or lexicon based language models BIBREF1 . 2) explicit modeling of controversy through platform-specific features, often in Wikipedia or social-media settings. Features such as mutual reverts BIBREF2 , user-provided flags BIBREF3 , interaction networks BIBREF4 or stance-distributions BIBREF5 have been used as platform-specific indicators of controversies. The downside of these approaches is the lack of generalizability due to their platform-specific nature. 3) matching models that combine lexical and explicit modelling approaches by looking at lexical similarities between a given text and a set of texts in a domain that provides explicit features BIBREF1 , BIBREF6 , BIBREF7 .", "preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural models to illustrate the overall increase in robustness by neural approaches. Interestingly, the SVM and HAN model show some unexpected improvement with regard to Precision when applied to unseen timeframes. For both models, this increase in Precision is offset by a greater loss in Recall, which seems to indicate both models `memorize` the controversial topics in a given timeframe instead of the controversial language. Overall, the neural approaches seem to compare favorably in terms of cross-temporal stability. Robustness of the model across topics. To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the", "are taken to express semantic relatedness, despite having different surface forms. By using embeddings, neural architectures are also able to leverage features learned on other texts (e.g. pretrained word embeddings) and create higher level representations of input (e.g. convolutional feature maps or hidden-states). These properties suggest that neural approaches are better able to generalize to unseen examples that poorly match the training set. We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question. RQ: Can we increase robustness of controversy detection using neural methods? Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to", "combine lexical and explicit modelling approaches by looking at lexical similarities between a given text and a set of texts in a domain that provides explicit features BIBREF1 , BIBREF6 , BIBREF7 . Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific", "size. The resulting confidence intervals based on percentiles provide a measure of significance. Baseline models. To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique", "10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics. Robustness of the model across domains. Most work on controversy has looked into using existing knowledge bases as a source of controversy information BIBREF6 , BIBREF1 . In this paper, we focus on text-based classification", "detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks. Methods. A proven approach in modelling text with neural networks is to use Recurrent Neural Networks (RNNs) which enjoy weight sharing capabilities to model words irrespective of their sequence location. A specific type, the Hierarchical Attention Network (HAN) proposed by BIBREF10 makes use of attention to build document representations in a hierarchical manner. It uses bi-directional Gated Recurrent Units (GRUs) BIBREF12 to selectively update representations of both words and sentences. This allows the network to both capture the hierarchy from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference. Recently, Convolutional Neural Networks (CNNs) have enjoyed increasing success"]}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "predicted_answer": "YouTube videos", "predicted_evidence": ["at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces. Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances. Experiments. For evaluation we used two", "the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features. Data and Experimental Setup. Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 ,", "with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances. Experiments. For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table . Conclusion. We present a simple strategy to augment a standard recurrent neural network language model with temporal visual features. Through an exploration of candidate architectures, we show that the Middle Fusion of visual and textual features leads to a 20-28% reduction in perplexity relative to a text only baseline. These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens \u2013 two orders of magnitude more than any previously published work. Our work is a first step", "These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens \u2013 two orders of magnitude more than any previously published work. Our work is a first step towards creating and deploying large-scale multimodal systems that properly situate themselves into a given context, by taking full advantage of every available signal.", "Introduction.  INLINEFORM0 Work performed while the author was an intern at Google. Language models are vital components of a wide variety of systems for Natural Language Processing (NLP) including Automatic Speech Recognition, Machine Translation, Optical Character Recognition, Spelling Correction, etc. However, most language models are trained and applied in a manner that is oblivious to the environment in which human language operates BIBREF0 . These models are typically trained only on sequences of words, ignoring the physical context in which the symbolic representations are grounded, or ignoring the social context that could inform the semantics of an utterance. For incorporating additional modalities, the NLP community has typically used datasets such as MS COCO BIBREF1 and Flickr BIBREF2 for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started", "integration of the two modalities: In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1  where INLINEFORM0 are learned matrices. Here, we apply the intuition that some words could provide information as to whether or not the visual context is helpful. In a simplistic example, if the word history is the article \u201cthe,\" then the visual context could provide relevant information needed for predicting the next word. For other word histories, though, the visual context might not be needed or be even irrelevant for the next word prediction: if the previous word is \u201ccarpe\", the next word is very likely to be \u201cdiem\", regardless of visual context. We implement a simple weighting mechanism that learns a scalar weight for the visual embedding prior to concatenation with the word embedding. The input to the RNNLM is now INLINEFORM0 , where: INLINEFORM1  This approach does not add any new parameters to the model, but since the word", "for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started investigating language grounding in images BIBREF8 , BIBREF9 , BIBREF10 and to lesser extent in videos BIBREF11 , BIBREF1 . However, language grounding has focused more on obtaining better word and sentence representations or other downstream tasks, and to lesser extent on language modeling. In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs. The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5\u20136% on three", "the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs. The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5\u20136% on three languages on the MS COCO dataset (with an English vocabulary of only 16K words). Our work is distinguishable from previous work with respect to three dimensions: Model. A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1  where each word is assigned a probability given the previous word history. For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames:"]}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "predicted_answer": "About 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens.", "predicted_evidence": ["at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces. Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances. Experiments. For evaluation we used two", "These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens \u2013 two orders of magnitude more than any previously published work. Our work is a first step towards creating and deploying large-scale multimodal systems that properly situate themselves into a given context, by taking full advantage of every available signal.", "the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features. Data and Experimental Setup. Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 ,", "with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances. Experiments. For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table . Conclusion. We present a simple strategy to augment a standard recurrent neural network language model with temporal visual features. Through an exploration of candidate architectures, we show that the Middle Fusion of visual and textual features leads to a 20-28% reduction in perplexity relative to a text only baseline. These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens \u2013 two orders of magnitude more than any previously published work. Our work is a first step", "Introduction.  INLINEFORM0 Work performed while the author was an intern at Google. Language models are vital components of a wide variety of systems for Natural Language Processing (NLP) including Automatic Speech Recognition, Machine Translation, Optical Character Recognition, Spelling Correction, etc. However, most language models are trained and applied in a manner that is oblivious to the environment in which human language operates BIBREF0 . These models are typically trained only on sequences of words, ignoring the physical context in which the symbolic representations are grounded, or ignoring the social context that could inform the semantics of an utterance. For incorporating additional modalities, the NLP community has typically used datasets such as MS COCO BIBREF1 and Flickr BIBREF2 for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started", "integration of the two modalities: In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1  where INLINEFORM0 are learned matrices. Here, we apply the intuition that some words could provide information as to whether or not the visual context is helpful. In a simplistic example, if the word history is the article \u201cthe,\" then the visual context could provide relevant information needed for predicting the next word. For other word histories, though, the visual context might not be needed or be even irrelevant for the next word prediction: if the previous word is \u201ccarpe\", the next word is very likely to be \u201cdiem\", regardless of visual context. We implement a simple weighting mechanism that learns a scalar weight for the visual embedding prior to concatenation with the word embedding. The input to the RNNLM is now INLINEFORM0 , where: INLINEFORM1  This approach does not add any new parameters to the model, but since the word", "the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs. The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5\u20136% on three languages on the MS COCO dataset (with an English vocabulary of only 16K words). Our work is distinguishable from previous work with respect to three dimensions: Model. A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1  where each word is assigned a probability given the previous word history. For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames:", "for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started investigating language grounding in images BIBREF8 , BIBREF9 , BIBREF10 and to lesser extent in videos BIBREF11 , BIBREF1 . However, language grounding has focused more on obtaining better word and sentence representations or other downstream tasks, and to lesser extent on language modeling. In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs. The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5\u20136% on three"]}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "predicted_answer": "1500-dimensional vectors", "predicted_evidence": ["the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames: INLINEFORM4  Combining the text and video modalities. There are several options for combining the text and video modalities. We opt for the simplest strategy, which concatenates the representations. For a word embedding INLINEFORM0 and corresponding visual representation INLINEFORM1 , the input to our RNNLM will be the concatenated vector INLINEFORM2 . For the examples where we were unable to compute visual features (see Section \u00a7 SECREF3 ), we set INLINEFORM3 to be a zero-vector. In addition to concatenating the word and visual embedding, we explore two variants of our model that allow for a finer-grained integration of the two modalities: In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1  where INLINEFORM0 are learned", "at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces. Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances. Experiments. For evaluation we used two", "the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features. Data and Experimental Setup. Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 ,", "with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances. Experiments. For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table . Conclusion. We present a simple strategy to augment a standard recurrent neural network language model with temporal visual features. Through an exploration of candidate architectures, we show that the Middle Fusion of visual and textual features leads to a 20-28% reduction in perplexity relative to a text only baseline. These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens \u2013 two orders of magnitude more than any previously published work. Our work is a first step", "for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started investigating language grounding in images BIBREF8 , BIBREF9 , BIBREF10 and to lesser extent in videos BIBREF11 , BIBREF1 . However, language grounding has focused more on obtaining better word and sentence representations or other downstream tasks, and to lesser extent on language modeling. In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs. The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5\u20136% on three", "the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs. The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5\u20136% on three languages on the MS COCO dataset (with an English vocabulary of only 16K words). Our work is distinguishable from previous work with respect to three dimensions: Model. A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1  where each word is assigned a probability given the previous word history. For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames:", "prior to concatenation with the word embedding. The input to the RNNLM is now INLINEFORM0 , where: INLINEFORM1  This approach does not add any new parameters to the model, but since the word representations INLINEFORM0 are learned, this mechanism has the potential to learn word embeddings that are also appropriate for weighting the visual context. Location of combination. We explore three locations for fusing visual features in an RNNLM (Figure ). Our Early Fusion strategy merges the text and the visual features at the input to the LSTM cells. This embodies the intuition that it is best to do feature combination at the earliest possible stage. The Middle Fusion merges the visual features at the output of the 1st LSTM layer while the Late Fusion strategies merges the two features after the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the", "integration of the two modalities: In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1  where INLINEFORM0 are learned matrices. Here, we apply the intuition that some words could provide information as to whether or not the visual context is helpful. In a simplistic example, if the word history is the article \u201cthe,\" then the visual context could provide relevant information needed for predicting the next word. For other word histories, though, the visual context might not be needed or be even irrelevant for the next word prediction: if the previous word is \u201ccarpe\", the next word is very likely to be \u201cdiem\", regardless of visual context. We implement a simple weighting mechanism that learns a scalar weight for the visual embedding prior to concatenation with the word embedding. The input to the RNNLM is now INLINEFORM0 , where: INLINEFORM1  This approach does not add any new parameters to the model, but since the word"]}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "predicted_answer": "Yes, the model described in Document 1 uses attention mechanism.", "predicted_evidence": ["BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism. After a small grid search we decided to inherit most of the hyperparameters of the model from the best results achieved in BIBREF3 where -to-Mizar translation is learned. We used relatively small LSTM cells consisting of 2 layers with 128 units. The \u201cscaled Luong\u201d version of the attention mechanism was used, as well as dropout with rate equal $0.2$. The number of training steps was 10000. (This setting was used for all our experiments described below.) Experiments ::: AIM data set. First, NMT models were trained for each of the 20 rewrite rules in the AIM data set. It turned out that the models, as long as the number of examples was greater than 1000, were able to learn the rewriting task very well, reaching $90\\%$ of accuracy on separated test sets. This means that the task of applying single rewrite step seems relatively easy to learn by NMT. See Table TABREF11 for all the results. We", "our question we prepared two data sets \u2013 the first consists of examples extracted from proofs found by ATP (automated theorem prover) in a mathematical domain (AIM loops), whereas the second is a synthetic set of polynomial terms. Data ::: The AIM data set. The data consists of sets of ground and nonground rewrites that came from Prover9 proofs of theorems about AIM loops produced by Veroff BIBREF8. Many of the inferences in the proofs are paramodulations from an equation and have the form s = t u[(s)] = vu[(t)] = v where $s, t, u, v$ are terms and $\\theta $ is a substitution. For the most common equations $s = t$, we gathered corresponding pairs of terms $\\big (u[\\theta (s)], u[\\theta (t)]\\big )$ which were rewritten from one to another with $s = t$. We put the pairs to separate data sets (depending on the corresponding $s = t$): in total 8 data sets for ground rewrites (where $\\theta $ is trivial) and 12 for nonground ones. The goal will be to learn rewriting for each of this 20", "sets (depending on the corresponding $s = t$): in total 8 data sets for ground rewrites (where $\\theta $ is trivial) and 12 for nonground ones. The goal will be to learn rewriting for each of this 20 rules separately. Terms in the examples are treated as linear sequences of tokens where tokens are single symbols (variable / costant / predicate names, brackets, commas). Numbers of examples in each of the data sets vary between 251 and 34101. Lengths of the sequences of tokens vary between 1 and 343, with mean around 35. These 20 data sets were split into training, validation and test sets for our experiments ($60 \\%, 10 \\%, 30 \\%$, respectively). In Table TABREF4 and Table TABREF5 there are presented examples of pairs of AIM terms in TPTP BIBREF9 format, before and after rewriting with, respectively, ground and nonground rewrite rules. Data ::: The polynomial data set. This is a synthetically created data set where the examples are pairs of equivalent polynomial terms. The first", "25 symbols. Terms longer than 50 are filtered out. Several data sets of various difficulty were created by varying the number of available symbols. This were quite limited \u2013 at most 5 different variables and constants being a few first natural numbers. The reason for this limited complexity of the input terms is because normalizing even a relatively simple polynomial can result in a very long term with very large constants \u2013 which is related especially to the operation of exponentiation in polynomials. Each data set consists of different 300 000 examples, see Table TABREF7 for examples. These data sets were split into training, validation and test sets for our experiments ($60 \\%, 10 \\%, 30 \\%$, respectively). Experiments. For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism. After a small grid search we decided to inherit most of the hyperparameters of the model from", "well, reaching $90\\%$ of accuracy on separated test sets. This means that the task of applying single rewrite step seems relatively easy to learn by NMT. See Table TABREF11 for all the results. We also run an experiment on the joint set of all rewrite rules (consisting of 41396 examples). Here the task was more difficult as a model needed not only to apply rewriting correctly, but also choose \u201cthe right\u201d rewrite rule applicable for a given term. Nevertheless, the performance was also very good, reaching $83\\%$ of accuracy. Experiments ::: Polynomial data set. Then experiments on more challenging but also much larger data sets for polynomial normalization were performed. Depending on the difficulty of the data, accuracy on the test sets achieved in our experiments varied between $70\\%$ and $99\\%$. The results in terms of accuracy are shown in Table TABREF13. This high performance of the model encouraged a closer inspection of the results. First, we checked if in the test sets there are", "this work can be extended. Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are relatively simple and with no direct relevance to the real unsolved symbolic problems. But the results on these simple problems are encouraging enough to try with more challenging ones, related to real difficulties \u2013 e.g. these from TPDB data base. Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact", "task. Thirdly, there is an idea of integrating neural rewriting architectures into the larger systems for automated reasoning. This can be motivated by the interesting contrast between some simpler ILP systems suffering for combinatorial explosion in presence of a large number of examples and neural methods which definitely benefit form large data sets. We hope that this work will inspire and trigger a discussion on the above (and other) ideas. Acknowledgements. Piotrowski was supported by the grant of National Science Center, Poland, no. 2018/29/N/ST6/02903, and by the European Agency COST action CA15123. Urban and Brown were supported by the ERC Consolidator grant no. 649043 AI4REASON and by the Czech project AI&Reasoning CZ.02.1.01/0.0/0.0/15_003/0000466 and the European Regional Development Fund. Kaliszyk was supported by ERC Starting grant no. 714034 SMART.", "$99\\%$. The results in terms of accuracy are shown in Table TABREF13. This high performance of the model encouraged a closer inspection of the results. First, we checked if in the test sets there are input examples which differs from these in training sets only by renaming of variables. Indeed, for each of the data sets in test sets are $5 - 15 \\%$ of such \u201crenamed\u201d examples. After filtering them out the measured accuracy drops \u2013 but only by $1 - 2 \\%$. An examination of the examples wrongly rewritten by the model was done. It turns out that the wrong outputs almost always parse (in $97 - 99 \\%$ of cases they are legal polynomial terms). Notably, depending on the difficulty of the data set, as much as $18 - 64 \\%$ of incorrect outputs are wrong only with respect to the constants in the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming. Conclusions and future work. NMT is not"]}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "predicted_answer": "Neural machine translation (NMT) models are explored, including their potential for use in the formal-to-formal setting and for learning symbolic rewriting and normalizing polynomials.", "predicted_evidence": ["Introduction. Neural networks (NNs) turned out to be very useful in several domains. In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting? In particular: Can", "this work can be extended. Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are relatively simple and with no direct relevance to the real unsolved symbolic problems. But the results on these simple problems are encouraging enough to try with more challenging ones, related to real difficulties \u2013 e.g. these from TPDB data base. Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact", "BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism. After a small grid search we decided to inherit most of the hyperparameters of the model from the best results achieved in BIBREF3 where -to-Mizar translation is learned. We used relatively small LSTM cells consisting of 2 layers with 128 units. The \u201cscaled Luong\u201d version of the attention mechanism was used, as well as dropout with rate equal $0.2$. The number of training steps was 10000. (This setting was used for all our experiments described below.) Experiments ::: AIM data set. First, NMT models were trained for each of the 20 rewrite rules in the AIM data set. It turned out that the models, as long as the number of examples was greater than 1000, were able to learn the rewriting task very well, reaching $90\\%$ of accuracy on separated test sets. This means that the task of applying single rewrite step seems relatively easy to learn by NMT. See Table TABREF11 for all the results. We", "$99\\%$. The results in terms of accuracy are shown in Table TABREF13. This high performance of the model encouraged a closer inspection of the results. First, we checked if in the test sets there are input examples which differs from these in training sets only by renaming of variables. Indeed, for each of the data sets in test sets are $5 - 15 \\%$ of such \u201crenamed\u201d examples. After filtering them out the measured accuracy drops \u2013 but only by $1 - 2 \\%$. An examination of the examples wrongly rewritten by the model was done. It turns out that the wrong outputs almost always parse (in $97 - 99 \\%$ of cases they are legal polynomial terms). Notably, depending on the difficulty of the data set, as much as $18 - 64 \\%$ of incorrect outputs are wrong only with respect to the constants in the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming. Conclusions and future work. NMT is not", "the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming. Conclusions and future work. NMT is not typically applied to symbolic problems, but surprisingly, it performed very well for both described tasks. The first one was easier in terms of complexity of the rewriting (only one application of a rewrite rule was performed) but the number of examples was quite limited. The second task involved more difficult rewriting \u2013 multiple different rewrite steps were performed to construct the examples. Nevertheless, provided many examples, NMT could learn normalizing polynomials. We hope this work provides a baseline and inspiration for continuing this line of research. We see several interesting directions this work can be extended. Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are", "well, reaching $90\\%$ of accuracy on separated test sets. This means that the task of applying single rewrite step seems relatively easy to learn by NMT. See Table TABREF11 for all the results. We also run an experiment on the joint set of all rewrite rules (consisting of 41396 examples). Here the task was more difficult as a model needed not only to apply rewriting correctly, but also choose \u201cthe right\u201d rewrite rule applicable for a given term. Nevertheless, the performance was also very good, reaching $83\\%$ of accuracy. Experiments ::: Polynomial data set. Then experiments on more challenging but also much larger data sets for polynomial normalization were performed. Depending on the difficulty of the data, accuracy on the test sets achieved in our experiments varied between $70\\%$ and $99\\%$. The results in terms of accuracy are shown in Table TABREF13. This high performance of the model encouraged a closer inspection of the results. First, we checked if in the test sets there are", "that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting? In particular: Can NMT models learn symbolic rewriting? The answer is relevant to various tasks in automated reasoning. For example, neural models could compete with symbolic methods such as inductive logic programming BIBREF5 (ILP) that have been previously experimented with to learn simple rewrite tasks and theorem-proving heuristics from large formal corpora BIBREF6. Unlike (early) ILP, neural methods can however easily cope with large and rich datasets, without combinatorial explosion. Our work is also an inquiry into the capabilities of NNs as such, in the spirit of works like BIBREF7. Data. To perform experiments answering our question we prepared two data sets \u2013 the first consists of examples extracted from proofs found by ATP (automated theorem prover) in a mathematical domain (AIM loops), whereas the second", "25 symbols. Terms longer than 50 are filtered out. Several data sets of various difficulty were created by varying the number of available symbols. This were quite limited \u2013 at most 5 different variables and constants being a few first natural numbers. The reason for this limited complexity of the input terms is because normalizing even a relatively simple polynomial can result in a very long term with very large constants \u2013 which is related especially to the operation of exponentiation in polynomials. Each data set consists of different 300 000 examples, see Table TABREF7 for examples. These data sets were split into training, validation and test sets for our experiments ($60 \\%, 10 \\%, 30 \\%$, respectively). Experiments. For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism. After a small grid search we decided to inherit most of the hyperparameters of the model from"]}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "predicted_answer": "the process of transforming symbolic expressions into other symbolic expressions using rewriting rules", "predicted_evidence": ["this work can be extended. Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are relatively simple and with no direct relevance to the real unsolved symbolic problems. But the results on these simple problems are encouraging enough to try with more challenging ones, related to real difficulties \u2013 e.g. these from TPDB data base. Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact", "the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming. Conclusions and future work. NMT is not typically applied to symbolic problems, but surprisingly, it performed very well for both described tasks. The first one was easier in terms of complexity of the rewriting (only one application of a rewrite rule was performed) but the number of examples was quite limited. The second task involved more difficult rewriting \u2013 multiple different rewrite steps were performed to construct the examples. Nevertheless, provided many examples, NMT could learn normalizing polynomials. We hope this work provides a baseline and inspiration for continuing this line of research. We see several interesting directions this work can be extended. Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are", "that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting? In particular: Can NMT models learn symbolic rewriting? The answer is relevant to various tasks in automated reasoning. For example, neural models could compete with symbolic methods such as inductive logic programming BIBREF5 (ILP) that have been previously experimented with to learn simple rewrite tasks and theorem-proving heuristics from large formal corpora BIBREF6. Unlike (early) ILP, neural methods can however easily cope with large and rich datasets, without combinatorial explosion. Our work is also an inquiry into the capabilities of NNs as such, in the spirit of works like BIBREF7. Data. To perform experiments answering our question we prepared two data sets \u2013 the first consists of examples extracted from proofs found by ATP (automated theorem prover) in a mathematical domain (AIM loops), whereas the second", "the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact structure of the expression, which in case of formal logic is always unambiguous and easy to extract. Perhaps this way the model could learn more efficiently from examples (and achieve higher results even on the small AIM data sets). The authors have a positive experience of applying TreeNNs to learn remainders of arithmetical expressions modulo small natural numbers \u2013 TreeNNs outperformed here neural models based on LSTM cells, giving almost perfect accuracy. However, this is unclear how to translate this TreeNN methodology to the tasks with the structured output, like the symbolic rewriting task. Thirdly, there is an idea of integrating neural rewriting architectures into the larger systems for automated reasoning. This can be motivated by the interesting contrast between some simpler", "sets (depending on the corresponding $s = t$): in total 8 data sets for ground rewrites (where $\\theta $ is trivial) and 12 for nonground ones. The goal will be to learn rewriting for each of this 20 rules separately. Terms in the examples are treated as linear sequences of tokens where tokens are single symbols (variable / costant / predicate names, brackets, commas). Numbers of examples in each of the data sets vary between 251 and 34101. Lengths of the sequences of tokens vary between 1 and 343, with mean around 35. These 20 data sets were split into training, validation and test sets for our experiments ($60 \\%, 10 \\%, 30 \\%$, respectively). In Table TABREF4 and Table TABREF5 there are presented examples of pairs of AIM terms in TPTP BIBREF9 format, before and after rewriting with, respectively, ground and nonground rewrite rules. Data ::: The polynomial data set. This is a synthetically created data set where the examples are pairs of equivalent polynomial terms. The first", "our question we prepared two data sets \u2013 the first consists of examples extracted from proofs found by ATP (automated theorem prover) in a mathematical domain (AIM loops), whereas the second is a synthetic set of polynomial terms. Data ::: The AIM data set. The data consists of sets of ground and nonground rewrites that came from Prover9 proofs of theorems about AIM loops produced by Veroff BIBREF8. Many of the inferences in the proofs are paramodulations from an equation and have the form s = t u[(s)] = vu[(t)] = v where $s, t, u, v$ are terms and $\\theta $ is a substitution. For the most common equations $s = t$, we gathered corresponding pairs of terms $\\big (u[\\theta (s)], u[\\theta (t)]\\big )$ which were rewritten from one to another with $s = t$. We put the pairs to separate data sets (depending on the corresponding $s = t$): in total 8 data sets for ground rewrites (where $\\theta $ is trivial) and 12 for nonground ones. The goal will be to learn rewriting for each of this 20", "respectively, ground and nonground rewrite rules. Data ::: The polynomial data set. This is a synthetically created data set where the examples are pairs of equivalent polynomial terms. The first element of each pair is a polynomial in an arbitrary form and the second element is the same polynomial in a normalized form. The arbitrary polynomials are created randomly in a recursive manner from a set of available (non-nullary) function symbols, variables and constants. First, one of the symbols is randomly chosen. If it is a constant or a variable it is returned and the process terminates. If a function symbol is chosen, its subterm(s) are constructed recursively in a similar way. The parameters of this process are set in such a way that it creates polynomial terms of average length around 25 symbols. Terms longer than 50 are filtered out. Several data sets of various difficulty were created by varying the number of available symbols. This were quite limited \u2013 at most 5 different", "Introduction. Neural networks (NNs) turned out to be very useful in several domains. In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting? In particular: Can"]}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "predicted_answer": "By specifying \"anchor terms\", which are words having high mutual information with one or more latent factors and chosen based on domain knowledge.", "predicted_evidence": ["theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx. We present preliminary experimental results on two text corpora (including a corpus of clinical notes), showing that anchors can be used to discover topics that are more specific and relevant. What is more, we demonstrate the potential for this framework to perform weakly supervised learning in settings where labeling documents is prohibitively expensive BIBREF5 , BIBREF6 . With respect to interpretable machine learning, our contributions are twofold. First, our framework provides a way for human users to share domain knowledge with a statistical learning algorithm that is both convenient for the human user and easily digestible by the machine. Second, our experimental", "Further, developing and testing such systems is time- and labor-intensive. We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics. In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain", "is a large body of work on integrating domain knowledge into topic models and other unsupervised latent variable models, often in the form of constraints BIBREF13 , prior distributions BIBREF14 , and token labels BIBREF15 . Like Anchored CorEx, seeded latent dirichlet allocation (SeededLDA) allows the specification of word-topic relationships BIBREF16 . However, SeededLDA assumes a more complex latent structure, in which each topic is a mixture of two distributions, one unseeded and one seeded.  BIBREF1 first proposed anchors in the context of topic modeling: words that are high precision indicators of underlying topics. In contrast to our approach, anchors are typically selected automatically, constrained to appear in only one topic, and used primarily to aid optimization BIBREF17 . In our information theoretic framework, anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors. The effects of anchors on the", "anchors to help learn and impose semantics on a discrete latent factor model with a directed acyclic graph structure. We utilize an information theoretic approach that makes no generative modeling assumptions. Results and Discussion. To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics. 20 Newsgroups. The 20 Newsgroups data set is suitable for a straightforward evaluation of anchored topic models. The latent classes represent mutually exclusive categories, and each document is known to originate from a single category. We find that the correlation structure among the latent classes is less complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from", "a separate classifier for each disease. Surprisingly, Anchored CorEx outperforms Naive Bayes (NB) by a large margin. Of course, Anchored CorEx is not a replacement for supervised learning: NB beats Anchored CorEx on 20 Newsgroups and does not represent a \u201cstrong\u201d baseline for Obesity 2008 (teams scored above 0.7 in Macro-F1 during the competition). It is nonetheless remarkable that Anchored CorEx performs as well as it does given that it is fundamentally unsupervised. Conclusion. We have introduced a simple information theoretic approach to topic modeling that can leverage domain knowledge specified informally as anchors. Our framework uses a novel combination of CorEx and the information bottleneck. Preliminary results suggest it can extract more precise, interpretable topics through a lightweight interactive process. We next plan to perform further empirical evaluations and to extend the algorithm to handle complex latent structures present in health care data. Acknowledgements.", "complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from BIBREF1 ). To prepare the data, we removed headers, footers, and quotes and reduced the vocabulary to the most frequent 20,000 words. Each document was represented as a binary bag-of-words vector. In all experiemnts, we used the standard training/test split. All CorEx models used three layers of 40, 3, and 1 factors. fig:big shows an example hierarchical topic model extracted by Anchored CorEx. i2b2 Obesity Challenge 2008. The Obesity Challenge 2008 data set includes 1237 deidentified clinical discharge summaries from the Partners HealthCare Research Patient Data Repository. All summaries have been labeled by clinical experts with obesity and 15 other conditions commonly comorbid with obesity, ranging from Coronary Artery Disease (663 positives) to Depression (247) to Hypertriglyceridemia", "Introduction. A clinician can look at a patient's electronic health record (EHR) and not only decide whether the patient has diabetes but also produce a succinct summary of the clinical evidence. Replicating this feat with computational tools has been the focus of much research in clinical informatics. There are major initiatives underway to codify clinical knowledge into formal representations, most often as deterministic rules that can be applied in a semi-automated fashion BIBREF0 . However, representing the intuitive judgments of human experts can be challenging, particularly when the formal system does not match the expert's knowledge. For example, many deterministic disease classifiers used in clinical informatics rely heavily upon administrative codes not available at time of diagnosis. Further, developing and testing such systems is time- and labor-intensive. We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it", "In our information theoretic framework, anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors. The effects of anchors on the interpretability of traditional topic models are often mixed BIBREF18 , but our experiments suggest that our approach yields more coherent topics. In health informatics, \u201canchor\u201d features chosen based on domain knowledge have been used to guide statistical learning BIBREF2 . In BIBREF6 , anchors are used as a source of distant supervision BIBREF19 , BIBREF20 for classifiers in the absence of ground truth labels. While Anchored CorEx can be used for discriminative tasks, it is essentially unsupervised. Recent work by BIBREF21 is perhaps most similar in spirit to ours: they exploit predefined anchors to help learn and impose semantics on a discrete latent factor model with a directed acyclic graph structure. We utilize an information theoretic approach that makes no generative modeling"]}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "predicted_answer": "20 Newsgroups and the i2b2 2008 Obesity Challenge data set.", "predicted_evidence": ["theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx. We present preliminary experimental results on two text corpora (including a corpus of clinical notes), showing that anchors can be used to discover topics that are more specific and relevant. What is more, we demonstrate the potential for this framework to perform weakly supervised learning in settings where labeling documents is prohibitively expensive BIBREF5 , BIBREF6 . With respect to interpretable machine learning, our contributions are twofold. First, our framework provides a way for human users to share domain knowledge with a statistical learning algorithm that is both convenient for the human user and easily digestible by the machine. Second, our experimental", "anchors to help learn and impose semantics on a discrete latent factor model with a directed acyclic graph structure. We utilize an information theoretic approach that makes no generative modeling assumptions. Results and Discussion. To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics. 20 Newsgroups. The 20 Newsgroups data set is suitable for a straightforward evaluation of anchored topic models. The latent classes represent mutually exclusive categories, and each document is known to originate from a single category. We find that the correlation structure among the latent classes is less complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from", "is a large body of work on integrating domain knowledge into topic models and other unsupervised latent variable models, often in the form of constraints BIBREF13 , prior distributions BIBREF14 , and token labels BIBREF15 . Like Anchored CorEx, seeded latent dirichlet allocation (SeededLDA) allows the specification of word-topic relationships BIBREF16 . However, SeededLDA assumes a more complex latent structure, in which each topic is a mixture of two distributions, one unseeded and one seeded.  BIBREF1 first proposed anchors in the context of topic modeling: words that are high precision indicators of underlying topics. In contrast to our approach, anchors are typically selected automatically, constrained to appear in only one topic, and used primarily to aid optimization BIBREF17 . In our information theoretic framework, anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors. The effects of anchors on the", "complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from BIBREF1 ). To prepare the data, we removed headers, footers, and quotes and reduced the vocabulary to the most frequent 20,000 words. Each document was represented as a binary bag-of-words vector. In all experiemnts, we used the standard training/test split. All CorEx models used three layers of 40, 3, and 1 factors. fig:big shows an example hierarchical topic model extracted by Anchored CorEx. i2b2 Obesity Challenge 2008. The Obesity Challenge 2008 data set includes 1237 deidentified clinical discharge summaries from the Partners HealthCare Research Patient Data Repository. All summaries have been labeled by clinical experts with obesity and 15 other conditions commonly comorbid with obesity, ranging from Coronary Artery Disease (663 positives) to Depression (247) to Hypertriglyceridemia", "a way for human users to share domain knowledge with a statistical learning algorithm that is both convenient for the human user and easily digestible by the machine. Second, our experimental results confirm that the introduction of simple anchor words can improve the coherence and human interpretability of topics discovered from data. Both are essential to successful and interactive collaboration between machine learning and human users. Methods. Anchored Correlation Explanation can be understood as a combination of Total Correlation Explanation (CorEx) BIBREF3 , BIBREF7 and the multivariate information bottleneck BIBREF4 , BIBREF8 . We search for a set of probabilistic functions of the inputs INLINEFORM0 for INLINEFORM1 that optimize the following information theoretic objective: INLINEFORM2  The first term is the CorEx objective INLINEFORM0 , which aims to construct latent variables INLINEFORM1 that best explain multivariate dependencies in the data INLINEFORM2 . Here the data", "and does not need to be tuned. Anchors allow us to both seed CorEx and impose semantics on latent factors: when analyzing medical documents, for example, we can anchor a diabetes latent factor to the word \u201cdiabetes.\u201d The INLINEFORM0 objective then discovers other words associated with \u201cdiabetes\u201d and includes them in this topic. While there is not space here for a full description of the optimization, it is similar in principle to the approaches in BIBREF3 , BIBREF7 . Two points are worth noting: first, the TC objective is replaced by a lower bound to make optimization feasible BIBREF7 . Second, we impose a sparse connection constraint (each word appears in only one topic) to speed up computation. Open source code implementing CorEx is available on github BIBREF12 . Related Work. There is a large body of work on integrating domain knowledge into topic models and other unsupervised latent variable models, often in the form of constraints BIBREF13 , prior distributions BIBREF14 , and", "a separate classifier for each disease. Surprisingly, Anchored CorEx outperforms Naive Bayes (NB) by a large margin. Of course, Anchored CorEx is not a replacement for supervised learning: NB beats Anchored CorEx on 20 Newsgroups and does not represent a \u201cstrong\u201d baseline for Obesity 2008 (teams scored above 0.7 in Macro-F1 during the competition). It is nonetheless remarkable that Anchored CorEx performs as well as it does given that it is fundamentally unsupervised. Conclusion. We have introduced a simple information theoretic approach to topic modeling that can leverage domain knowledge specified informally as anchors. Our framework uses a novel combination of CorEx and the information bottleneck. Preliminary results suggest it can extract more precise, interpretable topics through a lightweight interactive process. We next plan to perform further empirical evaluations and to extend the algorithm to handle complex latent structures present in health care data. Acknowledgements.", "Further, developing and testing such systems is time- and labor-intensive. We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics. In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain"]}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "predicted_answer": "Unanswerable", "predicted_evidence": ["anchors to help learn and impose semantics on a discrete latent factor model with a directed acyclic graph structure. We utilize an information theoretic approach that makes no generative modeling assumptions. Results and Discussion. To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics. 20 Newsgroups. The 20 Newsgroups data set is suitable for a straightforward evaluation of anchored topic models. The latent classes represent mutually exclusive categories, and each document is known to originate from a single category. We find that the correlation structure among the latent classes is less complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from", "Unsupervised and Anchored CorEx on the soc.religion.christianity category from 20 Newsgroups for different choices of anchors. For both types of CorEx, the topic containing the corresponding terms is used as the classifier, but for Anchored CorEx those terms are also used as anchors when estimating the latent factor. Unsupervised CorEx does a reasonable job of discovering a coherent religion topic that already contains the terms God, Christian, and Jesus. However, using the terms Jesus and Christian as anchors yields a topic that better predicts the actual soc.religion.christianity category.  tab:obesity:class shows the Macro-AUC and F1 scores (averaged across all diseases) on the Obesity Challenge data for the final anchored CorEx model and a Naive Bayes (NB) baseline, in which we train a separate classifier for each disease. Surprisingly, Anchored CorEx outperforms Naive Bayes (NB) by a large margin. Of course, Anchored CorEx is not a replacement for supervised learning: NB beats", "a separate classifier for each disease. Surprisingly, Anchored CorEx outperforms Naive Bayes (NB) by a large margin. Of course, Anchored CorEx is not a replacement for supervised learning: NB beats Anchored CorEx on 20 Newsgroups and does not represent a \u201cstrong\u201d baseline for Obesity 2008 (teams scored above 0.7 in Macro-F1 during the competition). It is nonetheless remarkable that Anchored CorEx performs as well as it does given that it is fundamentally unsupervised. Conclusion. We have introduced a simple information theoretic approach to topic modeling that can leverage domain knowledge specified informally as anchors. Our framework uses a novel combination of CorEx and the information bottleneck. Preliminary results suggest it can extract more precise, interpretable topics through a lightweight interactive process. We next plan to perform further empirical evaluations and to extend the algorithm to handle complex latent structures present in health care data. Acknowledgements.", "Further, developing and testing such systems is time- and labor-intensive. We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics. In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain", "theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx. We present preliminary experimental results on two text corpora (including a corpus of clinical notes), showing that anchors can be used to discover topics that are more specific and relevant. What is more, we demonstrate the potential for this framework to perform weakly supervised learning in settings where labeling documents is prohibitively expensive BIBREF5 , BIBREF6 . With respect to interpretable machine learning, our contributions are twofold. First, our framework provides a way for human users to share domain knowledge with a statistical learning algorithm that is both convenient for the human user and easily digestible by the machine. Second, our experimental", "complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from BIBREF1 ). To prepare the data, we removed headers, footers, and quotes and reduced the vocabulary to the most frequent 20,000 words. Each document was represented as a binary bag-of-words vector. In all experiemnts, we used the standard training/test split. All CorEx models used three layers of 40, 3, and 1 factors. fig:big shows an example hierarchical topic model extracted by Anchored CorEx. i2b2 Obesity Challenge 2008. The Obesity Challenge 2008 data set includes 1237 deidentified clinical discharge summaries from the Partners HealthCare Research Patient Data Repository. All summaries have been labeled by clinical experts with obesity and 15 other conditions commonly comorbid with obesity, ranging from Coronary Artery Disease (663 positives) to Depression (247) to Hypertriglyceridemia", "of CorEx models with 32 latent topics in the first layer, each using a different anchor strategy. tab:obesity:topics shows the Obesity and Obstructive Sleep Apnea (OSA) topics for three iterations of Anchored CorEx with the ten most important terms (highest weighted connections to the latent factor) listed for each topic. Unsupervised CorEx (first row) does not discover any topics obviously related to obesity or OSA, so we choose the topics to which the terms obesity and obstructive sleep apnea are assigned. No unambiguous Obesity or OSA topics emerge even as the number of latent factors is decreased or increased. In the second iteration (second row), we add the common name of each of the 16 diseases as an anchor to one factor (16 total). Adding obesity as an anchor produces a clear Obesity topic, including several medications known to cause weight gain (e.g., acebutolol, klonopin). The anchored OSA topic, however, is quite poor and in fact resembles the rather generic topic to which", "to explain OSA terms. In the third iteration, we correct this by adding sleep apnea as a second anchor to the OSA topic, and the resulting topic is clearly associated with OSA, including terms related to respiratory problems and medications used to treat (or believed to increase risk for) OSA. There is no noticeable reduction in quality in the Obesity topic. Anchored CorEx for Discriminative Tasks. In a series of follow-up experiments, we investigate the suitability of using anchored CorEx to perform weakly supervised classification. We interpret each anchored latent factor as a classifier for an associated class label and then compute test set F1 (using a threshold of 0.5) and area under the curve (AUC) scores (Obesity Challenge only). tab:class compares the classification performance of Unsupervised and Anchored CorEx on the soc.religion.christianity category from 20 Newsgroups for different choices of anchors. For both types of CorEx, the topic containing the corresponding terms is"]}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "predicted_answer": "Mean Average Precision (MAP) is reported, but the F-Score obtained depends on the specific part of the documents, so there is not a single answer that encompasses all of them.", "predicted_evidence": ["process to optimize the F-Score of training examples. Our new structured margin loss can be described as: DISPLAYFORM0  where INLINEFORM0 is the F-Score between corrected label sequence and predicted label sequence. F-Score and Label Accuracy Trigger Function The F-Score can be quite unstable in some situation. For instance, if there is no named entity in a sentence, F-Score will be always 0 regardless of the predicted label sequence. To take advantage of meaningful information provided by label accuracy, we introduce an integrated trigger function as follows: DISPLAYFORM0  where INLINEFORM0 is a factor to adjust the weight of label accuracy and F-Score. Because F-Score depends on the whole label sequence, we use beam search to find INLINEFORM0 label sequences with top sentece-level score INLINEFORM1 and then use trigger function to rerank the INLINEFORM2 label sequences and select the best. Word Segmentation Representation. Word segmentation takes an important part in Chinese text", "a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The results of models are depicted as Figure UID11 . From the figure, we can know our models perfrom better with little loss in time. Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated", "( EQREF9 ): INLINEFORM4  In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The margin loss INLINEFORM2 serves as a trigger function to conduct the training process of B-LSTM based MMNN. We can introduce a new trigger function to guide the training process of neural network. F-Score Trigger Function The main criterion of NER task is F-score. However, high label accuracy does not mean high F-score. For instance, if every named entity's last character is labeledas O, the label accuracy can be quite high, but the precision, recall and F-score are 0. We use the F-Score between corrected label sequence and predicted label sequence as trigger function, which can conduct the training process to optimize the F-Score of training examples. Our new structured margin loss can be described as: DISPLAYFORM0  where INLINEFORM0 is the F-Score between corrected label sequence and predicted", "The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention. To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media. Conclusions and Future Work. The results of our experiments also suggest directions for future work. We can observe all models in Table TABREF23 achieve a much lower recall than precision BIBREF25 . So we need to design some methods to solve the problem. Acknowledgements. Thanks to Shuming Ma for the help on improving the writing. This work was supported in part by", ". However, the two above approaches are implemented within CRF model. We construct a semi-supervised model based on B-LSTM neural network to learn from the limited labelled corpus by using lexical information provided by massive unlabeled text. To shrink the gap between label accuracy and F-Score, we propose a method to directly train on F-Score rather than label accuracy in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. Specifically, we make contributions as follows: Model. We construct a semi-supervised model which is based on B-LSTM neural network and combine transition probability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. Transition Probability. B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 . However,", "obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0  By minimizing the object, we can increase the score of correct label sequence INLINEFORM0 and decrease the score of incorrect label sequence INLINEFORM1 . F-Score Driven Training Method. Max Margin training method use structured margin loss INLINEFORM0 to describe the difference between the corrected label sequence INLINEFORM1 and predicted label sequence INLINEFORM2 . In fact, the structured margin loss INLINEFORM3 reflect the loss in label accuracy. Considering the gap between label accuracy and F-Score in NER, we introduce a new training method to train directly on F-Score. To introduce F-Score driven training method, we need to take a look at the subgradient of equation ( EQREF9 ): INLINEFORM4  In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The", "with a label sequence INLINEFORM4 , a sentence-level score is then given as: DISPLAYFORM0  where INLINEFORM0 indicates the probability of label INLINEFORM1 at position INLINEFORM2 by the network with parameters INLINEFORM3 , INLINEFORM4 indicates the matrix of transition probability. In our model, INLINEFORM5 is computed as: DISPLAYFORM0  We define a structured margin loss INLINEFORM0 as Pei et al. pei-ge-chang:2014:P14-1: DISPLAYFORM0  where INLINEFORM0 is the length of setence INLINEFORM1 , INLINEFORM2 is a discount parameter, INLINEFORM3 a given correct label sequence and INLINEFORM4 a predicted label sequence. For a given training instance INLINEFORM5 , our predicted label sequence is the label sequence with highest score: INLINEFORM6  The label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0  By minimizing the object, we can increase the score of correct label sequence", "and label accuracy. Transition Probability. B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 . However, B-LSTM cannot learn sentence level label information. Huang et al. huang2015bidirectional combine CRF to use sentence level label information. We combine transition probability into our model to gain sentence level label information. To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) BIBREF19 based on B-LSTM. The prediction of label in position INLINEFORM0 is given as: DISPLAYFORM0  where INLINEFORM0 are the transformation parameters, INLINEFORM1 the hidden vector and INLINEFORM2 the bias parameter. For a input sentence INLINEFORM3 with a label sequence INLINEFORM4 , a sentence-level score is then given as: DISPLAYFORM0  where INLINEFORM0 indicates the probability of label INLINEFORM1 at position INLINEFORM2 by the network with"]}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "predicted_answer": "Sina Weibo", "predicted_evidence": ["a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The results of models are depicted as Figure UID11 . From the figure, we can know our models perfrom better with little loss in time. Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated", "Introduction. With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . As the foundation of many downstream applications BIBREF4 , BIBREF5 , BIBREF6 such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . It is the informality of social media that discourages accuracy of NER systems. While efforts in English have narrowed the gap between social media and formal domains BIBREF3 , the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus", "The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention. To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media. Conclusions and Future Work. The results of our experiments also suggest directions for future work. We can observe all models in Table TABREF23 achieve a much lower recall than precision BIBREF25 . So we need to design some methods to solve the problem. Acknowledgements. Thanks to Shuming Ma for the help on improving the writing. This work was supported in part by", "rate INLINEFORM5 . For integrated model, INLINEFORM6 is INLINEFORM7 . We train 20 epochs and choose the best prediction for test. Results and Analysis. We evaluate two methods to incorporate word segmentation information. The results of two methods are shown as Table TABREF22 . We can see that positional character embeddings perform better in neural network. This is probably because positional character embeddings method can learn word segmentation information from unlabeled text while word segmentation can only use training corpus. We adopt positional character embeddings in our next four models. Our first model is a B-LSTM neural network (baseline). To take advantage of traditional model BIBREF23 , BIBREF24 such as CRF, we combine transition probability in our B-LSTM based MMNN. We design a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The results of models are depicted", "than precision BIBREF25 . So we need to design some methods to solve the problem. Acknowledgements. Thanks to Shuming Ma for the help on improving the writing. This work was supported in part by National Natural Science Foundation of China (No. 61673028), and National High Technology Research and Development Program of China (863 Program, No. 2015AA015404). Xu Sun is the corresponding author of this paper. The first author focuses on the design of the method and the experimental results. The corresponding author focuses on the design of the method.", "obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0  By minimizing the object, we can increase the score of correct label sequence INLINEFORM0 and decrease the score of incorrect label sequence INLINEFORM1 . F-Score Driven Training Method. Max Margin training method use structured margin loss INLINEFORM0 to describe the difference between the corrected label sequence INLINEFORM1 and predicted label sequence INLINEFORM2 . In fact, the structured margin loss INLINEFORM3 reflect the loss in label accuracy. Considering the gap between label accuracy and F-Score in NER, we introduce a new training method to train directly on F-Score. To introduce F-Score driven training method, we need to take a look at the subgradient of equation ( EQREF9 ): INLINEFORM4  In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The", "remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging BIBREF11 , BIBREF12 , BIBREF13 . To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze peng-dredze:2015:EMNLP evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER BIBREF14 . However, the two above approaches are implemented within CRF model. We construct a semi-supervised model based on B-LSTM neural network to learn from the limited labelled corpus by using lexical", "Segmentation Features We can treat word segmentation as discrete features in neural network model. The discrete features can be easily incorporated into neural network model BIBREF20 . We use word embeddings from a LSTM pretrained on MSRA 2006 corpus to initialize the word segmentation features. Datasets. We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs. Parameter Estimation. We pre-trained embeddings using word2vec BIBREF22 with the skip-gram training model, without negative sampling and other default parameter settings. Like Mao et al. mao2008chinese, we use bigram features as follow: INLINEFORM0  We"]}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "predicted_answer": "modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media", "predicted_evidence": ["a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The results of models are depicted as Figure UID11 . From the figure, we can know our models perfrom better with little loss in time. Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated", "The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention. To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media. Conclusions and Future Work. The results of our experiments also suggest directions for future work. We can observe all models in Table TABREF23 achieve a much lower recall than precision BIBREF25 . So we need to design some methods to solve the problem. Acknowledgements. Thanks to Shuming Ma for the help on improving the writing. This work was supported in part by", "process to optimize the F-Score of training examples. Our new structured margin loss can be described as: DISPLAYFORM0  where INLINEFORM0 is the F-Score between corrected label sequence and predicted label sequence. F-Score and Label Accuracy Trigger Function The F-Score can be quite unstable in some situation. For instance, if there is no named entity in a sentence, F-Score will be always 0 regardless of the predicted label sequence. To take advantage of meaningful information provided by label accuracy, we introduce an integrated trigger function as follows: DISPLAYFORM0  where INLINEFORM0 is a factor to adjust the weight of label accuracy and F-Score. Because F-Score depends on the whole label sequence, we use beam search to find INLINEFORM0 label sequences with top sentece-level score INLINEFORM1 and then use trigger function to rerank the INLINEFORM2 label sequences and select the best. Word Segmentation Representation. Word segmentation takes an important part in Chinese text", "obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0  By minimizing the object, we can increase the score of correct label sequence INLINEFORM0 and decrease the score of incorrect label sequence INLINEFORM1 . F-Score Driven Training Method. Max Margin training method use structured margin loss INLINEFORM0 to describe the difference between the corrected label sequence INLINEFORM1 and predicted label sequence INLINEFORM2 . In fact, the structured margin loss INLINEFORM3 reflect the loss in label accuracy. Considering the gap between label accuracy and F-Score in NER, we introduce a new training method to train directly on F-Score. To introduce F-Score driven training method, we need to take a look at the subgradient of equation ( EQREF9 ): INLINEFORM4  In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The", "( EQREF9 ): INLINEFORM4  In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The margin loss INLINEFORM2 serves as a trigger function to conduct the training process of B-LSTM based MMNN. We can introduce a new trigger function to guide the training process of neural network. F-Score Trigger Function The main criterion of NER task is F-score. However, high label accuracy does not mean high F-score. For instance, if every named entity's last character is labeledas O, the label accuracy can be quite high, but the precision, recall and F-score are 0. We use the F-Score between corrected label sequence and predicted label sequence as trigger function, which can conduct the training process to optimize the F-Score of training examples. Our new structured margin loss can be described as: DISPLAYFORM0  where INLINEFORM0 is the F-Score between corrected label sequence and predicted", "Introduction. With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . As the foundation of many downstream applications BIBREF4 , BIBREF5 , BIBREF6 such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . It is the informality of social media that discourages accuracy of NER systems. While efforts in English have narrowed the gap between social media and formal domains BIBREF3 , the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus", ". However, the two above approaches are implemented within CRF model. We construct a semi-supervised model based on B-LSTM neural network to learn from the limited labelled corpus by using lexical information provided by massive unlabeled text. To shrink the gap between label accuracy and F-Score, we propose a method to directly train on F-Score rather than label accuracy in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. Specifically, we make contributions as follows: Model. We construct a semi-supervised model which is based on B-LSTM neural network and combine transition probability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. Transition Probability. B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 . However,", "Segmentation Features We can treat word segmentation as discrete features in neural network model. The discrete features can be easily incorporated into neural network model BIBREF20 . We use word embeddings from a LSTM pretrained on MSRA 2006 corpus to initialize the word segmentation features. Datasets. We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs. Parameter Estimation. We pre-trained embeddings using word2vec BIBREF22 with the skip-gram training model, without negative sampling and other default parameter settings. Like Mao et al. mao2008chinese, we use bigram features as follow: INLINEFORM0  We"]}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "predicted_answer": "Yes.", "predicted_evidence": ["to further improve the cross-lingual effectiveness. Related Work ::: Representations from NMT. The encoder from an NMT model has been used as yet another effective way to contextualize word vectors BIBREF32. Additionally, recent progress in NMT has enabled one to train multilingual NMT systems that support translation from multiple source languages into multiple target languages within a single model BIBREF3. Our work is more closely related to two very recent works which explore the encoder from multilingual NMT model for cross-lingual transfer learning BIBREF4, BIBREF5. While BIBREF4 also consider multilingual systems, they do so on a much smaller scale, training it on only 2 languages. BIBREF5 uses a large scale model comparable to ours with 93 languages but they constrain the model by pooling encoder representations and therefore only obtain a single vector per sequence. Neither of these approaches have been used on token level sequence tagging tasks. Further, neither concern", "same time by optimizing the translation objective between language pairs. To train such a multilingual system within a single model, we use the strategy proposed in BIBREF3 which suggests prepending a target language token to every source sequence to be translated. This simple and effective strategy enables us to share the encoder, decoder, and attention mechanisms across all language pairs. Massively Multilingual Neural Machine Translation Model ::: Pre-training ::: Data. We train our multilingual NMT system on a massive scale, using an in-house corpus generated by crawling and extracting parallel sentences from the web BIBREF14. This corpus contains parallel documents for 102 languages, to and from English, comprising a total of 25 billion sentence pairs. The number of parallel sentences per language in our corpus ranges from around 35 thousand to almost 2 billion. Figure FIGREF10 illustrates the data distribution for all 204 language pairs used to train the NMT model. Language ids", "translation from multiple source languages into multiple target languages within a single model BIBREF2, BIBREF3, BIBREF0. Such multilingual NMT (mNMT) systems often demonstrate large improvements in translation quality on low resource languages. This positive transfer originates from the model's ability to learn representations which are transferable across languages. Previous work has shown that these representations can then be used for cross-lingual transfer in other downstream NLP tasks - albeit on only a pair of language pairs BIBREF4, or by limiting the decoder to use a pooled vector representation of the entire sentence from the encoder BIBREF5. In this paper we scale up the number of translation directions used in the NMT model to include 102 languages to and from English. Unlike BIBREF5, we do not apply any restricting operations such as pooling while training mNMT which allows us to obtain token level representations making it possible to transfer them to sequence tagging", "during mNMT training improves cross-lingual effectiveness on both POS tagging and XNLI task. Training a massively multilingual NMT model that supports translation of 102 languages to and from English without using the $<$2xx$>$ token in the encoder is another direction for future work. Related Work. We briefly review widely used approaches in cross-lingual transfer learning and some of the recent work in learning contextual word representations (CWR). Related Work ::: Multilingual Word Embeddings. For cross-lingual transfer, the most widely studied approach is to use multilingual word embeddings as features in neural network models. Several recent efforts have explored methods that align vector spaces for words in different languages BIBREF23, BIBREF24, BIBREF25. Related Work ::: Unsupervised CWR. More recent work has shown that CWRs obtained using unsupervised generative pre-training techniques such as language modeling or cloze task BIBREF26 have led to state-of-the-art results", "Introduction. English has an abundance of labeled data that can be used for various Natural Language Processing (NLP) tasks, such as part-of-speech tagging (POS), named entity recognition (NER), and natural language inference (NLI). This richness of labeled data manifests itself as a boost in accuracy in the current era of data-hungry deep learning algorithms. However, the same is not true for many other languages where task specific data is scarce and expensive to acquire. This motivates the need for cross-lingual transfer learning \u2013 the ability to leverage the knowledge from task specific data available in one or more languages to solve that task in languages with little or no task-specific data. Recent progress in NMT has enabled one to train multilingual systems that support translation from multiple source languages into multiple target languages within a single model BIBREF2, BIBREF3, BIBREF0. Such multilingual NMT (mNMT) systems often demonstrate large improvements in", "of the extracted representations. The gains observed on various tasks over mBERT suggest that the translation objective is competitive with specialized approaches to learn cross-lingual embeddings. We find that there is a trade off between the number of languages in the multilingual model and efficiency of the learned representations due to the limited capacity. Scaling up the model to include more languages without diminishing transfer learning capability is a direction for future work. Finally, one could also consider integrating mBERT's objective with the translation objective to pre-train the mNMT system. Supplementary Material. In this section we provide the list of languages codes used throughout this paper and the statistics of the datasets used for the downstream tasks.", "CWR. More recent work has shown that CWRs obtained using unsupervised generative pre-training techniques such as language modeling or cloze task BIBREF26 have led to state-of-the-art results beyond what was achieved with traditional word type representations on many monolingual NLP tasks BIBREF27, BIBREF1, BIBREF28, BIBREF29 such as sentence classification, sequence tagging, and question answering. Subsequently, these contextual methods have been extended to produce multilingual representations by training a single model on text from multiple languages which have proven to be very effective for cross lingual transfer BIBREF18, BIBREF30, BIBREF31. BIBREF19 show that adding a translation language modeling (TLM) objective to mBERT's MLM objective utilizes both monolingual and parallel data to further improve the cross-lingual effectiveness. Related Work ::: Representations from NMT. The encoder from an NMT model has been used as yet another effective way to contextualize word vectors", "effectiveness of representations learned from an mNMT model in various transfer-learning settings. Massively Multilingual Neural Machine Translation Model. In this section, we describe our massively multilingual NMT system. Similar to BERT, our transfer learning setup has two distinct steps: pre-training and fine-tuning. During pre-training, the NMT model is trained on large amounts of parallel data to perform translation. During fine-tuning, we initialize our downstream model with the pre-trained parameters from the encoder of the NMT system, and then all of the parameters are fine-tuned using labeled data from the downstream tasks. Massively Multilingual Neural Machine Translation Model ::: Model Architecture. We train our Massively Multilingual NMT system using the Transformer architecture BIBREF9 in the open-source implementation under the Lingvo framework BIBREF10. We use a larger version of Transformer Big containing 375M parameters (6 layers, 16 heads, 8192 hidden dimension)"]}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "predicted_answer": "Not provided", "predicted_evidence": ["translation from multiple source languages into multiple target languages within a single model BIBREF2, BIBREF3, BIBREF0. Such multilingual NMT (mNMT) systems often demonstrate large improvements in translation quality on low resource languages. This positive transfer originates from the model's ability to learn representations which are transferable across languages. Previous work has shown that these representations can then be used for cross-lingual transfer in other downstream NLP tasks - albeit on only a pair of language pairs BIBREF4, or by limiting the decoder to use a pooled vector representation of the entire sentence from the encoder BIBREF5. In this paper we scale up the number of translation directions used in the NMT model to include 102 languages to and from English. Unlike BIBREF5, we do not apply any restricting operations such as pooling while training mNMT which allows us to obtain token level representations making it possible to transfer them to sequence tagging", "Introduction. English has an abundance of labeled data that can be used for various Natural Language Processing (NLP) tasks, such as part-of-speech tagging (POS), named entity recognition (NER), and natural language inference (NLI). This richness of labeled data manifests itself as a boost in accuracy in the current era of data-hungry deep learning algorithms. However, the same is not true for many other languages where task specific data is scarce and expensive to acquire. This motivates the need for cross-lingual transfer learning \u2013 the ability to leverage the knowledge from task specific data available in one or more languages to solve that task in languages with little or no task-specific data. Recent progress in NMT has enabled one to train multilingual systems that support translation from multiple source languages into multiple target languages within a single model BIBREF2, BIBREF3, BIBREF0. Such multilingual NMT (mNMT) systems often demonstrate large improvements in", "to further improve the cross-lingual effectiveness. Related Work ::: Representations from NMT. The encoder from an NMT model has been used as yet another effective way to contextualize word vectors BIBREF32. Additionally, recent progress in NMT has enabled one to train multilingual NMT systems that support translation from multiple source languages into multiple target languages within a single model BIBREF3. Our work is more closely related to two very recent works which explore the encoder from multilingual NMT model for cross-lingual transfer learning BIBREF4, BIBREF5. While BIBREF4 also consider multilingual systems, they do so on a much smaller scale, training it on only 2 languages. BIBREF5 uses a large scale model comparable to ours with 93 languages but they constrain the model by pooling encoder representations and therefore only obtain a single vector per sequence. Neither of these approaches have been used on token level sequence tagging tasks. Further, neither concern", "while lagging behind SOTA by 0.2 points. Interestingly, MMTE beats SOTA on Japanese by more than 8 points. This may be due to the different nature and amount of data used for pre-training by these methods. Experiments and Results ::: Cross-lingual Intent Classification. BIBREF7 recently presented a dataset for multilingual task oriented dialog. This dataset contains 57k annotated utterances in English (43k), Spanish (8.6k), and Thai (5k) with 12 different intents across the domains weather, alarm, and reminder. The evaluation metric used is classification accuracy. We use this data for both in-language training and zero-shot transfer. The task-specific network and the optimizer used is the same as the one used for the above two tasks. The learning rate schedule is (0.1,100k). Results are reported in Table TABREF25. MMTE outperforms both mBERT and previous SOTA in both in-language and zero-shot setting on all 3 languages and establishes a new SOTA for this dataset. Experiments and", "of the extracted representations. The gains observed on various tasks over mBERT suggest that the translation objective is competitive with specialized approaches to learn cross-lingual embeddings. We find that there is a trade off between the number of languages in the multilingual model and efficiency of the learned representations due to the limited capacity. Scaling up the model to include more languages without diminishing transfer learning capability is a direction for future work. Finally, one could also consider integrating mBERT's objective with the translation objective to pre-train the mNMT system. Supplementary Material. In this section we provide the list of languages codes used throughout this paper and the statistics of the datasets used for the downstream tasks.", "Analysis ::: Few Shot Transfer. While zero-shot transfer is a good measure of a model's natural cross-lingual effectiveness, the more practical setting is the few-shot transfer scenario as we almost always have access to, or can cheaply acquire, a small amount of data in the target language. We report the few-shot transfer results of mBERT and MMTE on the POS tagging dataset in TABREF33. To simulate the few-shot setting, in addition to using English data, we use 10 examples from each language (upsampled to 1000). MMTE outperforms mBERT in few-shot setting by 0.6 points averaged over 48 languages. Once again, we see that the gains are more pronounced in low resource languages. Analysis ::: One Model for all Languages. Another setting of importance is the in-language training where instead of training one model for each language, we concatenate all the data and train one model jointly on all languages. We perform this experiment on the POS tagging dataset with 48 languages and report", "To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$. Massively Multilingual Neural Machine Translation Model ::: Pre-training ::: Model quality. We use BLEU score BIBREF15 to evaluate the quality of our translation model(s). Our mNMT model performs worse than the bilingual baseline on high", "during mNMT training improves cross-lingual effectiveness on both POS tagging and XNLI task. Training a massively multilingual NMT model that supports translation of 102 languages to and from English without using the $<$2xx$>$ token in the encoder is another direction for future work. Related Work. We briefly review widely used approaches in cross-lingual transfer learning and some of the recent work in learning contextual word representations (CWR). Related Work ::: Multilingual Word Embeddings. For cross-lingual transfer, the most widely studied approach is to use multilingual word embeddings as features in neural network models. Several recent efforts have explored methods that align vector spaces for words in different languages BIBREF23, BIBREF24, BIBREF25. Related Work ::: Unsupervised CWR. More recent work has shown that CWRs obtained using unsupervised generative pre-training techniques such as language modeling or cloze task BIBREF26 have led to state-of-the-art results"]}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "predicted_answer": "Automatic and human evaluation", "predicted_evidence": ["chit-chat models. Experiments ::: Evaluation Metrics ::: Human. Asking humans to evaluate the quality of a dialogue model is challenging, especially when multiple models have to be compared. The likert score (a.k.a. 1 to 5 scoring) has been widely used to evaluate the interactive experience with conversational models BIBREF70, BIBREF65, BIBREF0, BIBREF1. In such evaluation, a human interacts with the systems for several turns, and then they assign a score from 1 to 5 based on three questions BIBREF0 about fluency, engagingness, and consistency. This evaluation is both expensive to conduct and requires many samples to achieve statistically significant results BIBREF6. To cope with these issues, BIBREF6 proposed ACUTE-EVAL, an A/B test evaluation for dialogue systems. The authors proposed two modes: human-model chats and self-chat BIBREF71, BIBREF72. In this work, we opt for the latter since it is cheaper to conduct and achieves similar results BIBREF6 to the former. Another advantage", "the target language. Experiments ::: Evaluation Metrics. Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset. Experiments ::: Evaluation Metrics ::: Automatic. For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU BIBREF67 with reference to the human-annotated responses. Although these automatic measures are not perfect BIBREF68, they help to roughly estimate the performance of different models under the same test set. More recently, BIBREF69 has shown the correlation between perplexity and human judgment in open-domain chit-chat models. Experiments ::: Evaluation Metrics ::: Human. Asking humans to evaluate the quality of a dialogue model is challenging, especially when multiple models have to be compared.", "illustrates the details of the multilingual causal decoder and the multilingual encoder-decoder models. Human Evaluation. As illustrated in Figure FIGREF54, the annotator is provided with two full dialogues made by a self-chat model or human-dialogues. Then the annotators are asked the following questions: Who would you talk to for a long conversation? If you had to say one of these speakers is interesting and one is boring, who would you say is more interesting? Which speaker sounds more human? Generated Samples ::: Mixed-language Samples. We report more the mixed-language samples generated by M-CausalBert in Table TABREF61 and TABREF62. Generated Samples ::: Model Comparison Samples. We randomly sample one self-chat dialogue examples for each model in each language and report them in figure 5-32. in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert in CausalBert,M-CausalBert,PolyEncoder,M-Bert2Bert in", "two modes: human-model chats and self-chat BIBREF71, BIBREF72. In this work, we opt for the latter since it is cheaper to conduct and achieves similar results BIBREF6 to the former. Another advantage of using this method is the ability to evaluate multi-turn conversations instead of single-turn responses. Following ACUTE-EVAL, the annotator is provided with two full dialogues made by self-chat or human-dialogue. The annotator is asked to choose which of the two dialogues is better in terms of engagingness, interestingness, and humanness. For each comparison, we sample 60\u2013100 conversations from both models. In Appendix C, we report the exact questions and instructions given to the annotators, and the user interface used in the evaluation. We hired native speakers annotators for all six considered languages. The annotators were different from the dataset collection annotators to avoid any possible bias. Experiments ::: Implementation Details ::: Multilingual Models. We use the", "the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native speaker annotators with a fluent level of English and asked them to revise the machine-translated dialogues and persona sentences in the validation set and test set according to original English dialogues. The main goal of human annotation is to ensure the resulting conversations are coherent and fluent despite the cultural differences in target languages. Therefore, annotators are not restricted to only translate the English dialogues, and they are allowed to modify the original dialogues to improve the dialogue coherence in the corresponding language while retaining the persona information. The full annotation instructions are reported in Appendix A. Compared to collecting new persona sentences and dialogues in each language, human-annotating the dialogues by leveraging translation APIs has multiple advantages. First, it increases the data", "the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. Related Work ::: Dialogue Systems. are categorized as goal-oriented BIBREF7, BIBREF8 and chit-chat BIBREF9, BIBREF10. Interested readers may refer to BIBREF11 for a general overview. In this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18 such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. BIBREF19 was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, BIBREF0 and BIBREF1 introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4\u20135 sentences) is randomly assigned to each of", "approach XNLG BIBREF4 shows inferior performance on multi-turn dialogue tasks, and generates repetitive responses. Although cross-lingual dialogue generation is challenging, it reduces the human effort for data annotation in different languages. Therefore, the cross-language transfer is an important direction to investigate. Conclusion. In this paper, we studied both cross-lingual and multilingual approaches in end-to-end personalized dialogue modeling. We presented the XPersona dataset, a multilingual extension of Persona-Chat, for evaluating the multilingual personalized chatbots. We further provided both cross-lingual and multilingual baselines and compared them with the monolingual approach and two-stage translation approach. Extensive automatic evaluation and human evaluation were conducted to examine the models' performance. The experimental results showed that multilingual trained models, with a single model across multiple languages, can outperform the two-stage translation", "A. Compared to collecting new persona sentences and dialogues in each language, human-annotating the dialogues by leveraging translation APIs has multiple advantages. First, it increases the data distribution similarity across languages BIBREF3, which can better examine the system's cross-lingual transferability. Second, revising the machine-translated dialogues based on the original English dialogue improves the data construction efficiency. Third, it leverages the well-constructed English persona conversations as a reference to ensure the dialogue quality without the need for training a new pool of workers to generate new samples BIBREF3. On the other hand, human-translating the entire training-set ($\\sim $130K utterances) in six languages is expensive. Therefore, we propose an iterative method to improve the quality of the automatically translated training set. We firstly sample 200 dialogues from the training set ($\\sim $2600 utterances) in each language, and we assign human"]}
{"question_id": "6c8dc31a199b155e73c84173816c1e252137a0af", "predicted_answer": "cross-lingual performance is lower", "predicted_evidence": ["translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language. Thus, the response generation flow is: target query $\\rightarrow $ English query $\\rightarrow $ English response $\\rightarrow $ target response. We denote this model as Poly. Experiments ::: Implementation Details ::: Cross-lingual Models.. In the first pre-training stage, we use the pre-trained weights from XLMR-base BIBREF60. Then, we follow the second pre-training stage of XNLG BIBREF4 for pre-training Italian, Japanese, Korean, Indonesia cross-lingual transferable models. For Chinese and French, we directly apply the pre-trained XNLG BIBREF4 weights. Then, the pre-trained models are fine-tune on English PersonaChat training set and early stop based on the perplexity on target language validation set. Experiments ::: Results and Discussion ::: Quantitative Analysis. Table TABREF20 compares monolingual, multilingual, and", "approach XNLG BIBREF4 shows inferior performance on multi-turn dialogue tasks, and generates repetitive responses. Although cross-lingual dialogue generation is challenging, it reduces the human effort for data annotation in different languages. Therefore, the cross-language transfer is an important direction to investigate. Conclusion. In this paper, we studied both cross-lingual and multilingual approaches in end-to-end personalized dialogue modeling. We presented the XPersona dataset, a multilingual extension of Persona-Chat, for evaluating the multilingual personalized chatbots. We further provided both cross-lingual and multilingual baselines and compared them with the monolingual approach and two-stage translation approach. Extensive automatic evaluation and human evaluation were conducted to examine the models' performance. The experimental results showed that multilingual trained models, with a single model across multiple languages, can outperform the two-stage translation", "to learn a multilingual system directly from noisy multilingual data (e.g., translated data), thus getting rid of the translation system dependence at inference time. To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages. Furthermore, we propose competitive baselines in two training settings, namely, cross-lingual and multilingual, and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models. An extensive automatic and human evaluation BIBREF6 of", "context from the encoder and generate the response in an unconditional language model way. We leave the investigation of this problem to future work. On the other hand, M-CausalBert achieves a comparable or slightly better performance compared to CausalBert, which suggests that M-CausalBert leverages the data from other languages. As expected, we observe a significant gap between the cross-lingual model and other models, which indicates that cross-lingual zero-shot conversation modeling is very challenging. Table TABREF28 shows the human evaluation result of comparing M-CausalBert (Multi) against the human, translation-based Poly-encoder (Poly), and monolingual CausalBert (Mono). The results illustrate that Multi outperforms Mono in English and Chinese, and is on par with Mono in other languages. On the other hand, Poly shows a strong performance in English as it was pre-trained with a large-scale English conversation corpus. In contrast, the performance of Poly drops in other", "and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models. An extensive automatic and human evaluation BIBREF6 of our models shows that a multilingual system is able to outperform strong translation-based models and on par with or even improve the monolingual model. The cross-lingual performance is still lower than other models, which indicates that cross-lingual conversation modeling is very challenging. The main contribution of this paper are summarized as follows: We present the first multilingual non-goal-oriented dialogue benchmark for evaluating multilingual generative chatbots. We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. Related Work ::: Dialogue Systems. are categorized as goal-oriented BIBREF7,", "conducted to examine the models' performance. The experimental results showed that multilingual trained models, with a single model across multiple languages, can outperform the two-stage translation approach and is on par with monolingual models. On the other hand, the current state-of-the-art cross-lingual approach XNLG achieved lower performance than other baselines. In future work, we plan to research a more advanced cross-lingual generation approach and construct a mixed-language conversational benchmark for evaluating multilingual systems. Dataset Collection ::: Annotation Instructions. In this section, we show the instructions for French annotation: There are two existing columns of conversations: the first column (en) is the original conversations in English, the second column (fr) is the conversations translated by an automatic system (e.g., Google Translate). You should copy the conversation from the second column (the translated conversations) into the third column (named", "focused on classification and sequence labeling, while instead, BIBREF4 proposed to pre-train both the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. Data Collection. The proposed XPersona dataset is an extension of the persona-chat dataset BIBREF0, BIBREF1. Specifically, we extend the ConvAI2 BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. Since the test set of ConvAI2 is hidden, we split the original validation set into a new validation set and test sets. Then, we firstly automatically translate the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native speaker annotators with a fluent level of English", "six considered languages. The annotators were different from the dataset collection annotators to avoid any possible bias. Experiments ::: Implementation Details ::: Multilingual Models. We use the \"BERT-Base, Multilingual Cased\" checkpoint, and we denote the multilingual encoder-decoder model as M-Bert2Bert ($\\sim $220M parameters) and causal decoder model as M-CausalBert ($\\sim $110M parameters). We fine-tune both models in the combined training set (English in Persona-chat BIBREF0, six languages in Xpersona) for five epochs with AdamW optimizer and a learning rate of $6.25e$-5. Experiments ::: Implementation Details ::: Monolingual Models. To verify whether the multilingual agent will under-perform the monolingual agent in the monolingual conversational task, we build a monolingual encoder-decoder model and causal decoder model for each language. For a fair comparison, we initialize the monolingual models with a pre-trained monolingual BERT BIBREF5, BIBREF73, BIBREF74. We denote"]}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "predicted_answer": "Poly-encoder and Google Translate API-based pipelines.", "predicted_evidence": ["a two-step translation from any language to English and from English to any language. This comes with three major problems: 1) amplification of translation errors since the current dialogue systems are far from perfect, especially with noisy input; 2) the three-stage pipeline system is significantly slower in terms of inference speed; and 3) high translation costs since the current state-of-the-art models, especially in low resources languages, are only available using costly APIs. In this paper, we analyze two possible workarounds to alleviate the aforementioned challenges. The first is to build a cross-lingual transferable system by aligning cross-lingual representations, as in BIBREF3, in which the system is trained on one language and zero-shot to another language. The second is to learn a multilingual system directly from noisy multilingual data (e.g., translated data), thus getting rid of the translation system dependence at inference time. To evaluate the aforementioned", "and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models. An extensive automatic and human evaluation BIBREF6 of our models shows that a multilingual system is able to outperform strong translation-based models and on par with or even improve the monolingual model. The cross-lingual performance is still lower than other models, which indicates that cross-lingual conversation modeling is very challenging. The main contribution of this paper are summarized as follows: We present the first multilingual non-goal-oriented dialogue benchmark for evaluating multilingual generative chatbots. We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. Related Work ::: Dialogue Systems. are categorized as goal-oriented BIBREF7,", "A. Compared to collecting new persona sentences and dialogues in each language, human-annotating the dialogues by leveraging translation APIs has multiple advantages. First, it increases the data distribution similarity across languages BIBREF3, which can better examine the system's cross-lingual transferability. Second, revising the machine-translated dialogues based on the original English dialogue improves the data construction efficiency. Third, it leverages the well-constructed English persona conversations as a reference to ensure the dialogue quality without the need for training a new pool of workers to generate new samples BIBREF3. On the other hand, human-translating the entire training-set ($\\sim $130K utterances) in six languages is expensive. Therefore, we propose an iterative method to improve the quality of the automatically translated training set. We firstly sample 200 dialogues from the training set ($\\sim $2600 utterances) in each language, and we assign human", "to learn a multilingual system directly from noisy multilingual data (e.g., translated data), thus getting rid of the translation system dependence at inference time. To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages. Furthermore, we propose competitive baselines in two training settings, namely, cross-lingual and multilingual, and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models. An extensive automatic and human evaluation BIBREF6 of", "model and causal decoder model for each language. For a fair comparison, we initialize the monolingual models with a pre-trained monolingual BERT BIBREF5, BIBREF73, BIBREF74. We denote the monolingual encoder-decoder model as Bert2Bert ($\\sim $220M parameters) and causal decoder model as CausalBert ($\\sim $110M parameters). Then we fine-tune each model in each language independently for the same number of epoch and optimizer as the multilingual model. Experiments ::: Implementation Details ::: Translation-based Models. Another strong baseline we compare with is Poly-encoder BIBREF75, a large-scale pre-trained retrieval model that has shown state-of-the-art performance in the English Persona-chat dataset BIBREF6. We adapt this model to the other languages by using the Google Translate API to translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language. Thus, the response generation flow is: target", "translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language. Thus, the response generation flow is: target query $\\rightarrow $ English query $\\rightarrow $ English response $\\rightarrow $ target response. We denote this model as Poly. Experiments ::: Implementation Details ::: Cross-lingual Models.. In the first pre-training stage, we use the pre-trained weights from XLMR-base BIBREF60. Then, we follow the second pre-training stage of XNLG BIBREF4 for pre-training Italian, Japanese, Korean, Indonesia cross-lingual transferable models. For Chinese and French, we directly apply the pre-trained XNLG BIBREF4 weights. Then, the pre-trained models are fine-tune on English PersonaChat training set and early stop based on the perplexity on target language validation set. Experiments ::: Results and Discussion ::: Quantitative Analysis. Table TABREF20 compares monolingual, multilingual, and", "method to improve the quality of the automatically translated training set. We firstly sample 200 dialogues from the training set ($\\sim $2600 utterances) in each language, and we assign human annotators to list all frequent translation mistakes in the given dialogues. For example, daily colloquial English expressions such as \u201ccool\", \u201cI see\", and \u201clol\" are usually literally translated. After that, we use a simple string matching to revise the inappropriate translations in the whole training-set and return a revision log, which records all the revised utterances. Then, we assign human annotators to check all the revised utterances and list translation mistakes again. We repeat this process at least twice for each language. Finally, we summarize the statistics of the collected dataset in Table TABREF6. Multilingual Personalized Conversational Models. Let us define a dialogue $\\mathcal {D}=\\lbrace U_1,S_1,U_2,S_2, \\dots , U_n, S_n\\rbrace $ as an alternating set of utterances from two", "is the conversations translated by an automatic system (e.g., Google Translate). You should copy the conversation from the second column (the translated conversations) into the third column (named fr_annotation). In that column, you should then revise the incorrect or inappropriate translations. The goal of the revision is to make the conversations more coherent and fluent in the target language (French). Hence you can customize dialogues and persona sentences to make them fluent and coherent in the target language, including by deviating from the original translation. However, you should retain persona and conversation consistency. Dataset Collection ::: Training Set Statistics. We report our iterative revised training set statistics in Table TABREF53. Model Detail. Figure FIGREF55 and FIGREF56 illustrates the details of the multilingual causal decoder and the multilingual encoder-decoder models. Human Evaluation. As illustrated in Figure FIGREF54, the annotator is provided with two"]}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "predicted_answer": "Chinese, French, Indonesian, Italian, Korean, and Japanese", "predicted_evidence": ["to learn a multilingual system directly from noisy multilingual data (e.g., translated data), thus getting rid of the translation system dependence at inference time. To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. In XPersona, the training sets are automatically translated using translation APIs with several human-in-the-loop passes of mistake correction. In contrast, the validation and test sets are annotated by human experts to facilitate both automatic and human evaluations in multiple languages. Furthermore, we propose competitive baselines in two training settings, namely, cross-lingual and multilingual, and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models. An extensive automatic and human evaluation BIBREF6 of", "approach XNLG BIBREF4 shows inferior performance on multi-turn dialogue tasks, and generates repetitive responses. Although cross-lingual dialogue generation is challenging, it reduces the human effort for data annotation in different languages. Therefore, the cross-language transfer is an important direction to investigate. Conclusion. In this paper, we studied both cross-lingual and multilingual approaches in end-to-end personalized dialogue modeling. We presented the XPersona dataset, a multilingual extension of Persona-Chat, for evaluating the multilingual personalized chatbots. We further provided both cross-lingual and multilingual baselines and compared them with the monolingual approach and two-stage translation approach. Extensive automatic evaluation and human evaluation were conducted to examine the models' performance. The experimental results showed that multilingual trained models, with a single model across multiple languages, can outperform the two-stage translation", "focused on classification and sequence labeling, while instead, BIBREF4 proposed to pre-train both the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. Data Collection. The proposed XPersona dataset is an extension of the persona-chat dataset BIBREF0, BIBREF1. Specifically, we extend the ConvAI2 BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese. Since the test set of ConvAI2 is hidden, we split the original validation set into a new validation set and test sets. Then, we firstly automatically translate the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native speaker annotators with a fluent level of English", "Later on, BIBREF0 and BIBREF1 introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4\u20135 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona-consistent dialogue BIBREF0. Several works have improved on the initial baselines with various methodologies BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, especially using large pre-trained models BIBREF26, BIBREF27. Related Work ::: Multilingual. Extensive approaches have been introduced to construct multilingual systems, for example, multilingual semantic role labeling BIBREF28, BIBREF29, multilingual machine translation BIBREF30, multilingual automatic speech recognition BIBREF31, BIBREF32, BIBREF33, BIBREF34, and named entity recognition BIBREF35, BIBREF36. Multilingual deep contextualized model such as Multilingual BERT (M-BERT) BIBREF5 have been", "Table TABREF6. Multilingual Personalized Conversational Models. Let us define a dialogue $\\mathcal {D}=\\lbrace U_1,S_1,U_2,S_2, \\dots , U_n, S_n\\rbrace $ as an alternating set of utterances from two speakers, where $U$ and $S$ represent the user and the system, respectively. Each speaker has its corresponding persona description that consists of a set of sentences $\\mathcal {P}=\\lbrace P_1,\\dots ,P_m\\rbrace $. Given the system persona sentences $\\mathcal {P}_s$ and dialogue history $\\mathcal {D}_t=\\lbrace U_1,S_1,U_2, \\dots ,S_{t-1}, U_t\\rbrace $, we are interested in predicting the system utterances $S_t$. Multilingual Personalized Conversational Models ::: Model Architecture. We explore both encoder-decoder and causal decoder architectures, and we leverage existing pre-trained contextualized multilingual language models as weights initialization. Hence, we firstly define the multilingual embedding layer and then the two multilingual models used in our experiments. Multilingual", "and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models. An extensive automatic and human evaluation BIBREF6 of our models shows that a multilingual system is able to outperform strong translation-based models and on par with or even improve the monolingual model. The cross-lingual performance is still lower than other models, which indicates that cross-lingual conversation modeling is very challenging. The main contribution of this paper are summarized as follows: We present the first multilingual non-goal-oriented dialogue benchmark for evaluating multilingual generative chatbots. We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. Related Work ::: Dialogue Systems. are categorized as goal-oriented BIBREF7,", "Introduction. Personalized dialogue agents have been shown efficient in conducting human-like conversation. This progress has been catalyzed thanks to existing conversational dataset such as Persona-chat BIBREF0, BIBREF1. However, the training data are provided in a single language (e.g., English), and thus the resulting systems can perform conversations only in the training language. For wide, commercial dialogue systems are required to handle a large number of languages since the smart home devices market is increasingly international BIBREF2. Therefore, creating multilingual conversational benchmarks is essential, yet challenging since it is costly to perform human annotation of data in all languages. A possible solution is to use translation systems before and after the model inference, a two-step translation from any language to English and from English to any language. This comes with three major problems: 1) amplification of translation errors since the current dialogue systems", "training conversations; most models are pretrained with a larger corpus, and then fine-tuned on this set. PersonaChat was the subject of the NeurIPS 2018 ConvAI2 Challenge BIBREF8, in which competitor's models were first evaluated with respect to automatic metrics, and then with respect to human judgment via human-bot chats followed by the question \u201cHow much did you enjoy talking to this user?\" on a scale of 1\u20134. A total of 9 systems were evaluated using human annotators, 100 conversations for each. In this work, we leverage the human-model chat logs from the ConvAI2 competition for three models: Lost in Conversation (LIC), which won the competition, and Hugging Face (HF; BIBREF23, BIBREF23) which won the automatic evaluation track, and the KVMemNN BIBREF24 baseline released by the competition organizers (KV; BIBREF8, BIBREF8). LIC and HF are large pretrained and fine-tuned generative Transformer models, while KV is a retrieval model with no pretraining. Secondly, we also compare to"]}
{"question_id": "ae2142ee9e093ce485025168f4bcb3da4602739d", "predicted_answer": "No", "predicted_evidence": ["because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Test set with contrastive examples. Contrastive evaluation requires a large set of suitable examples that involve the translation of pronouns. As additional goals, our test set is designed to 1) focus on hard cases, so that it can be used as a benchmark to track progress in context-aware translation and 2) allow for fine-grained analysis. Section SECREF14 describes how we extract our data set. Section SECREF26 explains how, given a set of contrastive examples, contrastive evaluation works. Automatic extraction of contrastive examples from corpora. We automatically create a test set from the OpenSubtitles corpus BIBREF22 . The goal is to provide a large number of difficult test cases where an", "the annotated documents, such as the distance (in sentences) between pronouns and their antecedents, the document of origin, lemma, morphology and dependency information if available. Evaluation by scoring. Contrastive evaluation is different from conventional evaluation of machine translation in that it does not require any translation. Rather than testing a model's ability to translate, it is a method to test a model's ability to discriminate between given good and bad translations. We exploit the fact that NMT systems are in fact language models of the target language, conditioned on source text. Like language models, NMT systems can be used to compute a model score (the negative log probability) for an existing translation. Contrastive evaluation, then, means to compare the model score of two pairs of inputs: INLINEFORM0 and INLINEFORM1 . If the model score of the actual reference translation is higher, we assume that this model can detect wrong pronoun translations. However, this", "their nominal antecedents in the coreference chain are aligned on word level. This removes most candidate pairs, but is necessary to overcome the noise introduced by our preprocessing pipeline, most notably coreference resolution. From the filtered set, we create a balanced test set by randomly sampling 4000 instances of each of the three translations of it under consideration (er, sie, es). We do not balance antecedent distance. See Table TABREF25 for the distribution of pronoun pairs and antecedent distance in the test set. For each sentence pair in the resulting test set, we introduce contrastive translations. A contrastive translation is a translation variant where the correct pronoun is swapped with an incorrect one. For an example, see Table TABREF19 , where the pronoun it in the original translation corresponds to sie because the antecedent bat is a feminine noun in German (Fledermaus). We produce wrong translations by replacing sie with one of the other pronouns (er, es). Note", "that allows to specifically measure a model's capability to correctly translate pronouns. The test suite consists of pairs of source and target sentences, in combination with contrastive translation variants (for evaluation by model scoring) and additional linguistic and contextual information (for further analysis). The resource is freely available. Additionally, we evaluate several context-aware models that have recently been proposed in the literature on this test set, and extend existing models with parameter tying. The main contributions of our paper are: Section SECREF2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation. Section SECREF3 describes our test suite. The context-aware models we use in our experiments are detailed in Section SECREF4 . We discuss our experiments in Section SECREF5 and the results in Section SECREF6 . Related Work. Two lines of work are related to our paper: research on context-aware", "of two pairs of inputs: INLINEFORM0 and INLINEFORM1 . If the model score of the actual reference translation is higher, we assume that this model can detect wrong pronoun translations. However, this does not mean that systems actually produce the reference translation when given the source sentence for translation. An entirely different target sequence might rank higher in the system's beam during decoding. The only conclusion permitted by contrastive evaluation is whether or not the reference translation is more probable than a contrastive variant. If the model score of the reference is indeed higher, we refer to this outcome as a \u201ccorrect decision\u201d by the model. The model's decision is only correct if the reference translation has a higher score than any contrastive translation. In our evaluation, we aggregate model decisions on the whole test set and report the overall percentage of correct decisions as accuracy. During scoring, the model is provided with reference translations as", "it to its German counterparts er, sie and es, extracted automatically from OpenSubtitles BIBREF22 . We evaluate recently proposed context-aware models on our test set. Even though the increase in BLEU score is moderate for all context-aware models, the improvement in the translation of pronouns is considerable: The best model (s-hier-to-2.tied) achieves a +16 percentage points gain in accuracy over the baseline. Our experiments confirm the importance of careful architecture design, with multi-encoder architectures outperforming a model that simply concatenates context sentences. We also demonstrate the effectiveness of parameter sharing between encoders of a context-aware model. We hope the test set will prove useful for empirically validating novel architectures for context-aware NMT. So far, we have only evaluated models that consider one sentence of context, but the nominal antecedent is more distant for a sizable proportion of the test set, and the evaluation of variable-size", "additional context on the source side and target side. Evaluation of Pronoun Translation. Pronouns can serve a variety of functions with complex cross-lingual variation BIBREF12 , and hand-picked, manually annotated test suites have been presented for the evaluation of pronoun translation BIBREF13 , BIBREF14 , BIBREF9 . While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases. To overcome these problems, we opted for a fully automatic approach to constructing a large-scale test suite. Conceptually, our test set is most similar to the \u201ccross-lingual pronoun prediction\u201d task held at DiscoMT and WMT in recent years BIBREF15 , BIBREF16 , BIBREF17 : participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns. The first edition of the task focused on English INLINEFORM0 French, and it was found that local context", "coreferential pronouns. The Transformer-based models perform strongest on pronouns with intra-segmental antecedent, outperforming the recurrent baseline by 9\u201318 percentage points. This is likely an effect of increased model depth and the self-attentional architecture in this set of experiments. The model by BIBREF8 only uses source context, and outperforms the most comparable RNN system, s-hier.tied. However, the Transformer-based concat22 slightly underperforms the RNN-based concat22, and we consider it future research how to better exploit target context with Transformer-based models. Conclusions. We present a large-scale test suite to specifically test the capacity of NMT models to translate pronouns correctly. The test set contains 12,000 difficult cases of pronoun translations from English it to its German counterparts er, sie and es, extracted automatically from OpenSubtitles BIBREF22 . We evaluate recently proposed context-aware models on our test set. Even though the increase"]}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_answer": "baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 . concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model. concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .  BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial. Also, a standard bidirectional RNN model with attention, trained with Nematus.", "predicted_evidence": ["it follows an encoder-decoder architecture using stacked self-attention and fully connected layers for both the encoder and decoder. baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 . concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model. concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .  BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial", "evaluation, we aggregate model decisions on the whole test set and report the overall percentage of correct decisions as accuracy. During scoring, the model is provided with reference translations as target context, while during translation, the model needs to predict the full sequence. It is an open question to what extent performance deteriorates when context is itself predicted, and thus noisy. We highlight that the same problem arises for sentence-level NMT, and has been addressed with alternative training strategies BIBREF27 . Recurrent Models. We consider the following recurrent baselines: baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not. concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train", "translation: The best model s-hier-to-2.tied achieves a total of +16 percentage points accuracy on the test set over the baseline, see Table TABREF31 . Table TABREF32 shows that context-aware models perform better than the baseline when the antecedent is outside the current sentence. In our experiments, all context-aware models consider one preceding sentence as context. The evaluation according to the distance of the antecedent in Table TABREF35 confirms that the subset of sentences with antecedent distance 1 benefits most from the tested context-aware models (up to +20 percentage points accuracy). However, we note two surprising patterns: The first observation can be explained by the distribution of German pronouns in the test set. The further away the antecedent, the higher the percentage of it INLINEFORM0 es cases, which are the majority class, and thus the class that will be predicted most often if evidence for other classes is lacking. We speculate that this is due to our more", "models are trained with Nematus BIBREF28 . We learn a joint BPE model with 89.5k merge operations BIBREF29 . We train shallow models with an embedding size of 512, a hidden layer size of 1024 and layer normalization. Models are trained with Adam BIBREF30 , with an initial learning rate of 0.0001. We apply early stopping based on validation perplexity. The batch size for training is 80, and the maximum length of training sequences is 100 (if input sentences are concatenated) or 50 (if input lines are single sentences). For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from BIBREF2 , BIBREF8 . Systems are trained on lowercased text that was encoded using BPE (32k merge operations). Models consist of 6 encoder and decoder layers with 8 attention heads. The hidden state size is 512, the size of feedforward layers is 2048. Model performance is evaluated in terms of BLEU, on newstest2017, newstest2018 and all sentence pairs from our pronoun", "because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Test set with contrastive examples. Contrastive evaluation requires a large set of suitable examples that involve the translation of pronouns. As additional goals, our test set is designed to 1) focus on hard cases, so that it can be used as a benchmark to track progress in context-aware translation and 2) allow for fine-grained analysis. Section SECREF14 describes how we extract our data set. Section SECREF26 explains how, given a set of contrastive examples, contrastive evaluation works. Automatic extraction of contrastive examples from corpora. We automatically create a test set from the OpenSubtitles corpus BIBREF22 . The goal is to provide a large number of difficult test cases where an", "heads. The hidden state size is 512, the size of feedforward layers is 2048. Model performance is evaluated in terms of BLEU, on newstest2017, newstest2018 and all sentence pairs from our pronoun test set. We compute scores with SacreBLEU BIBREF31 . Evaluation with BLEU is done mainly to control for overall translation quality. To evaluate pronoun translation, we perform contrastive evaluation and report the accuracy of models on our contrastive test set. Evaluation. The BLEU scores in Table TABREF30 show a moderate improvement for most context-aware systems. This suggests that the architectural changes for the context-aware models do not degrade overall translation quality. The contrastive evaluation on our test set on the other hand shows a clear increase in the accuracy of pronoun translation: The best model s-hier-to-2.tied achieves a total of +16 percentage points accuracy on the test set over the baseline, see Table TABREF31 . Table TABREF32 shows that context-aware models", "and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work. Experiments. We train all models on the data from the WMT 2017 English INLINEFORM0 German news translation shared task ( INLINEFORM1 5.8 million sentence pairs). These corpora do not have document boundaries, therefore a small fraction of sentences will be paired with wrong context, but we expect the model to be robust against occasional random context (see also BIBREF8 ). Experimental setups for the RNN and Transformer models are different, and we describe them separately. All RNN-based models are trained with Nematus BIBREF28 . We learn a joint BPE model with 89.5k merge operations BIBREF29 . We train shallow models with an embedding size of 512, a hidden layer size of 1024 and", "coreferential pronouns. The Transformer-based models perform strongest on pronouns with intra-segmental antecedent, outperforming the recurrent baseline by 9\u201318 percentage points. This is likely an effect of increased model depth and the self-attentional architecture in this set of experiments. The model by BIBREF8 only uses source context, and outperforms the most comparable RNN system, s-hier.tied. However, the Transformer-based concat22 slightly underperforms the RNN-based concat22, and we consider it future research how to better exploit target context with Transformer-based models. Conclusions. We present a large-scale test suite to specifically test the capacity of NMT models to translate pronouns correctly. The test set contains 12,000 difficult cases of pronoun translations from English it to its German counterparts er, sie and es, extracted automatically from OpenSubtitles BIBREF22 . We evaluate recently proposed context-aware models on our test set. Even though the increase"]}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "predicted_answer": "Unanswerable", "predicted_evidence": ["the annotated documents, such as the distance (in sentences) between pronouns and their antecedents, the document of origin, lemma, morphology and dependency information if available. Evaluation by scoring. Contrastive evaluation is different from conventional evaluation of machine translation in that it does not require any translation. Rather than testing a model's ability to translate, it is a method to test a model's ability to discriminate between given good and bad translations. We exploit the fact that NMT systems are in fact language models of the target language, conditioned on source text. Like language models, NMT systems can be used to compute a model score (the negative log probability) for an existing translation. Contrastive evaluation, then, means to compare the model score of two pairs of inputs: INLINEFORM0 and INLINEFORM1 . If the model score of the actual reference translation is higher, we assume that this model can detect wrong pronoun translations. However, this", "additional context on the source side and target side. Evaluation of Pronoun Translation. Pronouns can serve a variety of functions with complex cross-lingual variation BIBREF12 , and hand-picked, manually annotated test suites have been presented for the evaluation of pronoun translation BIBREF13 , BIBREF14 , BIBREF9 . While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases. To overcome these problems, we opted for a fully automatic approach to constructing a large-scale test suite. Conceptually, our test set is most similar to the \u201ccross-lingual pronoun prediction\u201d task held at DiscoMT and WMT in recent years BIBREF15 , BIBREF16 , BIBREF17 : participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns. The first edition of the task focused on English INLINEFORM0 French, and it was found that local context", "that allows to specifically measure a model's capability to correctly translate pronouns. The test suite consists of pairs of source and target sentences, in combination with contrastive translation variants (for evaluation by model scoring) and additional linguistic and contextual information (for further analysis). The resource is freely available. Additionally, we evaluate several context-aware models that have recently been proposed in the literature on this test set, and extend existing models with parameter tying. The main contributions of our paper are: Section SECREF2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation. Section SECREF3 describes our test suite. The context-aware models we use in our experiments are detailed in Section SECREF4 . We discuss our experiments in Section SECREF5 and the results in Section SECREF6 . Related Work. Two lines of work are related to our paper: research on context-aware", "considers only source context, but for pronoun translation, target-side context is intuitively important for disambiguation, especially if the antecedent itself is ambiguous. In our evaluation, we therefore emphasize models that take into account both source and target context. Our experiments are based on models from BIBREF9 , who have released their source code. We extend their models with parameter sharing, which was shown to be beneficial by BIBREF8 . Additionally, we consider a concatenative baseline, similar to BIBREF5 , and Transformer-based models BIBREF8 . This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side. Evaluation of Pronoun Translation. Pronouns can serve a variety of functions with complex cross-lingual variation BIBREF12 , and hand-picked,", "because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Test set with contrastive examples. Contrastive evaluation requires a large set of suitable examples that involve the translation of pronouns. As additional goals, our test set is designed to 1) focus on hard cases, so that it can be used as a benchmark to track progress in context-aware translation and 2) allow for fine-grained analysis. Section SECREF14 describes how we extract our data set. Section SECREF26 explains how, given a set of contrastive examples, contrastive evaluation works. Automatic extraction of contrastive examples from corpora. We automatically create a test set from the OpenSubtitles corpus BIBREF22 . The goal is to provide a large number of difficult test cases where an", "it to its German counterparts er, sie and es, extracted automatically from OpenSubtitles BIBREF22 . We evaluate recently proposed context-aware models on our test set. Even though the increase in BLEU score is moderate for all context-aware models, the improvement in the translation of pronouns is considerable: The best model (s-hier-to-2.tied) achieves a +16 percentage points gain in accuracy over the baseline. Our experiments confirm the importance of careful architecture design, with multi-encoder architectures outperforming a model that simply concatenates context sentences. We also demonstrate the effectiveness of parameter sharing between encoders of a context-aware model. We hope the test set will prove useful for empirically validating novel architectures for context-aware NMT. So far, we have only evaluated models that consider one sentence of context, but the nominal antecedent is more distant for a sizable proportion of the test set, and the evaluation of variable-size", "extraction of contrastive examples from corpora. We automatically create a test set from the OpenSubtitles corpus BIBREF22 . The goal is to provide a large number of difficult test cases where an English pronoun has to be translated to a German pronoun. The most challenging cases are translating it to either er, sie or es, depending on the grammatical gender of the antecedent. Not only is the translation of it ambiguous, there is also class imbalance in the training data (see Table TABREF18 ). There is roughly a 30% probability that it is aligned to es, which makes it difficult to learn to translate er and sie. We use parsing and automatic co-reference resolution to find translation pairs that satisfy our constraints. To provide a basis for filtering with constraints, we tokenize the whole data set with the Moses tokenizer, generate symmetric word alignments with fast_align BIBREF23 , parse the English text with CoreNLP BIBREF24 , parse the German text with ParZu BIBREF25 and perform", "their nominal antecedents in the coreference chain are aligned on word level. This removes most candidate pairs, but is necessary to overcome the noise introduced by our preprocessing pipeline, most notably coreference resolution. From the filtered set, we create a balanced test set by randomly sampling 4000 instances of each of the three translations of it under consideration (er, sie, es). We do not balance antecedent distance. See Table TABREF25 for the distribution of pronoun pairs and antecedent distance in the test set. For each sentence pair in the resulting test set, we introduce contrastive translations. A contrastive translation is a translation variant where the correct pronoun is swapped with an incorrect one. For an example, see Table TABREF19 , where the pronoun it in the original translation corresponds to sie because the antecedent bat is a feminine noun in German (Fledermaus). We produce wrong translations by replacing sie with one of the other pronouns (er, es). Note"]}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "predicted_answer": "They use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score.", "predicted_evidence": ["BIBREF2. Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to make typing decisions because if a mention is correctly linked to its target entity, we can directly obtain the type information about this entity in the knowledge base (KB). For example, in the sentence \u201cThere were some great discussions on a variety of issues facing Federal Way,\u201d the mention \u201cFederal Way\u201d may be incorrectly labeled as a company by some FET models. Such a mistake can be avoided after linking it to the city Federal Way, Washington. For cases that require the understanding of the context, using entity linking results is also beneficial. In the aforementioned example where \u201cTrump\u201d is the mention, obtaining all the types of Donald Trump in the knowledge base (e.g., politician, businessman, TV personality, etc.) is still informative for inferring the correct type (i.e., politician)", "the KB type representation, and $\\lambda _P$ is set to 1; Ours (NoEL) does not use entity linking, i.e., the KB type representation and the entity linking confidence score are removed, and the model is trained in DirectTrain style; Ours (NonDeep) uses one BiLSTM layer and replaces the MLP with a dense layer; Ours (NonDeep NoEL) is the NoEL version of Ours (NonDeep); Ours (LocAttEL) uses the entity linking approach proposed in BIBREF19 instead of our own commonness based approach. Ours (Full), Ours (DirectTrain), and Ours (NonDeep) all use our own commonness based entity linking approach. Experiments ::: Results. The experimental results are listed in Table TABREF16. As we can see, our approach performs much better than existing approaches on both datasets. The benefit of using entity linking in our approach can be verified by comparing Ours (Full) and Ours (NoEL). The performance on both datasets decreases if the entity linking part is removed. Especially on FIGER (GOLD), the strict", "string representation $\\mathbf {f}_s=(\\sum _{i=1}^l \\mathbf {x}_i)/l$. Method ::: Fine-grained Entity Typing Model ::: KB Type Representation. To obtain the KB type representation, we run an EL algorithm for the current mention. If the EL algorithm returns an entity, we retrieve the types of of this entity from the KB. We use Freebase as our KB. Since the types in Freebase is different from $T$, the target type set, they are mapped to the types in $T$ with rules similar to those used in BIBREF14. Afterwards, we perform one hot encoding on these types to get the KB Type Representation $\\mathbf {f}_e$. If the EL algorithm returns NIL (i.e., the mention cannot be linked to an entity), we simply one hot encode the empty type set. Method ::: Fine-grained Entity Typing Model ::: Prediction. Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector", "it may incorrectly link \u201cMarch,\u201d the month, to an entity whose Wikipedia description fits the context better. 3) For some mentions, although the EL system links it to an incorrect entity, the type of this entity is the same with the correct entity. Conclusions. We propose a deep neural model to improve fine-grained entity typing with entity linking. The problem of overfitting the weakly labeled training data is addressed by using a variant of the hinge loss and introducing noise during training. We conduct experiments on two commonly used dataset. The experimental results demonstrates the effectiveness of our approach. Acknowledgments. This paper was supported by the Early Career Scheme (ECS, No. 26206717) from Research Grants Council in Hong Kong and WeChat-HKUST WHAT Lab on Artificial Intelligence Technology.", "$m$, $\\bar{\\tau }_m$ is the incorrect type set. $\\lambda (t)\\in [1,+\\infty )$ is a predefined parameter to impose a larger penalty if the type $t$ is incorrectly predicted as positive. Since the problem of overfitting the weakly annotated labels is more severe for person mentions, we set $\\lambda (t)=\\lambda _P$ if $t$ is a fine-grained person type, and $\\lambda (t)=1$ for all other types. During training, we also randomly set the EL results of half of the training samples to be NIL. So that the model can perform well for mentions that cannot be linked to the KB at test time. Method ::: Entity Linking Algorithm. In this paper, we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string. In our FET approach, the commonness score is also used as the confidence on the linking result", "in our approach can be verified by comparing Ours (Full) and Ours (NoEL). The performance on both datasets decreases if the entity linking part is removed. Especially on FIGER (GOLD), the strict accuracy drops from 75.5 to 69.8. Using entity linking improves less on BBN. We think this is because of three reasons: 1) BBN has a much smaller tag set than FIGER (GOLD); 2) BBN does not allow a mention to be annotated with multiple type paths (e.g., labeling a mention with both /building and /location is not allowed), thus the task is easier; 3) By making the model deep, the performance on BBN is already improved a lot, which makes further improvement harder. The improvement of our full approach over Ours (DirectTrain) on FIGER (GOLD) indicates that the techniques we use to avoid overfitting the weakly labeled data are also effective. Ours (LocAttEL), which uses a more advanced EL system, does not achieve better performance than Ours (Full), which uses our own EL approach. After manually", "approach to create FET training samples is to use the anchor links in Wikipedia BIBREF0, BIBREF3. Each anchor link is regarded as a mention, and is weakly labeled with all the types of its referred entity (the Wikipedia page the anchor link points to) in KB. Our approach, when links the mention correctly, also uses all the types of the referred entity in KB as extra information. This may cause the trained model to overfit the weakly labeled data. We design a variant of the hinge loss and introduce noise during training to address this problem. We conduct experiments on two commonly used FET datasets. Experimental results show that introducing information obtained through entity linking and having a deep neural model both helps to improve FET performance. Our model achieves more than 5% absolute strict accuracy improvement over the state of the art on both datasets. Our contributions are summarized as follows: We propose a deep neural fine-grained entity typing model that utilizes type", "Introduction. Given a piece of text and the span of an entity mention in this text, fine-grained entity typing (FET) is the task of assigning fine-grained type labels to the mention BIBREF0. The assigned labels should be context dependent BIBREF1. For example, in the sentence \u201cTrump threatens to pull US out of World Trade Organization,\u201d the mention \u201cTrump\u201d should be labeled as /person and /person/politician, although Donald Trump also had other occupations such as businessman, TV personality, etc. This task is challenging because it usually uses a relatively large tag set, and some mentions may require the understanding of the context to be correctly labeled. Moreover, since manual annotation is very labor-intensive, existing approaches have to rely on distant supervision to train models BIBREF0, BIBREF2. Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to"]}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_answer": "Two commonly used FET datasets.", "predicted_evidence": ["it may incorrectly link \u201cMarch,\u201d the month, to an entity whose Wikipedia description fits the context better. 3) For some mentions, although the EL system links it to an incorrect entity, the type of this entity is the same with the correct entity. Conclusions. We propose a deep neural model to improve fine-grained entity typing with entity linking. The problem of overfitting the weakly labeled training data is addressed by using a variant of the hinge loss and introducing noise during training. We conduct experiments on two commonly used dataset. The experimental results demonstrates the effectiveness of our approach. Acknowledgments. This paper was supported by the Early Career Scheme (ECS, No. 26206717) from Research Grants Council in Hong Kong and WeChat-HKUST WHAT Lab on Artificial Intelligence Technology.", "strict accuracy improvement over the state of the art on both datasets. Our contributions are summarized as follows: We propose a deep neural fine-grained entity typing model that utilizes type information from KB obtained through entity linking. We address the problem that our model may overfit the weakly labeled data by using a variant of the hinge-loss and introducing noise during training. We demonstrate the effectiveness of our approach with experimental results on commonly used FET datasets. Our code is available at https://github.com/HKUST-KnowComp/IFETEL. Related Work. An early effort of classifying named entities into fine-grained types can be found in BIBREF4, which only focuses on person names. Latter, datasets with larger type sets are constructed BIBREF5, BIBREF0, BIBREF6. These datasets are more preferred by recent studies BIBREF3, BIBREF7. Most of the existing approaches proposed for FET are learning based. The features used by these approaches can either be", "string representation $\\mathbf {f}_s=(\\sum _{i=1}^l \\mathbf {x}_i)/l$. Method ::: Fine-grained Entity Typing Model ::: KB Type Representation. To obtain the KB type representation, we run an EL algorithm for the current mention. If the EL algorithm returns an entity, we retrieve the types of of this entity from the KB. We use Freebase as our KB. Since the types in Freebase is different from $T$, the target type set, they are mapped to the types in $T$ with rules similar to those used in BIBREF14. Afterwards, we perform one hot encoding on these types to get the KB Type Representation $\\mathbf {f}_e$. If the EL algorithm returns NIL (i.e., the mention cannot be linked to an entity), we simply one hot encode the empty type set. Method ::: Fine-grained Entity Typing Model ::: Prediction. Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector", "BIBREF2. Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to make typing decisions because if a mention is correctly linked to its target entity, we can directly obtain the type information about this entity in the knowledge base (KB). For example, in the sentence \u201cThere were some great discussions on a variety of issues facing Federal Way,\u201d the mention \u201cFederal Way\u201d may be incorrectly labeled as a company by some FET models. Such a mistake can be avoided after linking it to the city Federal Way, Washington. For cases that require the understanding of the context, using entity linking results is also beneficial. In the aforementioned example where \u201cTrump\u201d is the mention, obtaining all the types of Donald Trump in the knowledge base (e.g., politician, businessman, TV personality, etc.) is still informative for inferring the correct type (i.e., politician)", "Introduction. Given a piece of text and the span of an entity mention in this text, fine-grained entity typing (FET) is the task of assigning fine-grained type labels to the mention BIBREF0. The assigned labels should be context dependent BIBREF1. For example, in the sentence \u201cTrump threatens to pull US out of World Trade Organization,\u201d the mention \u201cTrump\u201d should be labeled as /person and /person/politician, although Donald Trump also had other occupations such as businessman, TV personality, etc. This task is challenging because it usually uses a relatively large tag set, and some mentions may require the understanding of the context to be correctly labeled. Moreover, since manual annotation is very labor-intensive, existing approaches have to rely on distant supervision to train models BIBREF0, BIBREF2. Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to", "approach to create FET training samples is to use the anchor links in Wikipedia BIBREF0, BIBREF3. Each anchor link is regarded as a mention, and is weakly labeled with all the types of its referred entity (the Wikipedia page the anchor link points to) in KB. Our approach, when links the mention correctly, also uses all the types of the referred entity in KB as extra information. This may cause the trained model to overfit the weakly labeled data. We design a variant of the hinge loss and introduce noise during training to address this problem. We conduct experiments on two commonly used FET datasets. Experimental results show that introducing information obtained through entity linking and having a deep neural model both helps to improve FET performance. Our model achieves more than 5% absolute strict accuracy improvement over the state of the art on both datasets. Our contributions are summarized as follows: We propose a deep neural fine-grained entity typing model that utilizes type", "Fine-grained Entity Typing Model ::: Input. Each input sample to our FET system contains one mention and the sentence it belongs to. We denote $w_1,w_2,...,w_n$ as the words in the current sentence, $w_{p_1},w_{p_2},...,w_{p_l}$ as the words in the mention string, where $n$ is the number of words in the sentence, $p_1,...,p_l$ are the indices of the words in the mention string, $l$ is the number of words in the mention string. We also use a set of pretrained word embeddings. Our FET approach is illustrated in Figure FIGREF4. It first constructs three representations: context representation, mention string representation, and KB type representation. Note that the KB type representation is obtained from a knowledge base through entity linking and is independent of the context of the mention. Method ::: Fine-grained Entity Typing Model ::: Context Representation. To obtain the context representation, we first use a special token $w_m$ to represent the mention (the token \u201c[Mention]\u201d in", "use EL to help with clustering and type name selection, which is very different from how we use it to improve the performance of a supervised FET model. BIBREF14 finds related entities based on the context instead of directly applying EL. The types of these entities are then used for inferring the type of the mention. Method. Let $T$ be a predefined tag set, which includes all the types we want to assign to mentions. Given a mention $m$ and its context, the task is to predict a set of types $\\mathbf {\\tau }\\subset T$ suitable for this mention. Thus, this is a multi-class, multi-label classification problem BIBREF0. Next, we will introduce our approach for this problem in detail, including the neural model, the training of the model, and the entity linking algorithm we use. Method ::: Fine-grained Entity Typing Model ::: Input. Each input sample to our FET system contains one mention and the sentence it belongs to. We denote $w_1,w_2,...,w_n$ as the words in the current sentence,"]}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "predicted_answer": "not provided", "predicted_evidence": ["system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1. CONCEPT RECOGNITION ::: Fine tuning with BERT. Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each", "is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain. Overall structure of this work is summarized in Figure 1. Implementation with demo and dataset is available at: https://github.com/jitinkrishnan/NASA-SE . BACKGROUND AND MOTIVATION. Creating commonsense AI still", "with the lack of efficient knowledge transfer of generic lessons-learned makes most technology-based missions risk-averse. Thus, a comprehensive commonsense engine can significantly enhance the productivity of any mission by letting the experts focus on what they do best. Concept Recognition (CR) is a task identical to the traditional Named Entity Recognition (NER) problem. A typical NER task seeks to identify entities like name of a person such as `Shakespeare', a geographical location such as `London', or name of an organisation such as `NASA' from unstructured text. A supervised NER dataset consists of the above mentioned entities annotated at the word-token level using labelling schemes such as BIO which provides beginning (B), continuation or inside (I), and outside (O) representation for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge", "for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge scenario because the entities we want to extract are domain-specific concepts such as `system architecture' or `functional requirements' rather than physical entities such as `Shakespeare' or `London'. This requires defining new labels and fine-tuning. Relation extraction tasks extract semantic relationships from text. These extractors aim to connect named entities such as `Shakespeare' and `England' using relations such as `born-in'. Relations can be as simple as using hand-built patterns or as challenging as using unsupervised methods like Open IE BIBREF10; with bootstrapping, supervised, and semi-supervised methods in between. BIBREF11 and BIBREF12 are some of the high performing models that extract relations from New York Times Corpus BIBREF13 and TACRED challenges BIBREF14 respectively.", "INTRODUCTION. The Systems Engineer's Virtual Assistant (SEVA) BIBREF0 was introduced with the goal to assist systems engineers (SE) in their problem-solving abilities by keeping track of large amounts of information of a NASA-specific project and using the information to answer queries from the user. In this work, we address a system element by constructing a common-knowledge concept recognition system for improving the performance of SEVA, using the static knowledge collected from the Systems Engineering Handbook BIBREF1 that is widely used in projects across the organization as domain-specific commonsense knowledge. At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although", "`complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section. CONCEPT RECOGNITION ::: BIO Labelling Scheme. abb: represents abbreviations such as TRL representing Technology Readiness Level. grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager. syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts. opcon: represents operational concepts such as decision analysis process, technology maturity assessment,", "only once.  RELATION EXTRACTION ::: Hyponyms from POS tags. Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is performed on a few selected entity tags such as opcon and syscon. For example, consider the sentence `SE functions should be performed'. `SE' has tag NNP and `functions' has tag NNS. We create a relation called subset-of between `SE functions' and `SE'.  RELATION EXTRACTION ::: Relations from Abbreviations.  Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo. RELATION EXTRACTION ::: Relation Extraction using Verb Phrase Chunking. Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from"]}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "predicted_answer": "Unanswerable", "predicted_evidence": ["`complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section. CONCEPT RECOGNITION ::: BIO Labelling Scheme. abb: represents abbreviations such as TRL representing Technology Readiness Level. grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager. syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts. opcon: represents operational concepts such as decision analysis process, technology maturity assessment,", "engineering unit, product, hardware, software, etc. They mostly represent physical concepts. opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc. seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc. event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc. org: represents an organization such as `NASA', `aerospace industry', etc. art: represents names of artifacts or instruments such as `AS1300' cardinal: represents numerical values such as `1', `100', 'one' etc. loc: represents location-like entities such as component facilities or centralized facility. mea: represents measures, features, or behaviors such as cost, risk, or feasibility. CONCEPT RECOGNITION ::: Abbreviations. Abbreviations are used frequently in SE text. We", "or centralized facility. mea: represents measures, features, or behaviors such as cost, risk, or feasibility. CONCEPT RECOGNITION ::: Abbreviations. Abbreviations are used frequently in SE text. We automatically extract abbreviations using simple pattern-matching around parentheses. Given below is a sample regex that matches most abbreviations in the SE handbook. r\"\\([ ]*[A-Z][A-Za-z]*[ ]*\\)\" An iterative regex matching procedure using this pattern over the preceding words will produce the full phrase of the abbreviation. `A process to determine a system\u2019s technological maturity based on Technology Readiness Levels (TRLs)' produces the abbreviation TRL which stands for Technology Readiness Levels. `Define one or more initial Concept of Operations (ConOps) scenarios' produces the abbreviation ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is", "RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1. CONCEPT RECOGNITION ::: Fine tuning with BERT. Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each", "for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge scenario because the entities we want to extract are domain-specific concepts such as `system architecture' or `functional requirements' rather than physical entities such as `Shakespeare' or `London'. This requires defining new labels and fine-tuning. Relation extraction tasks extract semantic relationships from text. These extractors aim to connect named entities such as `Shakespeare' and `England' using relations such as `born-in'. Relations can be as simple as using hand-built patterns or as challenging as using unsupervised methods like Open IE BIBREF10; with bootstrapping, supervised, and semi-supervised methods in between. BIBREF11 and BIBREF12 are some of the high performing models that extract relations from New York Times Corpus BIBREF13 and TACRED challenges BIBREF14 respectively.", "with the lack of efficient knowledge transfer of generic lessons-learned makes most technology-based missions risk-averse. Thus, a comprehensive commonsense engine can significantly enhance the productivity of any mission by letting the experts focus on what they do best. Concept Recognition (CR) is a task identical to the traditional Named Entity Recognition (NER) problem. A typical NER task seeks to identify entities like name of a person such as `Shakespeare', a geographical location such as `London', or name of an organisation such as `NASA' from unstructured text. A supervised NER dataset consists of the above mentioned entities annotated at the word-token level using labelling schemes such as BIO which provides beginning (B), continuation or inside (I), and outside (O) representation for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge", "ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts. CONCEPT RECOGNITION ::: Common-Knowledge Definitions. Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below: Definition: Acceptable Risk Description: The risk that is understood and agreed to by the program/project, governing authority, mission directorate, and other customer(s) such that no further specific mitigating action is required. CONCEPT RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which", "purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results. RELATION EXTRACTION. In this work, for relation extraction, we focus on hyponyms and verb phrase chunking."]}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "predicted_answer": "Unanswerable.", "predicted_evidence": ["system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1. CONCEPT RECOGNITION ::: Fine tuning with BERT. Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each", "for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge scenario because the entities we want to extract are domain-specific concepts such as `system architecture' or `functional requirements' rather than physical entities such as `Shakespeare' or `London'. This requires defining new labels and fine-tuning. Relation extraction tasks extract semantic relationships from text. These extractors aim to connect named entities such as `Shakespeare' and `England' using relations such as `born-in'. Relations can be as simple as using hand-built patterns or as challenging as using unsupervised methods like Open IE BIBREF10; with bootstrapping, supervised, and semi-supervised methods in between. BIBREF11 and BIBREF12 are some of the high performing models that extract relations from New York Times Corpus BIBREF13 and TACRED challenges BIBREF14 respectively.", "with the lack of efficient knowledge transfer of generic lessons-learned makes most technology-based missions risk-averse. Thus, a comprehensive commonsense engine can significantly enhance the productivity of any mission by letting the experts focus on what they do best. Concept Recognition (CR) is a task identical to the traditional Named Entity Recognition (NER) problem. A typical NER task seeks to identify entities like name of a person such as `Shakespeare', a geographical location such as `London', or name of an organisation such as `NASA' from unstructured text. A supervised NER dataset consists of the above mentioned entities annotated at the word-token level using labelling schemes such as BIO which provides beginning (B), continuation or inside (I), and outside (O) representation for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge", "purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results. RELATION EXTRACTION. In this work, for relation extraction, we focus on hyponyms and verb phrase chunking.", "only once.  RELATION EXTRACTION ::: Hyponyms from POS tags. Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is performed on a few selected entity tags such as opcon and syscon. For example, consider the sentence `SE functions should be performed'. `SE' has tag NNP and `functions' has tag NNS. We create a relation called subset-of between `SE functions' and `SE'.  RELATION EXTRACTION ::: Relations from Abbreviations.  Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo. RELATION EXTRACTION ::: Relation Extraction using Verb Phrase Chunking. Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from", "is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain. Overall structure of this work is summarized in Figure 1. Implementation with demo and dataset is available at: https://github.com/jitinkrishnan/NASA-SE . BACKGROUND AND MOTIVATION. Creating commonsense AI still", "ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts. CONCEPT RECOGNITION ::: Common-Knowledge Definitions. Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below: Definition: Acceptable Risk Description: The risk that is understood and agreed to by the program/project, governing authority, mission directorate, and other customer(s) such that no further specific mitigating action is required. CONCEPT RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which"]}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "predicted_answer": "BIO Labelling Scheme", "predicted_evidence": ["`complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section. CONCEPT RECOGNITION ::: BIO Labelling Scheme. abb: represents abbreviations such as TRL representing Technology Readiness Level. grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager. syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts. opcon: represents operational concepts such as decision analysis process, technology maturity assessment,", "engineering unit, product, hardware, software, etc. They mostly represent physical concepts. opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc. seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc. event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc. org: represents an organization such as `NASA', `aerospace industry', etc. art: represents names of artifacts or instruments such as `AS1300' cardinal: represents numerical values such as `1', `100', 'one' etc. loc: represents location-like entities such as component facilities or centralized facility. mea: represents measures, features, or behaviors such as cost, risk, or feasibility. CONCEPT RECOGNITION ::: Abbreviations. Abbreviations are used frequently in SE text. We", "system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "or centralized facility. mea: represents measures, features, or behaviors such as cost, risk, or feasibility. CONCEPT RECOGNITION ::: Abbreviations. Abbreviations are used frequently in SE text. We automatically extract abbreviations using simple pattern-matching around parentheses. Given below is a sample regex that matches most abbreviations in the SE handbook. r\"\\([ ]*[A-Z][A-Za-z]*[ ]*\\)\" An iterative regex matching procedure using this pattern over the preceding words will produce the full phrase of the abbreviation. `A process to determine a system\u2019s technological maturity based on Technology Readiness Levels (TRLs)' produces the abbreviation TRL which stands for Technology Readiness Levels. `Define one or more initial Concept of Operations (ConOps) scenarios' produces the abbreviation ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is", "only once.  RELATION EXTRACTION ::: Hyponyms from POS tags. Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is performed on a few selected entity tags such as opcon and syscon. For example, consider the sentence `SE functions should be performed'. `SE' has tag NNP and `functions' has tag NNS. We create a relation called subset-of between `SE functions' and `SE'.  RELATION EXTRACTION ::: Relations from Abbreviations.  Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo. RELATION EXTRACTION ::: Relation Extraction using Verb Phrase Chunking. Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from", "purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results. RELATION EXTRACTION. In this work, for relation extraction, we focus on hyponyms and verb phrase chunking.", "with the lack of efficient knowledge transfer of generic lessons-learned makes most technology-based missions risk-averse. Thus, a comprehensive commonsense engine can significantly enhance the productivity of any mission by letting the experts focus on what they do best. Concept Recognition (CR) is a task identical to the traditional Named Entity Recognition (NER) problem. A typical NER task seeks to identify entities like name of a person such as `Shakespeare', a geographical location such as `London', or name of an organisation such as `NASA' from unstructured text. A supervised NER dataset consists of the above mentioned entities annotated at the word-token level using labelling schemes such as BIO which provides beginning (B), continuation or inside (I), and outside (O) representation for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge", "ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts. CONCEPT RECOGNITION ::: Common-Knowledge Definitions. Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below: Definition: Acceptable Risk Description: The risk that is understood and agreed to by the program/project, governing authority, mission directorate, and other customer(s) such that no further specific mitigating action is required. CONCEPT RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which"]}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "predicted_answer": "BERT", "predicted_evidence": ["purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results. RELATION EXTRACTION. In this work, for relation extraction, we focus on hyponyms and verb phrase chunking.", "RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1. CONCEPT RECOGNITION ::: Fine tuning with BERT. Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each", "EXTRACTION ::: Relation Extraction using Verb Phrase Chunking. Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from definitions. Only those phrases that connect two entities are selected for verb phrase extraction. Using NLTK's regex parser and chunker, a grammar such as VP: {(<MD>|<R.*>|<I.*>|<VB.*>|<JJ.*>| <TO>)*<VB.*>+(<MD>|<R.*>|<I.*>|<VB.*>| <JJ.*>|<TO>)*} with at least one verb, can extract relation-like phrases from the phrase that links two concepts. An example is shown in Figure FIGREF27. Further investigation of relation extraction from SE handbook is left as future work. CONCLUSION AND FUTURE WORK. We presented a common-knowledge concept extractor for the Systems Engineer's Virtual Assistant (SEVA) system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain", "system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "we also implemented CR using spaCy BIBREF18 which also produced similar results. RELATION EXTRACTION. In this work, for relation extraction, we focus on hyponyms and verb phrase chunking. Hyponyms are more specific concepts such as earth to planet or rose to flower. Verb phrase chunking connects the named entities recognized by the CR model through verbs. RELATION EXTRACTION ::: Hyponyms from Definitions. The definition document consists of 241 SE definitions and their descriptions. We iteratively construct entities in increasing order of number of words in the definitions with the help of their parts-of-speech tags. This helps in creating subset-of relation between a lower-word entity and a higher-word entity. Each root entity is lemmatized such that entities like processes and process appear only once.  RELATION EXTRACTION ::: Hyponyms from POS tags. Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is", "methods in between. BIBREF11 and BIBREF12 are some of the high performing models that extract relations from New York Times Corpus BIBREF13 and TACRED challenges BIBREF14 respectively. Hyponyms represent hierarchical connection between entities of a domain and represent important relationships. For instance, a well-known work by BIBREF15 uses syntactic patterns such as [Y such as A, B, C], [Y including X], or [Y, including X] to extract hyponyms. Our goal is to extract preliminary hyponym relations from the concepts extracted by the CR and to connect the entities through verb phrases. CONCEPT RECOGNITION. SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would", "or centralized facility. mea: represents measures, features, or behaviors such as cost, risk, or feasibility. CONCEPT RECOGNITION ::: Abbreviations. Abbreviations are used frequently in SE text. We automatically extract abbreviations using simple pattern-matching around parentheses. Given below is a sample regex that matches most abbreviations in the SE handbook. r\"\\([ ]*[A-Z][A-Za-z]*[ ]*\\)\" An iterative regex matching procedure using this pattern over the preceding words will produce the full phrase of the abbreviation. `A process to determine a system\u2019s technological maturity based on Technology Readiness Levels (TRLs)' produces the abbreviation TRL which stands for Technology Readiness Levels. `Define one or more initial Concept of Operations (ConOps) scenarios' produces the abbreviation ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is", "with the lack of efficient knowledge transfer of generic lessons-learned makes most technology-based missions risk-averse. Thus, a comprehensive commonsense engine can significantly enhance the productivity of any mission by letting the experts focus on what they do best. Concept Recognition (CR) is a task identical to the traditional Named Entity Recognition (NER) problem. A typical NER task seeks to identify entities like name of a person such as `Shakespeare', a geographical location such as `London', or name of an organisation such as `NASA' from unstructured text. A supervised NER dataset consists of the above mentioned entities annotated at the word-token level using labelling schemes such as BIO which provides beginning (B), continuation or inside (I), and outside (O) representation for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge"]}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "predicted_answer": "not specified", "predicted_evidence": ["system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which includes joining sentences that are split, removing URLs, shortening duplicate non-alpha characters, and replacing full forms of abbreviations with their shortened forms. We assume that the SE text is free of spelling errors. For the CR dataset, we select coherent paragraphs and full sentences by avoiding headers and short blurbs. Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level. An example is shown in Figure 2 and the unique tag count is shown in Table 1. CONCEPT RECOGNITION ::: Fine tuning with BERT. Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each", "purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings. In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results. RELATION EXTRACTION. In this work, for relation extraction, we focus on hyponyms and verb phrase chunking.", "with the lack of efficient knowledge transfer of generic lessons-learned makes most technology-based missions risk-averse. Thus, a comprehensive commonsense engine can significantly enhance the productivity of any mission by letting the experts focus on what they do best. Concept Recognition (CR) is a task identical to the traditional Named Entity Recognition (NER) problem. A typical NER task seeks to identify entities like name of a person such as `Shakespeare', a geographical location such as `London', or name of an organisation such as `NASA' from unstructured text. A supervised NER dataset consists of the above mentioned entities annotated at the word-token level using labelling schemes such as BIO which provides beginning (B), continuation or inside (I), and outside (O) representation for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge", "engineering unit, product, hardware, software, etc. They mostly represent physical concepts. opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc. seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc. event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc. org: represents an organization such as `NASA', `aerospace industry', etc. art: represents names of artifacts or instruments such as `AS1300' cardinal: represents numerical values such as `1', `100', 'one' etc. loc: represents location-like entities such as component facilities or centralized facility. mea: represents measures, features, or behaviors such as cost, risk, or feasibility. CONCEPT RECOGNITION ::: Abbreviations. Abbreviations are used frequently in SE text. We", "for each word of an entity. BIBREF8 is the current top-performing NER model for CoNLL-2003 shared task BIBREF9. Off-the-shelf named entity extractors do not suffice in the SE common-knowledge scenario because the entities we want to extract are domain-specific concepts such as `system architecture' or `functional requirements' rather than physical entities such as `Shakespeare' or `London'. This requires defining new labels and fine-tuning. Relation extraction tasks extract semantic relationships from text. These extractors aim to connect named entities such as `Shakespeare' and `England' using relations such as `born-in'. Relations can be as simple as using hand-built patterns or as challenging as using unsupervised methods like Open IE BIBREF10; with bootstrapping, supervised, and semi-supervised methods in between. BIBREF11 and BIBREF12 are some of the high performing models that extract relations from New York Times Corpus BIBREF13 and TACRED challenges BIBREF14 respectively.", "ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts. CONCEPT RECOGNITION ::: Common-Knowledge Definitions. Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below: Definition: Acceptable Risk Description: The risk that is understood and agreed to by the program/project, governing authority, mission directorate, and other customer(s) such that no further specific mitigating action is required. CONCEPT RECOGNITION ::: CR Dataset Construction and Pre-processing. Using python tools such as PyPDF2, NLTK, and RegEx we build a pipeline to convert PDF to raw text along with extensive pre-processing which", "only once.  RELATION EXTRACTION ::: Hyponyms from POS tags. Using the words (especially nouns) that surround an already identified named entity, more specific entities can be identified. This is performed on a few selected entity tags such as opcon and syscon. For example, consider the sentence `SE functions should be performed'. `SE' has tag NNP and `functions' has tag NNS. We create a relation called subset-of between `SE functions' and `SE'.  RELATION EXTRACTION ::: Relations from Abbreviations.  Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo. RELATION EXTRACTION ::: Relation Extraction using Verb Phrase Chunking. Finally, we explore creating contextual triples from sentences using all the entities extracted using the CR model and entities from"]}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "predicted_answer": "precision, recall, and F-measure", "predicted_evidence": ["averaged from 10 iterations. Results of classification for per category. Table. TABREF19 and TABREF20 show the classification performance of different methods.  The results were examined from the following aspects: When the feature dimension is set to 100 and the training corpus is ACL, the results generated by different models were compared (AVGWVEC, PARAVEC and AVGWVEC+BSWE for BAS category only). Looking at the F-measure, AVGWVEC performs better than PARAVEC, but PARAVEC gave a better precision results on several categories, such as AIM, CTR, TXT and OWN. The results showed that PARAVEC model is not robust, for example, it performs badly for the category of BAS. For specific category classification, take the BAS category for example, the BSWE model outperforms others in terms of F-measure. When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories,", "When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories, 300-dimension features perform better than the 100-dimension ones, for example, CTR and BKG, but they are not as good as 100-dimension features for some categories, such as BAS. When the model is set to AVGWVEC and the feature dimension is 100, the results computed from different training corpus were compared (ACL+AZ, MixedAbs and Brown corpus). ACL+AZ outperforms others and brown corpus is better than MixedAbs for most of the categories, but brown corpus is not as good as MixedAbs for the category of OWN. Finally, the results were compared between word embeddings and the methods of cuewords, Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is", "of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100 dimensions. Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ. In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results. Conclusion. In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features", ", the word to vector features were set up as follows: the Minimum word count is 40; The number of threads to run in parallel is 4 and the context window is 10. Strategy of dealing with unbalanced data. In imbalanced data sets, some classes are significantly outnumbered by other classes BIBREF27 , which affects the classification results. In this experiment, the test dataset is an imbalanced data set. Table. TABREF16 shows the distribution of rhetorical categories from the INLINEFORM0 test dataset. The categories OWN and OTH are significantly outnumbering other categories. To deal with the problem of classification on unbalanced data, synthetic Minority Over-sampling TEchnique (SMOTE) BIBREF28 were performed on the original dataset. 10-cross validation scheme was adopted and the results were averaged from 10 iterations. Results of classification for per category. Table. TABREF19 and TABREF20 show the classification performance of different methods.  The results were examined from the", "are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories. To evaluate the classification performance, precision, recall and F-measure were computed. Training Dataset.  INLINEFORM0 collection. ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which 622,144 sentences were generated after filtering out sentences with lower quality.  INLINEFORM0 collection contains 6,778 sentences, extracted from the titles and abstracts of publications provided by WEB OF SCIENCE . Test Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After", ", BIBREF22 , BIBREF23 and text classification BIBREF24 . Sadeghian and Sharafat BIBREF25 explored averaging of the word vectors in a sentiment review statement. Their results indicated that word2vec models significantly outperform the vanilla bag-of-words model. Amongst the word2vec based models, softmax provides the best form of classification. Tang et al. BIBREF21 used the concatenation of vectors derived from different convolutional layers to analyze the sentiment statements. They also trained sentiment-specific word embeddings to improve the twitter sentiment classification results. This work is aiming at learning word embeddings for the task of AZ. The results were compared from three aspects: the impact of the training corpus, the effectiveness of specific word embeddings and different ways of constructing sentence representations based on the learned word vectors. Le and Mikolov BIBREF26 introduced the concept of word vector representation in a formal way: Given a sequence of", "matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 . The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 . Classification and evaluation. The learned word embeddings are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be", "Document summarization from social media and news circles has received much attention for the past decades. Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these"]}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "predicted_answer": "qualifying adjectives and meta-discourse features", "predicted_evidence": ["Document summarization from social media and news circles has received much attention for the past decades. Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these", "of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community. There are plenty of cases where word embedding and sentence representations have been applied to short text classification BIBREF17 and paraphrase detection BIBREF18 . However, the effectiveness of this technique on AZ needs further study. The research question is, is it possible to extract word embeddings as features to classify sentences into the seven categories mentioned above using supervised machine learning approach? Related Work. The tool of word2vec proposed by Mikolov et al. BIBREF16 has", "When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories, 300-dimension features perform better than the 100-dimension ones, for example, CTR and BKG, but they are not as good as 100-dimension features for some categories, such as BAS. When the model is set to AVGWVEC and the feature dimension is 100, the results computed from different training corpus were compared (ACL+AZ, MixedAbs and Brown corpus). ACL+AZ outperforms others and brown corpus is better than MixedAbs for most of the categories, but brown corpus is not as good as MixedAbs for the category of OWN. Finally, the results were compared between word embeddings and the methods of cuewords, Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is", "information from multiple documents. Fortunately, computer algorithms have been introduced to solve this problem. With the development of artificial intelligence, machine learning and computational linguistics, Natural Language Processing (NLP) has become a popular research area BIBREF4 , BIBREF5 . NLP covers the applications from document retrieval, text categorization BIBREF6 , document summarization BIBREF7 to sentiment analysis BIBREF8 , BIBREF9 . Those applications are targeting different types of text resources, such as articles from social media BIBREF10 and scientific publications BIBREF2 . There are several approaches to tackle these tasks. From machine learning prospective, text can be analysed via supervised BIBREF2 , semi-supervised BIBREF11 and unsupervised BIBREF12 algorithms. Document summarization from social media and news circles has received much attention for the past decades. Those problems have been addressed from many angles, one of which is feature extraction", "of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100 dimensions. Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ. In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results. Conclusion. In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features", "For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual. Analyzing the rhetorical status of sentences manually requires huge amount of efforts, especially for structuring information from multiple documents. Fortunately, computer algorithms have been introduced to solve this problem. With the development of artificial intelligence, machine learning and computational", "are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories. To evaluate the classification performance, precision, recall and F-measure were computed. Training Dataset.  INLINEFORM0 collection. ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which 622,144 sentences were generated after filtering out sentences with lower quality.  INLINEFORM0 collection contains 6,778 sentences, extracted from the titles and abstracts of publications provided by WEB OF SCIENCE . Test Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After", "averaged from 10 iterations. Results of classification for per category. Table. TABREF19 and TABREF20 show the classification performance of different methods.  The results were examined from the following aspects: When the feature dimension is set to 100 and the training corpus is ACL, the results generated by different models were compared (AVGWVEC, PARAVEC and AVGWVEC+BSWE for BAS category only). Looking at the F-measure, AVGWVEC performs better than PARAVEC, but PARAVEC gave a better precision results on several categories, such as AIM, CTR, TXT and OWN. The results showed that PARAVEC model is not robust, for example, it performs badly for the category of BAS. For specific category classification, take the BAS category for example, the BSWE model outperforms others in terms of F-measure. When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories,"]}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "predicted_answer": "word2vec", "predicted_evidence": ["of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community. There are plenty of cases where word embedding and sentence representations have been applied to short text classification BIBREF17 and paraphrase detection BIBREF18 . However, the effectiveness of this technique on AZ needs further study. The research question is, is it possible to extract word embeddings as features to classify sentences into the seven categories mentioned above using supervised machine learning approach? Related Work. The tool of word2vec proposed by Mikolov et al. BIBREF16 has", "were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features of statements and they are easier to be obtained than hand engineered features. To improve the sentence classification for a specific category, integrating word specific embedding strategy helps. The size of the feature pool doesn't matter too much on the results, nor does the vocabulary size. In comparison, the domain of the training corpus affects the classification performance.", "as features to classify sentences into the seven categories mentioned above using supervised machine learning approach? Related Work. The tool of word2vec proposed by Mikolov et al. BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such", "matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 . The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 . Classification and evaluation. The learned word embeddings are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be", "words in one sentence, paragraph vectors and specific word vectors. The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process in this model is to learn the word embedding matrix INLINEFORM2 :  INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 (4) where INLINEFORM0 is the word embedding for word INLINEFORM1 , which is learned by the classical word2vec algorithm BIBREF16 . The second model, INLINEFORM0 , is aiming at training paragraph vectors. It is also called distributed memory model of paragraph vectors (PV-DM) BIBREF26 , which is an extension of word2vec. In comparison with the word2vec framework, the only change in PV-DM is in the equation (3), where INLINEFORM1 is constructed from INLINEFORM2 and INLINEFORM3 , where matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 . The", "Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is better than the method using cuewords matching. It also outperforms Teufel 2002 for most of the cases, except AIM, BAS and OWN. It won baseline for most of the categories, except OWN. Discussion. The classification results showed that the type of word embeddings and the training corpus affect the AZ performance. As the simple model, INLINEFORM0 performs better than others, which indicate averaging the word vectors in a sentence can capture the semantic property of statements. By training specific argumentation word embeddings, the performance can be improved, which can be seen from the case of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100", "ways of constructing sentence representations based on the learned word vectors. Le and Mikolov BIBREF26 introduced the concept of word vector representation in a formal way: Given a sequence of training words INLINEFORM0 , the objective of the word2vec model is to maximize the average log probability:  INLINEFORM0 INLINEFORM1 INLINEFORM2 p INLINEFORM3 (1) Using softmax technique, the prediction can be formalized as: p INLINEFORM0 = INLINEFORM1 (2) Each of INLINEFORM0 is un-normalized log probability for each output word INLINEFORM1 :  INLINEFORM0 (3) Models. In this study, sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors. The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process", "of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100 dimensions. Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ. In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results. Conclusion. In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features"]}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "predicted_answer": "Yes", "predicted_evidence": [", the word to vector features were set up as follows: the Minimum word count is 40; The number of threads to run in parallel is 4 and the context window is 10. Strategy of dealing with unbalanced data. In imbalanced data sets, some classes are significantly outnumbered by other classes BIBREF27 , which affects the classification results. In this experiment, the test dataset is an imbalanced data set. Table. TABREF16 shows the distribution of rhetorical categories from the INLINEFORM0 test dataset. The categories OWN and OTH are significantly outnumbering other categories. To deal with the problem of classification on unbalanced data, synthetic Minority Over-sampling TEchnique (SMOTE) BIBREF28 were performed on the original dataset. 10-cross validation scheme was adopted and the results were averaged from 10 iterations. Results of classification for per category. Table. TABREF19 and TABREF20 show the classification performance of different methods.  The results were examined from the", "For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual. Analyzing the rhetorical status of sentences manually requires huge amount of efforts, especially for structuring information from multiple documents. Fortunately, computer algorithms have been introduced to solve this problem. With the development of artificial intelligence, machine learning and computational", "Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After Concatenating sub-sentences, 7,347 labeled sentences were obtained. Training strategy. To compare the three models effectiveness on the AZ task, the three models on a same ACL dataset (introduced int he dataset section) were trained. The word2vec were also trained using different parameters, such as different dimension of features. To evaluate the impact from different domains, the first model was trained on different corpus. The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 . Parameters. Inspired by the work from Sadeghian and Sharafat BIBREF25 , the word to vector features were set up as follows: the Minimum word count is 40; The number of threads to run in parallel is 4 and the context window is 10. Strategy of dealing with unbalanced", "are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories. To evaluate the classification performance, precision, recall and F-measure were computed. Training Dataset.  INLINEFORM0 collection. ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which 622,144 sentences were generated after filtering out sentences with lower quality.  INLINEFORM0 collection contains 6,778 sentences, extracted from the titles and abstracts of publications provided by WEB OF SCIENCE . Test Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After", "Document summarization from social media and news circles has received much attention for the past decades. Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these", "When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories, 300-dimension features perform better than the 100-dimension ones, for example, CTR and BKG, but they are not as good as 100-dimension features for some categories, such as BAS. When the model is set to AVGWVEC and the feature dimension is 100, the results computed from different training corpus were compared (ACL+AZ, MixedAbs and Brown corpus). ACL+AZ outperforms others and brown corpus is better than MixedAbs for most of the categories, but brown corpus is not as good as MixedAbs for the category of OWN. Finally, the results were compared between word embeddings and the methods of cuewords, Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is", "of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100 dimensions. Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ. In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results. Conclusion. In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features", "Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is better than the method using cuewords matching. It also outperforms Teufel 2002 for most of the cases, except AIM, BAS and OWN. It won baseline for most of the categories, except OWN. Discussion. The classification results showed that the type of word embeddings and the training corpus affect the AZ performance. As the simple model, INLINEFORM0 performs better than others, which indicate averaging the word vectors in a sentence can capture the semantic property of statements. By training specific argumentation word embeddings, the performance can be improved, which can be seen from the case of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100"]}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "predicted_answer": "averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors.", "predicted_evidence": ["of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community. There are plenty of cases where word embedding and sentence representations have been applied to short text classification BIBREF17 and paraphrase detection BIBREF18 . However, the effectiveness of this technique on AZ needs further study. The research question is, is it possible to extract word embeddings as features to classify sentences into the seven categories mentioned above using supervised machine learning approach? Related Work. The tool of word2vec proposed by Mikolov et al. BIBREF16 has", "ways of constructing sentence representations based on the learned word vectors. Le and Mikolov BIBREF26 introduced the concept of word vector representation in a formal way: Given a sequence of training words INLINEFORM0 , the objective of the word2vec model is to maximize the average log probability:  INLINEFORM0 INLINEFORM1 INLINEFORM2 p INLINEFORM3 (1) Using softmax technique, the prediction can be formalized as: p INLINEFORM0 = INLINEFORM1 (2) Each of INLINEFORM0 is un-normalized log probability for each output word INLINEFORM1 :  INLINEFORM0 (3) Models. In this study, sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors. The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process", "were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features of statements and they are easier to be obtained than hand engineered features. To improve the sentence classification for a specific category, integrating word specific embedding strategy helps. The size of the feature pool doesn't matter too much on the results, nor does the vocabulary size. In comparison, the domain of the training corpus affects the classification performance.", "matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 . The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 . Classification and evaluation. The learned word embeddings are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be", "as features to classify sentences into the seven categories mentioned above using supervised machine learning approach? Related Work. The tool of word2vec proposed by Mikolov et al. BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such", "are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories. To evaluate the classification performance, precision, recall and F-measure were computed. Training Dataset.  INLINEFORM0 collection. ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which 622,144 sentences were generated after filtering out sentences with lower quality.  INLINEFORM0 collection contains 6,778 sentences, extracted from the titles and abstracts of publications provided by WEB OF SCIENCE . Test Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After", "words in one sentence, paragraph vectors and specific word vectors. The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process in this model is to learn the word embedding matrix INLINEFORM2 :  INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 (4) where INLINEFORM0 is the word embedding for word INLINEFORM1 , which is learned by the classical word2vec algorithm BIBREF16 . The second model, INLINEFORM0 , is aiming at training paragraph vectors. It is also called distributed memory model of paragraph vectors (PV-DM) BIBREF26 , which is an extension of word2vec. In comparison with the word2vec framework, the only change in PV-DM is in the equation (3), where INLINEFORM1 is constructed from INLINEFORM2 and INLINEFORM3 , where matrix INLINEFORM4 is the word vector and INLINEFORM5 holds the paragraph vectors in such a way that every paragraph is mapped to a unique vector represented by a column in matrix INLINEFORM6 . The", "Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is better than the method using cuewords matching. It also outperforms Teufel 2002 for most of the cases, except AIM, BAS and OWN. It won baseline for most of the categories, except OWN. Discussion. The classification results showed that the type of word embeddings and the training corpus affect the AZ performance. As the simple model, INLINEFORM0 performs better than others, which indicate averaging the word vectors in a sentence can capture the semantic property of statements. By training specific argumentation word embeddings, the performance can be improved, which can be seen from the case of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100"]}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "predicted_answer": "A process of assigning rhetorical status to sentences.", "predicted_evidence": ["of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100 dimensions. Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ. In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results. Conclusion. In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features", "Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After Concatenating sub-sentences, 7,347 labeled sentences were obtained. Training strategy. To compare the three models effectiveness on the AZ task, the three models on a same ACL dataset (introduced int he dataset section) were trained. The word2vec were also trained using different parameters, such as different dimension of features. To evaluate the impact from different domains, the first model was trained on different corpus. The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 . Parameters. Inspired by the work from Sadeghian and Sharafat BIBREF25 , the word to vector features were set up as follows: the Minimum word count is 40; The number of threads to run in parallel is 4 and the context window is 10. Strategy of dealing with unbalanced", "texts. Unlike document summarization from news circles, where the task is to identify centroid sentences BIBREF0 or to extract the first few sentences of the paragraphs BIBREF1 , summarization of scientific articles involves extra text processing stage BIBREF2 . After highest ranked texts are extracted, rhetorical status analysis will be conducted on the selected sentences. Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. The results of AZ provide readers with general discourse context from which the scientific ideas could be better linked, compared and analyzed. For example, given a specific task, which sentences should be shown to the reader is related to the features of the sentences. For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with", "For the task of identifying a paper's unique contribution, sentences expressing research purpose should be retrieved with higher priority. For comparing ideas, statements of comparison with other works would be more useful. Teufel et. al. BIBREF2 introduced their rhetorical annotation scheme which takes into account of the aspects of argumentation, metadiscourse and relatedness to other works. Their scheme resulted seven categories of rhetorical status and the categories are assigned to full sentences. Examples of human annotated sentences with their rhetorical status are shown in Table. TABREF2 . The seven categories are aim, contrast, own, background, other, basis and textual. Analyzing the rhetorical status of sentences manually requires huge amount of efforts, especially for structuring information from multiple documents. Fortunately, computer algorithms have been introduced to solve this problem. With the development of artificial intelligence, machine learning and computational", "Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is better than the method using cuewords matching. It also outperforms Teufel 2002 for most of the cases, except AIM, BAS and OWN. It won baseline for most of the categories, except OWN. Discussion. The classification results showed that the type of word embeddings and the training corpus affect the AZ performance. As the simple model, INLINEFORM0 performs better than others, which indicate averaging the word vectors in a sentence can capture the semantic property of statements. By training specific argumentation word embeddings, the performance can be improved, which can be seen from the case of detecting BAS status using INLINEFORM1 model. Feature dimension doesn't dominate the results. There is no significant difference between the resutls generated by 300-dimension of features and 100", "are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories. To evaluate the classification performance, precision, recall and F-measure were computed. Training Dataset.  INLINEFORM0 collection. ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which 622,144 sentences were generated after filtering out sentences with lower quality.  INLINEFORM0 collection contains 6,778 sentences, extracted from the titles and abstracts of publications provided by WEB OF SCIENCE . Test Dataset. Argumentative Zoning Corpus ( INLINEFORM0 corpus) consists of 80 AZ INLINEFORM1 annotated conference articles in computational linguistics, originally drawn from the Cmplg arXiv. . After", ", the word to vector features were set up as follows: the Minimum word count is 40; The number of threads to run in parallel is 4 and the context window is 10. Strategy of dealing with unbalanced data. In imbalanced data sets, some classes are significantly outnumbered by other classes BIBREF27 , which affects the classification results. In this experiment, the test dataset is an imbalanced data set. Table. TABREF16 shows the distribution of rhetorical categories from the INLINEFORM0 test dataset. The categories OWN and OTH are significantly outnumbering other categories. To deal with the problem of classification on unbalanced data, synthetic Minority Over-sampling TEchnique (SMOTE) BIBREF28 were performed on the original dataset. 10-cross validation scheme was adopted and the results were averaged from 10 iterations. Results of classification for per category. Table. TABREF19 and TABREF20 show the classification performance of different methods.  The results were examined from the", "When the model is fixed to AVGWVEC and the training corpus is ACL, the feature size impact (300 and 100 dimensions) was investigated. From the F-measure, it can be seen that for some categories, 300-dimension features perform better than the 100-dimension ones, for example, CTR and BKG, but they are not as good as 100-dimension features for some categories, such as BAS. When the model is set to AVGWVEC and the feature dimension is 100, the results computed from different training corpus were compared (ACL+AZ, MixedAbs and Brown corpus). ACL+AZ outperforms others and brown corpus is better than MixedAbs for most of the categories, but brown corpus is not as good as MixedAbs for the category of OWN. Finally, the results were compared between word embeddings and the methods of cuewords, Teufel 2002 and baseline. To evaluate word embeddings on AZ, the model AVGWVEC trained on ACL+AZ was used for the comparison. It can be seen from the table. TABREF19 , the model of word embeddings is"]}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "predicted_answer": "using Twitter's REST API to collect the tweet objects", "predicted_evidence": ["Introduction. Twitter is an important medium for people and companies to promote their products, ideologies, or to reach out and connect with other people in the form of micro-conversations. Twitter provides users with multiple ways of showing their support towards a tweet in the form of Likes, Retweets and Quotes. These content-level appraisals help in spreading the content further and act as a measure of users' agreement on the value of the content. The count of these content-level appraisals therefore determines the influence of a particular tweet and its author. This has led to the creation of certain blackmarket services such as FreeFollowers (https://www.freefollowers.io/), Like4Like (https://like4like.org/), YouLikeHits (https://www.youlikehits.com/), JustRetweet (http://justretweet.com), which allow users to post their tweets in order to gain inorganic appraisals in the form of Likes, Retweets and Quotes BIBREF0 , BIBREF1 . There has been a lot of research on the detection of", "other than the tweets from Others class included at least one URL, and 100% of the URLs in the blackmarket tweets were shortened. Proposed Approach. This section describes the features and tweet representation methodology, and the proposed model to solve the problem. Tweet Content Features. We use the following features based on the tweet content:  INLINEFORM0 : Number of user mentions in the tweet  INLINEFORM0 : Number of hashtags in the tweet  INLINEFORM0 : Number of URLs in the tweet  INLINEFORM0 : Count of media content in the tweet  INLINEFORM0 : Is the tweet a reply to another tweet?  INLINEFORM0 : Number of special characters (non alpha-numeric) in the tweet  INLINEFORM0 : Length of the content (number of characters) in the tweet  INLINEFORM0 : Sentiment score of the tweet obtained using SentiWordNet, ranging from -1 (negative) to +1 (positive)  INLINEFORM0 : Number of noun words in the tweet  INLINEFORM0 : Number of adjective words in the tweet  INLINEFORM0 : Number of pronoun", "gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites. Dataset Description. In total, we collected INLINEFORM0 tweets posted on blackmarket sites. Out of these, we removed non-English tweets and tweets with a length of less than two characters. Finally, we were left with INLINEFORM1 blackmarket tweets. Then, from the timelines of the authors of these tweets, we randomly sampled INLINEFORM2 genuine tweets that were not posted on these blackmarket sites during the same period. Both the blackmarket and genuine tweets were also inspected manually. Analysis of Blackmarket Tweets. To further understand the purpose of the collusive users behind the usage of blackmarket services, we annotated blackmarket tweets in our test set into a few discrete categories. The statistics of the", "using SentiWordNet, ranging from -1 (negative) to +1 (positive)  INLINEFORM0 : Number of noun words in the tweet  INLINEFORM0 : Number of adjective words in the tweet  INLINEFORM0 : Number of pronoun words in the tweet  INLINEFORM0 : Number of verbs in the tweet Tweet Content Representation. We use the Tweet2Vec model BIBREF5 to generate a vector-space representation of each of the tweets. Tweet2Vec is a character-level deep learning based encoder for social media posts trained on the task of predicting the associated hashtags. It considers the assumption that posts with the same hashtags should have similar representation. It uses a bi-directional Gated Recurrent Unit (Bi-GRU) for learning the tweet representation. To get the representation for a particular tweet, the model combines the final GRU states by going through a forward and backward pass over the entire sequence. We use the pre-trained model provided by Dhingra et al. BIBREF5 , which is trained on a dataset of 2 million", "in identifying individual tweets as blackmarket tweets (as shown in Table TABREF29 ). Table TABREF1 shows a sample tweet that was posted on a blackmarket service and another sample tweet that was not. In this paper, we make the first attempt to detect tweets that are posted on blackmarket services. Our aim is to build a system that can flag tweets soon after they are posted, which is why we do not consider temporal features such as the number of retweets or likes that a tweet keeps gaining over time. Instead, we only rely on the features and representations extracted from the content of the tweets. We curate a novel dataset of tweets that have been posted to blackmarket services, and a corresponding set of tweets that haven't. We propose a multitask learning approach to combine properties from the characterization of blackmarket tweets via traditional feature extraction, with a deep learning based feature representation of the tweets. We train a neural network which takes as input", "further understand the purpose of the collusive users behind the usage of blackmarket services, we annotated blackmarket tweets in our test set into a few discrete categories. The statistics of the categories are as follows: Promotional - 43.75%, Entertainment - 15.89%, Spam - 13.57%, News - 7.86%, Politics - 4.82%, and Others - 14.11%. We considered a tweet as Promotional only if the tweet attempts to promote a website/product. Most of the tweets in the Others category include personal tweets without any call to action or promotion, but this also can be considered as self-promotion. We further noticed that there were about 5% of normal tweets on concerning issues such as \u201cpray for ...\", indicating that blackmarket services are also being used for non-business purposes. 99% of tweets other than the tweets from Others class included at least one URL, and 100% of the URLs in the blackmarket tweets were shortened. Proposed Approach. This section describes the features and tweet", "which allow users to post their tweets in order to gain inorganic appraisals in the form of Likes, Retweets and Quotes BIBREF0 , BIBREF1 . There has been a lot of research on the detection of fraudulent activities on Twitter such as detection of bots BIBREF2 , fake followers BIBREF3 , collusive retweeters BIBREF0 , BIBREF1 , and social spam BIBREF4 . However, the problem of detecting tweets that are posted to these blackmarket services has not been tackled before. The tweets submitted to blackmarket services are not necessarily spam or promotional tweets. As we observe in our data, there is some intersection between spammers and blackmarket users since spammers may also try to gain more appraisals by using these services. However, existing spam tweet detection approaches do not work that well in identifying individual tweets as blackmarket tweets (as shown in Table TABREF29 ). Table TABREF1 shows a sample tweet that was posted on a blackmarket service and another sample tweet that was", "final GRU states by going through a forward and backward pass over the entire sequence. We use the pre-trained model provided by Dhingra et al. BIBREF5 , which is trained on a dataset of 2 million tweets, to get the tweet representation. This gives us a 500-dimensional representation of each tweet, based on its content. Proposed Model. The architecture of our model is shown in Figure FIGREF21 . We adopt multitask learning to develop our model. The primary task is set as a binary classification problem, wherein the tweets are classified as blackmarket or genuine. The secondary task is set as a regression problem, wherein the number of likes and retweets that a tweet will gain after five days of being posted is predicted. The model takes a different input feature vector for each of the tasks. Primary Input: The primary task takes as input the tweet content representation generated by the Tweet2Vec model, which is a 500-dimensional vector for each of the tweets, as described above."]}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "predicted_answer": "Spam Detector 2 BIBREF29", "predicted_evidence": ["results are reported after 5-fold cross-validation. Experimental Results. As shown in Table TABREF29 , we observe that the multitask learning based model which uses the Tweet2Vec encoding and the content features as inputs to two separate tasks outperforms all the baselines, achieving an F1-score of 0.89 for classification of tweets as Blackmarket or Genuine. The best baseline is Spam Detector 2 which achieves an F1-score of 0.77. blackWe analyse the false negatives generated by our model to find which type of tweets the model finds difficult to classify. The percentage of each class in the false negatives is as follows: Promotional - 23.29%, Politics - 10.96%, Entertainment - 21.92%, News - 9.59%, Spam - 5.48%, and Others - 28.77%. We observe that the tweets belonging to the category Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the", "Random Forest classifier on our dataset. We generate a combined feature vector by concatenating the tweet content features and the encoding generated by Tweet2Vec. This feature vector is then fed to state-of-the-art machine learning classifiers - Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM). Evaluation Setup. We consider the problem as a binary classification problem, where the tweets are classified into two classes - blackmarket and genuine. The performance of each competing method is measured using the following metrics: Precision, Recall, and F1-score. The primary output of the multitask learning model gives us the classification result, which is what we use to evaluate our model. All hyperparameters of the models are appropriately tuned. The average results are reported after 5-fold cross-validation. Experimental Results. As shown in Table TABREF29 , we observe that the multitask learning based model which uses the Tweet2Vec encoding and the", "gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites. Dataset Description. In total, we collected INLINEFORM0 tweets posted on blackmarket sites. Out of these, we removed non-English tweets and tweets with a length of less than two characters. Finally, we were left with INLINEFORM1 blackmarket tweets. Then, from the timelines of the authors of these tweets, we randomly sampled INLINEFORM2 genuine tweets that were not posted on these blackmarket sites during the same period. Both the blackmarket and genuine tweets were also inspected manually. Analysis of Blackmarket Tweets. To further understand the purpose of the collusive users behind the usage of blackmarket services, we annotated blackmarket tweets in our test set into a few discrete categories. The statistics of the", "shares the parameters from the other network using linear combinations. The network also employs batch-normalization and dropout to avoid overfitting. The output layer of the first task classifies tweets as blackmarket or genuine using a cross entropy loss function. The output layer of the second task predicts the numerical values for the number of retweets and likes that a tweet will gain after five days of being posted by using a Mean Squared Error (MSE) loss. Note that the performance of the secondary task is not of importance to us, however, the secondary task helps the primary task. Therefore, we focus on the performance of the model in the primary task during training and evaluation. Baseline Methods. Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset. Spam Detection 1: We use the Twitter spam", "tasks. Primary Input: The primary task takes as input the tweet content representation generated by the Tweet2Vec model, which is a 500-dimensional vector for each of the tweets, as described above. Secondary Input: The secondary task takes as input the vector of tweet content features, which is a 12-dimensional vector, as described above. As shown in Figure FIGREF21 , the inputs are fed into separate fully connected (FC) layers with cross-stitch units stacked between successive layers. The cross-stitch units find the best shared representations using linear combinations, and learn the optimal linear combinations for a given set of tasks. The cross-stitch units essentially allow us to unify two separate networks for two separate tasks into a single network wherein each layer of the network shares the parameters from the other network using linear combinations. The network also employs batch-normalization and dropout to avoid overfitting. The output layer of the first task classifies", "of each approach. Cross-Stitch units were introduced by Misra et al. BIBREF6 , which can learn an optimal combination of shared and task-specific representations. Gupta et al. BIBREF15 proposed GIRNet, a unified position-sensitive multitask recurrent neural network architecture. Blackmarket Services. blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to", "Introduction. Twitter is an important medium for people and companies to promote their products, ideologies, or to reach out and connect with other people in the form of micro-conversations. Twitter provides users with multiple ways of showing their support towards a tweet in the form of Likes, Retweets and Quotes. These content-level appraisals help in spreading the content further and act as a measure of users' agreement on the value of the content. The count of these content-level appraisals therefore determines the influence of a particular tweet and its author. This has led to the creation of certain blackmarket services such as FreeFollowers (https://www.freefollowers.io/), Like4Like (https://like4like.org/), YouLikeHits (https://www.youlikehits.com/), JustRetweet (http://justretweet.com), which allow users to post their tweets in order to gain inorganic appraisals in the form of Likes, Retweets and Quotes BIBREF0 , BIBREF1 . There has been a lot of research on the detection of", "from the characterization of blackmarket tweets via traditional feature extraction, with a deep learning based feature representation of the tweets. We train a neural network which takes as input both the traditional feature representation as well as the deep learning based representation generated using the Tweet2Vec model BIBREF5 , and utilizes cross-stitch units BIBREF6 to learn an optimal combination of shared and task-specific knowledge via soft parameter sharing. We show that our multitask learning approach outperforms Twitter spam detection approaches, as well as state-of-the-art classifiers by 14.1% (in terms of F1-score), achieving an F1-score of 0.89 on our dataset. In short, the contributions of the paper are threefold: a new dataset, characterization of blackmarket tweets, and a novel multitask learning framework to detect tweets posted on blackmarket services. Related Work. Several studies have focused on detecting malicious activities such as spam, fake content and"]}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "predicted_answer": "German", "predicted_evidence": ["from the characterization of blackmarket tweets via traditional feature extraction, with a deep learning based feature representation of the tweets. We train a neural network which takes as input both the traditional feature representation as well as the deep learning based representation generated using the Tweet2Vec model BIBREF5 , and utilizes cross-stitch units BIBREF6 to learn an optimal combination of shared and task-specific knowledge via soft parameter sharing. We show that our multitask learning approach outperforms Twitter spam detection approaches, as well as state-of-the-art classifiers by 14.1% (in terms of F1-score), achieving an F1-score of 0.89 on our dataset. In short, the contributions of the paper are threefold: a new dataset, characterization of blackmarket tweets, and a novel multitask learning framework to detect tweets posted on blackmarket services. Related Work. Several studies have focused on detecting malicious activities such as spam, fake content and", "Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the following categories \u2013 News, Spam and Politics. Conclusion. In this paper, we presented a novel multitask learning approach to solve the problem of identification of tweets that are submitted to blackmarket services, without the use of any temporal features. To sum up, our contributions are three-fold: (i) Characterization: We proposed 12 tweet content based features that are useful in the task of identifying blackmarket tweets, (ii) Classification: We developed a novel Multitask Learning based model to classify tweets as blackmarket tweets or genuine tweets, (iii) Dataset: We collected a dataset consisting of tweets that have been submitted to blackmarket services in order to gain inorganic appraisals. Acknowledgements. The work was partially funded by DST (ECR/2017/00l691,", "and a novel multitask learning framework to detect tweets posted on blackmarket services. Related Work. Several studies have focused on detecting malicious activities such as spam, fake content and blackmarket services. Here, we mention some of these studies which we deem as pertinent to our work. We also mention the prior usage of multitask learning in a similar context. Spam/Fake Tweet Detection: The problem of fake and spam tweets is not new. Many solutions have been proposed to tackle this problem. Yardi et al. BIBREF7 showed that the network structure of spammers and non-spammers is different, and also tracked the life cycle of endogenous Twitter content. Chen et al. BIBREF8 conducted a comprehensive evaluation of several machine learning algorithms for timely detection of spam. Fake tweets, on the other hand, are the tweets which spread misinformation. Serrano et al. BIBREF9 provided an extensive survey on fake tweet detection. Unlike spam tweets, fake tweets are mostly", "final GRU states by going through a forward and backward pass over the entire sequence. We use the pre-trained model provided by Dhingra et al. BIBREF5 , which is trained on a dataset of 2 million tweets, to get the tweet representation. This gives us a 500-dimensional representation of each tweet, based on its content. Proposed Model. The architecture of our model is shown in Figure FIGREF21 . We adopt multitask learning to develop our model. The primary task is set as a binary classification problem, wherein the tweets are classified as blackmarket or genuine. The secondary task is set as a regression problem, wherein the number of likes and retweets that a tweet will gain after five days of being posted is predicted. The model takes a different input feature vector for each of the tasks. Primary Input: The primary task takes as input the tweet content representation generated by the Tweet2Vec model, which is a 500-dimensional vector for each of the tweets, as described above.", "problem. Motoyama et al. BIBREF12 provided a detailed analysis of six underground forms, examining the properties of those social network structures that are formed and services that are being exchanged. Dutta et al. BIBREF0 investigated the customers involved in gaining fake retweets. Chetan et al. BIBREF1 proposed CoReRank, an unsupervised model and CoReRank+, a semi-supervised model which extends CoReRank to detect collusive users involved in retweeting activities. Multitask Learning: Multitask learning is used whenever we have two or more similar tasks to optimise together. Most of the related studies on multitask learning are based on how the tasks can be better learned together. Zhang et al. BIBREF14 classified multitask learning models into five types and reported the characteristics of each approach. Cross-Stitch units were introduced by Misra et al. BIBREF6 , which can learn an optimal combination of shared and task-specific representations. Gupta et al. BIBREF15 proposed", "Random Forest classifier on our dataset. We generate a combined feature vector by concatenating the tweet content features and the encoding generated by Tweet2Vec. This feature vector is then fed to state-of-the-art machine learning classifiers - Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM). Evaluation Setup. We consider the problem as a binary classification problem, where the tweets are classified into two classes - blackmarket and genuine. The performance of each competing method is measured using the following metrics: Precision, Recall, and F1-score. The primary output of the multitask learning model gives us the classification result, which is what we use to evaluate our model. All hyperparameters of the models are appropriately tuned. The average results are reported after 5-fold cross-validation. Experimental Results. As shown in Table TABREF29 , we observe that the multitask learning based model which uses the Tweet2Vec encoding and the", "shares the parameters from the other network using linear combinations. The network also employs batch-normalization and dropout to avoid overfitting. The output layer of the first task classifies tweets as blackmarket or genuine using a cross entropy loss function. The output layer of the second task predicts the numerical values for the number of retweets and likes that a tweet will gain after five days of being posted by using a Mean Squared Error (MSE) loss. Note that the performance of the secondary task is not of importance to us, however, the secondary task helps the primary task. Therefore, we focus on the performance of the model in the primary task during training and evaluation. Baseline Methods. Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset. Spam Detection 1: We use the Twitter spam", "results are reported after 5-fold cross-validation. Experimental Results. As shown in Table TABREF29 , we observe that the multitask learning based model which uses the Tweet2Vec encoding and the content features as inputs to two separate tasks outperforms all the baselines, achieving an F1-score of 0.89 for classification of tweets as Blackmarket or Genuine. The best baseline is Spam Detector 2 which achieves an F1-score of 0.77. blackWe analyse the false negatives generated by our model to find which type of tweets the model finds difficult to classify. The percentage of each class in the false negatives is as follows: Promotional - 23.29%, Politics - 10.96%, Entertainment - 21.92%, News - 9.59%, Spam - 5.48%, and Others - 28.77%. We observe that the tweets belonging to the category Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the"]}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "predicted_answer": "The paper focuses on freemium services that offer paid and unpaid options, with the unpaid services requiring users to provide appraisals for other users' content.", "predicted_evidence": ["of each approach. Cross-Stitch units were introduced by Misra et al. BIBREF6 , which can learn an optimal combination of shared and task-specific representations. Gupta et al. BIBREF15 proposed GIRNet, a unified position-sensitive multitask recurrent neural network architecture. Blackmarket Services. blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to", "in identifying individual tweets as blackmarket tweets (as shown in Table TABREF29 ). Table TABREF1 shows a sample tweet that was posted on a blackmarket service and another sample tweet that was not. In this paper, we make the first attempt to detect tweets that are posted on blackmarket services. Our aim is to build a system that can flag tweets soon after they are posted, which is why we do not consider temporal features such as the number of retweets or likes that a tweet keeps gaining over time. Instead, we only rely on the features and representations extracted from the content of the tweets. We curate a novel dataset of tweets that have been posted to blackmarket services, and a corresponding set of tweets that haven't. We propose a multitask learning approach to combine properties from the characterization of blackmarket tweets via traditional feature extraction, with a deep learning based feature representation of the tweets. We train a neural network which takes as input", "gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites. Dataset Description. In total, we collected INLINEFORM0 tweets posted on blackmarket sites. Out of these, we removed non-English tweets and tweets with a length of less than two characters. Finally, we were left with INLINEFORM1 blackmarket tweets. Then, from the timelines of the authors of these tweets, we randomly sampled INLINEFORM2 genuine tweets that were not posted on these blackmarket sites during the same period. Both the blackmarket and genuine tweets were also inspected manually. Analysis of Blackmarket Tweets. To further understand the purpose of the collusive users behind the usage of blackmarket services, we annotated blackmarket tweets in our test set into a few discrete categories. The statistics of the", "further understand the purpose of the collusive users behind the usage of blackmarket services, we annotated blackmarket tweets in our test set into a few discrete categories. The statistics of the categories are as follows: Promotional - 43.75%, Entertainment - 15.89%, Spam - 13.57%, News - 7.86%, Politics - 4.82%, and Others - 14.11%. We considered a tweet as Promotional only if the tweet attempts to promote a website/product. Most of the tweets in the Others category include personal tweets without any call to action or promotion, but this also can be considered as self-promotion. We further noticed that there were about 5% of normal tweets on concerning issues such as \u201cpray for ...\", indicating that blackmarket services are also being used for non-business purposes. 99% of tweets other than the tweets from Others class included at least one URL, and 100% of the URLs in the blackmarket tweets were shortened. Proposed Approach. This section describes the features and tweet", "a dataset consisting of tweets that have been submitted to blackmarket services in order to gain inorganic appraisals. Acknowledgements. The work was partially funded by DST (ECR/2017/00l691, DST/INT/UK/P158/2017), Ramanujan Fellowship, and the Infosys Centre of AI, IIIT-Delhi, India.", "Others are difficult to classify since they are similar to genuine tweets in terms of content. The results also indicate that our model is robust while classifying blackmarket tweets belonging to the following categories \u2013 News, Spam and Politics. Conclusion. In this paper, we presented a novel multitask learning approach to solve the problem of identification of tweets that are submitted to blackmarket services, without the use of any temporal features. To sum up, our contributions are three-fold: (i) Characterization: We proposed 12 tweet content based features that are useful in the task of identifying blackmarket tweets, (ii) Classification: We developed a novel Multitask Learning based model to classify tweets as blackmarket tweets or genuine tweets, (iii) Dataset: We collected a dataset consisting of tweets that have been submitted to blackmarket services in order to gain inorganic appraisals. Acknowledgements. The work was partially funded by DST (ECR/2017/00l691,", "which allow users to post their tweets in order to gain inorganic appraisals in the form of Likes, Retweets and Quotes BIBREF0 , BIBREF1 . There has been a lot of research on the detection of fraudulent activities on Twitter such as detection of bots BIBREF2 , fake followers BIBREF3 , collusive retweeters BIBREF0 , BIBREF1 , and social spam BIBREF4 . However, the problem of detecting tweets that are posted to these blackmarket services has not been tackled before. The tweets submitted to blackmarket services are not necessarily spam or promotional tweets. As we observe in our data, there is some intersection between spammers and blackmarket users since spammers may also try to gain more appraisals by using these services. However, existing spam tweet detection approaches do not work that well in identifying individual tweets as blackmarket tweets (as shown in Table TABREF29 ). Table TABREF1 shows a sample tweet that was posted on a blackmarket service and another sample tweet that was", "from the characterization of blackmarket tweets via traditional feature extraction, with a deep learning based feature representation of the tweets. We train a neural network which takes as input both the traditional feature representation as well as the deep learning based representation generated using the Tweet2Vec model BIBREF5 , and utilizes cross-stitch units BIBREF6 to learn an optimal combination of shared and task-specific knowledge via soft parameter sharing. We show that our multitask learning approach outperforms Twitter spam detection approaches, as well as state-of-the-art classifiers by 14.1% (in terms of F1-score), achieving an F1-score of 0.89 on our dataset. In short, the contributions of the paper are threefold: a new dataset, characterization of blackmarket tweets, and a novel multitask learning framework to detect tweets posted on blackmarket services. Related Work. Several studies have focused on detecting malicious activities such as spam, fake content and"]}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "predicted_answer": "English/Chinese and English/French/Chinese", "predicted_evidence": ["with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines. Experiments ::: Training Details ::: Pre-Training. We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword", "signals across all language pairs. BIBREF10 kumar2019cross introduce a cross-lingual model for question generation, which uses training data annotated in multiple languages to jointly train a sequence-to-sequence model. In contrast, our method can also be applied to zero-shot settings across languages. Related Work ::: Monolingual Pre-Training. Various training objectives are designed to pretrain text encoders used for general-purpose language representations, such as language modeling BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, auto-encoding BIBREF16, and machine translation BIBREF17. Apart from pre-training encoders, several pre-trained models BIBREF18, BIBREF19 are proposed for generation tasks. In comparison, our goal is to investigate a pre-training method for cross-lingual NLG tasks. Related Work ::: Cross-Lingual Pre-Training. Cross-lingual pre-training aims at building universal cross-lingual encoders that can encode multilingual sentences to a shared embedding space.", "tasks. Related Work ::: Cross-Lingual Pre-Training. Cross-lingual pre-training aims at building universal cross-lingual encoders that can encode multilingual sentences to a shared embedding space. BIBREF20 artetxe2018massively use the sequence encoder of the multilingual translation model BIBREF3 to produce cross-lingual sentence embeddings. However, as shown in the experiments (Section SECREF4), it is difficult to control the target language by directly fine-tuning the pre-trained translation model on downstream NLG tasks. BIBREF4 xnli propose an alignment loss function to encourage parallel sentences to have similar representations. By pre-training BERT BIBREF13 on corpora of multiple languages, it shows a surprising ability to produce cross-lingual representations BIBREF21. More recently, BIBREF5 xlm extend mask language modeling pre-training to cross-lingual settings, which shows significant improvements on cross-lingual text classification and unsupervised machine translation. By", "BIBREF5 xlm extend mask language modeling pre-training to cross-lingual settings, which shows significant improvements on cross-lingual text classification and unsupervised machine translation. By comparison, we pretrain both encoder and decoder for cross-lingual generation tasks, rather than only focusing on encoder. Methods. Xnlg is a pre-trained sequence-to-sequence model, which is based on Transformer BIBREF22. Both the encoder and the decoder are supposed to support multiple languages. Following BIBREF5, we use language tag embeddings to distinguish the source and target languages. Given a sentence and its corresponding language tag, Xnlg encodes the input into vector representations. By conditioning on the encoding vectors and a specific language tag, the decoder generates the output sequence in the target language. Figure FIGREF6 illustrates the pre-training objectives and the pre-training protocol designed for Xnlg. Methods ::: Pre-Training Tasks ::: Monolingual MLM. The", "is first translated to English, and fed into the NLG model that is trained by English data. Then the generated English text is translated back to the target language. Another strand of work employs MT to generate pseudo training data for other language pairs that are lack of annotations BIBREF7, BIBREF8. However, such methods have to use multiple MT systems, which renders them suffering from error propagation. Moreover, because the pipeline-based methods do not explicitly share the same parameter space across the languages, we can not directly transfer the task-specific supervision to other low-resource languages. In this paper, we propose a cross-lingual pre-trained model (named as Xnlg) in order to transfer monolingual NLG supervision to other pre-trained languages by fine-tuning. Specifically, Xnlg shares the same sequence-to-sequence model across languages, and is pre-trained with both monolingual and cross-lingual objectives. The model not only learns to understand multilingual", "from the error propagation issue, especially when the source and target languages are all different from the training data. For example, when the pipeline model performs Zh-Zh-QG, keywords are translated twice, increasing the risk of mistranslation. In the second example, \u201catomic bomb\u201d is mistranslated to \u201cnuclear bomb\u201d, resulting in its low correctness. On the contrary, by directly transferring English supervision signals to the other generation directions, the generated questions of Xnlg match the references better than baselines. Conclusion. In this paper, we propose a pre-training method for cross-lingual natural language generation (NLG) that can transfer monolingual NLG supervision signals to all pre-trained languages. With the pre-trained model, we achieve zero-shot cross-lingual NLG on several languages by only fine-tuning once. Experimental results show that our model outperforms the machine-translation-based pipeline model on several cross-lingual NLG tasks. For future work,", "output sequence in the target language. Figure FIGREF6 illustrates the pre-training objectives and the pre-training protocol designed for Xnlg. Methods ::: Pre-Training Tasks ::: Monolingual MLM. The masked language modeling (MLM) BIBREF13 task, also known as the Cloze task BIBREF23, aims at predicting the randomly masked words according to their context. The objective pretrains the bidirectional encoder to obtain contextual representations. Following BIBREF13, we randomly mask 15% of the tokens in a monolingual sentence. For each masked token, we substitute it with a special token M, a random token, or the unchanged token with a probability of 0.8, 0.1, and 0.1, respectively. Let $x$ denote a sentence from the monolingual training corpus, and $M_{x}$ the set of randomly masked positions. The monolingual MLM loss is defined as: MLM(x) = -i Mxp( xi | xMx) where $x_{\\setminus M_{x}}$ is the masked version of input $x$. Notice that language tags are fed into the model for all", "by conditioning on $\\hat{x}$. The DAE loss is computed via: DAE(x) = -p(x|x) = -i = 1|x|p(xi | x, x<i) where $x_{<i}$ represents the tokens of previous time steps $x_1,\\cdots ,x_{i-1}$. Methods ::: Pre-Training Tasks ::: Cross-Lingual MLM (XMLM). Similar to monolingual MLM, the masked token prediction task can be extended to cross-lingual settings BIBREF5. To be specific, given a parallel corpus, we concatenate the pair of bilingual sentences $(x,y)$ to a whole sequence, and use it as the input of MLM. The language tags are also fed into the model to indicate the languages of tokens. During training, we adopt the same masking strategy as monolingual MLM. Apart from using monolingual context to predict the masked tokens, XMLM encourages the model to utilize the alignment of bilingual sentences, so that the model learns to map cross-lingual texts into a shared vector space. Similar to eq:mlm, the cross-lingual MLM loss is: XMLM(x,y) = -i Mxp( xi | xMx , yMy) -i Myp( yi | xMx , yMy)"]}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "predicted_answer": "6-layer decoder", "predicted_evidence": ["the encoder parameters during fine-tuning. In practice, we found that the proposed fine-tuning method prevents the model from only decoding English words for the Any-to-Others setting. Methods ::: Fine-Tuning on Downstream NLG Tasks ::: Fine-Tuning for Any-to-English NLG. For the Any-to-English NLG transfer, the decoder always generates English. So we can freeze the encoder parameters, and update the decoder parameters to retain the cross-lingual ability. As an alternative way, we can also fine-tune all the parameters to obtain the best results on the English dataset while having a slight drop in performance. Experiments. We conduct experiments over two cross-lingual NLG downstream tasks, i.e., cross-lingual question generation, and cross-lingual abstractive summarization. We compare Xnlg with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines. Experiments ::: Training Details ::: Pre-Training. We use a pre-trained Xnlg with a 10-layer encoder", "QG, which shows the strong cross-lingual transfer ability of our model. When fine-tuning all the parameters, the model achieves the best score for English QG, but it suffers a performance drop when evaluating on Chinese QG. We find that fine-tuning decoder hurts cross-lingual decoding, and the model learns to only decodes English words. For only fine-tuning decoder, the performance degrades by a large margin for both languages because of the underfitting issue, which indicates the necessity of fine-tuning encoder. Experiments ::: Ablation Studies ::: Effects of Cross-Lingual Transfer. We examine whether low-resource NLG can benefit from cross-lingual transfer. We consider English as the rich-resource language, and conduct experiments for few-shot French/Chinese AS. Specifically, we first fine-tune Xnlg on the English AS data, and then fine-tune it on the French or Chinese AS data. We compare with the monolingual supervised model that Xnlg is only fine-tuned on the dataset of the", "stage is to minimize: 1= (x,y) p XMLM(x,y) + x m MLM(x) where ${_{\\textnormal {p}}}$ indicates the parallel corpus, and ${_{\\textnormal {m}}}$ is the monolingual corpus. Although the pre-trained encoder in the first stage enables the model to encode multilingual sentences. However, it cannot directly be used in cross-lingual NLG because: 1) encoder-decoder attention is not pre-trained; 2) the decoding algorithm is different between masked language modeling and autoregressive decoding, resulting in the mismatch between pre-training and fine-tuning. Therefore, we conduct decoding pre-training in the second stage by using DAE and XAE as the tasks. Besides, we only update decoder parameters and keep the encoder fixed. The objective of the second stage is to minimize: 2 = (x,y) pXAE(x,y) + x mDAE(x) Methods ::: Fine-Tuning on Downstream NLG Tasks. In the fine-tuning procedure, let us assume that we only have English training data for downstream NLG tasks. According to whether the target", "with a special token S between them. During decoding Chinese, we utilize a subset of vocabulary, which is obtained from the passage sentences of the WebQA dataset. Experiments ::: Question Generation ::: English-English Question Generation. We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines: CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder. Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism. Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM. We evaluate models with BLEU-4 (BL-4), ROUGE (RG) and METEOR (MTR) metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG. Experiments ::: Question Generation :::", "mDAE(x) Methods ::: Fine-Tuning on Downstream NLG Tasks. In the fine-tuning procedure, let us assume that we only have English training data for downstream NLG tasks. According to whether the target language is English, the directions of NLG can be categorized into two classes: any languages to non-English languages (Any-to-Others), and any languages to English (Any-to-English). Methods ::: Fine-Tuning on Downstream NLG Tasks ::: Fine-Tuning for Any-to-Others NLG. Ideally, the model can be fine-tuned towards a new task without losing its cross-lingual ability. However, we observe the catastrophic forgetting phenomenon of target language controllability, if we fine-tune all the model parameters for Any-to-Others NLG. So we keep the decoder and word embeddings frozen and only update the encoder parameters during fine-tuning. In practice, we found that the proposed fine-tuning method prevents the model from only decoding English words for the Any-to-Others setting. Methods :::", "NLG tasks remains relatively understudied. The cross-lingual generation problem is challenging due to the following reasons. First, it requires the models to understand multilingual input texts, and generate multilingual target sequences. So both encoder and decoder should be pre-trained together. Second, the many-to-many nature of cross-lingual NLG increases language pairs with the square of the number of languages. Third, the prediction space of cross-lingual NLG is much larger than classification tasks, which makes the knowledge transfer of decoders quite critical. Previous work mainly relies on machine translation (MT) systems to map texts to different languages. The first strand of research directly uses MT in a pipeline manner BIBREF6. For example, the input written in other languages is first translated to English, and fed into the NLG model that is trained by English data. Then the generated English text is translated back to the target language. Another strand of work employs", "with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines. Experiments ::: Training Details ::: Pre-Training. We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword", "positions. The monolingual MLM loss is defined as: MLM(x) = -i Mxp( xi | xMx) where $x_{\\setminus M_{x}}$ is the masked version of input $x$. Notice that language tags are fed into the model for all pre-training tasks. Methods ::: Pre-Training Tasks ::: Denoising Auto-Encoding (DAE). We use the denoising auto-encoding (DAE) objective BIBREF24 to pretrain the encoder-decoder attention mechanism. Given sentence $x$ from the monolingual corpus, we use three types of noise to obtain the randomly perturbed text $\\hat{x}$. First, the word order is locally shuffled. Second, we randomly drop tokens of the sentence with a probability of $0.1$. Third, we substitute tokens with the special padding token P with a probability of $0.1$. The pre-training objective is to recover the original sentence $x$ by conditioning on $\\hat{x}$. The DAE loss is computed via: DAE(x) = -p(x|x) = -i = 1|x|p(xi | x, x<i) where $x_{<i}$ represents the tokens of previous time steps $x_1,\\cdots ,x_{i-1}$. Methods :::"]}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "predicted_answer": "Unanswerable.", "predicted_evidence": ["the encoder parameters during fine-tuning. In practice, we found that the proposed fine-tuning method prevents the model from only decoding English words for the Any-to-Others setting. Methods ::: Fine-Tuning on Downstream NLG Tasks ::: Fine-Tuning for Any-to-English NLG. For the Any-to-English NLG transfer, the decoder always generates English. So we can freeze the encoder parameters, and update the decoder parameters to retain the cross-lingual ability. As an alternative way, we can also fine-tune all the parameters to obtain the best results on the English dataset while having a slight drop in performance. Experiments. We conduct experiments over two cross-lingual NLG downstream tasks, i.e., cross-lingual question generation, and cross-lingual abstractive summarization. We compare Xnlg with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines. Experiments ::: Training Details ::: Pre-Training. We use a pre-trained Xnlg with a 10-layer encoder", "stage is to minimize: 1= (x,y) p XMLM(x,y) + x m MLM(x) where ${_{\\textnormal {p}}}$ indicates the parallel corpus, and ${_{\\textnormal {m}}}$ is the monolingual corpus. Although the pre-trained encoder in the first stage enables the model to encode multilingual sentences. However, it cannot directly be used in cross-lingual NLG because: 1) encoder-decoder attention is not pre-trained; 2) the decoding algorithm is different between masked language modeling and autoregressive decoding, resulting in the mismatch between pre-training and fine-tuning. Therefore, we conduct decoding pre-training in the second stage by using DAE and XAE as the tasks. Besides, we only update decoder parameters and keep the encoder fixed. The objective of the second stage is to minimize: 2 = (x,y) pXAE(x,y) + x mDAE(x) Methods ::: Fine-Tuning on Downstream NLG Tasks. In the fine-tuning procedure, let us assume that we only have English training data for downstream NLG tasks. According to whether the target", "QG, which shows the strong cross-lingual transfer ability of our model. When fine-tuning all the parameters, the model achieves the best score for English QG, but it suffers a performance drop when evaluating on Chinese QG. We find that fine-tuning decoder hurts cross-lingual decoding, and the model learns to only decodes English words. For only fine-tuning decoder, the performance degrades by a large margin for both languages because of the underfitting issue, which indicates the necessity of fine-tuning encoder. Experiments ::: Ablation Studies ::: Effects of Cross-Lingual Transfer. We examine whether low-resource NLG can benefit from cross-lingual transfer. We consider English as the rich-resource language, and conduct experiments for few-shot French/Chinese AS. Specifically, we first fine-tune Xnlg on the English AS data, and then fine-tune it on the French or Chinese AS data. We compare with the monolingual supervised model that Xnlg is only fine-tuned on the dataset of the", "we use machine translation as the cross-lingual auto-encoding (XAE) task, which decreases mutual information between the target sentences and the source language tag. XAE can be viewed as the multilingual-version DAE task in the sense that both of them recover the sentence by conditioning on the encoded representations. The cross-lingual auto-encoding loss is defined as: XAE(x,y) = -p(y|x) - p(x|y) where $(x,y)$ is a pair of sentences in the parallel corpus. Methods ::: Pre-Training Protocol. As shown in Figure FIGREF6(b), we propose a two-stage pre-training protocol for Xnlg. The first stage pretrains the encoding components, where the model learns to encode multilingual sentences to a shared embedding space. We consider using MLM and XMLM as the pre-training tasks. The objective of the first stage is to minimize: 1= (x,y) p XMLM(x,y) + x m MLM(x) where ${_{\\textnormal {p}}}$ indicates the parallel corpus, and ${_{\\textnormal {m}}}$ is the monolingual corpus. Although the pre-trained", "positions. The monolingual MLM loss is defined as: MLM(x) = -i Mxp( xi | xMx) where $x_{\\setminus M_{x}}$ is the masked version of input $x$. Notice that language tags are fed into the model for all pre-training tasks. Methods ::: Pre-Training Tasks ::: Denoising Auto-Encoding (DAE). We use the denoising auto-encoding (DAE) objective BIBREF24 to pretrain the encoder-decoder attention mechanism. Given sentence $x$ from the monolingual corpus, we use three types of noise to obtain the randomly perturbed text $\\hat{x}$. First, the word order is locally shuffled. Second, we randomly drop tokens of the sentence with a probability of $0.1$. Third, we substitute tokens with the special padding token P with a probability of $0.1$. The pre-training objective is to recover the original sentence $x$ by conditioning on $\\hat{x}$. The DAE loss is computed via: DAE(x) = -p(x|x) = -i = 1|x|p(xi | x, x<i) where $x_{<i}$ represents the tokens of previous time steps $x_1,\\cdots ,x_{i-1}$. Methods :::", "mDAE(x) Methods ::: Fine-Tuning on Downstream NLG Tasks. In the fine-tuning procedure, let us assume that we only have English training data for downstream NLG tasks. According to whether the target language is English, the directions of NLG can be categorized into two classes: any languages to non-English languages (Any-to-Others), and any languages to English (Any-to-English). Methods ::: Fine-Tuning on Downstream NLG Tasks ::: Fine-Tuning for Any-to-Others NLG. Ideally, the model can be fine-tuned towards a new task without losing its cross-lingual ability. However, we observe the catastrophic forgetting phenomenon of target language controllability, if we fine-tune all the model parameters for Any-to-Others NLG. So we keep the decoder and word embeddings frozen and only update the encoder parameters during fine-tuning. In practice, we found that the proposed fine-tuning method prevents the model from only decoding English words for the Any-to-Others setting. Methods :::", "with a special token S between them. During decoding Chinese, we utilize a subset of vocabulary, which is obtained from the passage sentences of the WebQA dataset. Experiments ::: Question Generation ::: English-English Question Generation. We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines: CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder. Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism. Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM. We evaluate models with BLEU-4 (BL-4), ROUGE (RG) and METEOR (MTR) metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG. Experiments ::: Question Generation :::", "summarization, respectively. When the target language is the same as the language of training data, we fine-tune all parameters. When the target language is different from the language of training data, we fine-tune the Transformer layers of the encoder. We truncate the input sentences to the first 256 tokens. During decoding, we use beam search with beam size of 3, and limit the length of the target sequence to 80 tokens. Experiments ::: Question Generation. We evaluate our model on the zero-shot cross-lingual answer-aware question generation task. The goal of question generation (QG) is to generate a question that asks towards the answer with the given passage and the expected answer. In the following experiments, we extend the QG task to the cross-lingual setting. By only using English QG training data, our goal is to generate questions in English or Chinese with the given passage-answer pair in English or Chinese. We use SQuAD 1.1 BIBREF30 as the English QG dataset. It is a"]}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "predicted_answer": "XLM and Pipeline (Xlm)", "predicted_evidence": ["TABREF16 as reference. In the automatic evaluation, our model consistently performs better than baselines in both zero-shot and monolingual supervised setting. In the human evaluation, our model also obtains significant improvements in terms of relatedness and correctness. Experiments ::: Question Generation ::: English-Chinese Question Generation. In the zero-shot English-Chinese question generation experiments, we use Xlm and Pipeline (Xlm) as our baselines. Pipeline (Xlm) is a pipeline method that uses En-En-QG with Xlm to generate questions, and then translates the results to Chinese. Because there is no annotations for En-Zh-QG, we perform human evaluation studies for this setting. Table TABREF19 shows the human evaluation results, where our model surpasses all the baselines especially in terms of relatedness and correctness. Experiments ::: Question Generation ::: Chinese-English Question Generation. We also conduct experiments for zero-shot Chinese-English question generation,", "and regard them as input document and predicted summaries, respectively. For each language, we sample 500k/5k/5k examples for training/validation/test. Experiments ::: Abstractive Summarization ::: Zero-Shot Summarization. In the zero-shot setting, we only use English data for training, and directly evaluate the model on other languages. In Table TABREF22 and Table TABREF23, we present the results for French/Chinese AS, which are evaluated by the ROUGE-1, ROUGE-2 and ROUGE-L metrics. We also report the results of supervised AS in Table TABREF21 for reference. We find that Xnlg outperforms all the baseline models on both French and Chinese AS. Comparing with French, there is a larger gap between baselines and our model on zero-shot Chinese AS, which indicates that the error propagation issue is more serious on distant language pairs. Experiments ::: Ablation Studies ::: Effects of Pre-Training. We conduct ablation studies for pre-training objectives, and the results can be seen in", "issue is more serious on distant language pairs. Experiments ::: Ablation Studies ::: Effects of Pre-Training. We conduct ablation studies for pre-training objectives, and the results can be seen in Table TABREF40. We observe that our model greatly benefits from the DAE objective for the zero-shot Chinese question generation task. The results also demonstrate that combining DAE and XAE can alleviate the spurious correlation issue and improves cross-lingual NLG. Experiments ::: Ablation Studies ::: Effects of Fine-Tuning Strategies. As shown in Table TABREF41, we use the En-En-QG and Zh-Zh-QG tasks to analyze the effects of using different fine-tuning strategies. It can be observed that fine-tuning encoder parameters, our model obtain an impressive performance for both English and Chinese QG, which shows the strong cross-lingual transfer ability of our model. When fine-tuning all the parameters, the model achieves the best score for English QG, but it suffers a performance drop when", "metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG. Experiments ::: Question Generation ::: Chinese-Chinese Question Generation. We conduct experiments on the zero-shot Chinese-Chinese QG task to evaluate the cross-lingual transfer ability. In this task, models are trained with English QG data but evaluated with Chinese QG examples. We include the following models as our baselines: Xlm Fine-tuning XLM with the English QG data. Pipeline (Xlm) The pipeline of translating input Chinese sentences into English first, then performing En-En-QG with the XLM model, and finally translating back to the Chinese. We use the Transformer as the translator, which is also trained on the MultiUN dataset. Pipeline (Xlm) with Google Translator Same to Pipeline (Xlm) but using Google Translator to translate the texts. We evaluate models by both automatic evaluation metrics and human experts. The", "respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29. We use Adam optimizer with a linear warm-up over the first 4,000 steps and linear decay for later steps, and the learning rate is set to $10^{-4}$. The pre-training batch size is 64, and the sequence length is set to 256. It takes about 30 hours to run 23,000 steps for the pre-training procedure by using 4 Nvidia Telsa V100-16GB GPUs. Experiments ::: Training Details ::: Fine-Tuning. For fine-tuning on downstream NLG tasks, we use Adam optimizer with a learning rate of $5\\times 10^{-6}$. We set the batch size as 16 and 32 for question generation and abstractive summarization, respectively. When the target language is the same as the language of training data, we fine-tune all parameters. When the target language is different from the language of training", "from the error propagation issue, especially when the source and target languages are all different from the training data. For example, when the pipeline model performs Zh-Zh-QG, keywords are translated twice, increasing the risk of mistranslation. In the second example, \u201catomic bomb\u201d is mistranslated to \u201cnuclear bomb\u201d, resulting in its low correctness. On the contrary, by directly transferring English supervision signals to the other generation directions, the generated questions of Xnlg match the references better than baselines. Conclusion. In this paper, we propose a pre-training method for cross-lingual natural language generation (NLG) that can transfer monolingual NLG supervision signals to all pre-trained languages. With the pre-trained model, we achieve zero-shot cross-lingual NLG on several languages by only fine-tuning once. Experimental results show that our model outperforms the machine-translation-based pipeline model on several cross-lingual NLG tasks. For future work,", "Pipeline (Xlm) with Google Translator Same to Pipeline (Xlm) but using Google Translator to translate the texts. We evaluate models by both automatic evaluation metrics and human experts. The automatic metrics scores are computed by regarding each Chinese character as a token. For human evaluation, we consider three metrics for the generated questions: relatedness, fluency, and correctness, which are represented as integers ranged from 1 to 3. We randomly select 100 passage-answer pairs from the English QG test set, and use the models to generate questions. Then we present these examples to three experts to ask for the above scores. In Table TABREF17 and Table TABREF18, we present the results for the zero-shot Zh-Zh-QG. The results of monolingual supervised models are also reported in Table TABREF16 as reference. In the automatic evaluation, our model consistently performs better than baselines in both zero-shot and monolingual supervised setting. In the human evaluation, our model", "with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines. Experiments ::: Training Details ::: Pre-Training. We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword"]}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "predicted_answer": "Triple pairing task, hierarchical generation importance", "predicted_evidence": ["the story. In our evaluation, we examined 1000 stories from the test set for each model. For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges. Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test. Results. We analyze the effect of our modeling improvements on the WritingPrompts dataset. Generation Quality. Our proposed fusion", "We do not aim to generate a specific story; we want to generate viable and novel stories. We focus on measuring both the fluency of our models and their ability to adhere to the prompt. For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts\u20149 randomly sampled prompts and 1 true corresponding prompt\u2014and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model. For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model", "prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation. Experiments show that our fusion and self-attention mechanisms improve over existing techniques on both automated and human evaluation measures. Our new dataset and neural architectures allow for models which can creatively generate longer, more consistent and more fluent passages of text. Human judges prefer our hierarchical model's stories twice as often as those of a non-hierarchical baseline. Writing Prompts Dataset. We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general", "and do not generate unknown word tokens. For prompt generation, we use a self-attentive GCNN language model trained with the same prompt-side vocabulary as the sequence-to-sequence story generation models. The language model to generate prompts has a validation perplexity of 63.06. Prompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is completed when the language model generates the end of prompt token. Evaluation. We propose a number of evaluation metrics to quantify the performance of our models. Many commonly used metrics, such as BLEU for machine translation or ROUGE for summarization, compute an n-gram overlap between the generated text and the human text\u2014however, in our open-ended generation setting, these are not useful. We do not aim to generate a specific story; we want to generate viable and novel stories. We focus on measuring both the fluency of our models and their ability to adhere to the prompt. For automatic", "must focus on the link between the prompt and the story. For the first time, we show that fusion mechanisms can help seq2seq models build dependencies between their input and output. Another major challenge in story generation is the inefficiency of modeling long documents with standard recurrent architectures\u2014stories contain 734 words on average in our dataset. We improve efficiency using a convolutional architecture, allowing whole stories to be encoded in parallel. Existing convolutional architectures only encode a bounded amount of context BIBREF4 , so we introduce a novel gated self-attention mechanism that allows the model to condition on its previous outputs at different time-scales. To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation. Experiments show that our", "pairs of stories were evaluated by 5 judges each in a blind test. Results. We analyze the effect of our modeling improvements on the WritingPrompts dataset. Generation Quality. Our proposed fusion model is capable of generating unique text without copying directly from the training set. When analyzing 500 150-word generated stories from test-set prompts, the average longest common subsequence is 8.9. In contrast, the baseline Conv seq2seq model copies 10.2 words on average and the KNN baseline copies all 150 words from a story in the training set. Figure FIGREF27 shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a language model producing common words and punctuation. The second seq2seq model learns to focus on rare words, such as horned and robe. However, the fusion model has limitations. Using random sampling to generate can produce errors. For example, can't is tokenized to ca n't, and the model", "INLINEFORM2 , 4 decoder self-attention heads. Conv seq2seq + self-attention. 3 layers in encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 8 layers in the decoder with hidden unit sizes INLINEFORM2 with convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 nomalization INLINEFORM4 , 4 decoder self-attention heads. Ensemble: Conv seq2seq + self-attention. Two different Conv seq2seq models were trained and ensembled together by averaging with equal weights. Fusion: Conv seq2seq + self-attention. The pretrained seq2seq model is the model in Section SECREF37 . The additional fused model has the following architecture: 5 layers in the encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 5 layers in the decoder with hidden unit sizes INLINEFORM2 and convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout", "to look at words generated very far in the past, at many timesteps the self-attention focuses on the recent past. Conclusion. We have collected the first dataset for creative text generation based on short writing prompts. This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise. Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories. GCNN Language Model + Self-Attention. 9 layers with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . Learning rate 1, momentum 0.99, dropout 0.1, embedding size 300, l2 normalization INLINEFORM2 , 4 decoder self-attention heads. Conv seq2seq + self-attention. 3 layers in encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 8 layers"]}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "predicted_answer": "Perplexity and prompt ranking accuracy.", "predicted_evidence": ["prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation. Experiments show that our fusion and self-attention mechanisms improve over existing techniques on both automated and human evaluation measures. Our new dataset and neural architectures allow for models which can creatively generate longer, more consistent and more fluent passages of text. Human judges prefer our hierarchical model's stories twice as often as those of a non-hierarchical baseline. Writing Prompts Dataset. We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general", "the story. In our evaluation, we examined 1000 stories from the test set for each model. For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges. Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test. Results. We analyze the effect of our modeling improvements on the WritingPrompts dataset. Generation Quality. Our proposed fusion", "BIBREF5 , BIBREF4 , but accurately modeling several paragraphs is an open problem. While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts. We will evaluate strategies to address these challenges in the following sections. Hierarchical Story Generation. High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases. Efficient Learning", "Introduction. Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation BIBREF0 . We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of standard sequence models to drift off topic. We find that standard sequence-to-sequence (seq2seq) models BIBREF1 applied to hierarchical story generation are prone to degenerating into language models that pay little attention to the writing prompt (a problem that", "We do not aim to generate a specific story; we want to generate viable and novel stories. We focus on measuring both the fluency of our models and their ability to adhere to the prompt. For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts\u20149 randomly sampled prompts and 1 true corresponding prompt\u2014and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model. For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model", "must focus on the link between the prompt and the story. For the first time, we show that fusion mechanisms can help seq2seq models build dependencies between their input and output. Another major challenge in story generation is the inefficiency of modeling long documents with standard recurrent architectures\u2014stories contain 734 words on average in our dataset. We improve efficiency using a convolutional architecture, allowing whole stories to be encoded in parallel. Existing convolutional architectures only encode a bounded amount of context BIBREF4 , so we introduce a novel gated self-attention mechanism that allows the model to condition on its previous outputs at different time-scales. To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation. Experiments show that our", "then sample event representations using MCMC. They find this technique can generate text of the desired genre, but the movie plots are not interpretable (as the model outputs events, not raw text). However, we are not aware of previous work that has used hierarchical generation from a textual premise to improve the coherence and structure of stories. Hierarchical Text Generation. Previous work has proposed decomposing the challenge of generating long sequences of text into a hierarchical generation task. For instance, BIBREF19 use an LSTM to hierarchically learn word, then sentence, then paragraph embeddings, then transform the paragraph embeddings into text. BIBREF20 generate a discrete latent variable based on the context, then generates text conditioned upon it. Fusion Models. Previous work has investigated the integration of language models with seq2seq models. The two models can be leveraged together without architectural modifications: BIBREF21 use language models to initialize", "in this work, we formulate the OG task as a hierarchical structured prediction problem and introduce a novel hierarchical structured neural generation model, named HiStGen for short, to solve it. The overall architecture of HiStGen is illustrated in Figure FIGREF8 . Basically, the HiStGen employs the encoder-decoder framework. In the encoding phase, to obtain the representation of a multi-paragraph document, HiStGen utilizes the hierarchical encoder framework BIBREF36 to obtain the document representation. The decoding phase is hierarchical, where we exploit three-level coherence for better OG prediction. Specifically, we employ a Markov paragraph dependency mechanism between context paragraphs for the section boundary prediction problem. Moreover, HiStGen employs a section-aware attention mechanism between a section and its heading, and a Markov heading dependency mechanism and a review mechanism between context headings for the heading generation problem whenever a new section is"]}
{"question_id": "d66550f65484696c1284903708b87809ea705786", "predicted_answer": "Language Models, seq2seq using LSTMs and convolutional seq2seq at each layer of the decoder.", "predicted_evidence": ["pairs of stories were evaluated by 5 judges each in a blind test. Results. We analyze the effect of our modeling improvements on the WritingPrompts dataset. Generation Quality. Our proposed fusion model is capable of generating unique text without copying directly from the training set. When analyzing 500 150-word generated stories from test-set prompts, the average longest common subsequence is 8.9. In contrast, the baseline Conv seq2seq model copies 10.2 words on average and the KNN baseline copies all 150 words from a story in the training set. Figure FIGREF27 shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a language model producing common words and punctuation. The second seq2seq model learns to focus on rare words, such as horned and robe. However, the fusion model has limitations. Using random sampling to generate can produce errors. For example, can't is tokenized to ca n't, and the model", "dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story. We propose a fusion-based approach to encourage conditioning on the prompt. We train a seq2seq model that has access to the hidden states of a pretrained seq2seq model. Doing so can be seen as a type of boosting or residual learning that allows the second model to focus on what the first model failed to learn\u2014such as conditioning on the prompt. To our knowledge, this paper is the first to show that fusion reduces the problem of seq2seq models degenerating into language models that capture primarily syntactic and grammatical information. The cold fusion mechanism of BIBREF3 pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure FIGREF13 ): DISPLAYFORM0   where the", "work has investigated the integration of language models with seq2seq models. The two models can be leveraged together without architectural modifications: BIBREF21 use language models to initialize the encoder and decoder side of the seq2seq model independently, and BIBREF22 combine the predictions of the language model and seq2seq model solely at inference time. Recent work has also proposed deeper integration. BIBREF23 combined a trained language model with a trained seq2seq model to learn a gating function that joins them. BIBREF3 propose training the seq2seq model given the fixed language model then learning a gate to filter the information from the language model. Baselines. We evaluate a number of baselines: (1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism. (2) seq2seq: using LSTMs and convolutional seq2seq", "at each layer of the decoder. Modeling Unbounded Context with Gated Multi-Scale Self-attention. CNNs can only model a bounded context window, preventing the modeling of long-range dependencies within the output story. To enable modeling of unbounded context, we supplement the decoder with a self-attention mechanism BIBREF8 , BIBREF9 , which allows the model to refer to any previously generated words. The self-attention mechanism improves the model's ability to extract long-range context with limited computational impact due to parallelism. Gated Attention: Similar to BIBREF9 , we use multi-head attention to allow each head to attend to information at different positions. However, the queries, keys and values are not given by linear projections but by more expressive gated deep neural nets with Gated Linear Unit BIBREF4 activations. We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections. Multi-Scale Attention: Further, we propose to have", "with Gated Linear Unit BIBREF4 activations. We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections. Multi-Scale Attention: Further, we propose to have each head operating at a different time scale, depicted in Figure FIGREF7 . Thus the input to each head is downsampled a different amount\u2014the first head sees the full input, the second every other input timestep, the third every third input timestep, etc. The different scales encourage the heads to attend to different information. The downsampling operation limits the number of tokens in the attention maps, making them sharper. The output of a single attention head is given by DISPLAYFORM0   where INLINEFORM0 contains the hidden states up to time INLINEFORM1 at layer INLINEFORM2 , and INLINEFORM3 are gated downsampling networks as shown in Figure FIGREF7 . Unlike BIBREF9 , we allow the model to optionally attend to a 0 vector at each timestep, if it chooses to ignore the information of", "BIBREF5 , BIBREF4 , but accurately modeling several paragraphs is an open problem. While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts. We will evaluate strategies to address these challenges in the following sections. Hierarchical Story Generation. High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases. Efficient Learning", "Introduction. Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation BIBREF0 . We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of standard sequence models to drift off topic. We find that standard sequence-to-sequence (seq2seq) models BIBREF1 applied to hierarchical story generation are prone to degenerating into language models that pay little attention to the writing prompt (a problem that", "generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases. Efficient Learning with Convolutional Sequence-to-Sequence Model. The length of stories in our dataset is a challenge for RNNs, which process tokens sequentially. To transform prompts into stories, we instead build on the convolutional seq2seq model of BIBREF6 , which uses deep convolutional networks as the encoder and decoder. Convolutional models are ideally suited to modeling long sequences, because they allow parallelism of computation within the sequence. In the Conv seq2seq model, the encoder and decoder are connected with attention modules BIBREF7 that perform a weighted sum of encoder outputs, using attention at each layer of the decoder. Modeling Unbounded Context with Gated Multi-Scale Self-attention. CNNs can only model a bounded context window, preventing the modeling of long-range dependencies"]}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "predicted_answer": "No", "predicted_evidence": ["INLINEFORM2 , 4 decoder self-attention heads. Conv seq2seq + self-attention. 3 layers in encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 8 layers in the decoder with hidden unit sizes INLINEFORM2 with convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 nomalization INLINEFORM4 , 4 decoder self-attention heads. Ensemble: Conv seq2seq + self-attention. Two different Conv seq2seq models were trained and ensembled together by averaging with equal weights. Fusion: Conv seq2seq + self-attention. The pretrained seq2seq model is the model in Section SECREF37 . The additional fused model has the following architecture: 5 layers in the encoder with hidden unit sizes INLINEFORM0 and convolutional kernel widths INLINEFORM1 . 5 layers in the decoder with hidden unit sizes INLINEFORM2 and convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout", "dataset. This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time. The two models are integrated with the fusion mechanism described in Section SECREF11 . Training. We implement models with the fairseq-py library in PyTorch. Similar to BIBREF6 , we train using the Nesterov accelerated gradient method BIBREF26 using gradient clipping BIBREF27 . We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set. We provide model architectures in the appendix. Generation. We generate stories from our models using a top-k random sampling scheme. At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the INLINEFORM0 most likely candidates from this distribution. Then, subsequent timesteps generate words based on the previously selected words. We find this sampling strategy substantially more", "work has investigated the integration of language models with seq2seq models. The two models can be leveraged together without architectural modifications: BIBREF21 use language models to initialize the encoder and decoder side of the seq2seq model independently, and BIBREF22 combine the predictions of the language model and seq2seq model solely at inference time. Recent work has also proposed deeper integration. BIBREF23 combined a trained language model with a trained seq2seq model to learn a gating function that joins them. BIBREF3 propose training the seq2seq model given the fixed language model then learning a gate to filter the information from the language model. Baselines. We evaluate a number of baselines: (1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism. (2) seq2seq: using LSTMs and convolutional seq2seq", "to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure FIGREF13 ): DISPLAYFORM0   where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by INLINEFORM0 ) are concatenated to learn gates INLINEFORM1 . The gates are computed using a linear projection with the weight matrix INLINEFORM2 . The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see Appendix). We use layer normalization BIBREF10 after each fully connected layer. Story Generation. Sequence-to-sequence neural networks BIBREF1 have achieved state of the art performance on a variety of text generation tasks, such as machine translation BIBREF1 and summarization BIBREF11 . Recent work has applied these models to more open-ended generation tasks, including writing Wikipedia articles BIBREF12 and poetry BIBREF13 . Previous", "dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story. We propose a fusion-based approach to encourage conditioning on the prompt. We train a seq2seq model that has access to the hidden states of a pretrained seq2seq model. Doing so can be seen as a type of boosting or residual learning that allows the second model to focus on what the first model failed to learn\u2014such as conditioning on the prompt. To our knowledge, this paper is the first to show that fusion reduces the problem of seq2seq models degenerating into language models that capture primarily syntactic and grammatical information. The cold fusion mechanism of BIBREF3 pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure FIGREF13 ): DISPLAYFORM0   where the", "do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism. (2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention. (3) Ensemble: an ensemble of two Conv seq2seq with self-attention models. (4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search. The retrieved story from the training set is limited to 150 words to match the length of generated stories. Fusion Training. To train the fusion model, we first pretrain a Conv seq2seq with self-attention model on the WritingPrompts dataset. This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time. The two models are integrated with the fusion mechanism described in", "generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases. Efficient Learning with Convolutional Sequence-to-Sequence Model. The length of stories in our dataset is a challenge for RNNs, which process tokens sequentially. To transform prompts into stories, we instead build on the convolutional seq2seq model of BIBREF6 , which uses deep convolutional networks as the encoder and decoder. Convolutional models are ideally suited to modeling long sequences, because they allow parallelism of computation within the sequence. In the Conv seq2seq model, the encoder and decoder are connected with attention modules BIBREF7 that perform a weighted sum of encoder outputs, using attention at each layer of the decoder. Modeling Unbounded Context with Gated Multi-Scale Self-attention. CNNs can only model a bounded context window, preventing the modeling of long-range dependencies", "and convolutional kernel widths INLINEFORM1 . 5 layers in the decoder with hidden unit sizes INLINEFORM2 and convolutional kernel widths INLINEFORM3 . Learning rate 0.25, momentum 0.99, dropout 0.3, embedding size 256, output embedding size 256, l2 normalization INLINEFORM4 , 4 decoder self-attention heads."]}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "predicted_answer": "933 manually identified adpositions.", "predicted_evidence": ["such as 14 and coverbs such as 4. Postpositions are localizers such as 4 and 1. We observe that prepositions 4 and 4 are dominant in the corpus (greater than 10%). Other top adpositions are distributed quite evenly between prepositions and postpositions. On the low end, 27 out of the 70 attested adposition types occur only once in the corpus. Corpus Analysis ::: Supersense & Construal Distributions in Chinese versus English. The distribution of scene role and function types in Chinese and English reflects the differences and similarities of adposition semantics in both languages. In tab:statssupersensezhen we compare this corpus with the largest English adposition supersense corpus, STREUSLE version 4.1 BIBREF0, which consists of web reviews. We note that the Chinese corpus is proportionally smaller than the English one in terms of token and adposition counts. Moreover, there are fewer scene role, function and construal types attested in Chinese. The proportion of construals in which", "necessary as an automatic POS tagger was found unsuitable for our criteria (sec:adpositionidentification). Corpus Annotation ::: Preprocessing ::: Data Format. Though parsing is not essential to this annotation project, we ran the StanfordNLP BIBREF40 dependency parser to obtain POS tags and dependency trees. These are stored alongside supersense annotations in the CoNLL-U-Lex format BIBREF41, BIBREF0. CoNLL-U-Lex extends the CoNLL-U format used by the Universal Dependencies BIBREF42 project to add additional columns for lexical semantic annotations. Corpus Annotation ::: Reliability of Annotation. The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was", "smaller than the English one in terms of token and adposition counts. Moreover, there are fewer scene role, function and construal types attested in Chinese. The proportion of construals in which the scene role differs from the function (scene$\\ne $fxn) is also halved in Chinese. In this section, we delve into comparisons regarding scene roles, functions, and full construals between the two corpora both quantitatively and qualitatively. Corpus Analysis ::: Supersense & Construal Distributions in Chinese versus English ::: Overall Distribution of Supersenses. fig:barscenezhen,fig:barfunctionzhen present the top 10 scene roles and functions in Mandarin Chinese and their distributions in English. It is worth noting that since more scene role and function types are attested in the larger STREUSLE dataset, the percentages of these supersenses in English are in general lower than the ones in Chinese.  There are a few observations in these distributions that are of particular interest. For", "1-year interval between the two annotation phases. Corpus Analysis. Our corpus contains 933 manually identified adpositions. Of these, 70 distinct adpositions, 28 distinct scene roles, 26 distinct functions, and 41 distinct full construals are attested in annotation. Full statistics of token and type frequencies are shown in tab:stats. This section presents the most frequent adpositions in Mandarin Chinese, as well as quantitative and qualitative comparisons of scene roles, functions, and construals between Chinese and English annotations. Corpus Analysis ::: Adpositions in Chinese. We analyze semantic and distributional properties of adpositions in Mandarin Chinese. The top 5 most frequent prepositions and postpositions are shown in tab:statstoptoks. Prepositions include canonical adpositions such as 14 and coverbs such as 4. Postpositions are localizers such as 4 and 1. We observe that prepositions 4 and 4 are dominant in the corpus (greater than 10%). Other top adpositions are", "dataset, the percentages of these supersenses in English are in general lower than the ones in Chinese.  There are a few observations in these distributions that are of particular interest. For some of the examples, we use an annotated subset of the English Little Prince corpus for qualitative comparisons, whereas all quantitative results in English refer to the larger STREUSLE corpus of English Web Treebank reviews BIBREF0. Corpus Analysis ::: Supersense & Construal Distributions in Chinese versus English ::: Fewer Adpositions in Chinese. As shown in tab:statssupersensezhen, the percentage of adposition targets over tokens in Chinese is only half of that in English. This is due to the fact that Chinese has a stronger preference to convey semantic information via verbal or nominal forms. Examples eg:enmoreadpositions,eg:zhlessadpositions show that the prepositions used in English, of and in, are translated as copula verbs (4) and progressives (44) in Chinese. Corresponding to", "paper, we presented the first corpus annotated with adposition supersenses in Mandarin Chinese. The corpus is a valuable resource for examining similarities and differences between adpositions in different languages with parallel corpora and can further support automatic disambiguation of adpositions in Chinese. We intend to annotate additional genres\u2014including native (non-translated) Chinese and learner corpora\u2014in order to more fully capture the semantic behavior of adpositions in Chinese as compared to other languages. Acknowledgements. We thank anonymous reviewers for their feedback. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States\u2013Israel Binational Science Foundation (BSF), Jerusalem, Israel.", "frameworks can facilitate cross-framework comparisons. Prior to supersense annotation, we conducted the following preprocessing steps in order to identify the adposition targets that merit supersense annotation. Corpus Annotation ::: Preprocessing ::: Tokenization. After automatic tokenization using Jieba, we conducted manual corrections to ensure that all potential adpositions occur as separate tokens, closely following the Chinese Penn Treebank segmentation guidelines BIBREF39. The final corpus includes all 27 chapters of The Little Prince, with a total of 20k tokens. Corpus Annotation ::: Preprocessing ::: Adposition Targets. All annotators jointly identified adposition targets according to the criteria discussed in subsec:adpositioncriteria. Manual identification of adpositions was necessary as an automatic POS tagger was found unsuitable for our criteria (sec:adpositionidentification). Corpus Annotation ::: Preprocessing ::: Data Format. Though parsing is not essential to this", "lc:one's-regard this-way tell res seems real `It looks real to those who know about life.' (zh_lpp_1943.185) Corpus Analysis ::: Supersense & Construal Distributions in Chinese versus English ::: Divergence of Functions across Languages. Among all possible types of construals between scene role and function, here we are only concerned with construals where the scene role differs from the function (scene$\\ne $fxn). The basis of hwang-etal-2017-double construal analysis is that a scene role is construed as a function to express the contexual meaning of the adposition that is different from its lexical one. fig:barconstrualzhen presents the top 10 divergent (scene$\\ne $fxn) construals in Chinese and their corresponding proportions in English. Strikingly fewer types of construals are formed in Chinese. Nevertheless, Chinese is replete with RecipientDirection adpositions, which constitute nearly half of the construals.  The 2 adpositions annotated with RecipientDirection are 4 and 4, both"]}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "predicted_answer": "CNN, RNN, FFNN, LSTM, Logistic Regression, Random Forest.", "predicted_evidence": ["like content, the author\u2019s name and job-title, using lots of models like the convolutional neural network (CNN), recurrent neural network (RNN), feed-forward neural network (FFNN), long-short term memory (LSTM) and logistic regression to find the most optimal model and return its results. In [1], the author built a classifier using natural language processing and used models like CNN, RNN, FFNN, and Logistic Regression and concluded that the CNN classifiers could not be as competitive as the RNN classifiers. The authors in [2] think that their study can be improved by having more features like knowing the history of lies spoken by the news reporter or the speaker. Moreover, apart from the traditional machine learning methods, new models have also been developed. One of the newer models, TraceMiner, creates an LSTM-RNN model inferring from the embedding of social media users in the social network structure to propagate through the path of messages and has provided high classification", "an output layer with just one node for the classification (real or fake) which uses sigmoid as an activation function. We chose sigmoid as the output layer activation and the binary_crossentropy as the loss since it is a binary classification problem and the use of softmax normalizes the results which is not needed for this problem and since we use only one output node to return the activation, we applied sigmoid for the output layer activation. We performed Grid Search strategy to find the best hyper-parameters such as activations, optimizers, number of hidden layers and number of hidden neurons. We had used Keras Sequential model and we used Dense Layers which contains connections to every hidden node in the next layer. Due to the limitation of computing resource, the grid search for Neural Networks is divided into three sequential steps. Instead of performing grid search on all the hyperparameters all at once, we chose to do grid search for the activations for the hidden layers,", "sequential steps. Instead of performing grid search on all the hyperparameters all at once, the grid search is first done on the number of hidden layers and all other hyperparameters are randomly selected from the subset. Then, the grid search is done on the number of nodes in the hidden layer(s), using the best number of hidden layer found in step 1. The grid search completes when all four steps are finished. In each step we used K-fold cross validation with $K = 3$. Methods ::: Fine-tuning ::: Random Forest. A random forest is an ensemble classifier that estimates based on the combination of different decision trees. So random forest will fit a number of decision tree classifiers on various subsamples of the dataset. A random best subsets are built by each tree in the forest. In the end, it gives the best subset of features among all the random subsets of features. In our project, 3 random forest algorithms have been applied with models count vectorizer, tfidf and word-to-vector.", "it gives the best subset of features among all the random subsets of features. In our project, 3 random forest algorithms have been applied with models count vectorizer, tfidf and word-to-vector. Random forest algorithm requires 4 hyperparameters to tune, such as the number of trees in the forest (i.e., {200, 400, 800}); the maximum depth of the tree (i.e., {1,5,9}); the minimum number of samples required to be at a lead node (i.e., {2, 4}); The minimum number of samples at each leaf node has the effect of smoothing the model, especially during regression; the minimum number of samples required to be at a leaf node (i.e., {5, 10}). All parameters are applied to grid search and in the end, the best set of parameters can be determined as we used K-fold cross validation with $K = 3$. Methods ::: Fine-tuning ::: Logistic Regression. Logistic regression is a statistical machine learning algorithm that classifies the data by considering outcome variables on extreme ends and this algorithm", "requires specific parameters such as a kernel type, $C$, maximum iterations, etc. In our case, we needed to determine the optimal $C$ as well as the optimal kernel for each fit. We used K-fold cross validation with $K = 3$. A grid search of kernel types and $C$ was performed in order to give us the most accurate svm model. The parameters we used for each kernel were linear and rbf while the values we used for $C$ were 0.25 ,0.5, and 0.75. Once the grid search was completed for these hyperparameters, the model was evaluated with the most optimal hyperparameters using cross validation of 3 splits. Results. Grid Search Results  Mean Test Scores     ANN Loss and Accuracy  LSTM Loss and Accuracy  The model is evaluated using a 3-fold of cross validation. Out of the fifteen models, CountVectorizer with LSTMs performs the best. Word2Vec performs the worst among the three pre-training algorithms. Random forest performs the worst among the five fine-tuning algorithms. Discussion. Among our", "are included. Methods ::: Fine-tuning. Once the representations of text are pre-trained from previous unsupervised learning, the representations are then fed into 5 different models to perform supervised learning on the downstream task. In this case, the downstream task is a binary classification of the fake news as either real or fake. A k-fold prediction error is obtained from each of the 5 models, and since we have 3 different pre-training models, we have a total of 15 models to compare. Methods ::: Fine-tuning ::: Artificial Neural Network (ANN). We trained simple Artificial Neural Networks which contains an input layer, particular number of output layers (specified by a hyperparameter) in which each hidden layer contains the same number of neurons and the same activation function, and an output layer with just one node for the classification (real or fake) which uses sigmoid as an activation function. We chose sigmoid as the output layer activation and the binary_crossentropy as", "only on context words. We can use different forms for encoding texts which can be used to be trained using these algorithms to achieve a better model. In another State-of-the-art pre-trained models can be used if the task is no longer a binary classification. Models like Transformer and BERT will be strong candidates as they have learned a very strong representation that takes the context into account when computing an embedding for a word. Unlike LSTMs whose sequential nature prohibits parallelization, the Transformer and the BERT can achieve parallelization by replacing recurrence with the attention mechanism. Thus, they require less computation power and can be easily fine-tuned in downstream tasks. Github Repo. https://github.com/Sairamvinay/Fake-News-Dataset Author Contributions. Sairamvinay Vijayaraghavan: Project Planning, Problem Formation, DataSet Search, POS Distribution graph, Code for CountVectorizer, Word2Vec, ANN, Randomforest,To parse csv files (readdata), Code", "TraceMiner, creates an LSTM-RNN model inferring from the embedding of social media users in the social network structure to propagate through the path of messages and has provided high classification accuracy$^{5}$. FAKEDETECTOR is another inference model developed to detect the credibility of the fake news which is considered to be quite reliable and accurate$^{7}$. There also have been studies that have a different approach. A paper surveys the current state-of-the-art technologies that are imperative when adopting and developing fake news detection and provides a classification of several accurate assessment methods that analyze the text and detect anomalies$^{3}$. These previous approaches lack a clear contextual analysis used in NLP. We considered the semantic meaning of each word and we feel that the presence of particular words influence the meaning. We reckoned this important since we felt the contextual meaning of the text needs to be preserved and analyzed for better"]}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "predicted_answer": "readability, syntactic complexity, and content", "predicted_evidence": ["and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels. Feature Extaction. In this section I present the extracted features partitioned in six groups and detail each of them separately. Model Selection and Evaluation. As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the", "of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent. Conclusion. In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem. While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however,", "achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics. Acknoledgements. I would like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's", "Introduction. Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data.", "Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics. In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that", "a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research. Problem Definition. In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most", "matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's employer."]}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_answer": "cost measure that uses the confusion matrix of the prediction and prior knowledge.", "predicted_evidence": ["Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics. In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that", "of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent. Conclusion. In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem. While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however,", "matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics. Acknoledgements. I would like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's", "and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels. Feature Extaction. In this section I present the extracted features partitioned in six groups and detail each of them separately. Model Selection and Evaluation. As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the", "Introduction. Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data.", "a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research. Problem Definition. In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most", "like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's employer."]}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "predicted_answer": "Gradient Boosted Trees", "predicted_evidence": ["Introduction. Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data.", "a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research. Problem Definition. In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most", "achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics. Acknoledgements. I would like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's", "Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics. In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that", "and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels. Feature Extaction. In this section I present the extracted features partitioned in six groups and detail each of them separately. Model Selection and Evaluation. As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the", "of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent. Conclusion. In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem. While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however,", "matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's employer."]}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "predicted_answer": "Further analysis and validation to answer whether LDA was just a clever way to model information leak in the given data or not.", "predicted_evidence": ["Introduction. Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data.", "a variety of classifiers. The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research. Problem Definition. In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most", "of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the infrequent. Conclusion. In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem. While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however,", "like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's employer.", "Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics. In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that", "achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics. Acknoledgements. I would like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's", "matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0  where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.", "and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels. Feature Extaction. In this section I present the extracted features partitioned in six groups and detail each of them separately. Model Selection and Evaluation. As the class distribution in the training data is not balanced, I have used stratified cross-validation for validation purposes and for hyper-parameter selection. As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft.. The depth of the trees was set to 3, the learning rate to 0.06 and the number of trees to 4,000. Also, to combat the class imbalance in the training labels I assigned class weights at each class so that errors in the frequent classes incur less penalties than error in the"]}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "predicted_answer": "Unanswerable", "predicted_evidence": ["were beyond the 95th percentile BIBREF16. Corpus statistics are given in Table TABREF12. The final data are (1) a parallel corpus of 180k biased sentences and their neutral counterparts, and (2) 385k neutral sentences that were adjacent to a revised sentence at the time of editing but were not changed by the editor. Note that following BIBREF2, the neutralizing experiments in Section SECREF4 focus on the subset of WNC where the editor modified or deleted a single word in the source text (\u201cBiased-word\u201d in Table TABREF12). Table TABREF12 also gives a categorization of these sample pairs using a slight extension of the typology of BIBREF2. They defined framing bias as using subjective words or phrases linked with a particular point of view (like using words like best or deepest or using pilfered from instead of based on, and epistemological bias as linguistic features that subtly (often via presupposition) focus on the believability of a proposition. We add to their two a third kind of", "Introduction. Writers and editors of texts like encyclopedias, news, and textbooks strive to avoid biased language. Yet bias remains ubiquitous. 62% of Americans believe their news is biased BIBREF0 and bias is the single largest source of distrust in the media BIBREF1. This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d. For example a news headline like \u201cJohn McCain exposed as an unprincipled politician\" (Figure FIGREF1) is biased because the verb expose is a factive verb that", "Inter-rater agreement was fair to substantial (Krippendorff's alpha of 0.65 for fluency, 0.33 for meaning, and 0.51 for bias). We report statistical significance with a t-test and 95% confidence interval. Experiments ::: Wikipedia (WNC). Results on WNC are presented in Table TABREF35. In addition to methods from the literature we include (1) a BERT-based system which simply predicts and deletes subjective words, and (2) a system which predicts replacements (including deletion) for subjective words directly from their BERT embeddings. All methods appear to successfully reduce bias according to the human evaluators. However, many methods appear to lack fluency. Adding a token-weighted loss function and pretraining the decoder help the model's coherence according to BLEU and accuracy. Adding the detector (modular) or a BERT encoder (concurrent) provide additional benefits. The proposed models retain the strong effects of systems from the literature while also producing target-level", "and 2019 where editors provided NPOV-related justification BIBREF11, BIBREF2, BIBREF12. To maximize the precision of bias-related changes, we ignored revisions where [noitemsep] More than a single sentence was changed. Minimal edits (character Levenshtein distance $<$ 4). Maximal edits (more than half of the words changed). Edits where more than half of the words were proper nouns. Edits that fixed spelling or grammatical errors. Edits that added references or hyperlinks. Edits that changed non-literary elements like tables or punctuation. We align sentences in the pre and post text by computing a sliding window (of size $k = 5$) of pairwise BLEU BIBREF13 between sentences and matching sentences with the biggest score BIBREF14, BIBREF15. Last, we discarded pairs whose length ratios were beyond the 95th percentile BIBREF16. Corpus statistics are given in Table TABREF12. The final data are (1) a parallel corpus of 180k biased sentences and their neutral counterparts, and (2) 385k", "force the model to target particular words. This can let human advisors correct errors or push the model's behavior towards some desired outcome. We find that the model is indeed capable of being controlled, letting users target specific words for rewording in case they disagree with the model's output or seek recommendations on specific language. However, doing so can also introduce errors into downstream language generation (Table TABREF52). Related Work. Subjectivity Bias. The study of subjectivity in NLP was pioneered by the late Janyce Wiebe and colleagues BIBREF41, BIBREF42. Several studies develop methods for highlighting subjective or persuasive frames in a text BIBREF43, BIBREF44, or detecting biased sentences BIBREF45, BIBREF46, BIBREF12, BIBREF47 of which the most similar to ours is BIBREF2, whose early, smaller version of WNC and logistic regression-based bias detector inspired our study. Debiasing. Many scholars have worked on removing demographic prejudice from meaning", "Turk. Workers were shown the BIBREF2 and Wikipedia definition of a \u201cbiased statement\u201d and six example sentences, then subjected to a five-question qualification test where they had to identify subjectivity bias. Approximately half of the 30,000 workers who took the qualification test passed. Those who passed were asked to compare pairs of original and edited sentences (not knowing which was the original) along three criteria: fluency, meaning preservation, and bias. Fluency and bias were evaluated on a Semantic Differential scale from -2 to 2. Here, a semantic differential scale can better evaluate attitude oriented questions with two polarized options (e.g., \u201cis text A or B more fluent?\u201d). Meaning was evaluated on a Likert scale from 0 to 4, ranging from \u201ctotally different\u201d to \u201cidentical\u201d. Inter-rater agreement was fair to substantial (Krippendorff's alpha of 0.65 for fluency, 0.33 for meaning, and 0.51 for bias). We report statistical significance with a t-test and 95% confidence", "reducing bias is thus an important new challenge for the Natural Language Processing and Artificial Intelligence community. By learning models to automatically detect and correct subjective bias in text, this work is a first step in this important direction. Nonetheless our scope was limited to single-word edits, which only constitute a quarter of the edits in our data, and are probably among the simplest instances of bias. We therefore encourage future work to tackle broader instances of multi-word, multi-lingual, and cross-sentence bias. Another important direction is integrating aspects of fact-checking BIBREF55, since a more sophisticated system would be able to know when a presupposition is in fact true and hence not subjective. Finally, our new join embedding mechanism can be applied to other modular neural network architectures. Acknowledgements. We thank the Japan-United States Educational Commission (Fulbright Japan) for their generous support. We thank Chris Potts, Hirokazu", "Introduction. Natural language inference (NLI) is a widely-studied problem in natural language processing. It aims at comparing a pair of sentences (i.e. a premise and a hypothesis), and inferring the relationship between them (i.e., entailment, neutral and contradiction). Large-scaled datasets like SNLI BIBREF0 and MultiNLI BIBREF1 have been created by crowd-sourcing and fertilized NLI research substantially. However, several works BIBREF2, BIBREF3, BIBREF4 have pointed out that crowd-sourcing workers have brought a bias pattern named annotation artifacts in these NLI datasets. Such artifacts in hypotheses can reveal the labels and make it possible to predict the labels solely by looking at the hypotheses. For example, models trained on SNLI with only the hypotheses can achieve an accuracy of 67.0%, despite the always predicting the majority-class baseline is only 34.3% BIBREF2. Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic"]}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "predicted_answer": "The sentence alignment quality is evaluated using inter-annotator reliability measured by Krippendorff's $\\alpha$-reliability score for ordinal ratings, and rated on a 5-point scale for text-to-text alignments and a 3-point scale for audio-text alignments. The inter-annotator reliability for text-to-text alignment quality ratings scored 0.77, while for audio-text alignment quality ratings it scored 1.00.", "predicted_evidence": ["agreement between two raters was measured by Krippendorff's $\\alpha $-reliability score BIBREF11 for ordinal ratings. The inter-annotator reliability for text-to-text alignment quality ratings scored 0.77, while for audio-text alignment quality ratings it scored 1.00. Corpus Evaluation ::: Examples. In the following, we present selected examples for text-text alignments for each bin. A closer inspection reveals properties and shortcomings of hunalign scores which are based on a combination of dictionary-based alignments and sentence-length information. Shorter sentence pairs are in general aligned correctly, irrespective of the score (compare examples with score $0.30$. $0.78$ and $1.57$, $2.44$ below). Longer sentences can include exact matches of longer substrings, however, they are scored based on a bag-of-words overlap (see the examples with scores $0.41$ and $0.84$ below). Schigolch Yes, yes; und mir tr\u00e4umte von einem St\u00fcck Christmas Pudding. She only does that to revive old", "Partial alignment, some words or sentences may be missing Correct alignment, allowing non-spoken syllables at start or end. The evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability. Corpus Evaluation ::: Evaluation Results. Table TABREF54 shows the results of our manual evaluation. The audio-text alignment was rated as in general as high quality. The text-text alignment rating increases corresponding to increasing hunalign confidence score which shows that the latter can be safely used to find a threshold for corpus filtering. Overall, the audio-text and text-text alignment scores are very similar to those reported by KocabiyikogluETAL:18. The inter-annotator agreement between two raters was measured by Krippendorff's $\\alpha $-reliability score BIBREF11 for ordinal ratings. The inter-annotator reliability for text-to-text alignment quality ratings", "Corpus Evaluation ::: Human Evaluation. For a manual evaluation of our dataset, we split the corpus into three bins according to ranges $(-0.3,0.3]$, $(0.3,0.8]$ and $(0.8,\\infty )$ of the hunalign confidence score (see Table TABREF56). The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18: Wrong alignment Partial alignment with slightly compositional translational equivalence Partial alignment with compositional translation and additional or missing information Correct alignment with compositional translation and few additional or missing information Correct alignment and fully compositional translation The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale: Wrong alignment Partial alignment, some words or sentences may be missing Correct alignment, allowing non-spoken syllables at start or end. The evaluation experiment was performed by two annotators who each rated 30", "of aligned triples of German audio, German text, and English translations for speech translation from German to English. The audio data in our corpus are read speech, based on German audio books, ensuring a low amount of speech disfluencies. The audio-text alignment and text-to-text sentence alignment was done with state-of-the-art alignment tools and checked to be of high quality in a manual evaluation. The audio-text alignment was generally rated very high. The text-text sentence alignment quality is comparable to widely used corpora such as that of KocabiyikogluETAL:18. A cutoff on a sentence alignment quality score allows to filter the text alignments further for speech translation, resulting in a clean corpus of $50,427$ German-English sentence pairs aligned to 110 hours of German speech. A larger version of the corpus, comprising 133 hours of German speech and high-quality alignments to German transcriptions is available for speech recognition. Acknowledgments. The research", "alignments that may only have been produced when aligning in a specific direction. Statistics on the resulting text alignments are given in Table TABREF37. Data Filtering and Corpus Structure ::: Corpus Filtering. A last step in our corpus creation procedure consisted out filtering out empty and incomplete alignments, i.e., alignments that did not consist of a DE-EN sentence pair. This was achieved by dropping all entries with a hunalign score of -0.3 or below. Table TABREF38 shows the resulting corpus after this filtering step. Moreover, many-to-many alignments by hunalign were re-segmented to source-audio sentence level for German, while keeping the merged English sentence to provide a complete audio lookup. The corresponding English sentences were duplicated and tagged with <MERGE> to mark that the German sentence was involved into a many-to-many alignment. The size of our final cleaned and filtered corpus is thus comparable to the cleaned Augmented LibriSpeech corpus that has been", "that the German sentence was involved into a many-to-many alignment. The size of our final cleaned and filtered corpus is thus comparable to the cleaned Augmented LibriSpeech corpus that has been used in speech translation experiments by BerardETAL:18. Statistics on the resulting filtered text alignments are given in Table TABREF38. Data Filtering and Corpus Structure ::: Corpus Structure. Our corpus is structured in following folders: contains German text files for each book contains English text files for each book alignment maps produced by aeneas sentence level audio files text2speech, a lookup table for speech alignments text2text, a lookup table for text-to-text alignments Further information about the corpus and a download link can be found here: https://www.cl.uni-heidelberg.de/statnlpgroup/librivoxdeen/. Corpus Evaluation ::: Human Evaluation. For a manual evaluation of our dataset, we split the corpus into three bins according to ranges $(-0.3,0.3]$, $(0.3,0.8]$ and", "Furthermore we added rules to adjust the segmenting behavior for direct speech and for semicolon-separated sentences. Source Corpus Creation ::: Text-to-Speech Alignment. To align sentences to onsets and endings of corresponding audio segments we made use of aeneas \u2013 a tool for an automatic synchronization of text and audio. In contrast to most forced aligners, aeneas does not use automatic speech recognition (ASR) to compare an obtained transcript with the original text. Instead, it works in the opposite direction by using dynamic time warping to align the mel-frequency cepstral coefficients extracted from the real audio to the audio representation synthesized from the text, thus aligning the text file to a time interval in the real audio. Furthermore, we used the maps pointing to the beginning and the end of each text row in the audio file produced with SoX to split the audio into sentence level chunks. The timestamps were also used to filter boilerplate information about the book,", "::: Text-to-Text Alignment. To produce text-to-text alignments we used hunalign with a custom dictionary of parallel sentences, generated from the WikiMatrix corpus. Using this additional dictionary improved our alignment scores. Furthermore we availed ourselves of a realign option enabling to save a dictionary generated in a first pass and profiting from it in a second pass. The final dictionary we used for the alignments consisted of a combination of entries of our corpora as well as the parallel corpus WikiMatrix. For further completeness we reversed the arguments in hunalign to not only obtain German to English alignments, but also English to German. These tables were merged to build the union by dropping duplicate entries and keeping those with a higher confidence score, while also appending alignments that may only have been produced when aligning in a specific direction. Statistics on the resulting text alignments are given in Table TABREF37. Data Filtering and Corpus Structure"]}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "predicted_answer": "Human evaluation using different scales.", "predicted_evidence": ["of aligned triples of German audio, German text, and English translations for speech translation from German to English. The audio data in our corpus are read speech, based on German audio books, ensuring a low amount of speech disfluencies. The audio-text alignment and text-to-text sentence alignment was done with state-of-the-art alignment tools and checked to be of high quality in a manual evaluation. The audio-text alignment was generally rated very high. The text-text sentence alignment quality is comparable to widely used corpora such as that of KocabiyikogluETAL:18. A cutoff on a sentence alignment quality score allows to filter the text alignments further for speech translation, resulting in a clean corpus of $50,427$ German-English sentence pairs aligned to 110 hours of German speech. A larger version of the corpus, comprising 133 hours of German speech and high-quality alignments to German transcriptions is available for speech recognition. Acknowledgments. The research", "Partial alignment, some words or sentences may be missing Correct alignment, allowing non-spoken syllables at start or end. The evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability. Corpus Evaluation ::: Evaluation Results. Table TABREF54 shows the results of our manual evaluation. The audio-text alignment was rated as in general as high quality. The text-text alignment rating increases corresponding to increasing hunalign confidence score which shows that the latter can be safely used to find a threshold for corpus filtering. Overall, the audio-text and text-text alignment scores are very similar to those reported by KocabiyikogluETAL:18. The inter-annotator agreement between two raters was measured by Krippendorff's $\\alpha $-reliability score BIBREF11 for ordinal ratings. The inter-annotator reliability for text-to-text alignment quality ratings", "Corpus Evaluation ::: Human Evaluation. For a manual evaluation of our dataset, we split the corpus into three bins according to ranges $(-0.3,0.3]$, $(0.3,0.8]$ and $(0.8,\\infty )$ of the hunalign confidence score (see Table TABREF56). The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18: Wrong alignment Partial alignment with slightly compositional translational equivalence Partial alignment with compositional translation and additional or missing information Correct alignment with compositional translation and few additional or missing information Correct alignment and fully compositional translation The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale: Wrong alignment Partial alignment, some words or sentences may be missing Correct alignment, allowing non-spoken syllables at start or end. The evaluation experiment was performed by two annotators who each rated 30", "agreement between two raters was measured by Krippendorff's $\\alpha $-reliability score BIBREF11 for ordinal ratings. The inter-annotator reliability for text-to-text alignment quality ratings scored 0.77, while for audio-text alignment quality ratings it scored 1.00. Corpus Evaluation ::: Examples. In the following, we present selected examples for text-text alignments for each bin. A closer inspection reveals properties and shortcomings of hunalign scores which are based on a combination of dictionary-based alignments and sentence-length information. Shorter sentence pairs are in general aligned correctly, irrespective of the score (compare examples with score $0.30$. $0.78$ and $1.57$, $2.44$ below). Longer sentences can include exact matches of longer substrings, however, they are scored based on a bag-of-words overlap (see the examples with scores $0.41$ and $0.84$ below). Schigolch Yes, yes; und mir tr\u00e4umte von einem St\u00fcck Christmas Pudding. She only does that to revive old", "Furthermore we added rules to adjust the segmenting behavior for direct speech and for semicolon-separated sentences. Source Corpus Creation ::: Text-to-Speech Alignment. To align sentences to onsets and endings of corresponding audio segments we made use of aeneas \u2013 a tool for an automatic synchronization of text and audio. In contrast to most forced aligners, aeneas does not use automatic speech recognition (ASR) to compare an obtained transcript with the original text. Instead, it works in the opposite direction by using dynamic time warping to align the mel-frequency cepstral coefficients extracted from the real audio to the audio representation synthesized from the text, thus aligning the text file to a time interval in the real audio. Furthermore, we used the maps pointing to the beginning and the end of each text row in the audio file produced with SoX to split the audio into sentence level chunks. The timestamps were also used to filter boilerplate information about the book,", "that the German sentence was involved into a many-to-many alignment. The size of our final cleaned and filtered corpus is thus comparable to the cleaned Augmented LibriSpeech corpus that has been used in speech translation experiments by BerardETAL:18. Statistics on the resulting filtered text alignments are given in Table TABREF38. Data Filtering and Corpus Structure ::: Corpus Structure. Our corpus is structured in following folders: contains German text files for each book contains English text files for each book alignment maps produced by aeneas sentence level audio files text2speech, a lookup table for speech alignments text2text, a lookup table for text-to-text alignments Further information about the corpus and a download link can be found here: https://www.cl.uni-heidelberg.de/statnlpgroup/librivoxdeen/. Corpus Evaluation ::: Human Evaluation. For a manual evaluation of our dataset, we split the corpus into three bins according to ranges $(-0.3,0.3]$, $(0.3,0.8]$ and", "speech. A larger version of the corpus, comprising 133 hours of German speech and high-quality alignments to German transcriptions is available for speech recognition. Acknowledgments. The research reported in this paper was supported in part by the German research foundation (DFG) under grant RI-2221/4-1.", "alignments that may only have been produced when aligning in a specific direction. Statistics on the resulting text alignments are given in Table TABREF37. Data Filtering and Corpus Structure ::: Corpus Filtering. A last step in our corpus creation procedure consisted out filtering out empty and incomplete alignments, i.e., alignments that did not consist of a DE-EN sentence pair. This was achieved by dropping all entries with a hunalign score of -0.3 or below. Table TABREF38 shows the resulting corpus after this filtering step. Moreover, many-to-many alignments by hunalign were re-segmented to source-audio sentence level for German, while keeping the merged English sentence to provide a complete audio lookup. The corresponding English sentences were duplicated and tagged with <MERGE> to mark that the German sentence was involved into a many-to-many alignment. The size of our final cleaned and filtered corpus is thus comparable to the cleaned Augmented LibriSpeech corpus that has been"]}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "predicted_answer": "Yes", "predicted_evidence": ["Experiments ::: Error Analysis ::: Isolated Sentence in Bag. Each sentence in a bag can be regarded as independent individual and do not have any relationship with other sentences in the bag, which possibly leads to information loss among the multiple sentences in the bag when considering classification over bag level. Conclusion. In this paper, we propose a brand-new framework for distantly supervised relation extraction, i.e., selective gate (SeG) framework, as a new alternative to previous ones. It incorporates an entity-aware embedding module and a self-attention enhanced selective gate mechanism to integrate task-specific entity information into word embedding and then generates a complementary context-enriched representation for PCNN. The proposed framework has certain merits over previously prevalent selective attention when handling wrongly labeled data, especially for a usual case that there are only one sentence in the most of bags. The experiments conduct on popular NYT", "this framework assigns a label to a bag of sentences containing a common entity pair, and the label is a relationship of the entity pair on knowledge graph. Recently, based on the labeled data at bag level, a line of works BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7 under selective attention framework BIBREF5 let model implicitly focus on the correctly labeled sentence(s) by an attention mechanism and thus learn a stable and robust model from the noisy data. However, such selective attention framework is vulnerable to situations where a bag is merely comprised of one single sentence labeled; and what is worse, the only one sentence possibly expresses inconsistent relation information with the bag-level label. This scenario is not uncommon. For a popular distantly supervised relation extraction benchmark, e.g., NYT dataset BIBREF2, up to $80\\%$ of its training examples (i.e., bags) are one-sentence bags. From our data inspection, we randomly sample 100 one-sentence bags and find $35\\%$", "problem, selective attention tries to produce a distribution over all sentence in a bag; but if there is only one sentence in the bag, even the only sentence is wrongly labeled, the selective attention mechanism will be low-effective or even completely useless. Note that almost $80\\%$ of bags from popular relation extraction benchmark consist of only one sentence, and many of them suffer from the wrong label problem. In contrast, our proposed gate mechanism is competent to tackle such case by directly and dynamically aligning low gating value to the wrongly labeled instances and thus preventing noise representation being propagated. Particularly, a two-layer feed forward network is applied to each $\\mathbf {u}_j$ to sentence-wisely produce gating value, which is formally denoted as where, $\\mathbf {W}^{(g1)} \\in \\mathbb {R}^{3d_c \\times d_h}$, $\\mathbf {W}^{(g2)} \\in \\mathbb {R}^{d_h \\times d_h}$, $\\sigma (\\cdot )$ denotes an activation function and $g_j \\in (0, 1)$. Then, given the", "which is capable of capturing long-term or global dependencies to produce complementary and more powerful sentence representation. Hence, we employ a self-attention mechanism in our model due to its parallelizable computation and state-of-the-art performance. Unlike existing approaches that sequentially stack self-attention and CNN layers in a cascade form BIBREF9, BIBREF14, we arrange these two modules in parallel so they can generate features describing both local and long-term relations for the same input sequence. Since each bag may contain many sentences (up to 20), a light-weight networks that can can efficiently process these sentences simultaneously is more preferable, such as PCNN that is the most popular module for relation extraction. For this reason, there is only one light-weight self-attention layer in our model. This is contrast to BIBREF9 yu2018qanet and BIBREF14 wu2019pay who stack both modules many times repeatedly. Our experiments show that two modules arranged in", "Given a sentence bag $B = [s_1, \\dots , s_m]$ with common entity pair, where $m$ is the number of sentences. As elaborated in Section SECREF6, we can obtain $\\mathbf {S} = [\\mathbf {s}_1, \\dots , \\mathbf {s}_m]$ and $\\mathbf {U} = [\\mathbf {u}_1, \\dots , \\mathbf {u}_m]$ for each sentence in the bag, which are derived from PCNN and self-attention respectively. Unlike previous works under multi-instance framework that frequently use a selective attention module to aggregate sentence-level representations into bag-level one, we propose a innovative selective gate mechanism to perform this aggregation. The selective gate can mitigate problems existing in distantly supervised relation extraction and achieve a satisfactory empirical effectiveness. Specifically, when handling the noisy instance problem, selective attention tries to produce a distribution over all sentence in a bag; but if there is only one sentence in the bag, even the only sentence is wrongly labeled, the selective", "benchmark, e.g., NYT dataset BIBREF2, up to $80\\%$ of its training examples (i.e., bags) are one-sentence bags. From our data inspection, we randomly sample 100 one-sentence bags and find $35\\%$ of them is incorrectly labeled. Two examples of one-sentence bag are shown in Table TABREF1. These results indicate that, in training phrase the selective attention module is enforced to output a single-valued scalar for $80\\%$ examples, leading to an ill-trained attention module and thus hurting the performance. Motivated by aforementioned observations, in this paper, we propose a novel Selective Gate (SeG) framework for distantly supervised relation extraction. In the proposed framework, 1) we employ both the entity embeddings and relative position embeddings BIBREF8 for relation extraction, and an entity-aware embedding approach is proposed to dynamically integrate entity information into each word embedding, yielding more expressively-powerful representations for downstream modules; 2) to", "gate with selective attention to fulfill aggregation instead mean-pooling, namely, SeG+Attn. Across the board, the proposed SeG still deliver the best results in terms of both metrics even if extra selective attention module is applied. Lastly, to explore the influence of the way to combine PCNN with self-attention mechanism, we stack them by following the previous works BIBREF9, i.e., SeG w/ Stack. And we observe a notable performance drop after stacking PCNN and self-attention in Table TABREF36. This verifies that our model combining self-attention mechanism and PCNN in parallel can achieve a satisfactory result. To further empirically evaluate the performance of our method in solving one-sentence bag problem, we extract only the one-sentence bags from NYT's training and test sets, which occupy 80% of the original dataset. The evaluation and comparison results in Table TABREF33 show that compared to PCNN+ATT, the AUC improvement (+0.13) between our model and PCNN+ATT on one-sentence", "occupy 80% of the original dataset. The evaluation and comparison results in Table TABREF33 show that compared to PCNN+ATT, the AUC improvement (+0.13) between our model and PCNN+ATT on one-sentence bags is higher than the improvement of full NYT dataset, which verifies SeG's effectiveness on one-sentence bags. In addition, PCNN+ATT shows a light decrease compared with PCNN, which can also support the claim that selective attention is vulnerable to one-sentence bags. Experiments ::: Case Study. In this section, we conduct a case study to qualitatively analyze the effects of entity-aware embedding and self-attention enhanced selective gate. The case study of four examples is shown in Table TABREF38. First, comparing Bag 1 and 2, we find that, without the support of the self-attention enhanced selective gate, the model will misclassify both bags into NA, leading to a degraded performance. Further, as shown in Bag 2, even if entity-aware embedding module is absent, proposed framework"]}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "predicted_answer": "23.6%", "predicted_evidence": ["(i.e., PCNN+ATT) and all recently-promoted works in terms of all P@N metric. Compared to PCNN with selective attention (i.e., PCNN+ATT), our proposed SeG can significantly improve the performance by 23.6% in terms of P@N mean for all sentences; even if a soft label technique is applied (i.e., PCNN+ATT+SL) to alleviate wrongly labeled problem, our performance improvement is also very significant, i.e., 7.8%. Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3% , even if they propose sophisticated techniques to handle the noisy training data. These verify the effectiveness of our approach over previous works when solving the wrongly labeled problem that frequently appears in distantly supervised relation extraction. Moreover, for proposed approach and comparative ones, we also show AUC curves and available numerical values in Figure FIGREF31 and Table TABREF32", "a selective attention over multiple instances to alleviate the wrongly labeled problem, which is the principal baseline of our work. PCNN+ATT+SL BIBREF21 introduces an entity-pair level denoising method, namely employing a soft label to alleviate the impact of wrongly labeled problem. PCNN+HATT BIBREF6 employs hierarchical attention to exploit correlations among relations. PCNN+BAG-ATT BIBREF7 uses an intra-bag to deal with the noise at sentence-level and an inter-bag attention to deal with noise at the bag-level. Experiments ::: Relation Extraction Performance. We first compare our proposed SeG with aforementioned approaches in Table TABREF19 for top-N precision (i.e., P@N). As shown in the top panel of the table, our proposed model SeG can consistently and significantly outperform baseline (i.e., PCNN+ATT) and all recently-promoted works in terms of all P@N metric. Compared to PCNN with selective attention (i.e., PCNN+ATT), our proposed SeG can significantly improve the performance", "appears in distantly supervised relation extraction. Moreover, for proposed approach and comparative ones, we also show AUC curves and available numerical values in Figure FIGREF31 and Table TABREF32 respectively. The empirical results for AUC are coherent with those of P@N, which shows that, our proposed approach can significantly improve previous ones and reach a new state-of-the-art performance by handling wrongly labeled problem using context-aware selective gate mechanism. Specifically, our approach substantially improves both PCNN+HATT and PCNN+BAG-ATT by 21.4% in aspect of AUC for precision-recall. Experiments ::: Ablation Study. To further verify the effectiveness of each module in the proposed framework, we conduct an extensive ablation study in this section. In particular, SeG w/o Ent denotes removing entity-aware embedding, SeG w/o Gate denotes removing selective gate and concatenating two representations from PCNN and self-attention, SeG w/o Gate w/o Self-Attn denotes", "previously prevalent selective attention when handling wrongly labeled data, especially for a usual case that there are only one sentence in the most of bags. The experiments conduct on popular NYT dataset show that our model SeG can consistently deliver a new benchmark in state-of-the-art performance in terms of all P@N and precision-recall AUC. And further ablation study and case study also demonstrate the significance of the proposed modules to handle wrongly labeled data and thus set a new state-of-the-art performance for the benchmark dataset. In the future, we plan to incorporate an external knowledge base into our framework, which may further boost the prediction quality by overcoming the problems with a lack of background information as discussed in our error analysis. Acknowledgements. This research was funded by the Australian Government through the Australian Research Council (ARC) under grants LP180100654 partnership with KS computer. We also acknowledge the support of", "gate with selective attention to fulfill aggregation instead mean-pooling, namely, SeG+Attn. Across the board, the proposed SeG still deliver the best results in terms of both metrics even if extra selective attention module is applied. Lastly, to explore the influence of the way to combine PCNN with self-attention mechanism, we stack them by following the previous works BIBREF9, i.e., SeG w/ Stack. And we observe a notable performance drop after stacking PCNN and self-attention in Table TABREF36. This verifies that our model combining self-attention mechanism and PCNN in parallel can achieve a satisfactory result. To further empirically evaluate the performance of our method in solving one-sentence bag problem, we extract only the one-sentence bags from NYT's training and test sets, which occupy 80% of the original dataset. The evaluation and comparison results in Table TABREF33 show that compared to PCNN+ATT, the AUC improvement (+0.13) between our model and PCNN+ATT on one-sentence", "in our experiments on the held-out test set from the NYT dataset. To directly show the perfomance on one sentence bag, we also calculate the accuracy of classification (Acc.) on non-NA sentences. Experiments ::: Training Setup. For a fair and rational comparison with baselines and competitive approaches, we set most of the hyper-parameters by following prior works BIBREF10, BIBREF6, and also use 50D word embedding and 5D position embedding released by BIBREF5, BIBREF6 for initialization, where the dimension of $d_h$ equals to 150. The filters number of CNN $d_c$ equals to 230 and the kernel size $m$ in CNN equals to 3. In output layer, we employ dropout BIBREF22 for regularization, where the drop probability is set to $0.5$. To minimize the loss function defined in Eq.DISPLAY_FORM18, we use stochastic gradient descent with initial learning rate of $0.1$, and decay the learning rate to one tenth every 100K steps. Experiments ::: Baselines and Competitive Approaches. We compare our", "w/o Ent denotes removing entity-aware embedding, SeG w/o Gate denotes removing selective gate and concatenating two representations from PCNN and self-attention, SeG w/o Gate w/o Self-Attn denotes removing self-attention enhanced selective gate. In addition, we also replace the some parts of the proposed framework with baseline module for an in-depth comparison. SeG+ATT denotes replacing mean-pooing with selective attention, and SeG w/ stack denotes using stacked PCNN and self-attention rather than in parallel. The P@N results are listed in the bottom panel of Table TABREF19, and corresponding AUC results are shown in Table TABREF36 and Figure FIGREF37. According to the results, we find that our proposed modules perform substantially better than those of the baseline in terms of both metrics. Particularly, by removing entity-aware embedding (i.e, SeG w/o Ent) and self-attention enhanced selective gate (i.e., SeG w/o Gate w/o Self-Attn), it shows 11.5% and 1.8% decreases respectively", "the performance of our model, we adopt New York Times (NYT) dataset BIBREF2, a widely-used standard benchmark for distantly supervised relation extraction in most of previous works BIBREF5, BIBREF3, BIBREF6, BIBREF4, which contains 53 distinct relations including a null class NA relation. This dataset generates by aligning Freebase with the New York Times (NYT) corpus automatically. In particular, NYT dataset contains 53 distinct relations including a null class NA relation referred to as the relation of an entity pair is unavailable. There are 570K and 172K sentences respectively in training and test set. Experiments ::: Metrics. Following previous works BIBREF3, BIBREF5, BIBREF6, BIBREF4, we use precision-recall (PR) curves, area under curve (AUC) and top-N precision (P@N) as metrics in our experiments on the held-out test set from the NYT dataset. To directly show the perfomance on one sentence bag, we also calculate the accuracy of classification (Acc.) on non-NA sentences."]}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "predicted_answer": "State-of-the-art", "predicted_evidence": ["matching feature planes (FPs) with below equations: DISPLAYFORM0  where INLINEFORM0 is the element-wise multiplication. The INLINEFORM1 measure can be interpreted as an element-wise comparison of the signs of the input representations. The INLINEFORM2 measure can be interpreted as the distance between the input representations. In addition to the above measures, we also found the following feature plane can improve the performance: DISPLAYFORM0  In INLINEFORM0 , the INLINEFORM1 means one-dimensional convolution. Join mean concatenate the two representation. The intuition behind INLINEFORM2 is let the one-dimensional convolution preserves the common information between sentence pairs. Reshape Feature Planes. Recall that the multi-layer bidirectional LSTM generates sentence representation matrix INLINEFORM0 by concatenating sentence hidden matrix INLINEFORM1 and reversed sentence hidden matrix INLINEFORM2 . Then we conduct element-wise merge to form feature plane INLINEFORM3 .", "best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks. Note that for semantic relatedness task, the latest research BIBREF10 proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.863. Compared with their approach, our method didn't use dependency parsing and can be used to predict tasks contains multiple languages. We hope to point out that we implemented the method in BIBREF10 , but the results are not as good as our method. Here we use the results reported in their paper. Based on our experiments, we believe the method in BIBREF10 is very sensitive to the initializations, thus it may not achieve the good performance in different settings. However, our method is pretty stable which may benefit from the joint tasks training. Tree LSTM vs Sequence LSTM. In this experiment, we will compare tree LSTM", "representation by concatenating forward and backward representations. We found that adding CNN layer will decrease the accuracy in this scenario. Because when feeding into CNN, we have to reshape the feature planes otherwise convolution will not work. For example, we set convolution kernel width as 2, the input 2D tensor will have the shape lager than 2. To boost performance with CNN, we need more matching features. We found Multi-layer Bidirectional LSTM can incorporate more features and achieve best performance compared with single-layer Bidirectional LSTM. Related Work. Existing neural sentence models mainly fall into two groups: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In regular 1D CNNs BIBREF6 , BIBREF8 , BIBREF19 , a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning. The", "the states of possibly many child units. Additionally, instead of a single forget gate, the tree LSTM unit contains one forget gate INLINEFORM5 for each child INLINEFORM6 . This allows the tree LSTM unit to selectively incorporate information from each child. We use dependency tree child-sum tree LSTM proposed by BIBREF10 as our baseline. Given a tree, let INLINEFORM0 denote the set of children of node INLINEFORM1 . The child-sum tree LSTM transition equations are the following: DISPLAYFORM0  Table TABREF35 show the comparisons between tree and sequential based methods. We can see that, if we don't deploy CNN, simple Tree LSTM yields better result than traditional LSTM, but worse than Bidirectional LSTM. This is reasonable due to the fact that Bidirectional LSTM can enhance sentence representation by concatenating forward and backward representations. We found that adding CNN layer will decrease the accuracy in this scenario. Because when feeding into CNN, we have to reshape the", "good performance in different settings. However, our method is pretty stable which may benefit from the joint tasks training. Tree LSTM vs Sequence LSTM. In this experiment, we will compare tree LSTM with sequential LSTM. A limitation of the sequence LSTM architectures is that they only allow for strictly sequential information propagation. However, tree LSTMs allow richer network topologies where each LSTM unit is able to incorporate information from multiple child units. As in standard LSTM units, each Tree-LSTM unit (indexed by INLINEFORM0 ) contains input and output gates INLINEFORM1 and INLINEFORM2 , a memory cell INLINEFORM3 and hidden state INLINEFORM4 . The difference between the standard LSTM unit and tree LSTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units. Additionally, instead of a single forget gate, the tree LSTM unit contains one forget gate INLINEFORM5 for each child INLINEFORM6 . This allows the tree LSTM", "negative log-likelihood of the true class labels INLINEFORM0 : DISPLAYFORM0  where INLINEFORM0 is the number of training pairs and the superscript INLINEFORM1 indicates the INLINEFORM2 th sentence pair. Results and Discussions. Table TABREF31 and TABREF32 show the Pearson correlation and accuracy comparison results of semantic relatedness and text entailment tasks. We can see that combining CharCNN with multi-layer bidirectional LSTM yields better performance compared with other traditional machine learning methods such as SVM and MaxEnt approach BIBREF17 , BIBREF0 that served with many handcraft features. Note that our method doesn't need extra handcrafted feature extraction procedure. Also our method doesn't leverage external linguistic resources such as wordnet or parsing which get best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks.", "architecture that jointly leverage pre-trained word embedding and character embedding to learn sentence meanings. Our new approach first generates two kinds of word sequence representations as inputs into bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Our model shows that combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Experimental results on benchmark datasets demonstrate that our new framework achieved the state-of-the-art performance compared with other deep neural networks based approaches.", "on top of each feature plane. After that, we deploy another temporal convolution and tanh activation operation with kernel width as 2. In Topology ii@, however, we first stack temporal convolution and tanh activation with kernel width as 2. Then we deploy another temporal convolution and tanh activation operation with kernel width as 1. Experiment results demonstrate that the Topology i@ is slightly better than the Topology ii@. This conclusion is reasonable. The feature planes are heterogeneous. After conducting convolution and tanh activation transformation, it makes sense to compare values across different feature planes. Experiments. We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another"]}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_answer": "SICK dataset", "predicted_evidence": ["by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment. Hyperparameters and Training Details. We first initialize our word representations using publicly available 300-dimensional Glove word vectors . LSTM memory dimension is 100, the number of layers is 2. On the other hand, for CharCNN model we use threshold activation function on top of each temporal convolution and max pooling pairs . The CharCNN input frame size equals alphabet size, output frame size is 100. The maximum sentence length is 37. The kernel width of each temporal convolution is set to 3, the step is 1, the hidden units of HighwayMLP is 50. Training is done through", "on top of each feature plane. After that, we deploy another temporal convolution and tanh activation operation with kernel width as 2. In Topology ii@, however, we first stack temporal convolution and tanh activation with kernel width as 2. Then we deploy another temporal convolution and tanh activation operation with kernel width as 1. Experiment results demonstrate that the Topology i@ is slightly better than the Topology ii@. This conclusion is reasonable. The feature planes are heterogeneous. After conducting convolution and tanh activation transformation, it makes sense to compare values across different feature planes. Experiments. We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another", "and then quantize each character using one-hot encoding. Then, the sequence of characters is transformed to a sequence of such INLINEFORM1 sized vectors with fixed length INLINEFORM2 . Any character exceeding length INLINEFORM3 is ignored, and any characters that are not in the alphabet are quantized as all-zero vectors. The alphabet used in our model consists of 36 characters, including 26 english letters and 10 digits. Below, we will introduce character-level temporal convolution neural network. Temporal Convolution. Temporal Convolution applies one-dimensional convolution over an input sequence. The one-dimensional convolution is an operation between a vector of weights INLINEFORM0 and a vector of inputs viewed as a sequence INLINEFORM1 . The vector INLINEFORM2 is the filter of the convolution. Concretely, we think of INLINEFORM3 as the input token and INLINEFORM4 as a single feature value associated with the INLINEFORM5 -th character in this token. The idea behind the", "real-valued similarity score in a range of INLINEFORM0 , where INLINEFORM1 is an integer. The sequence INLINEFORM2 is the ordinal scale of similarity, where higher scores indicate greater degrees of similarity. We can predict the similarity score INLINEFORM3 by predicting the probability that the learned hidden representation INLINEFORM4 belongs to the ordinal scale. This is done by projecting an input representation onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input will located in corresponding scale. Mathematically, the similarity score INLINEFORM0 can be written as: DISPLAYFORM0  where INLINEFORM0 and the weight matrix INLINEFORM1 and INLINEFORM2 are parameters. In order to introduce the task objective function, we define a sparse target distribution INLINEFORM0 that satisfies INLINEFORM1 : DISPLAYFORM0  where INLINEFORM0 . The objective function then can be defined as the regularized", "of the convolution. Concretely, we think of INLINEFORM3 as the input token and INLINEFORM4 as a single feature value associated with the INLINEFORM5 -th character in this token. The idea behind the one-dimensional convolution is to take the dot product of the vector INLINEFORM6 with each INLINEFORM7 -gram in the token INLINEFORM8 to obtain another sequence INLINEFORM9 : DISPLAYFORM0  Usually, INLINEFORM0 is not a single value, but a INLINEFORM1 -dimensional vector so that INLINEFORM2 . There exist two types of 1d convolution operations. One is called Time Delay Neural Networks (TDNNs). The other one was introduced by BIBREF6 . In TDNN, weights INLINEFORM3 form a matrix. Each row of INLINEFORM4 is convolved with the corresponding row of INLINEFORM5 . In BIBREF6 architecture, a sequence of length INLINEFORM6 is represented as: DISPLAYFORM0  where INLINEFORM0 is the concatenation operation. In general, let INLINEFORM1 refer to the concatenation of characters INLINEFORM2 . A convolution", "best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks. Note that for semantic relatedness task, the latest research BIBREF10 proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.863. Compared with their approach, our method didn't use dependency parsing and can be used to predict tasks contains multiple languages. We hope to point out that we implemented the method in BIBREF10 , but the results are not as good as our method. Here we use the results reported in their paper. Based on our experiments, we believe the method in BIBREF10 is very sensitive to the initializations, thus it may not achieve the good performance in different settings. However, our method is pretty stable which may benefit from the joint tasks training. Tree LSTM vs Sequence LSTM. In this experiment, we will compare tree LSTM", "output frame size is 100. The maximum sentence length is 37. The kernel width of each temporal convolution is set to 3, the step is 1, the hidden units of HighwayMLP is 50. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule BIBREF16 . The learning rate is set to 0.05. The mini-batch size is 25. The model parameters were regularized with a per-minibatch L2 regularization strength of INLINEFORM0 . Note that word embeddings were fixed during training. Objective Functions. The task of semantic relatedness prediction tries to measure the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related). More formally, given a sentence pair, we wish to predict a real-valued similarity score in a range of INLINEFORM0 , where INLINEFORM1 is an integer. The sequence INLINEFORM2 is the ordinal scale of similarity, where higher scores indicate greater degrees of", "representation by concatenating forward and backward representations. We found that adding CNN layer will decrease the accuracy in this scenario. Because when feeding into CNN, we have to reshape the feature planes otherwise convolution will not work. For example, we set convolution kernel width as 2, the input 2D tensor will have the shape lager than 2. To boost performance with CNN, we need more matching features. We found Multi-layer Bidirectional LSTM can incorporate more features and achieve best performance compared with single-layer Bidirectional LSTM. Related Work. Existing neural sentence models mainly fall into two groups: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In regular 1D CNNs BIBREF6 , BIBREF8 , BIBREF19 , a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector, usually taking the maximum value in each dimension, for supervised learning. The"]}
{"question_id": "ad8411edf11d3429c9bdd08b3e07ee671464d73c", "predicted_answer": "Unanswerable", "predicted_evidence": ["matching feature planes (FPs) with below equations: DISPLAYFORM0  where INLINEFORM0 is the element-wise multiplication. The INLINEFORM1 measure can be interpreted as an element-wise comparison of the signs of the input representations. The INLINEFORM2 measure can be interpreted as the distance between the input representations. In addition to the above measures, we also found the following feature plane can improve the performance: DISPLAYFORM0  In INLINEFORM0 , the INLINEFORM1 means one-dimensional convolution. Join mean concatenate the two representation. The intuition behind INLINEFORM2 is let the one-dimensional convolution preserves the common information between sentence pairs. Reshape Feature Planes. Recall that the multi-layer bidirectional LSTM generates sentence representation matrix INLINEFORM0 by concatenating sentence hidden matrix INLINEFORM1 and reversed sentence hidden matrix INLINEFORM2 . Then we conduct element-wise merge to form feature plane INLINEFORM3 .", "real-valued similarity score in a range of INLINEFORM0 , where INLINEFORM1 is an integer. The sequence INLINEFORM2 is the ordinal scale of similarity, where higher scores indicate greater degrees of similarity. We can predict the similarity score INLINEFORM3 by predicting the probability that the learned hidden representation INLINEFORM4 belongs to the ordinal scale. This is done by projecting an input representation onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input will located in corresponding scale. Mathematically, the similarity score INLINEFORM0 can be written as: DISPLAYFORM0  where INLINEFORM0 and the weight matrix INLINEFORM1 and INLINEFORM2 are parameters. In order to introduce the task objective function, we define a sparse target distribution INLINEFORM0 that satisfies INLINEFORM1 : DISPLAYFORM0  where INLINEFORM0 . The objective function then can be defined as the regularized", "architecture that jointly leverage pre-trained word embedding and character embedding to learn sentence meanings. Our new approach first generates two kinds of word sequence representations as inputs into bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Our model shows that combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Experimental results on benchmark datasets demonstrate that our new framework achieved the state-of-the-art performance compared with other deep neural networks based approaches.", "and semantic role labeling BIBREF6 , BIBREF7 , to high level tasks such as machine translation, information retrieval and semantic analysis BIBREF8 , BIBREF9 , BIBREF10 . Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. On the other hand, some researchers have found character-level convolutional networks BIBREF11 , BIBREF12 are useful in extracting information from raw signals for the task such as language modeling or text classification. In this work, we focus on deep neural network based sentence relation modeling tasks. We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) BIBREF6 , Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) BIBREF13 to learn sentence representations. We", "best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks. Note that for semantic relatedness task, the latest research BIBREF10 proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.863. Compared with their approach, our method didn't use dependency parsing and can be used to predict tasks contains multiple languages. We hope to point out that we implemented the method in BIBREF10 , but the results are not as good as our method. Here we use the results reported in their paper. Based on our experiments, we believe the method in BIBREF10 is very sensitive to the initializations, thus it may not achieve the good performance in different settings. However, our method is pretty stable which may benefit from the joint tasks training. Tree LSTM vs Sequence LSTM. In this experiment, we will compare tree LSTM", "output frame size is 100. The maximum sentence length is 37. The kernel width of each temporal convolution is set to 3, the step is 1, the hidden units of HighwayMLP is 50. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule BIBREF16 . The learning rate is set to 0.05. The mini-batch size is 25. The model parameters were regularized with a per-minibatch L2 regularization strength of INLINEFORM0 . Note that word embeddings were fixed during training. Objective Functions. The task of semantic relatedness prediction tries to measure the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related). More formally, given a sentence pair, we wish to predict a real-valued similarity score in a range of INLINEFORM0 , where INLINEFORM1 is an integer. The sequence INLINEFORM2 is the ordinal scale of similarity, where higher scores indicate greater degrees of", "negative log-likelihood of the true class labels INLINEFORM0 : DISPLAYFORM0  where INLINEFORM0 is the number of training pairs and the superscript INLINEFORM1 indicates the INLINEFORM2 th sentence pair. Results and Discussions. Table TABREF31 and TABREF32 show the Pearson correlation and accuracy comparison results of semantic relatedness and text entailment tasks. We can see that combining CharCNN with multi-layer bidirectional LSTM yields better performance compared with other traditional machine learning methods such as SVM and MaxEnt approach BIBREF17 , BIBREF0 that served with many handcraft features. Note that our method doesn't need extra handcrafted feature extraction procedure. Also our method doesn't leverage external linguistic resources such as wordnet or parsing which get best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks.", "by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment. Hyperparameters and Training Details. We first initialize our word representations using publicly available 300-dimensional Glove word vectors . LSTM memory dimension is 100, the number of layers is 2. On the other hand, for CharCNN model we use threshold activation function on top of each temporal convolution and max pooling pairs . The CharCNN input frame size equals alphabet size, output frame size is 100. The maximum sentence length is 37. The kernel width of each temporal convolution is set to 3, the step is 1, the hidden units of HighwayMLP is 50. Training is done through"]}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "predicted_answer": "NIST 2019 speaker recognition evaluation (SRE) CTS challenge.", "predicted_evidence": ["Introduction. This paper describes the systems developed by the department of electronic engineering, institute of microelectronics of Tsinghua university and TsingMicro Co. Ltd. (THUEE) for the NIST 2019 speaker recognition evaluation (SRE) CTS challenge BIBREF0. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet, multitask and c-vector are developed in this evaluation. All the subsystems consists of a deep neural network followed by dimension deduction, score normalization and calibration. For each system, we begin with a summary of the data usage, followed by a description of the system setup along with their hyperparameters. Finally, we report experimental results obtained by each subsystem and fusion system on the SRE18 development and SRE18 evaluation datasets. Data Usage. For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4. Systems ::: Etdnn/ams. Etdnn/ams system is an", "For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4. Systems ::: Etdnn/ams. Etdnn/ams system is an extended version of tdnn with the additive margin softmax loss BIBREF1. Etdnn is used in speaker verification in BIBREF2. Compared with the traditional tdnn in BIBREF3, it has wider context and interleaving dense layers between each two tdnn layers. The architecture of our etdnn network is shown in table TABREF6. It is the same as the etdnn architecture in BIBREF2, except that the context of layer 5 of our system is t-3:t+3 instead of t-3, t, t+3. The x-vector is extracted from layer 12 prior to the ReLU non-linearity. For the loss, we use additive margin softmax with $m=0.15$ instead of traditional softmax loss or angular softmax loss. Additive margin softmax is proposed in BIBREF4 and then used in speaker verification in our paper BIBREF1. It is easier to train and generally performs better", "for c-vector, 0.132RT for eftdnn, 0.0639RT for ftdnn, and 0.112RT for ResNet. Single trial cost around 1.2ms for etdnn_ams, 0.9ms for multitask, 0.9ms for c-vector, 0.059s for eftdnn, 0.0288s for ftdnn, 1.0ms for ResNet. The memory cost about 1G for an embedding extraction and a single trial. In the inference, we just use CPU. Fusion. Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019.", "t - 2; t - 1; t; t + 1; t + 2 }, { t - 1; t; t + 1 }, { t - 1; t; t + 1 }, { t - 3; t; t + 3}, { t - 6; t - 3; t}. The 5-th layer is the BN layer containing 128 nodes and other layers have 650 nodes. A GMM-HMM is also trained as like in section SECREF12 to do phonetic alignment for training datasets. feature and back-end. 23-dimensional MFCC (20-3700Hz) is extracted as feature for etdnn/ams, ftdnn/as, eftdnn/ams, multitask and c-vector subsystems. 23-dimensional Fbank is used as feature for ResNet 16kHz subsystems. A simple energy-based VAD is used based on the C0 component of the MFCC feature BIBREF8. For each neural network, its training data are augmented using the public accessible MUSAN and RIRS_NOISES as the noise source. Two-fold data augmentation is applied for etdnn/ams, ftdnn/as, resnet, multitask and cvector subsystems. For eftdnn/ams subsystem, five-fold data augmentation is applied. After the embeddings are extracted, they are then transformed to 150 dimension using LDA.", "softmax loss or angular softmax loss. Additive margin softmax is proposed in BIBREF4 and then used in speaker verification in our paper BIBREF1. It is easier to train and generally performs better than angular softmax. Systems ::: ftdnn/as. Factorized TDNN (ftdnn) architecture is listed in table TABREF8. It is the same to BIBREF2 except that we use 1024 nodes instead of 512 nodes in layer 12 and 13. The x-vector is extracted from layer 12 prior to the ReLU non-linearity. So our x-vector is 1024 dimensional. More details about the architecture can be found in BIBREF2. Systems ::: eftdnn/ams. Extended ftdnn (eftdnn) is a combination of etdnn and ftdnn. Its architecture is listed in table TABREF10. The x-vector is extracted from layer 22 prior to the ReLU non-linearity. Systems ::: resnet. ResNet architecture is also based on tdnn x-vector BIBREF3. The five frame level tdnn layers in BIBREF3 are replaced by ResNet34 (512 nodes) + DNN(512 nodes) + DNN(1000 nodes). Further details about", "of 20-dimensional MFCCs with delta and delta-delta, totally 60-dimensional. The total number of senones is 3800. After training, forced alignment is applied to the SRE, Switchboard, and Voxceleb datasets using a fMLLR-SAT system. Systems ::: c-vector. C-vector architecture is also one of our proposed systems in paper BIBREF7. As shown in Fig. FIGREF15, it is an extension of multitask architecture. It combines multitask architecture with an extra ASR Acoustic Model. The output of ASR Acoustic Model is concatenated with x-vector's frame-level output as the input of statistics pooling. Refer to BIBREF7 for more details. The multitask part of c-vector has the same architecture as in the above section SECREF12 ASR Acoustic Model of c-vector is a 5-layer TDNN network. The slicing parameter is { t - 2; t - 1; t; t + 1; t + 2 }, { t - 1; t; t + 1 }, { t - 1; t; t + 1 }, { t - 3; t; t + 3}, { t - 6; t - 3; t}. The 5-th layer is the BN layer containing 128 nodes and other layers have 650 nodes.", "ResNet architecture is also based on tdnn x-vector BIBREF3. The five frame level tdnn layers in BIBREF3 are replaced by ResNet34 (512 nodes) + DNN(512 nodes) + DNN(1000 nodes). Further details about ResNet34 can be found in BIBREF5. In our realization, acoustic features are regarded as a single channel picture and feed into the ResNet34. If the dimensions in the residual network don't match, zeros are added. The statistic pooling and segment level network stay the same. For the loss function, we use angular softmax with $m=4$. The x-vector is extracted from first DNN layer in segment level prior to the ReLU non-linearity. It has 512 dimensions. Systems ::: multitask. Multitask architecture is proposed in BIBREF6. It is a hybrid multi-task learning based on x-vector network and ASR network. It aims to introduce phonetic information by another neural acoustic model in ASR to help speaker recognition task. The architecture is shown in Fig. FIGREF13. The frame-level part of the x-vector", "network is a 7-layer TDNN. The input of each layer is the sliced output of the previous layer. The slicing parameter is: {t - 2; t - 1; t; t + 1; t + 2}, {t - 2; t; t + 2}, {t - 3; t; t + 3}, {t}, {t}, {t}, {t}. It has 512 nodes in layer 1 to 7. Only the first TDNN layer of the x-vector network is shared with the ASR network. The phonetic classification is done at the frame level, while the speaker labels are classified at the segment level. To train the multitask network, we need training data with speaker and ASR transcribed. But only Phonetic dataset fits this condition and the data amount is too small to train a neural network. So, we need to train a GMM-HMM speech recognition system to do phonetic alignment for other datasets. The GMM-HMM is trained using Phonetic dataset with features of 20-dimensional MFCCs with delta and delta-delta, totally 60-dimensional. The total number of senones is 3800. After training, forced alignment is applied to the SRE, Switchboard, and Voxceleb"]}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "predicted_answer": "Unanswerable", "predicted_evidence": ["resnet, multitask and cvector subsystems. For eftdnn/ams subsystem, five-fold data augmentation is applied. After the embeddings are extracted, they are then transformed to 150 dimension using LDA. Then, embeddings are projected into unit sphere. At last, adapted PLDA with no dimension reduction is applied. The execution time is test on Intel Xeon E5-2680 v4. Extracting x-vector cost about 0.087RT. Single trial cost around 0.09RT. The memory cost about 1G for a x-vector extraction and a single trial. In the inference, only CPU is used. The speed test was performed on Intel Xeon E5-2680 v4 for etdnn_ams, multitask, c-vector and ResNet system. Test on Intel Xeon Platinum 8168 for ftdnn and eftdnn system. Extracting embedding cost about 0.103RT for etdnn_ams, 0.089RT for multitask, 0.092RT for c-vector, 0.132RT for eftdnn, 0.0639RT for ftdnn, and 0.112RT for ResNet. Single trial cost around 1.2ms for etdnn_ams, 0.9ms for multitask, 0.9ms for c-vector, 0.059s for eftdnn, 0.0288s for", "For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4. Systems ::: Etdnn/ams. Etdnn/ams system is an extended version of tdnn with the additive margin softmax loss BIBREF1. Etdnn is used in speaker verification in BIBREF2. Compared with the traditional tdnn in BIBREF3, it has wider context and interleaving dense layers between each two tdnn layers. The architecture of our etdnn network is shown in table TABREF6. It is the same as the etdnn architecture in BIBREF2, except that the context of layer 5 of our system is t-3:t+3 instead of t-3, t, t+3. The x-vector is extracted from layer 12 prior to the ReLU non-linearity. For the loss, we use additive margin softmax with $m=0.15$ instead of traditional softmax loss or angular softmax loss. Additive margin softmax is proposed in BIBREF4 and then used in speaker verification in our paper BIBREF1. It is easier to train and generally performs better", "Introduction. This paper describes the systems developed by the department of electronic engineering, institute of microelectronics of Tsinghua university and TsingMicro Co. Ltd. (THUEE) for the NIST 2019 speaker recognition evaluation (SRE) CTS challenge BIBREF0. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet, multitask and c-vector are developed in this evaluation. All the subsystems consists of a deep neural network followed by dimension deduction, score normalization and calibration. For each system, we begin with a summary of the data usage, followed by a description of the system setup along with their hyperparameters. Finally, we report experimental results obtained by each subsystem and fusion system on the SRE18 development and SRE18 evaluation datasets. Data Usage. For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4. Systems ::: Etdnn/ams. Etdnn/ams system is an", "for c-vector, 0.132RT for eftdnn, 0.0639RT for ftdnn, and 0.112RT for ResNet. Single trial cost around 1.2ms for etdnn_ams, 0.9ms for multitask, 0.9ms for c-vector, 0.059s for eftdnn, 0.0288s for ftdnn, 1.0ms for ResNet. The memory cost about 1G for an embedding extraction and a single trial. In the inference, we just use CPU. Fusion. Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019.", "of 20-dimensional MFCCs with delta and delta-delta, totally 60-dimensional. The total number of senones is 3800. After training, forced alignment is applied to the SRE, Switchboard, and Voxceleb datasets using a fMLLR-SAT system. Systems ::: c-vector. C-vector architecture is also one of our proposed systems in paper BIBREF7. As shown in Fig. FIGREF15, it is an extension of multitask architecture. It combines multitask architecture with an extra ASR Acoustic Model. The output of ASR Acoustic Model is concatenated with x-vector's frame-level output as the input of statistics pooling. Refer to BIBREF7 for more details. The multitask part of c-vector has the same architecture as in the above section SECREF12 ASR Acoustic Model of c-vector is a 5-layer TDNN network. The slicing parameter is { t - 2; t - 1; t; t + 1; t + 2 }, { t - 1; t; t + 1 }, { t - 1; t; t + 1 }, { t - 3; t; t + 3}, { t - 6; t - 3; t}. The 5-th layer is the BN layer containing 128 nodes and other layers have 650 nodes.", "softmax loss or angular softmax loss. Additive margin softmax is proposed in BIBREF4 and then used in speaker verification in our paper BIBREF1. It is easier to train and generally performs better than angular softmax. Systems ::: ftdnn/as. Factorized TDNN (ftdnn) architecture is listed in table TABREF8. It is the same to BIBREF2 except that we use 1024 nodes instead of 512 nodes in layer 12 and 13. The x-vector is extracted from layer 12 prior to the ReLU non-linearity. So our x-vector is 1024 dimensional. More details about the architecture can be found in BIBREF2. Systems ::: eftdnn/ams. Extended ftdnn (eftdnn) is a combination of etdnn and ftdnn. Its architecture is listed in table TABREF10. The x-vector is extracted from layer 22 prior to the ReLU non-linearity. Systems ::: resnet. ResNet architecture is also based on tdnn x-vector BIBREF3. The five frame level tdnn layers in BIBREF3 are replaced by ResNet34 (512 nodes) + DNN(512 nodes) + DNN(1000 nodes). Further details about", "t - 2; t - 1; t; t + 1; t + 2 }, { t - 1; t; t + 1 }, { t - 1; t; t + 1 }, { t - 3; t; t + 3}, { t - 6; t - 3; t}. The 5-th layer is the BN layer containing 128 nodes and other layers have 650 nodes. A GMM-HMM is also trained as like in section SECREF12 to do phonetic alignment for training datasets. feature and back-end. 23-dimensional MFCC (20-3700Hz) is extracted as feature for etdnn/ams, ftdnn/as, eftdnn/ams, multitask and c-vector subsystems. 23-dimensional Fbank is used as feature for ResNet 16kHz subsystems. A simple energy-based VAD is used based on the C0 component of the MFCC feature BIBREF8. For each neural network, its training data are augmented using the public accessible MUSAN and RIRS_NOISES as the noise source. Two-fold data augmentation is applied for etdnn/ams, ftdnn/as, resnet, multitask and cvector subsystems. For eftdnn/ams subsystem, five-fold data augmentation is applied. After the embeddings are extracted, they are then transformed to 150 dimension using LDA.", "It aims to introduce phonetic information by another neural acoustic model in ASR to help speaker recognition task. The architecture is shown in Fig. FIGREF13. The frame-level part of the x-vector network is a 10-layer TDNN. The input of each layer is the sliced output of the previous layer. The slicing parameter is: {t - 2; t - 1; t; t + 1; t + 2}, { t }, { t - 2; t; t + 2 }, {t}, { t - 3; t; t + 3 }, {t }, {t - 4; t; t + 4 }, { t }, { t } , { t }. It has 512 nodes in layer 1 to 9, and the 10-th layer has 1500 nodes. The segment-level part of x-vector network is a 2-layer fully-connected network with 512 nodes per layer. The output is predicted by softmax and the size is the same as the number of speakers. The ASR network has no statistics pooling component. The frame-level part of the x-vector network is a 7-layer TDNN. The input of each layer is the sliced output of the previous layer. The slicing parameter is: {t - 2; t - 1; t; t + 1; t + 2}, {t - 2; t; t + 2}, {t - 3; t; t + 3},"]}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "predicted_answer": "Unanswerable", "predicted_evidence": ["Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words. The results are shown in Table 3. It can be seen that inclusion of EuroVoc synsets improves the topic coherence and increases kernel uniqueness (in contrast to results with WordNet). Adding ngrams further improves the topic coherence and kernel uniqueness. At last we experimented with the Russian banking collection and utilized RuThes thesaurus. In this case we obtained improvement already on RuThes synsets and again adding ngrams further improved topic coherence and kernel uniqueness (Table 4). It is worth noting that adding ngrams sometimes worsens the TC-NPMI measure, especially on the JRC collection. This is due to the fact that in these evaluation frameworks, the topics' top elements contain a lot of multiword expressions, which rarely occur in Wikipedia, used for the coherence calculation, therefore", "Models. For evaluating topics with automatic quality measures, we used several English text collections and one Russian collection (Table TABREF7 ). We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 . At the preprocessing step, documents were processed by morphological analyzers. Also, we extracted noun groups as described in BIBREF16 . As baselines, we use the unigram LDA topic model and LDA topic model with added 1000 ngrams with maximal NC-value BIBREF20 extracted from the collection under analysis. As it was found before BIBREF14 , BIBREF16 , the addition of ngrams without accounting relations between their components considerably worsens the perplexity because of the vocabulary growth (for perplexity the less is the better) and practically does not change other automatic quality measures (Table 2). We add the Wordnet data in", "be calculated on an external corpus BIBREF17 . In our case, we use Wikipedia dumps. DISPLAYFORM0  Human-constructed topics usually have unique main words. The measure of kernel uniqueness shows to what extent topics are different from each other and is calculated as the number of unique elements among most probable elements of topics (kernels) in relation to the whole number of elements in kernels. DISPLAYFORM0  If uniqueness of the topic kernels is closer to zero then many topics are similar to each other, contain the same words in their kernels. In this paper the kernel of a topic means the ten most probable words in the topic. We also calculated perplexity as the measure of language models. We use it for additional checking the model quality. Use of Automatic Measures to Assess Combined Models. For evaluating topics with automatic quality measures, we used several English text collections and one Russian collection (Table TABREF7 ). We experiment with three thesauri: WordNet (155", "manual topic labels include a considerable number of phrases; users prefer shorter labels with more general words and tend to incorporate phrases and more generic terminology when using more complex network graph. Blei and Lafferty BIBREF3 visualize topics with ngrams consisting of words mentioned in these topics. These works show that phrases and knowledge about hyponyms/hypernyms are important for topic representation. In this paper we describe an approach to integrate large manual lexical resources such as WordNet or EuroVoc into probabilistic topic models, as well as automatically extracted n-grams to improve coherence and informativeness of generated topics. The structure of the paper is as follows. In Section 2 we consider related works. Section 3 describes the proposed approach. Section 4 enumerates automatic quality measures used in experiments. Section 5 presents the results obtained on several text collections according to automatic measures. Section 6 describes the results", "accounts both component relations and hypernym thesaurus relations. All topics are of high quality, quite understandable. The experts evaluated them with the same high scores. Phrase-enriched and thesaurus-enriched topics convey the content using both single words and phrases. It can be seen that phrase-enriched topics contain more phrases. Sometimes the phrases can create not very convincing relations such as Russian church - Russian language. It is explainable but does not seem much topical in this case. The thesaurus topics seem to convey the contents in the most concentrated way. In the Syrian topic general word country is absent; instead of UN (United Nations), it contains word rebel, which is closer to the Syrian situation. In the Orthodox church topic, the unigram variant contains extra word year, relations of words Moscow and Kirill to other words in the topic can be inferred only from the encyclopedic knowledge. Conclusion. In this paper we presented the approach for", "However, all such methods incorporate knowledge in a hard and topic-independent way, which is a simplification since two words that are similar in one topic are not necessarily of equal importance for another topic. Xie et al. BIBREF7 proposed a Markov Random Field regularized LDA model (MRF-LDA), which utilizes the external knowledge to improve the coherence of topic modeling. Within a document, if two words are labeled as similar according to the external knowledge, their latent topic nodes are connected by an undirected edge and a binary potential function is defined to encourage them to share the same topic label. Distributional similarity of words is calculated beforehand on a large text corpus. In BIBREF8 , the authors gather so-called lexical relation sets (LR-sets) for word senses described in WordNet. The LR-sets include synonyms, antonyms and adjective-attribute related words. To adapt LR-sets to a specific domain corpus and to remove inappropriate lexical relations, the", "having the same components as described in BIBREF16 . All measures significantly improve for all collections (Table 2:LDA-Sim+WNsr/hyp+Ngrams). At the last step, we try to apply the same approach to ngrams that was previously utilized to hyponym-hypernym relations: frequencies of shorter ngrams and words are summed to frequencies of longer ngrams but not vice versa. In this case we try to increase the contribution of more specific longer ngrams into topics. It can be seen (Table 2) that the kernel uniqueness grows significantly, at this step it is 1.3-1.6 times greater than for the baseline models achieving 0.76 on the ACL collection (Table 2:LDA-Sim+WNsr/hyp+Ngrams/l). At the second series of the experiments, we applied EuroVoc information retrieval thesaurus to two European Union collections: Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words. The results", "were used in texts to discuss specific events and objects, therefore, we change the constructions of the similarity sets in the following way: we do not add word hyponyms to its similarity set. Thus, hyponyms, which are usually more specific and concrete, should obtain additional frequencies from upper synsets and increase their contributions into the document topics. But the frequencies and contribution of hypernyms into the topic of the document are not changed. And we see the great improvement of the model quality: the kernel uniqueness considerably improves, perplexity decreases to levels comparable with the unigram model, topic coherence characteristics also improve for most collections (Table 2:LDA-Sim+WNsynrel/hyp). We further use the WordNet-based similarity sets with n-grams having the same components as described in BIBREF16 . All measures significantly improve for all collections (Table 2:LDA-Sim+WNsr/hyp+Ngrams). At the last step, we try to apply the same approach to"]}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "predicted_answer": "Economic, political, and Islamic.", "predicted_evidence": ["makes it much better than the initial model in several measures. Adding ngrams in a similar manner further improves the model. Introducing information from domain-specific thesaurus EuroVoc led to improving the initial model without the additional assumption, which can be explained by the absence of general abstract words in such information-retrieval thesauri. We also considered thematic analysis of an Islam Internet site and evaluated the combined topic models manually. We found that the best, understandable topics are obtained by adding domain-specific thesaurus knowledge (domain terms, synonyms, and relations).", "Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words. The results are shown in Table 3. It can be seen that inclusion of EuroVoc synsets improves the topic coherence and increases kernel uniqueness (in contrast to results with WordNet). Adding ngrams further improves the topic coherence and kernel uniqueness. At last we experimented with the Russian banking collection and utilized RuThes thesaurus. In this case we obtained improvement already on RuThes synsets and again adding ngrams further improved topic coherence and kernel uniqueness (Table 4). It is worth noting that adding ngrams sometimes worsens the TC-NPMI measure, especially on the JRC collection. This is due to the fact that in these evaluation frameworks, the topics' top elements contain a lot of multiword expressions, which rarely occur in Wikipedia, used for the coherence calculation, therefore", "is due to the fact that in these evaluation frameworks, the topics' top elements contain a lot of multiword expressions, which rarely occur in Wikipedia, used for the coherence calculation, therefore the utilized automatic coherence measures can have insufficient evidence for correct estimates. Manual Evaluation of Combined Topic Models. To estimate the quality of topic models in a real task, we chose Islam informational portal \"Golos Islama\" (Islam Voice) (in Russian). This portal contains both news articles related to Islam and articles discussing Islam basics. We supposed that the thematic analysis of this specialized site can be significantly improved with domain-specific knowledge described in the thesaurus form. We extracted the site contents using Open Web Spider and obtained 26,839 pages. To combine knowledge with a topic model, we used RuThes thesaurus together with the additional block of the Islam thesaurus. The Islam thesaurus contains more than 5 thousand Islam-related", "described in WordNet. The LR-sets include synonyms, antonyms and adjective-attribute related words. To adapt LR-sets to a specific domain corpus and to remove inappropriate lexical relations, the correlation matrix for word pairs in each LR-set is calculated. This matrix at the first step is used for filtrating inappropriate senses, then it is used to modify the initial LDA topic model according to the generalized Polya urn model described in BIBREF9 . The generalized Polya urn model boosts probabilities of related words in word-topic distributions. Gao and Wen BIBREF10 presented Semantic Similarity-Enhanced Topic Model that accounts for corpus-specific word co-occurrence and word semantic similarity calculated on WordNet paths between corresponding synsets using the generalized Polya urn model. They apply their topic model for categorizing short texts. All above-mentioned approaches on adding knowledge to topic models are limited to single words. Approaches using ngrams in topic", "accounts both component relations and hypernym thesaurus relations. All topics are of high quality, quite understandable. The experts evaluated them with the same high scores. Phrase-enriched and thesaurus-enriched topics convey the content using both single words and phrases. It can be seen that phrase-enriched topics contain more phrases. Sometimes the phrases can create not very convincing relations such as Russian church - Russian language. It is explainable but does not seem much topical in this case. The thesaurus topics seem to convey the contents in the most concentrated way. In the Syrian topic general word country is absent; instead of UN (United Nations), it contains word rebel, which is closer to the Syrian situation. In the Orthodox church topic, the unigram variant contains extra word year, relations of words Moscow and Kirill to other words in the topic can be inferred only from the encyclopedic knowledge. Conclusion. In this paper we presented the approach for", "urn model. They apply their topic model for categorizing short texts. All above-mentioned approaches on adding knowledge to topic models are limited to single words. Approaches using ngrams in topic models can be subdivided into two groups. The first group of methods tries to create a unified probabilistic model accounting unigrams and phrases. Bigram-based approaches include the Bigram Topic Model BIBREF11 and LDA Collocation Model BIBREF12 . In BIBREF13 the Topical N-Gram Model was proposed to allow the generation of ngrams based on the context. However, all these models are enough complex and hard to compute on real datasets. The second group of methods is based on preliminary extraction of ngrams and their further use in topics generation. Initial studies of this approach used only bigrams BIBREF14 , BIBREF15 . Nokel and Loukachevitch BIBREF16 proposed the LDA-SIM algorithm, which integrates top-ranked ngrams and terms of information-retrieval thesauri into topic models (thesaurus", "contribution into all topics of the current document. The Table 2 shows that these two steps lead to great degradation of the topic model in most measures in comparison to the initial unigram model: uniqueness of kernels abruptly decreases, perplexity at the second step grows by several times (Table 2: LDA-Sim+WNsynrel). It is evident that at this step the model has a poor quality. When we look at the topics, the cause of the problem seems to be clear. We can see the overgeneralization of the obtained topics. The topics are built around very general words such as \"person\", \"organization\", \"year\", etc. These words were initially frequent in the collection and then received additional frequencies from their frequent synonyms and related words. Then we suppose that these general words were used in texts to discuss specific events and objects, therefore, we change the constructions of the similarity sets in the following way: we do not add word hyponyms to its similarity set. Thus,", "However, all such methods incorporate knowledge in a hard and topic-independent way, which is a simplification since two words that are similar in one topic are not necessarily of equal importance for another topic. Xie et al. BIBREF7 proposed a Markov Random Field regularized LDA model (MRF-LDA), which utilizes the external knowledge to improve the coherence of topic modeling. Within a document, if two words are labeled as similar according to the external knowledge, their latent topic nodes are connected by an undirected edge and a binary potential function is defined to encourage them to share the same topic label. Distributional similarity of words is calculated beforehand on a large text corpus. In BIBREF8 , the authors gather so-called lexical relation sets (LR-sets) for word senses described in WordNet. The LR-sets include synonyms, antonyms and adjective-attribute related words. To adapt LR-sets to a specific domain corpus and to remove inappropriate lexical relations, the"]}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "predicted_answer": "WordNet and EuroVoc", "predicted_evidence": ["extra word year, relations of words Moscow and Kirill to other words in the topic can be inferred only from the encyclopedic knowledge. Conclusion. In this paper we presented the approach for introducing thesaurus information into topic models. The main idea of the approach is based on the assumption that if related words or phrases co-occur in the same text, their frequencies should be enhanced and this action leads to their mutual larger contribution into topics found in this text. In the experiments on four English collections, it was shown that the direct implementation of this idea using WordNet synonyms and/or direct relations leads to great degradation of the unigram model. But the correction of initial assumptions and excluding hyponyms from frequencies adding improve the model and makes it much better than the initial model in several measures. Adding ngrams in a similar manner further improves the model. Introducing information from domain-specific thesaurus EuroVoc led to", "Introduction. Currently, probabilistic topic models are important tools for improving automatic text processing including information retrieval, text categorization, summarization, etc. Besides, they can be useful in supporting expert analysis of document collections, news flows, or large volumes of messages in social networks BIBREF0 , BIBREF1 , BIBREF2 . To facilitate this analysis, such approaches as automatic topic labeling and various visualization techniques have been proposed BIBREF1 , BIBREF3 . Boyd-Graber et al. BIBREF4 indicate that to be understandable by humans, topics should be specific, coherent, and informative. Relationships between the topic components can be inferred. In BIBREF1 four topic visualization approaches are compared. The authors of the experiment concluded that manual topic labels include a considerable number of phrases; users prefer shorter labels with more general words and tend to incorporate phrases and more generic terminology when using more complex", "manual topic labels include a considerable number of phrases; users prefer shorter labels with more general words and tend to incorporate phrases and more generic terminology when using more complex network graph. Blei and Lafferty BIBREF3 visualize topics with ngrams consisting of words mentioned in these topics. These works show that phrases and knowledge about hyponyms/hypernyms are important for topic representation. In this paper we describe an approach to integrate large manual lexical resources such as WordNet or EuroVoc into probabilistic topic models, as well as automatically extracted n-grams to improve coherence and informativeness of generated topics. The structure of the paper is as follows. In Section 2 we consider related works. Section 3 describes the proposed approach. Section 4 enumerates automatic quality measures used in experiments. Section 5 presents the results obtained on several text collections according to automatic measures. Section 6 describes the results", "Introduction. Statistic topic models such as Latent Dirichlet Allocation (LDA) and its variants BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 have been proven to be effective in modeling textual documents. In these models, a word token in a document is assumed to be generated by a hidden mixture model, where the hidden variables are the topic indexes for each word and the topic assignments for words are related to document level topic weights. Due to the effectiveness and efficiency in modeling the document generation process, topic models are widely adopted in quite a lot of real world tasks such as sentiment classification BIBREF5 , social network analysis BIBREF6 , BIBREF5 , and recommendation systems BIBREF7 . Most topic models take the bag-of-words assumption, in which every document is treated as an unordered set of words and the word tokens in such a document are sampled independently with each other. The bag-of-words assumption brings computational convenience, however, it", "However, all such methods incorporate knowledge in a hard and topic-independent way, which is a simplification since two words that are similar in one topic are not necessarily of equal importance for another topic. Xie et al. BIBREF7 proposed a Markov Random Field regularized LDA model (MRF-LDA), which utilizes the external knowledge to improve the coherence of topic modeling. Within a document, if two words are labeled as similar according to the external knowledge, their latent topic nodes are connected by an undirected edge and a binary potential function is defined to encourage them to share the same topic label. Distributional similarity of words is calculated beforehand on a large text corpus. In BIBREF8 , the authors gather so-called lexical relation sets (LR-sets) for word senses described in WordNet. The LR-sets include synonyms, antonyms and adjective-attribute related words. To adapt LR-sets to a specific domain corpus and to remove inappropriate lexical relations, the", "Introduction. With the rapid growth of the internet, huge amounts of text data are generated in social networks, online shopping and news websites, etc. These data create demand for powerful and efficient text analysis techniques. Probabilistic topic models such as Latent Dirichlet Allocation (LDA) BIBREF0 are popular approaches for this task, by discovering latent topics from text collections. Many conventional topic models discover topics purely based on the word-occurrences, ignoring the meta information (a.k.a., side information) associated with the content. In contrast, when we humans read text it is natural to leverage meta information to improve our comprehension, which includes categories, authors, timestamps, the semantic meanings of the words, etc. Therefore, topic models capable of using meta information should yield improved modelling accuracy and topic quality. In practice, various kinds of meta information are available at the document level and the word level in many", "described in WordNet. The LR-sets include synonyms, antonyms and adjective-attribute related words. To adapt LR-sets to a specific domain corpus and to remove inappropriate lexical relations, the correlation matrix for word pairs in each LR-set is calculated. This matrix at the first step is used for filtrating inappropriate senses, then it is used to modify the initial LDA topic model according to the generalized Polya urn model described in BIBREF9 . The generalized Polya urn model boosts probabilities of related words in word-topic distributions. Gao and Wen BIBREF10 presented Semantic Similarity-Enhanced Topic Model that accounts for corpus-specific word co-occurrence and word semantic similarity calculated on WordNet paths between corresponding synsets using the generalized Polya urn model. They apply their topic model for categorizing short texts. All above-mentioned approaches on adding knowledge to topic models are limited to single words. Approaches using ngrams in topic", "BIBREF14 , BIBREF15 . Nokel and Loukachevitch BIBREF16 proposed the LDA-SIM algorithm, which integrates top-ranked ngrams and terms of information-retrieval thesauri into topic models (thesaurus relations were not utilized). They create similarity sets of expressions having the same word components and sum up frequencies of similarity set members if they co-occur in the same text. In this paper we describe the approach to integrate whole manual thesauri into topic models together with multiword expressions. Approach to Integration Whole Thesauri into Topic Models. In our approach we develop the idea of BIBREF16 that proposed to construct similarity sets between ngram phrases between each other and single words. Phrases and words are included in the same similarity set if they have the same component word, for example, weapon \u2013 nuclear weapon \u2013 weapon of mass destruction; discrimination \u2013 racial discrimination. It was supposed that if expressions from the same similarity set co-occur"]}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "predicted_answer": "language that is used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases this may also be language that threatens or incites violence, but limiting our definition only to such cases would exclude a large proportion of hate speech.", "predicted_evidence": ["These laws extend to the internet and social media, leading many sites to create their own provisions against hate speech. Both Facebook and Twitter have responded to criticism for not doing enough to prevent hate speech on their sites by instituting policies to prohibit the use of their platforms for attacks on people based on characteristics like race, ethnicity, gender, and sexual orientation, or threats of violence towards others. Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases this may also be language that threatens or incites violence, but limiting our definition only to such cases would exclude a large proportion of hate speech. Importantly, our definition does not include all instances of offensive language because people often use terms that are highly offensive to certain groups but in a", "Introduction. What constitutes hate speech and when does it differ from offensive language? No formal definition exists but there is a consensus that it is speech that targets disadvantaged social groups in a manner that is potentially harmful to them BIBREF0 , BIBREF1 . In the United States, hate speech is protected under the free speech provisions of the First Amendment, but it has been extensively debated in the legal sphere and with regards to speech codes on college campuses. In many countries, including the United Kingdom, Canada, and France, there are laws prohibiting hate speech, which tends to be defined as speech that targets minority groups in a way that could promote violence or social disorder. People convicted of using hate speech can often face large fines and even imprisonment. These laws extend to the internet and social media, leading many sites to create their own provisions against hate speech. Both Facebook and Twitter have responded to criticism for not doing", "hateful without necessarily using particular keywords or offensive language. Our results also illustrate how hate speech can be used in different ways: it can be directly send to a person or group of people targeted, it can be espoused to nobody in particular, and it can be used in conversation between people. Future work should distinguish between these different uses and look more closely at the social contexts and conversations in which hate speech occurs. We must also study more closely the people who use hate speech, focusing both on their individual characteristics and motivations and on the social structures they are embedded in. Hate speech is a difficult phenomenon to define and is not monolithic. Our classifications of hate speech tend to reflect our own subjective biases. People identify racist and homophobic slurs as hateful but tend to see sexist language as merely offensive. While our results show that people perform well at identifying some of the more egregious", "it contained offensive words. Given the relatively high prevalence of offensive language and curse words on social media this makes hate speech detection particularly challenging BIBREF3 . The difference between hate speech and other offensive language is often based upon subtle linguistic distinctions, for example tweets containing the word n*gger are more likely to be labeled as hate speech than n*gga BIBREF4 . Many can be ambiguous, for example the word gay can be used both pejoratively and in other contexts unrelated to hate speech BIBREF3 . Syntactic features have been leveraged to better identify the targets and intensity of hate speech, for example sentences where a relevant noun and verb occur (e.g. kill and Jews) BIBREF6 , the POS trigram DT jewish NN BIBREF2 , and the syntactic structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 . Other supervised approaches to hate speech classification have unfortunately conflated hate speech", "presence or absence of particular offensive or hateful terms can both help and hinder accurate classification. Consistent with previous work, we find that certain terms are particularly useful for distinguishing between hate speech and offensive language. While f*g, b*tch, and n*gga are used in both hate speech and offensive language, the terms f*ggot and n*gger are generally associated with hate speech. Many of the tweets considered most hateful contain multiple racial and homophobic slurs. While this allows us to easily identify some of the more egregious instances of hate speech it means that we are more likely to misclassify hate speech if it doesn't contain any curse words or offensive terms. To more accurately classify such cases we should find sources of training data that are hateful without necessarily using particular keywords or offensive language. Our results also illustrate how hate speech can be used in different ways: it can be directly send to a person or group of", "to be correctly identified as hate when they contained strongly racist or homophobic terms like n*gger and f*ggot. Interestingly, we also find cases where people use hate speech to respond to other hate speakers, such as this tweet where someone uses a homophobic slur to criticize someone else's racism: @MrMoonfrog @RacistNegro86 f*ck you, stupid ass coward b*tch f*ggot racist piece of sh*t. Turning to true hate speech classified as offensive it appears that tweets with the highest predicted probability of being offensive are genuinely less hateful and were perhaps mislabeled, for example When you realize how curiosity is a b*tch #CuriosityKilledMe may have been erroneously coded as hate speech if people thought that curiosity was a person, and Why no boycott of racist \"redskins\"? #Redskins #ChangeTheName contains a slur but is actually against racism. It is likely that coders skimmed these tweets too quickly, picking out words or phrases that appeared to be hateful without", "structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 . Other supervised approaches to hate speech classification have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which they are really identifying hate speech BIBREF5 , BIBREF8 . Neural language models show promise in the task but existing work has used training data has a similarly broad definition of hate speech BIBREF9 . Non-linguistic features like the gender or ethnicity of the author can help improve hate speech classification but this information is often unavailable or unreliable on social media BIBREF8 . Data. We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user,", "speech and offensive language then we erroneously consider many people to be hate speakers (errors in the lower triangle of Figure 1) and fail differentiate between commonplace offensive language and serious hate speech (errors in the upper triangle of Figure 1). Given the legal and moral implications of hate speech it is important that we are able to accurately distinguish between the two. Lexical methods are effective ways to identify potentially offensive terms but are inaccurate at identifying hate speech; only a small percentage of tweets flagged by the Hatebase lexicon were considered hate speech by human coders. While automated classification methods can achieve relatively high accuracy at differentiating between these different classes, close analysis of the results shows that the presence or absence of particular offensive or hateful terms can both help and hinder accurate classification. Consistent with previous work, we find that certain terms are particularly useful for"]}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "predicted_answer": "a model to differentiate between hate speech, offensive language, and neither", "predicted_evidence": ["speech, offensive language, or neither. We train a model to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. Our results show that fine-grained labels can help in the task of hate speech detection and highlights some of the key challenges to accurate classification. We conclude that future work must better account for context and the heterogeneity in hate speech usage. Related Work. Bag-of-words approaches tend to have high recall but lead to high rates of false positives since the presence of offensive words can lead to the misclassification of tweets as hate speech BIBREF4 , BIBREF5 . Focusing on anti-black racism, BIBREF4 find that 86% of the time the reason a tweet was categorized as racist was because it contained offensive words. Given the relatively high prevalence of offensive language and curse words on social media this makes hate speech detection particularly challenging BIBREF3 . The", "it contained offensive words. Given the relatively high prevalence of offensive language and curse words on social media this makes hate speech detection particularly challenging BIBREF3 . The difference between hate speech and other offensive language is often based upon subtle linguistic distinctions, for example tweets containing the word n*gger are more likely to be labeled as hate speech than n*gga BIBREF4 . Many can be ambiguous, for example the word gay can be used both pejoratively and in other contexts unrelated to hate speech BIBREF3 . Syntactic features have been leveraged to better identify the targets and intensity of hate speech, for example sentences where a relevant noun and verb occur (e.g. kill and Jews) BIBREF6 , the POS trigram DT jewish NN BIBREF2 , and the syntactic structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 . Other supervised approaches to hate speech classification have unfortunately conflated hate speech", "structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 . Other supervised approaches to hate speech classification have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which they are really identifying hate speech BIBREF5 , BIBREF8 . Neural language models show promise in the task but existing work has used training data has a similarly broad definition of hate speech BIBREF9 . Non-linguistic features like the gender or ethnicity of the author can help improve hate speech classification but this information is often unavailable or unreliable on social media BIBREF8 . Data. We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user,", "presence or absence of particular offensive or hateful terms can both help and hinder accurate classification. Consistent with previous work, we find that certain terms are particularly useful for distinguishing between hate speech and offensive language. While f*g, b*tch, and n*gga are used in both hate speech and offensive language, the terms f*ggot and n*gger are generally associated with hate speech. Many of the tweets considered most hateful contain multiple racial and homophobic slurs. While this allows us to easily identify some of the more egregious instances of hate speech it means that we are more likely to misclassify hate speech if it doesn't contain any curse words or offensive terms. To more accurately classify such cases we should find sources of training data that are hateful without necessarily using particular keywords or offensive language. Our results also illustrate how hate speech can be used in different ways: it can be directly send to a person or group of", "a large proportion of hate speech. Importantly, our definition does not include all instances of offensive language because people often use terms that are highly offensive to certain groups but in a qualitatively different manner. For example some African Americans often use the term n*gga in everyday language online BIBREF2 , people use terms like h*e and b*tch when quoting rap lyrics, and teenagers use homophobic slurs like f*g as they play video games. Such language is prevalent on social media BIBREF3 , making this boundary condition crucial for any usable hate speech detection system . Previous work on hate speech detection has identified this problem but many studies still tend to conflate hate speech and offensive language. In this paper we label tweets into three categories: hate speech, offensive language, or neither. We train a model to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. Our", "speech and offensive language then we erroneously consider many people to be hate speakers (errors in the lower triangle of Figure 1) and fail differentiate between commonplace offensive language and serious hate speech (errors in the upper triangle of Figure 1). Given the legal and moral implications of hate speech it is important that we are able to accurately distinguish between the two. Lexical methods are effective ways to identify potentially offensive terms but are inaccurate at identifying hate speech; only a small percentage of tweets flagged by the Hatebase lexicon were considered hate speech by human coders. While automated classification methods can achieve relatively high accuracy at differentiating between these different classes, close analysis of the results shows that the presence or absence of particular offensive or hateful terms can both help and hinder accurate classification. Consistent with previous work, we find that certain terms are particularly useful for", "People identify racist and homophobic slurs as hateful but tend to see sexist language as merely offensive. While our results show that people perform well at identifying some of the more egregious instances of hate speech, particularly anti-black racism and homophobia, it is important that we are cognizant of the social biases that enter into our algorithms and future work should aim to identify and correct these biases.", "for example He's a damn good actor. As a gay man it's awesome to see an openly queer actor given the lead role for a major film contains the potentially the offensive terms gay and queer but uses them in a positive sense. This problem has been encountered in previous research BIBREF2 and illustrates the importance of taking context into account. We also found a small number of cases where the coders appear to have missed hate speech that was correctly identified by our model, e.g. @mayormcgunn @SenFeinstein White people need those weapons to defend themselves from the subhuman trash your sort unleashes on us. This finding is consistent with previous work that has found amateur coders to often be unreliable at identifying abusive content BIBREF13 , BIBREF14 . Conclusions. If we conflate hate speech and offensive language then we erroneously consider many people to be hate speakers (errors in the lower triangle of Figure 1) and fail differentiate between commonplace offensive language"]}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "predicted_answer": "85.4 million tweets", "predicted_evidence": ["for hashtags, mentions, retweets, and URLs, as well as features for the number of characters, words, and syllables in each tweet. Model. We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to", "allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 . Results. The best performing model has an overall precision 0.91, recall of 0.90, and F1 score of 0.90. Looking at Figure 1, however, we see that almost 40% of hate speech is misclassified: the precision and recall scores for the hate class are 0.44 and 0.61 respectively. Most of the misclassification occurs in the upper triangle of this matrix, suggesting that the model is biased towards classifying tweets as less hateful or offensive than the human coders. Far fewer tweets are classified as more", "by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We", "structure I <intensity > <user intent > <hate target >, e.g. I f*cking hate white people BIBREF7 . Other supervised approaches to hate speech classification have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which they are really identifying hate speech BIBREF5 , BIBREF8 . Neural language models show promise in the task but existing work has used training data has a similarly broad definition of hate speech BIBREF9 . Non-linguistic features like the gender or ethnicity of the author can help improve hate speech classification but this information is often unavailable or unreliable on social media BIBREF8 . Data. We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user,", "2/3, 53% at 3/3) and the remainder were considered to be non-offensive (16.6% at 2/3, 11.8% at 3/3). We then constructed features from these tweets and used them to train a classifier. Features. We lowercased each tweet and stemmed it using the Porter stemmer, then create bigram, unigram, and trigram features, each weighted by its TF-IDF. To capture information about the syntactic structure we use NLTK BIBREF10 to construct Penn Part-of-Speech (POS) tag unigrams, bigrams, and trigrams. To capture the quality of each tweet we use modified Flesch-Kincaid Grade Level and Flesch Reading Ease scores, where the number of sentences is fixed at one. We also use a sentiment lexicon designed for social media to assign sentiment scores to each tweet BIBREF11 . We also include binary and count indicators for hashtags, mentions, retweets, and URLs, as well as features for the number of characters, words, and syllables in each tweet. Model. We first use a logistic regression with L1 regularization", "of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets. Only 5% of tweets were coded as hate speech by the majority of coders and only 1.3% were coded unanimously, demonstrating the imprecision of the Hatebase lexicon. This is much lower than a comparable study using Twitter, where 11.6% of tweets were flagged as hate speech BIBREF5 , likely because we use a stricter criteria for hate speech. The majority of the tweets were considered to be offensive language (76% at 2/3, 53% at 3/3) and the remainder were considered to be non-offensive (16.6% at 2/3, 11.8% at 3/3). We then constructed features from these tweets and used them to train a classifier. Features. We", "speech, offensive language, or neither. We train a model to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. Our results show that fine-grained labels can help in the task of hate speech detection and highlights some of the key challenges to accurate classification. We conclude that future work must better account for context and the heterogeneity in hate speech usage. Related Work. Bag-of-words approaches tend to have high recall but lead to high rates of false positives since the presence of offensive words can lead to the misclassification of tweets as hate speech BIBREF4 , BIBREF5 . Focusing on anti-black racism, BIBREF4 find that 86% of the time the reason a tweet was categorized as racist was because it contained offensive words. Given the relatively high prevalence of offensive language and curse words on social media this makes hate speech detection particularly challenging BIBREF3 . The", "are able to avoid the vast majority of these errors by differentiating between the two. Finally, turning to the neither class, we see that tweets with the highest predicted probability of belonging to this class all appear to be innocuous and were included in the sample because they contained terms included in the Hatebase lexicon such as charlie and bird that are generally not used in a hateful manner. Tweets with overall positive sentiment and higher readability scores are more likely to belong to this class. The tweets in this category that have been misclassified as hate or offensive tend to mention race, sexuality, and other social categories that are targeted by hate speakers. Most appear to be misclassifications appear to be caused by on the presence of potentially offensive language, for example He's a damn good actor. As a gay man it's awesome to see an openly queer actor given the lead role for a major film contains the potentially the offensive terms gay and queer but uses"]}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "predicted_answer": "SST-2 (a sentiment classification dataset)", "predicted_evidence": ["initial layers. Fine-tuning subsequent layers shows diminishing returns, with every model rapidly approaching the baseline quality at fine-tuning half of the network; hence, we believe that half is a reasonable cutoff point for characterizing the models. Finally, for the large variants of BERT and RoBERTa on SST-2 (second subfigure from both the top and the left), we observe a surprisingly consistent increase in quality when freezing 12\u201316 layers. This finding suggests that these models may be overparameterized for SST-2. Conclusions and Future Work. In this paper, we present a comprehensive evaluation of the number of final layers that need to be fine-tuned for pretrained transformer-based language models. We find that only a fourth of the layers necessarily need to be fine-tuned to obtain 90% of the original quality. One line of future work is to conduct a similar, more fine-grained analysis on the contributions of the attention heads. Acknowledgments. This research was supported by", "the nonoutput layers. The latter denotes the number of necessary layers for reaching at least 90% of the full model quality, excluding CoLA, which is an outlier. From the reported results in Tables TABREF6\u2013TABREF9, fine-tuning the last output layer and task-specific layers is insufficient for all tasks\u2014see the rows corresponding to 0, 12, and 24 frozen layers. However, we find that the first half of the model is unnecessary; the base models, for example, need fine-tuning of only 3\u20135 layers out of the 12 to reach 90% of the original quality\u2014see Table TABREF7, middle subrow of each row group. Similarly, fine-tuning only a fourth of the layers is sufficient for the large models (see Table TABREF9); only 6 layers out of 24 for BERT and 7 for RoBERTa. Analysis ::: Per-Layer Study. In Figure FIGREF10, we examine how the relative quality changes with the number of frozen layers. To compute a relative score, we subtract each frozen model's results from its corresponding full model. The", "a few attention heads need to be retained in each layer for acceptable effectiveness. BIBREF4 find that, on many tasks, just the last few layers change the most after the fine-tuning process. We take these observations as evidence that only the last few layers necessarily need to be fine-tuned. The central objective of our paper is, then, to determine how many of the last layers actually need fine-tuning. Why is this an important subject of study? Pragmatically, a reasonable cutoff point saves computational memory across fine-tuning multiple tasks, which bolsters the effectiveness of existing parameter-saving methods BIBREF5. Pedagogically, understanding the relationship between the number of fine-tuned layers and the resulting model quality may guide future works in modeling. Our research contribution is a comprehensive evaluation, across multiple pretrained transformers and datasets, of the number of final layers needed for fine-tuning. We show that, on most tasks, we need to", "FIGREF10, we examine how the relative quality changes with the number of frozen layers. To compute a relative score, we subtract each frozen model's results from its corresponding full model. The relative score aligns the two baselines at zero, allowing the fair comparison of the transformers. The graphs report the average of five trials to reduce the effects of outliers. When every component except the output layer and the task-specific layer is frozen, the fine-tuned model achieves only 64% of the original quality, on average. As more layers are fine-tuned, the model effectiveness often improves drastically\u2014see CoLA and STS-B, the first and fourth vertical pairs of subfigures from the left. This demonstrates that gains decompose nonadditively with respect to the number of frozen initial layers. Fine-tuning subsequent layers shows diminishing returns, with every model rapidly approaching the baseline quality at fine-tuning half of the network; hence, we believe that half is a", "contribution is a comprehensive evaluation, across multiple pretrained transformers and datasets, of the number of final layers needed for fine-tuning. We show that, on most tasks, we need to fine-tune only one fourth of the final layers to achieve within 10% parity with the full model. Surprisingly, on SST-2, a sentiment classification dataset, we find that not fine-tuning all of the layers leads to improved quality. Background and Related Work ::: Pretrained Language Models. In the pretrained language modeling paradigm, a language model (LM) is trained on vast amounts of text, then fine-tuned on a specific downstream task. BIBREF6 are one of the first to successfully apply this idea, outperforming state of the art in question answering, textual entailment, and sentiment classification. Their model, dubbed ELMo, comprises a two-layer BiLSTM pretrained on the Billion Word Corpus BIBREF7. Furthering this approach with more data and improved modeling, BIBREF0 pretrain deep 12- and", "consist of the binary-polarity Stanford Sentiment Treebank (SST-2; BIBREF22) and the Corpus of Linguistic Acceptability (CoLA; BIBREF23). Experimental Setup ::: Fine-Tuning Procedure. Our fine-tuning procedure closely resembles those of BERT and RoBERTa. We choose the Adam optimizer BIBREF24 with a batch size of 16 and fine-tune BERT for 3 epochs and RoBERTa for 10, following the original papers. For hyperparameter tuning, the best learning rate is different for each task, and all of the original authors choose one between $1 \\times 10^{-5}$ and $5 \\times 10^{-5}$; thus, we perform line search over the interval with a step size of $1 \\times 10^{-5}$. We report the best results in Table TABREF5. On each model, we freeze the embeddings and the weights of the first $N$ layers, then fine-tune the rest using the best hyperparameters of the full model. Specifically, if $L$ is the number of layers, we explore $N = \\frac{L}{2}, \\frac{L}{2} + 1, \\dots , L$. Due to computational limitations, we", "lower layers capturing content and higher layers style. Pretrained transformers. In the NLP literature, similar observations have been made for pretrained language models. BIBREF14 analyze BERT's attention and observe that the bottom layers attend broadly, while the top layers capture linguistic syntax. BIBREF4 find that the last few layers of BERT change the most after task-specific fine-tuning. Similar to our work, BIBREF5 fine-tune the top layers of BERT, as part of their baseline comparison for their model compression approach. However, none of the studies comprehensively examine the number of necessary final layers across multiple pretrained transformers and datasets. Experimental Setup. We conduct our experiments on NVIDIA Tesla V100 GPUs with CUDA v10.1. We run the models from the Transformers library (v2.1.1; BIBREF15) using PyTorch v1.2.0. Experimental Setup ::: Models and Datasets. We choose BERT BIBREF0 and RoBERTa BIBREF2 as the subjects of our study, since they represent", "the rest using the best hyperparameters of the full model. Specifically, if $L$ is the number of layers, we explore $N = \\frac{L}{2}, \\frac{L}{2} + 1, \\dots , L$. Due to computational limitations, we set half as the cutoff point. Additionally, we restrict our comprehensive all-datasets exploration to the base variant of BERT, since the large model variants and RoBERTa are much more computationally intensive. On the smaller CoLA, SST-2, MRPC, and STS-B datasets, we comprehensively evaluate both models. These choices do not substantially affect our analysis. Analysis ::: Operating Points. We report three relevant operating points in Tables TABREF6\u2013TABREF9: two extreme operating points and an intermediate one. The former is self-explanatory, indicating fine-tuning all or none of the nonoutput layers. The latter denotes the number of necessary layers for reaching at least 90% of the full model quality, excluding CoLA, which is an outlier. From the reported results in Tables"]}
{"question_id": "7f11f128fd39b8060f5810fa84102f000d94ea33", "predicted_answer": "Unanswerable", "predicted_evidence": ["of Hyp, we can find a trend that the larger the smooth value is, the lower the level of debiasing is, while with a very small or even no smooth value, the AUC may be lower than $0.5$. As mentioned before, we owe this to the imperfect estimation of $P(y|h)$, and we can conclude that a proper smooth value is a prerequisite for the best debiasing effect. Experimental Results ::: Debiasing Results ::: Benefits of Debiasing. Debiasing may improve models' generalization ability from two aspects: (1) Mitigate the misleading effect of annotation artifacts. (2) Improve models' semantic learning ability. When the annotation artifacts of the training set cannot be generalized to the testing set, which should be more common in the real-world, predicting by artifacts may hurt models' performance. Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested.", "Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested. Furthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions. Norm trained with SNLI exceed baseline in JOCI with smooth terms $0.01$ and $0.1$. With larger smooth terms, Norm trained with both SNLI and MultiNLI exceeds baseline in SICK. Given the fact that JOCI is almost neutral to artifacts in SNLI, and the bias pattern of both SNLI and MultiNLI are even predictive in SICK, we owe these promotions to that our method improves models' semantic learning ability. As to other testing sets like SNLI, MMatch and MMismatch, we notice that the performance of Norm always decreases compared with the baseline. As mentioned before, both SNLI and MultiNLI are prepared by Huamn Elicited, and their", "1. However, it is difficult to precisely estimate the probability $P(y|h)$. A minor error might lead to a significant difference to the weight, especially when the probability is close to zero. Thus, in practice, we use $w = \\frac{1}{(1-\\epsilon )P(y|h) + \\epsilon }$ as sample weights during training in order to improve the robustness. We can find that as $\\epsilon $ increases, the weights tend to be uniform, indicating that the debiasing effect decreases as the smooth term grows. Moreover, in order to keep the prior probability $P(Y)$ unchanged, we normalize the sum of weights of the three labels to the same. Experimental Results. In this section, we present the experimental results for cross-dataset testing of artifacts and artifact-balanced learning. We show that cross-dataset testing is less affected by annotation artifacts, while there are still some influences more or less in different datasets. We also demonstrate that our proposed framework can mitigate the bias and improve", "SNLI, MMatch and MMismatch, we notice that the performance of Norm always decreases compared with the baseline. As mentioned before, both SNLI and MultiNLI are prepared by Huamn Elicited, and their artifacts can be generalized across each other. We owe the drop to that the detrimental effect of mitigating the predictable bias pattern exceeds the beneficial effect of the improvement of semantic learning ability. Conclusion. In this paper, we take a close look at the annotation artifacts in NLI datasets. We find that the bias pattern could be predictive or misleading in cross-dataset testing. Furthermore, we propose a debiasing framework and experiments demonstrate that it can effectively mitigate the impacts of the bias pattern and improve the cross-dataset generalization ability of models. However, it remains an open problem that how we should treat the annotation artifacts. We cannot assert whether the bias pattern should not exist at all or it is actually some kind of nature. We", "set using fastText BIBREF18. And we summarize the size of the datasets used in Hard-Easy Testing below.  Experiment Setting ::: Experiment Setup. For DIIN, we use settings same as BIBREF5 but do not use syntactical features. Priors of labels are normalized to be the same. For hypothesis-only-model, we implement a na\u00efve model with one LSTM layer and a three-layer MLP behind, implemented with Keras and Tensorflow backend BIBREF16. We use the 300-dimensional GloVe embeddings trained on the Common Crawl 840B tokens dataset BIBREF19 and keep them fixed during training. Batch Normalization BIBREF17 are applied after every hidden layer in MLP and we use Dropout BIBREF20 with rate 0.5 after the last hidden layer. We use RMSPropBIBREF21 as optimizer and set the learning rate as 1e-3. We set the gradient clipping to 1.0 and the batch size to 256.", "smooth equals $0.02$ for MultiNLI, we observe that the AUC of Hyp for all testing sets are approximately $0.5$, indicating Hyp's predictions are approximately equivalent to randomly guessing. Also, the gap between Hard and Easy for Norm significantly decreases comparing with the baseline. With the smooth, we can conclude that our method effectively alleviates the bias pattern. With other smooth terms, our method still has more or less debiasing abilities. In those testing sets which are not neutral to the bias pattern, the AUC of Hyp always come closer to $0.5$ comparing with the baseline with whatever smooth values. Performances of Norm on Hard and Easy also come closer comparing with the baseline. Norm trained with SNLI even exceed baseline in Hard with most smooth terms. From the results of Hyp, we can find a trend that the larger the smooth value is, the lower the level of debiasing is, while with a very small or even no smooth value, the AUC may be lower than $0.5$. As mentioned", "is less affected by annotation artifacts, while there are still some influences more or less in different datasets. We also demonstrate that our proposed framework can mitigate the bias and improve the generalization ability of models. Experimental Results ::: Evaluation Scheme ::: Cross-dataset Testing. We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing. SNLI and MultiNLI are prepared by Human Elicited, in which workers are given a context and asked to produce hypotheses corresponding to labels. SICK and JOCI are created by Human Judged, referring that hypotheses and premises are automatically paired while labels are generated by humans BIBREF6. In order to maximumly mitigate the impacts of annotation artifacts during evaluations, we train and validate models respectively on SNLI and MultiNLI and test on both SICK and JOCI. We also report models' performances on SNLI and MultiNLI. As to SNLI, we use the same partition as BIBREF0. For", "We investigate the impacts of annotation artifacts in cross-dataset testing. Furthermore, we propose an easy-adopting debiasing training framework, which doesn't require any additional data or annotations, and apply it to the high-performing Densely Interactive Inference Network BIBREF5. Experiments show that our method can effectively mitigate the bias pattern and improve the cross-dataset generalization ability of models. To the best of our knowledge, our work is the first attempt to alleviate the annotation artifacts without any extra resources. Related Work. Frequently-used NLI datasets such as SNLI and MultiNLI are created by crowd-sourcing BIBREF0, BIBREF1, during which they present workers a premise and ask them to produce three hypotheses corresponding to labels. As BIBREF2 pointed out, workers may adopt some specific annotation strategies and heuristics when authoring hypotheses to save efforts, which produces certain patterns called annotation artifacts in the data. Models'"]}
{"question_id": "2a55076a66795793d79a3edfae1041098404fbc3", "predicted_answer": "Unanswerable", "predicted_evidence": ["Introduction. Natural language inference (NLI) is a widely-studied problem in natural language processing. It aims at comparing a pair of sentences (i.e. a premise and a hypothesis), and inferring the relationship between them (i.e., entailment, neutral and contradiction). Large-scaled datasets like SNLI BIBREF0 and MultiNLI BIBREF1 have been created by crowd-sourcing and fertilized NLI research substantially. However, several works BIBREF2, BIBREF3, BIBREF4 have pointed out that crowd-sourcing workers have brought a bias pattern named annotation artifacts in these NLI datasets. Such artifacts in hypotheses can reveal the labels and make it possible to predict the labels solely by looking at the hypotheses. For example, models trained on SNLI with only the hypotheses can achieve an accuracy of 67.0%, despite the always predicting the majority-class baseline is only 34.3% BIBREF2. Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic", "Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested. Furthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions. Norm trained with SNLI exceed baseline in JOCI with smooth terms $0.01$ and $0.1$. With larger smooth terms, Norm trained with both SNLI and MultiNLI exceeds baseline in SICK. Given the fact that JOCI is almost neutral to artifacts in SNLI, and the bias pattern of both SNLI and MultiNLI are even predictive in SICK, we owe these promotions to that our method improves models' semantic learning ability. As to other testing sets like SNLI, MMatch and MMismatch, we notice that the performance of Norm always decreases compared with the baseline. As mentioned before, both SNLI and MultiNLI are prepared by Huamn Elicited, and their", "how biased the models are, we partition the testing set of SNLI and MMatch into two subsets: examples that the hypothesis-only model can be correctly classified as Easy and the rest as Hard as seen in BIBREF2. More detailed information is presented in Appendix SECREF14. Experimental Results ::: Experiment Setup. We refer models trained only with hypotheses as hypothesis-only-model (Hyp), and models that utilize both premises and hypotheses as normal-model (Norm). We implement a simple LSTM model for Hyp and use DIIN BIBREF5 as Norm. We report AUC for Hyp and ACC for Norm. More details can be seen in Appendix SECREF15 We estimate $P(y|h)$ for SNLI and MultiNLI respectively using BERT BIBREF15 with 10-fold predictions. To investigate the impacts of smooth terms, we choose a series of smooth values and present the results. Considering models may jiggle during the training phase due to the varied scale of weights, we sample examples with probabilities proportional to the weights for every", "is less affected by annotation artifacts, while there are still some influences more or less in different datasets. We also demonstrate that our proposed framework can mitigate the bias and improve the generalization ability of models. Experimental Results ::: Evaluation Scheme ::: Cross-dataset Testing. We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing. SNLI and MultiNLI are prepared by Human Elicited, in which workers are given a context and asked to produce hypotheses corresponding to labels. SICK and JOCI are created by Human Judged, referring that hypotheses and premises are automatically paired while labels are generated by humans BIBREF6. In order to maximumly mitigate the impacts of annotation artifacts during evaluations, we train and validate models respectively on SNLI and MultiNLI and test on both SICK and JOCI. We also report models' performances on SNLI and MultiNLI. As to SNLI, we use the same partition as BIBREF0. For", "set using fastText BIBREF18. And we summarize the size of the datasets used in Hard-Easy Testing below.  Experiment Setting ::: Experiment Setup. For DIIN, we use settings same as BIBREF5 but do not use syntactical features. Priors of labels are normalized to be the same. For hypothesis-only-model, we implement a na\u00efve model with one LSTM layer and a three-layer MLP behind, implemented with Keras and Tensorflow backend BIBREF16. We use the 300-dimensional GloVe embeddings trained on the Common Crawl 840B tokens dataset BIBREF19 and keep them fixed during training. Batch Normalization BIBREF17 are applied after every hidden layer in MLP and we use Dropout BIBREF20 with rate 0.5 after the last hidden layer. We use RMSPropBIBREF21 as optimizer and set the learning rate as 1e-3. We set the gradient clipping to 1.0 and the batch size to 256.", "accuracy of 67.0%, despite the always predicting the majority-class baseline is only 34.3% BIBREF2. Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic relationships between given sentence pairs. However, it is shown that models are unintentionally utilizing the annotation artifacts BIBREF4, BIBREF2. If the evaluation is conducted under a similar distribution as the training data, e.g., with the given testing set, models will enjoy additional advantages, making the evaluation results over-estimated. On the other hand, if the bias pattern cannot be generalized to the real-world, it may introduce noise to models, thus hurting the generalization ability. In this paper, we use cross-dataset testing to better assess models' generalization ability. We investigate the impacts of annotation artifacts in cross-dataset testing. Furthermore, we propose an easy-adopting debiasing training framework, which doesn't require any additional data or", "SNLI, MMatch and MMismatch, we notice that the performance of Norm always decreases compared with the baseline. As mentioned before, both SNLI and MultiNLI are prepared by Huamn Elicited, and their artifacts can be generalized across each other. We owe the drop to that the detrimental effect of mitigating the predictable bias pattern exceeds the beneficial effect of the improvement of semantic learning ability. Conclusion. In this paper, we take a close look at the annotation artifacts in NLI datasets. We find that the bias pattern could be predictive or misleading in cross-dataset testing. Furthermore, we propose a debiasing framework and experiments demonstrate that it can effectively mitigate the impacts of the bias pattern and improve the cross-dataset generalization ability of models. However, it remains an open problem that how we should treat the annotation artifacts. We cannot assert whether the bias pattern should not exist at all or it is actually some kind of nature. We", "can get $0.6250$ (AUC) trained with SNLI and $0.6079$ (AUC) with MultiNLI when tested on SICK, indicating that the bias pattern of SNLI and MultiNLI are predictive on SICK. The results imply that the bias pattern can even be generalized across datasets prepared by different methods. Annotation Artifacts of SNLI are nearly neutral in JOCI, while MultiNLI is misleading. We find that AUC of Hyp baseline trained with SNLI is very close to $0.5$ on JOCI, indicating that JOCI is nearly neutral to artifacts in SNLI. However, when it comes to training with MultiNLI, the AUC of Hyp baseline is lower than $0.5$, indicating that the artifacts are misleading in JOCI. Experimental Results ::: Debiasing Results ::: Effectiveness of Debiasing. Focusing on the results when smooth equals $0.01$ for SNLI and smooth equals $0.02$ for MultiNLI, we observe that the AUC of Hyp for all testing sets are approximately $0.5$, indicating Hyp's predictions are approximately equivalent to randomly guessing. Also,"]}
{"question_id": "8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "predicted_answer": "SNLI, MultiNLI, JOCI, SICK", "predicted_evidence": ["is less affected by annotation artifacts, while there are still some influences more or less in different datasets. We also demonstrate that our proposed framework can mitigate the bias and improve the generalization ability of models. Experimental Results ::: Evaluation Scheme ::: Cross-dataset Testing. We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing. SNLI and MultiNLI are prepared by Human Elicited, in which workers are given a context and asked to produce hypotheses corresponding to labels. SICK and JOCI are created by Human Judged, referring that hypotheses and premises are automatically paired while labels are generated by humans BIBREF6. In order to maximumly mitigate the impacts of annotation artifacts during evaluations, we train and validate models respectively on SNLI and MultiNLI and test on both SICK and JOCI. We also report models' performances on SNLI and MultiNLI. As to SNLI, we use the same partition as BIBREF0. For", "values and present the results. Considering models may jiggle during the training phase due to the varied scale of weights, we sample examples with probabilities proportional to the weights for every mini-batch instead of adding weights to the loss directly. The evaluation results are reported in Table TABREF3. Experimental Results ::: Can Artifacts Generalize Across Datasets?. Anotation Artifacts can be generalized across Human Elicited datasets. From the AUC of Hyp baseline trained with SNLI, we can see that the bias pattern of SNLI has a strong predictive ability in itself and the other two testing sets of Human Elicited. The behavior of those trained with MultiNLI is similar. Anotation Artifacts of SNLI and MultiNLI can be generalized to SICK. Unexpectedly, it is shown that Hyp baseline can get $0.6250$ (AUC) trained with SNLI and $0.6079$ (AUC) with MultiNLI when tested on SICK, indicating that the bias pattern of SNLI and MultiNLI are predictive on SICK. The results imply that", "and validate models respectively on SNLI and MultiNLI and test on both SICK and JOCI. We also report models' performances on SNLI and MultiNLI. As to SNLI, we use the same partition as BIBREF0. For MultiNLI, we separately use two origin validation sets (Matched and Mismatched) as the testing sets for convenience, and refer them as MMatch and MMismatch. We randomly select 10000 samples out of the origin training set for validation and use the rest for training. As to JOCI, we use the whole \u201cB\u201d subsets for testing, whose premises are from SNLI-train while hypotheses are generated based on world knowledge BIBREF13, and convert the score to NLI labels following BIBREF6. As to SICK, we use the whole dataset for testing. Experimental Results ::: Evaluation Scheme ::: Hard-Easy Testing. To determine how biased the models are, we partition the testing set of SNLI and MMatch into two subsets: examples that the hypothesis-only model can be correctly classified as Easy and the rest as Hard as", "1. However, it is difficult to precisely estimate the probability $P(y|h)$. A minor error might lead to a significant difference to the weight, especially when the probability is close to zero. Thus, in practice, we use $w = \\frac{1}{(1-\\epsilon )P(y|h) + \\epsilon }$ as sample weights during training in order to improve the robustness. We can find that as $\\epsilon $ increases, the weights tend to be uniform, indicating that the debiasing effect decreases as the smooth term grows. Moreover, in order to keep the prior probability $P(Y)$ unchanged, we normalize the sum of weights of the three labels to the same. Experimental Results. In this section, we present the experimental results for cross-dataset testing of artifacts and artifact-balanced learning. We show that cross-dataset testing is less affected by annotation artifacts, while there are still some influences more or less in different datasets. We also demonstrate that our proposed framework can mitigate the bias and improve", "We investigate the impacts of annotation artifacts in cross-dataset testing. Furthermore, we propose an easy-adopting debiasing training framework, which doesn't require any additional data or annotations, and apply it to the high-performing Densely Interactive Inference Network BIBREF5. Experiments show that our method can effectively mitigate the bias pattern and improve the cross-dataset generalization ability of models. To the best of our knowledge, our work is the first attempt to alleviate the annotation artifacts without any extra resources. Related Work. Frequently-used NLI datasets such as SNLI and MultiNLI are created by crowd-sourcing BIBREF0, BIBREF1, during which they present workers a premise and ask them to produce three hypotheses corresponding to labels. As BIBREF2 pointed out, workers may adopt some specific annotation strategies and heuristics when authoring hypotheses to save efforts, which produces certain patterns called annotation artifacts in the data. Models'", "accuracy of 67.0%, despite the always predicting the majority-class baseline is only 34.3% BIBREF2. Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic relationships between given sentence pairs. However, it is shown that models are unintentionally utilizing the annotation artifacts BIBREF4, BIBREF2. If the evaluation is conducted under a similar distribution as the training data, e.g., with the given testing set, models will enjoy additional advantages, making the evaluation results over-estimated. On the other hand, if the bias pattern cannot be generalized to the real-world, it may introduce noise to models, thus hurting the generalization ability. In this paper, we use cross-dataset testing to better assess models' generalization ability. We investigate the impacts of annotation artifacts in cross-dataset testing. Furthermore, we propose an easy-adopting debiasing training framework, which doesn't require any additional data or", "can prove the equivalence of training with weight $\\frac{1}{P(Y|A)}$ and fitting the artifact-balanced distribution. We first present an equation as follows, Without loss of generality, we can assume $Q(Y=i)=\\frac{1}{3}~(i=0,1,2)$ and get that, With the above derivation, we can prove the equivalence like following, As $Q(S=Y)$ is just a constant, training with the loss is equivalent to fitting the artifact-balanced distribution. Given hypotheses variable H, the probability $P(Y|A)$ can be replaced by $P(Y|H)$ since the predictive ability of hypotheses totally comes from the annotation artifacts, and we can have $w=\\frac{1}{P(Y|H)}$ as weights during training. Experiment Setting ::: Hard-Easy Datasets Setting. For SNLI, we use Hard released by BIBREF2. For MMatch, we manually partition the set using fastText BIBREF18. And we summarize the size of the datasets used in Hard-Easy Testing below.  Experiment Setting ::: Experiment Setup. For DIIN, we use settings same as BIBREF5 but do not", "SNLI, MMatch and MMismatch, we notice that the performance of Norm always decreases compared with the baseline. As mentioned before, both SNLI and MultiNLI are prepared by Huamn Elicited, and their artifacts can be generalized across each other. We owe the drop to that the detrimental effect of mitigating the predictable bias pattern exceeds the beneficial effect of the improvement of semantic learning ability. Conclusion. In this paper, we take a close look at the annotation artifacts in NLI datasets. We find that the bias pattern could be predictive or misleading in cross-dataset testing. Furthermore, we propose a debiasing framework and experiments demonstrate that it can effectively mitigate the impacts of the bias pattern and improve the cross-dataset generalization ability of models. However, it remains an open problem that how we should treat the annotation artifacts. We cannot assert whether the bias pattern should not exist at all or it is actually some kind of nature. We"]}
{"question_id": "0aca0a208a1e28857fab44e397dc7880e010dbca", "predicted_answer": "Entropy", "predicted_evidence": ["of life events based on individual tweets. We analyzed the quality of crowdsourcing annotations and then experimented with different machine/deep learning classifiers combined with different active learning strategies to answer the following two research questions (RQs): RQ1. How does (1) the amount of time that a human worker spends on and (2) the number of workers assigned to each annotation task impact the quality of an-notation results? RQ2. Which active learning strategy is most efficient and cost-effective to build event classification models using Twitter data? -5pt Methods. We first collected tweets based on a list of job loss-related keywords. We then randomly selected a set of sample tweets and had these tweets annotated (i.e., whether the tweet is a job loss event) using the Amazon MTurk platform. With these annotated tweets, we then evaluated 4 different active learning strategies (i.e., least confi-dent, entropy, vote entropy, and Kullback-Leibler (KL) divergence) through", "on which active learning strategy is most efficient and cost-effective to build event classification models using crowdsourcing labels. Other research questions such as how the correctness of the crowdsourced labels would impact classifier performance warrant future investigations. In sum, our study demonstrated that crowdsourcing with active learning is a possible way to build up machine learning classifiers efficiently. However, active learning strategies do not benefit deep learning classifiers in our study. Acknowledgement. This study was supported by NSF Award #1734134.", "loss-related tweets annotated using Amazon MTurk, tested 8 classification models, and evaluated 4 active learning strategies to answer our two RQs. The key benefit of crowdsourcing is to have a large number of workers available to carry out tasks on a piecework basis. This means that it is likely to get the crowd to start work on tasks almost immediately and be able to have a large number of tasks completed quickly. However, even welltrained workers are only human and can make mistakes. Our first RQ was to find an optimal and economical way to get reliable annotations from crowdsourcing. Beyond using control tweets, we tested different cut-off time to assess how the amount of time workers spent on the task would affect annotation quality. We found that the annotation quality is low if the tasks were finished within 90 seconds. We also found that the annotation quality is not affected by the number of workers (i.e., between 3 worker group vs 5 worker group), which was also demonstrated", "classifier and active learning strategy is most efficient to build up an event classification classifier of Twitter data. We queried the top 300 most \u201cinformative\u201d tweets from the rest of the pool (i.e., excluding the tweets used for training the classifiers) at each iteration. Table 3 shows the active learning and classifier combinations that we evaluated. The performance of the classifiers was measured by F-score. Fig 3 shows the results of the different active learning strategies combined with LR (i.e., the baseline), RF (i.e., the best performed machine learning model), and CNN (i.e., the best performed deep learning model). For both machine learning models (i.e., LR and RF), using the entropy strategy can reach the optimal performance the quickest (i.e., the least amount of tweets). While, the least confident algorithm does not have any clear advantages compared with random selection. For deep learning model (i.e., CNN), none of the active learning strategies tested are useful to", "the limitations of our study. First, we only tested 5 classifiers (i.e., LR, RF, CNN, a machine learning ensemble classifier, and a deep learning classifier) and 4 active learning strategies (i.e., least confident, entropy, vote entropy, KL divergence). Other state-of-art methods for building tweet classifiers (e.g., BERT BIBREF29) and other active learning strategies (e.g., variance reduction BIBREF30) are worth exploring. Second, other crowdsourcing quality control methods such as using prequalification questions to identify high-quality workers also warrant further investigations. Third, the crowdsourcing and active learning pipeline can potentially be applied to other data and tasks. However, more experiments are needed to test the fea-sibility. Fourth, the current study only focused on which active learning strategy is most efficient and cost-effective to build event classification models using crowdsourcing labels. Other research questions such as how the correctness of the", "had a better performance which is consistent with other studies BIBREF24, BIBREF25 We then selected one machine learning and one deep learning classifiers based on the prediction performance (i.e., F-score). Logistic regression was used as the baseline classifier. Methods ::: Pool-based Active Learning. In pool-based sampling for active learning, instances are drawn from a pool of samples according to some sort of informativeness measure, and then the most informative instances are selected to be annotated. This is the most common scenario in active learning studies BIBREF26. The informativeness measures of the pool instances are called active learning strategies (or query strategies). We evaluated 4 active learning strategies (i.e., least confident, entropy, vote entropy and KL divergence). Fig 1.C shows the workflow of our pool-based active learning experiments: for a given active learning strategy and classifiers trained with an initial set of training data (1) the classifiers make", "to proactively select a subset of available examples to be manually labeled next from a pool of yet unlabeled instances. The fundamental idea behind the concept is that a machine learning algorithm could potentially achieve a better accuracy quicker and using fewer training data if it were allowed to choose the most informative data it wants to learn from. In our experiment, we found that the entropy algorithm is the best way to build machine learning models fast and efficiently. Vote entropy and KL divergence, the query-by-committee active learning methods are helpful for the training of machine learning ensemble classifiers. However, all the active learning strategies we tested do not work well with deep learning model (i.e., CNN) or deep learning-based ensemble classifier. We also recognize the limitations of our study. First, we only tested 5 classifiers (i.e., LR, RF, CNN, a machine learning ensemble classifier, and a deep learning classifier) and 4 active learning strategies", "MTurk platform. With these annotated tweets, we then evaluated 4 different active learning strategies (i.e., least confi-dent, entropy, vote entropy, and Kullback-Leibler (KL) divergence) through simulations. Methods ::: Data Collection. Our data were collected from two data sources based on a list of job loss-related keywords. The keywords were developed using a snowball sampling process, where we started with an initial list of 8 keywords that indicates a job-loss event (e.g., \u201cgot fired\u201d and \u201clost my job\u201d). Using these keywords, we then queried (1) Twitter\u2019s own search engine (i.e., https://twitter.com/search-home?lang=en), and (2) a database of public random tweets that we have collected using the Twitter steaming application programming interface (API) from January 1, 2013 to December 30, 2017, to identify job loss-related tweets. We then manually reviewed a sample of randomly selected tweets to discover new job loss-related keywords. We repeated the search then review process"]}
{"question_id": "471683ba6251b631f38a24d42b6dba6f52dee429", "predicted_answer": "2,803,164 tweets were collected from December 10, 2018 to December 26, 2018 base on a list of job loss-related keywords (n = 68), and 1,733,905 relevant tweets from a database of historical random public tweets collected from January 1, 2013 to December 30, 2017.", "predicted_evidence": ["Introduction. Micro-blogging social media platforms have become very popular in recent years. One of the most popular platforms is Twitter, which allows users to broadcast short texts (i.e., 140 characters initially, and 280 characters in a recent platform update) in real time with almost no restrictions on content. Twitter is a source of people\u2019s attitudes, opinions, and thoughts toward the things that happen in their daily life. Twitter data are publicly accessible through Twitter application programming interface (API); and there are several tools to download and process these data. Twitter is being increasingly used as a valuable instrument for surveillance research and predictive analytics in many fields including epidemiology, psychology, and social sciences. For example, Bian et al. explored the relation between promotional information and laypeople\u2019s discussion on Twitter by using topic modeling and sentiment analysis BIBREF0. Zhao et al. assessed the mental health signals", "upon three deep learning classifiers (i.e., CNN, RNN and LSTM) and three machine learning classifiers (i.e., LR, RF, and SVM) as two separate committees, respectively. Results ::: Data Collection. Our data came from two different sources as shown in Table 1. First, we collected 2,803,164 tweets using the Twitter search API BIBREF27 from December 10, 2018 to December 26, 2018 base on a list of job loss-related keywords (n = 68). After filtering out duplicates and non-English tweets, 1,952,079 tweets were left. Second, we used the same list of keywords to identify relevant tweets from a database of historical random public tweets we collected from January 1, 2013 to December 30, 2017. We found 1,733,905 relevant tweets from this database. Due to the different mechanisms behind the two Twitter APIs (i.e., streaming API vs. search API), the volumes of the tweets from the two data sources were significantly different. For the Twitter search API, users can retrieve most of the public tweets", "al. explored the relation between promotional information and laypeople\u2019s discussion on Twitter by using topic modeling and sentiment analysis BIBREF0. Zhao et al. assessed the mental health signals among sexual and gender minorities using Twitter data BIBREF1. Twitter data can be used to study and predict population-level targets, such as disease incidence BIBREF2, political trends BIBREF3, earthquake detection BIBREF4, and crime perdition BIBREF5, and individual-level outcomes or life events, such as job loss BIBREF6, depression BIBREF7, and adverse events BIBREF8. Since tweets are unstructured textual data, natural language processing (NLP) and machine learning, especially deep learning nowadays, are often used for preprocessing and analytics. However, for many studiesBIBREF9, BIBREF10, BIBREF11, especially those that analyze individual-level targets, manual annotations of several thousands of tweets, often by experts, is needed to create gold-standard training datasets, to be fed", "APIs (i.e., streaming API vs. search API), the volumes of the tweets from the two data sources were significantly different. For the Twitter search API, users can retrieve most of the public tweets related to the provided keywords within 10 to 14 days before the time of data collection; while the Twitter streaming API returns a random sample (i.e., roughly 1% to 20% varying across the years) of all public tweets at the time and covers a wide range of topics. After integrating the tweets from the two data sources, there were 3,685,984 unique tweets. Results ::: RQ1. How does (1) the amount of time that a human worker spends on and (2) the number of workers assigned to each annotation task impact the quality of annotation results?. We randomly selected 7,220 tweets from our Twitter data based on keyword distributions and had those tweets annotated using workers recruited through Amazon MTurk. Each tweet was also annotated by an expert annotator (i.e., one of the authors). We treated the", "the tweets following the preprocessing steps used by GloVe BIBREF23 with minor modifications as follows: (1) all hashtags (e.g., \u201c#gotfired\u201d) were replaced with \u201c$<$hashtag$>$ PHRASE\u201d (e.g.,, \u201c$<$hashtag$>$ gotfired\u201d); (2) user mentions (e.g., \u201c$@$Rob_Bradley\u201d) were replaced with \u201c$<$user$>$\u201d; (3) web links (eg, \u201chttps://t.co/ fMmFWAHEuM\u201d) were replaced with \u201c$<$url$>$\u201d; and (4) all emojis were replaced with \u201c$<$emoji$>$.\u201d Methods ::: Classifier Selection. Machine learning and deep learning have been wildly used in classification of tweets tasks. We evaluated 8 different classifiers: 4 traditional machine learning models (i.e., logistic regress [LR], Na\u00efve Bayes [NB], random forest [RF], and support vector machine [SVM]) and 4 deep learning models (i.e., convolutional neural network [CNN], recurrent neural network [RNN], long short-term memory [LSTM] RNN, and gated recurrent unit [GRU] RNN). 3,000 tweets out of 7,220 Amazon MTurk annotated dataset was used for classifier training (n =", "recurrent neural network [RNN], long short-term memory [LSTM] RNN, and gated recurrent unit [GRU] RNN). 3,000 tweets out of 7,220 Amazon MTurk annotated dataset was used for classifier training (n = 2,000) and testing (n = 1,000). The rest of MTurk annotated dataset were used for the subsequent active learning experiments. Each classifier was trained 10 times and 95 confidence intervals (CI) for mean value were reported. We explored two language models as the features for the classifiers (i.e., n-gram and word-embedding). All the machine learning classifiers were developed with n-gram features; while we used both n-gram and word-embedding features on the CNN classifier to test which feature set is more suitable for deep learning classifiers. CNN classifier with word embedding features had a better performance which is consistent with other studies BIBREF24, BIBREF25 We then selected one machine learning and one deep learning classifiers based on the prediction performance (i.e.,", "30, 2017, to identify job loss-related tweets. We then manually reviewed a sample of randomly selected tweets to discover new job loss-related keywords. We repeated the search then review process iteratively until no new keywords were found. Through this process, we found 33 keywords from the historical random tweet database and 57 keywords through Twitter web search. We then (1) not only collected tweets based on the over-all of 68 unique keywords from the historical random tweet database, but also (2) crawled new Twitter data using Twitter search API from December 10, 2018 to December 26, 2018 (17 days). Methods ::: Data Preprocessing. We preprocessed the collected data to eliminate tweets that were (1) duplicated or (2) not written in English. For building classifiers, we preprocessed the tweets following the preprocessing steps used by GloVe BIBREF23 with minor modifications as follows: (1) all hashtags (e.g., \u201c#gotfired\u201d) were replaced with \u201c$<$hashtag$>$ PHRASE\u201d (e.g.,,", "(i.e., Fleiss\u2019 Kappa score) between using 3 workers vs. using 5 workers. The Fleiss\u2019 kappa score of 3 workers is 0.53 (95% CI [0.46, 0.61]. The Fleiss\u2019 kappa score of 5 workers is 0.56 (95% CI [0.51, 0.61]. Thus, using 3 workers vs. 5 workers does not make any difference on the annotation reliability, while it is obviously cheaper to use only 3 workers. Results ::: RQ2. Which active learning strategy is most efficient and cost-effective to build event classification models using Twitter data?. We randomly selected 3,000 tweets from the 7,220 MTurk annotated dataset to build the initial classifiers. Two thousands out of 3,000 tweets were used to train the clas-sifiers and the rest 1,000 tweets were used as independent test dataset to benchmark their performance. We explored 4 machine learning classifiers (i.e., Logistic Regression [LR], Na\u00efve Bayes [NB], Random Forest [RF], and Support Vector Machine [SVM]) and 4 deep learning classifiers (i.e., Convolutional Neural Network [CNN],"]}
{"question_id": "5dfd58f91e7740899c23ebfe04b7176edce9ead2", "predicted_answer": "10K sentences and 250K tokens.", "predicted_evidence": ["cases respectively. We remark that the choice of macro-averaged INLINEFORM1 score is more suitable than accuracy for POS tagging because of the class imbalance in the dataset. There are too many words with NN as the true POS tag, so accuracy is not the best metric in such case. Dataset. We used the IDN Tagged Corpus proposed in BIBREF11 . The corpus contains 10K sentences and 250K tokens that are tagged manually. Due to the small size, we used 5-fold cross-validation to split the corpus into training, development, and test sets. We did not split multi-word expressions but treated them as if they are a single token. All 5 folds of the dataset are available publicly to serve as a benchmark for future work. Baselines. We used two simple baselines: majority tag (Major) and memorization (Memo). Major simply predicts the majority POS tag found in the training set for all words. Memo remembers the word-tag assignments from the training set and uses them to predict the tags on the test set.", "corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is developed in BIBREF7 using the aformentioned dataset, and is able to achieve an accuracy of 80%. One of the neural network-based POS taggers for Indonesian is proposed in BIBREF8 . They used a feedforward neural network with an architecture similar to that proposed in BIBREF13 . They evaluated their methods on the new POS tagging corpus BIBREF11 and separated the evaluation of multi- and single-word expressions. They experimented with several word embedding algorithms trained on Indonesian Wikipedia data and reported macro-averaged INLINEFORM0 score of 91 and 73 for the single- and multi-word expression cases respectively. We remark that the choice of macro-averaged INLINEFORM1 score is more suitable than accuracy for POS tagging because of the class imbalance in the dataset. There are too many", "on the IDN Tagged Corpus BIBREF11 , a manually annotated and publicly available Indonesian POS tagging dataset. Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result on the dataset. We make our cross-validation split available publicly to serve as a benchmark for future work.", "For the CRF model, we used L2 regularization whose coefficient was tuned to the development set. As we mentioned previously, we tuned the context window size INLINEFORM0 to the development set as well. For the neural tagger, we set the size of the word, affix, and character embedding to 100, 20, and 30 respectively. We applied dropout regularization to the embedding layers. The max-pooled CNN has 30 filters for each filter width. We set the feedforward network and the biLSTM to have 100 hidden units. We put a dropout layer before the biLSTM input layer. We tuned the learning rate, dropout rate, context window size, and CNN filter width to the development set. As we said earlier, we experimented with different configurations in the embedding, encoding, and prediction step. We evaluated each configuration on the development set as well. At training time, we used a batch size of 8, decayed the learning rate by half if the INLINEFORM0 score on the development set did not improve after 2", "an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on PANL10N dataset tagged manually using their proposed tagset. The dataset consists of 15K sentences. Another work used a hidden Markov model enhanced with an affix tree to better handle out-of-vocabulary (OOV) words BIBREF6 . They evaluated their models on the same PANL10N dataset and achieved more than 90% overall accuracy and roughly 70% accuracy for the OOV cases. We note that while the datasets are the same, the split could be different. Thus, making a fair comparison between them is difficult. Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is", "Major simply predicts the majority POS tag found in the training set for all words. Memo remembers the word-tag assignments from the training set and uses them to predict the tags on the test set. If there is an unknown word, it simply outputs the majority tag found in the training set. Comparisons. We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons. Firstly, the tagger tags named entities and multi-word expressions based on a dictionary. Then, it uses MorphInd BIBREF15 to tag the rest of the words. Finally, they employ 15 hand-crafted rules to resolve ambiguous tags in the post-processing step. We want to note that we did not use their provided tokenizer since the IDN Tagged Corpus dataset is already tokenized. Their implementation is available online. We used CRF BIBREF12 as another comparison since it is the most common non-neural model for sequence labeling tasks. We employed contextual words as well as affixes as features. For some context", "are not the same. This lack of a common benchmark dataset makes a fair comparison among these works difficult. Moreover, despite the success of neural network models for English POS tagging BIBREF9 , BIBREF10 , the use of neural networks is generally unexplored for Indonesian. As a result, published results may not reflect the actual state-of-the-art performance of Indonesian POS tagger. In this work, we explored different neural network architectures for Indonesian POS tagging. We evaluated our experiments on the IDN Tagged Corpus BIBREF11 . Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result for Indonesian POS tagging on the dataset. We release the dataset split that we used to serve as a benchmark for future work. Related Work. Pisceldo et al. BIBREF5 built an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on", "(first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the performance further by 0.08 and 0.10 points respectively. From this result, we see that it is the character features that positively affect the neural tagger the most. Conclusion. We experimented with several baselines and comparisons for Indonesian POS tagging task. Our comparisons include a rule-based tagger, a well-established probabilistic model for sequence labeling (CRF), and a neural model. We tested many configurations for our neural model: the features (words, affixes, characters), the architecture (feedforward, biLSTM), and the output layer (softmax, CRF). We evaluated all our models on the IDN Tagged Corpus BIBREF11 , a manually annotated and publicly available Indonesian POS tagging dataset. Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result on the"]}
{"question_id": "c09bceea67273c10a0621da1a83b409f53342fd9", "predicted_answer": "Different architectures were explored.", "predicted_evidence": ["are not the same. This lack of a common benchmark dataset makes a fair comparison among these works difficult. Moreover, despite the success of neural network models for English POS tagging BIBREF9 , BIBREF10 , the use of neural networks is generally unexplored for Indonesian. As a result, published results may not reflect the actual state-of-the-art performance of Indonesian POS tagger. In this work, we explored different neural network architectures for Indonesian POS tagging. We evaluated our experiments on the IDN Tagged Corpus BIBREF11 . Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result for Indonesian POS tagging on the dataset. We release the dataset split that we used to serve as a benchmark for future work. Related Work. Pisceldo et al. BIBREF5 built an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on", "many previous work in neural sequence labeling that a bidirectional LSTM with CRF layer performs best BIBREF10 , BIBREF17 , BIBREF9 . Therefore, we will use this tagger to represent the neural model hereinafter. To understand the performance of the neural model for each tag, we plot the confusion matrix from the development set of the first fold in Fig. FIGREF30 . The figure shows that the model can predict most tags almost perfectly, except for X and WH tag. The X tag is described as \"a word or part of a sentence which its category is unknown or uncertain\". The X tag is rather rare, as it only appears 397 times out of over 250K tokens. Some words annotated as X are typos and slang words. Some foreign terms and abbreviations are also annotated with X. The model might get confused as such words are usually tagged with a noun tag (NN or NNP). We also see that the model seems to confuse question words (WH) such as apa (what) or siapa (who) as SC since these words may be used in", "we used weighted macro-average INLINEFORM6 score which takes into account the tag proportion imbalance. It computes the weighted average of the scores where each weight is equal to the corresponding tag's proportion in the dataset. This functionality is available in the scikit-learn library. Results and Discussion. Firstly, we report on our tuning experiments for the neural tagger. Table TABREF27 shows the evaluation results of the many configurations of our neural tagger on the development set. We group the results by the encoding and prediction step configuration. For each group, we show the highest INLINEFORM0 score among many embedding configurations. As we can see, biLSTM with CRF layer achieves 97.60 INLINEFORM1 score, the best score on the development set. This result agrees with many previous work in neural sequence labeling that a bidirectional LSTM with CRF layer performs best BIBREF10 , BIBREF17 , BIBREF9 . Therefore, we will use this tagger to represent the neural model", "(first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the performance further by 0.08 and 0.10 points respectively. From this result, we see that it is the character features that positively affect the neural tagger the most. Conclusion. We experimented with several baselines and comparisons for Indonesian POS tagging task. Our comparisons include a rule-based tagger, a well-established probabilistic model for sequence labeling (CRF), and a neural model. We tested many configurations for our neural model: the features (words, affixes, characters), the architecture (feedforward, biLSTM), and the output layer (softmax, CRF). We evaluated all our models on the IDN Tagged Corpus BIBREF11 , a manually annotated and publicly available Indonesian POS tagging dataset. Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result on the", "For the CRF model, we used L2 regularization whose coefficient was tuned to the development set. As we mentioned previously, we tuned the context window size INLINEFORM0 to the development set as well. For the neural tagger, we set the size of the word, affix, and character embedding to 100, 20, and 30 respectively. We applied dropout regularization to the embedding layers. The max-pooled CNN has 30 filters for each filter width. We set the feedforward network and the biLSTM to have 100 hidden units. We put a dropout layer before the biLSTM input layer. We tuned the learning rate, dropout rate, context window size, and CNN filter width to the development set. As we said earlier, we experimented with different configurations in the embedding, encoding, and prediction step. We evaluated each configuration on the development set as well. At training time, we used a batch size of 8, decayed the learning rate by half if the INLINEFORM0 score on the development set did not improve after 2", "feedforward network with context window or a bidirectional LSTM (biLSTM). The feedforward network accepts as input the concatenation of the embedding of the current word and INLINEFORM0 preceding and succeeding words for some context window size INLINEFORM1 . Formally, given a sequence of word embedding INLINEFORM2 , the input of the feedforward network at timestep INLINEFORM3 is DISPLAYFORM0  where INLINEFORM0 denotes a concatenation. The feedforward network then computes DISPLAYFORM0   where INLINEFORM0 is the output vector, INLINEFORM1 is a dropout mask vector, and INLINEFORM2 are parameters. The output vector INLINEFORM3 has length equal to the number of possible tags. Its INLINEFORM4 -th component defines the (unnormalized) log probability of the INLINEFORM5 -th word having tag INLINEFORM6 . On the other hand, the biLSTM accepts as input the sequence of word embeddings, and for each timestep, the output from the forward and backward LSTM are concatenated to form the final output.", "This advantage is particularly useful for Indonesian which does not have well-established tools for such purposes. We padded the input sentence with padding tokens to ensure that every token has enough preceding and succeeding words for context window size INLINEFORM0 . For the implementation, we used pycrfsuite. Our neural network-based POS tagger can be divided into 3 steps: embedding, encoding, and prediction. First, the tagger embeds the words and optionally additional features of such words (e.g., affixes). From this embedding process, we get vector representations of the words and the features. Next, the tagger learns contextual information in the encoding step via either a feedforward network with context window or a bidirectional LSTM BIBREF16 . Finally, in prediction step, the tagger predicts the POS tags from the output of the encoding step using either a softmax or a CRF layer. Embedding. In the embedding step, the tagger obtains vector representations of each word and", "corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is developed in BIBREF7 using the aformentioned dataset, and is able to achieve an accuracy of 80%. One of the neural network-based POS taggers for Indonesian is proposed in BIBREF8 . They used a feedforward neural network with an architecture similar to that proposed in BIBREF13 . They evaluated their methods on the new POS tagging corpus BIBREF11 and separated the evaluation of multi- and single-word expressions. They experimented with several word embedding algorithms trained on Indonesian Wikipedia data and reported macro-averaged INLINEFORM0 score of 91 and 73 for the single- and multi-word expression cases respectively. We remark that the choice of macro-averaged INLINEFORM1 score is more suitable than accuracy for POS tagging because of the class imbalance in the dataset. There are too many"]}
{"question_id": "732bd97ae34541f215c436e2a1b98db1649cba27", "predicted_answer": "Rashel et al.'s rule-based tagger was evaluated in Document 3.", "predicted_evidence": ["(first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the performance further by 0.08 and 0.10 points respectively. From this result, we see that it is the character features that positively affect the neural tagger the most. Conclusion. We experimented with several baselines and comparisons for Indonesian POS tagging task. Our comparisons include a rule-based tagger, a well-established probabilistic model for sequence labeling (CRF), and a neural model. We tested many configurations for our neural model: the features (words, affixes, characters), the architecture (feedforward, biLSTM), and the output layer (softmax, CRF). We evaluated all our models on the IDN Tagged Corpus BIBREF11 , a manually annotated and publicly available Indonesian POS tagging dataset. Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result on the", "this dataset in contrast with Major. The result also provides evidence to the usefulness of our evaluation metric which heavily penalizes a simple majority vote model. Furthermore, we notice that the rule-based tagger by Rashel et al. BIBREF7 performs worse than Memo, indicating that Memo is not just suitable but also quite a strong baseline. Moving on, we observe how CRF has 6 points advantage over Memo, signaling that incorporating contextual features and modeling tag-to-tag transitions are useful. Lastly, the biLSTM with CRF tagger performs the best with 97.47 INLINEFORM2 score. To understand how each feature in the embedding step affects the neural tagger, we performed feature ablation on the development set and put the result in Table TABREF29 . We see that with only words as features (first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the", "Major simply predicts the majority POS tag found in the training set for all words. Memo remembers the word-tag assignments from the training set and uses them to predict the tags on the test set. If there is an unknown word, it simply outputs the majority tag found in the training set. Comparisons. We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons. Firstly, the tagger tags named entities and multi-word expressions based on a dictionary. Then, it uses MorphInd BIBREF15 to tag the rest of the words. Finally, they employ 15 hand-crafted rules to resolve ambiguous tags in the post-processing step. We want to note that we did not use their provided tokenizer since the IDN Tagged Corpus dataset is already tokenized. Their implementation is available online. We used CRF BIBREF12 as another comparison since it is the most common non-neural model for sequence labeling tasks. We employed contextual words as well as affixes as features. For some context", "corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is developed in BIBREF7 using the aformentioned dataset, and is able to achieve an accuracy of 80%. One of the neural network-based POS taggers for Indonesian is proposed in BIBREF8 . They used a feedforward neural network with an architecture similar to that proposed in BIBREF13 . They evaluated their methods on the new POS tagging corpus BIBREF11 and separated the evaluation of multi- and single-word expressions. They experimented with several word embedding algorithms trained on Indonesian Wikipedia data and reported macro-averaged INLINEFORM0 score of 91 and 73 for the single- and multi-word expression cases respectively. We remark that the choice of macro-averaged INLINEFORM1 score is more suitable than accuracy for POS tagging because of the class imbalance in the dataset. There are too many", "an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on PANL10N dataset tagged manually using their proposed tagset. The dataset consists of 15K sentences. Another work used a hidden Markov model enhanced with an affix tree to better handle out-of-vocabulary (OOV) words BIBREF6 . They evaluated their models on the same PANL10N dataset and achieved more than 90% overall accuracy and roughly 70% accuracy for the OOV cases. We note that while the datasets are the same, the split could be different. Thus, making a fair comparison between them is difficult. Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is", "as such words are usually tagged with a noun tag (NN or NNP). We also see that the model seems to confuse question words (WH) such as apa (what) or siapa (who) as SC since these words may be used in subordinate clauses as well. Looking at the data closely, we found that the tagging of such words are inconsistent. This inconsistency contributes to the inability of the model to distinguish the two tags well. Next, we present the result of evaluating the baselines and other comparisons on the test set in Table TABREF28 . The INLINEFORM0 scores are averaged over the 5 cross-validation folds. We see that Major baseline performs very poorly compared to the Memo baseline, which surprisingly achieves over 90 INLINEFORM1 points. This result suggests that Memo is a more suitable baseline for this dataset in contrast with Major. The result also provides evidence to the usefulness of our evaluation metric which heavily penalizes a simple majority vote model. Furthermore, we notice that the", "we used weighted macro-average INLINEFORM6 score which takes into account the tag proportion imbalance. It computes the weighted average of the scores where each weight is equal to the corresponding tag's proportion in the dataset. This functionality is available in the scikit-learn library. Results and Discussion. Firstly, we report on our tuning experiments for the neural tagger. Table TABREF27 shows the evaluation results of the many configurations of our neural tagger on the development set. We group the results by the encoding and prediction step configuration. For each group, we show the highest INLINEFORM0 score among many embedding configurations. As we can see, biLSTM with CRF layer achieves 97.60 INLINEFORM1 score, the best score on the development set. This result agrees with many previous work in neural sequence labeling that a bidirectional LSTM with CRF layer performs best BIBREF10 , BIBREF17 , BIBREF9 . Therefore, we will use this tagger to represent the neural model", "For the CRF model, we used L2 regularization whose coefficient was tuned to the development set. As we mentioned previously, we tuned the context window size INLINEFORM0 to the development set as well. For the neural tagger, we set the size of the word, affix, and character embedding to 100, 20, and 30 respectively. We applied dropout regularization to the embedding layers. The max-pooled CNN has 30 filters for each filter width. We set the feedforward network and the biLSTM to have 100 hidden units. We put a dropout layer before the biLSTM input layer. We tuned the learning rate, dropout rate, context window size, and CNN filter width to the development set. As we said earlier, we experimented with different configurations in the embedding, encoding, and prediction step. We evaluated each configuration on the development set as well. At training time, we used a batch size of 8, decayed the learning rate by half if the INLINEFORM0 score on the development set did not improve after 2"]}
{"question_id": "183b385fb59ff1e3f658d4555a08b67c005a8734", "predicted_answer": "PANL10N dataset, IDN Tagged Corpus", "predicted_evidence": ["Introduction. Part-of-speech (POS) tagging is a process to tag tokens in a string with their corresponding part-of-speech (e.g., noun, verb, etc). POS tagging is considered as one of the most basic tasks in NLP, as it is usually the first component in an NLP pipeline. This is because POS tags are shown to be useful features in various NLP tasks, such as named entity recognition BIBREF0 , BIBREF1 , machine translation BIBREF2 , BIBREF3 and constituency parsing BIBREF4 . Therefore, for any language, building a successful NLP system usually requires a well-performing POS tagger. There are quite a number of research on Indonesian POS tagging BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 . However, almost all of them are not evaluated on a common dataset. Even when they are, their train-test split are not the same. This lack of a common benchmark dataset makes a fair comparison among these works difficult. Moreover, despite the success of neural network models for English POS tagging BIBREF9 ,", "We used CRF BIBREF12 as another comparison since it is the most common non-neural model for sequence labeling tasks. We employed contextual words as well as affixes as features. For some context window size INLINEFORM0 , the complete list of features is: the current word, as well as INLINEFORM0 preceding and succeeding words; two and three leading characters of the current word and INLINEFORM0 preceding and succeeding words; two and three trailing characters of the current word and INLINEFORM0 preceding and succeeding words. The last two features are meant to capture prefixes and suffixes in Indonesian which usually consist of two or three characters. One advantage of this feature extraction approach is that it does not require language-specific tools such as stemmer or morphological segmenter. This advantage is particularly useful for Indonesian which does not have well-established tools for such purposes. We padded the input sentence with padding tokens to ensure that every token", "corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is developed in BIBREF7 using the aformentioned dataset, and is able to achieve an accuracy of 80%. One of the neural network-based POS taggers for Indonesian is proposed in BIBREF8 . They used a feedforward neural network with an architecture similar to that proposed in BIBREF13 . They evaluated their methods on the new POS tagging corpus BIBREF11 and separated the evaluation of multi- and single-word expressions. They experimented with several word embedding algorithms trained on Indonesian Wikipedia data and reported macro-averaged INLINEFORM0 score of 91 and 73 for the single- and multi-word expression cases respectively. We remark that the choice of macro-averaged INLINEFORM1 score is more suitable than accuracy for POS tagging because of the class imbalance in the dataset. There are too many", "an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on PANL10N dataset tagged manually using their proposed tagset. The dataset consists of 15K sentences. Another work used a hidden Markov model enhanced with an affix tree to better handle out-of-vocabulary (OOV) words BIBREF6 . They evaluated their models on the same PANL10N dataset and achieved more than 90% overall accuracy and roughly 70% accuracy for the OOV cases. We note that while the datasets are the same, the split could be different. Thus, making a fair comparison between them is difficult. Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset. The corpus is available online. A rule-based tagger is", "This advantage is particularly useful for Indonesian which does not have well-established tools for such purposes. We padded the input sentence with padding tokens to ensure that every token has enough preceding and succeeding words for context window size INLINEFORM0 . For the implementation, we used pycrfsuite. Our neural network-based POS tagger can be divided into 3 steps: embedding, encoding, and prediction. First, the tagger embeds the words and optionally additional features of such words (e.g., affixes). From this embedding process, we get vector representations of the words and the features. Next, the tagger learns contextual information in the encoding step via either a feedforward network with context window or a bidirectional LSTM BIBREF16 . Finally, in prediction step, the tagger predicts the POS tags from the output of the encoding step using either a softmax or a CRF layer. Embedding. In the embedding step, the tagger obtains vector representations of each word and", "(first row), the neural tagger only achieves 96.06 INLINEFORM0 score. Employing character features boosts the score up to 97.42, a gain of 1.36 points. Adding prefix and suffix features improves the performance further by 0.08 and 0.10 points respectively. From this result, we see that it is the character features that positively affect the neural tagger the most. Conclusion. We experimented with several baselines and comparisons for Indonesian POS tagging task. Our comparisons include a rule-based tagger, a well-established probabilistic model for sequence labeling (CRF), and a neural model. We tested many configurations for our neural model: the features (words, affixes, characters), the architecture (feedforward, biLSTM), and the output layer (softmax, CRF). We evaluated all our models on the IDN Tagged Corpus BIBREF11 , a manually annotated and publicly available Indonesian POS tagging dataset. Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result on the", "on the IDN Tagged Corpus BIBREF11 , a manually annotated and publicly available Indonesian POS tagging dataset. Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result on the dataset. We make our cross-validation split available publicly to serve as a benchmark for future work.", "are not the same. This lack of a common benchmark dataset makes a fair comparison among these works difficult. Moreover, despite the success of neural network models for English POS tagging BIBREF9 , BIBREF10 , the use of neural networks is generally unexplored for Indonesian. As a result, published results may not reflect the actual state-of-the-art performance of Indonesian POS tagger. In this work, we explored different neural network architectures for Indonesian POS tagging. We evaluated our experiments on the IDN Tagged Corpus BIBREF11 . Our best model achieves 97.47 INLINEFORM0 score, a new state-of-the-art result for Indonesian POS tagging on the dataset. We release the dataset split that we used to serve as a benchmark for future work. Related Work. Pisceldo et al. BIBREF5 built an Indonesian POS tagger by employing a conditional random field (CRF) BIBREF12 and a maximum entropy model. They used contextual unigram and bigram features and achieved accuracy scores of 80-90% on"]}
{"question_id": "5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "predicted_answer": "11,529,432 segmented words", "predicted_evidence": ["between those characters that are identical in strokes. After that, we combine the representation of words with the representation of characters and define the word embedding for INLINEFORM0 as follows: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are compositional operation. INLINEFORM6 is the word ID embedding and INLINEFORM7 is the number of characters in INLINEFORM8 . According to the previous work BIBREF0 , we compute the similarity between current word INLINEFORM0 and one of its context words INLINEFORM1 by defining a score function as INLINEFORM2 , where INLINEFORM3 and INLINEFORM4 are embedding vectors of INLINEFORM5 and INLINEFORM6 respectively. Following the previous works BIBREF0 , BIBREF21 , the objective function is defined as follows: DISPLAYFORM0  where INLINEFORM0 is the number of negative samples and INLINEFORM1 is the expectation term. For each INLINEFORM2 in training corpus INLINEFORM3 , a set of negative samples INLINEFORM4 will be selected according to the", "BIBREF10 , BIBREF11 , and some studies have proved that the morphology of words can indeed enrich the semantics of word embeddings BIBREF12 , BIBREF13 , BIBREF2 . More recently, Wieting et al. wieting2016charagram proposed to represent words using character n-gram count vectors. Further, Bojanowski et al. bojanowski2017enriching improved the classic skip-gram model BIBREF0 by taking subwords into account in the acquisition of word embeddings, which is instructive for us to regard certain stroke sequences as roots in English. Embedding for Chinese Language. The complexity of Chinese itself has given birth to a lot of research on Chinese embedding, including the utilization of character features BIBREF14 and radicals BIBREF15 , BIBREF16 , BIBREF17 . Considering the 2-D graphic structure of Chinese characters, Su and Lee su2017learning creatively proposed to enhance word representations by character glyphs. Lately, Cao et al. cao2018cw2vec proposed that a Chinese word can be decomposed", "system as shown in Figure FIGREF3 . After retrieving the stroke sequence, we add special boundary symbols INLINEFORM1 and INLINEFORM2 at the beginning and end of it and adopt an efficient approach by utilizing the stroke n-gram method BIBREF3 to extract strokes order information for each character. More precisely, we firstly scan each character throughout the training corpus and obtain a stroke n-gram dictionary INLINEFORM3 . Then, we use INLINEFORM4 to denote the collection of stroke n-grams of each character INLINEFORM5 in INLINEFORM6 . While in spatial channel, to capture the semantics hidden in glyphs, we render the glyph INLINEFORM7 for each character INLINEFORM8 and apply a well-known CNN structure, LeNet BIBREF20 , to process each character glyph, which is also helpful to distinguish between those characters that are identical in strokes. After that, we combine the representation of words with the representation of characters and define the word embedding for INLINEFORM0 as", "in root-like strokes order, and the spatial information hidden in graph-like character glyphs. Along this line, we propose a novel Dual-channel Word Embedding (DWE) model for Chinese to realize the joint learning of sequential and spatial information in characters. Finally, we evaluate DWE on two representative tasks, where the experimental results exactly validate the superiority of DWE in capturing the morphological information of Chinese. Morphological Word Representations. Traditional methods on getting word embeddings are mainly based on the distributional hypothesis BIBREF9 : words with similar contexts tend to have similar semantics. To explore more interpretable models, some scholars have gradually noticed the importance of the morphology of words in conveying semantics BIBREF10 , BIBREF11 , and some studies have proved that the morphology of words can indeed enrich the semantics of word embeddings BIBREF12 , BIBREF13 , BIBREF2 . More recently, Wieting et al.", "and render each character glyph to a 28 INLINEFORM0 28 1-bit grayscale bitmap by using Pillow. Experimental Setup. We choose adagrad BIBREF23 as our optimizing algorithm, and we set the batch size as 4,096 and learning rate as 0.05. In practice, the slide window size INLINEFORM0 of stroke INLINEFORM1 -grams is set as INLINEFORM2 . The dimension of all word embeddings of different models is consistently set as 300. We use two test tasks to evaluate the performance of different models: one is word similarity, and the other is word analogy. A word similarity test consists of multiple word pairs and similarity scores annotated by humans. Good word representations should make the calculated similarity have a high rank correlation with human annotated scores, which is usually measured by the Spearman's correlation INLINEFORM3 BIBREF24 . The form of an analogy problem is like \u201cking\":\u201cqueen\" = \u201cman\":\u201c?\", and \u201cwoman\" is the most proper answer to \u201c?\". That is, in this task, given three words", "the number of negative samples and INLINEFORM1 is the expectation term. For each INLINEFORM2 in training corpus INLINEFORM3 , a set of negative samples INLINEFORM4 will be selected according to the distribution INLINEFORM5 , which is usually set as the word unigram distribution. And INLINEFORM6 is the sigmoid function. Dataset Preparation. We download parts of Chinese Wikipedia articles from Large-Scale Chinese Datasets for NLP. For word segmentation and filtering the stopwords, we apply the jieba toolkit based on the stopwords table. Finally, we get 11,529,432 segmented words. In accordance with their work BIBREF14 , all items whose Unicode falls into the range between 0x4E00 and 0x9FA5 are Chinese characters. We crawl the stroke information of all 20,402 characters from an online dictionary and render each character glyph to a 28 INLINEFORM0 28 1-bit grayscale bitmap by using Pillow. Experimental Setup. We choose adagrad BIBREF23 as our optimizing algorithm, and we set the batch", "Chinese characters, Su and Lee su2017learning creatively proposed to enhance word representations by character glyphs. Lately, Cao et al. cao2018cw2vec proposed that a Chinese word can be decomposed into a sequence of strokes which correspond to subwords in English, and Wu et al. wu2019glyce designed a Tianzige-CNN to model the spatial structure of Chinese characters from the perspective of image processing. However, their methods are either somewhat loose for the stroke criteria or unable to capture the interactions between strokes and character glyphs. DWE Model. As we mentioned earlier, it is reasonable and imperative to learn Chinese word embeddings from two channels, i.e., a sequential stroke n-gram channel and a spatial glyph channel. Inspired by the previous works BIBREF14 , BIBREF18 , BIBREF4 , BIBREF19 , we propose to combine the representation of Chinese words with the representation of characters to obtain finer-grained semantics, so that unknown words can be identified and", "Introduction. Word embeddings are fixed-length vector representations for words BIBREF0 , BIBREF1 . In recent years, the morphology of words is drawing more and more attention BIBREF2 , especially for Chinese whose writing system is based on logograms. UTF8gbsn With the gradual exploration of the semantic features of Chinese, scholars have found that not only words and characters are important semantic carriers, but also stroke feature of Chinese characters is crucial for inferring semantics BIBREF3 . Actually, a Chinese word usually consists of several characters, and each character can be further decomposed into a stroke sequence which is certain and changeless, and this kind of stroke sequence is very similar to the construction of English words. In Chinese, a particular sequence of strokes can reflect the inherent semantics. As shown in the upper half of Figure FIGREF3 , the Chinese character \u201c\u9a7e\" (drive) can be decomposed into a sequence of eight strokes, where the last three"]}
{"question_id": "d427e9d181434078c78b7ee33a26b269f160f6d2", "predicted_answer": "Unanswerable", "predicted_evidence": ["findings reported in BIBREF4 . For instance, in an analogy pair \u201c\u897f\u73ed\u7259\" (Spain) : \u201c\u9a6c\u5fb7\u91cc\" (Madrid) = \u201c\u6cd5\u56fd\" (France) : \u201c\u5df4\u9ece\" (Paris), we cannot infer any relevance among these four words literally because they are all translated by pronunciation. In summary, since different words that are morphologically similar tend to have similar semantics in Chinese, simultaneously modeling the sequential and spatial information of characters from both stroke n-grams and glyph features can indeed improve the modeling of Chinese word representations substantially. Conclusions. In this article, we first analyzed the similarities and differences in terms of morphology between alphabetical languages and Chinese. Then, we delved deeper into the particularity of Chinese morphology and proposed our DWE model by taking into account the sequential information of strokes order and the spatial information of glyphs. Through the evaluation on two representative tasks, our model shows its superiority in capturing the", "in the lower half of Figure 1, three Chinese characters \u201c\u5165\" (enter), \u201c\u516b\" (eight) and \u201c\u4eba\" (man) share exactly a common stroke sequence, but they have completely different semantics because of their different spatial configurations. In addition, some biological investigations have confirmed that there are actually two processing channels for Chinese language. Specifically, Chinese readers not only activate the left brain which is a dominant hemisphere in processing alphabetic languages BIBREF5 , BIBREF6 , BIBREF7 , but also activate the areas of the right brain that are responsible for image processing and spatial information at the same time BIBREF8 . Therefore, we argue that the morphological information of characters in Chinese consists of two parts, i.e., the sequential information hidden in root-like strokes order, and the spatial information hidden in graph-like character glyphs. Along this line, we propose a novel Dual-channel Word Embedding (DWE) model for Chinese to realize the", "strokes can reflect the inherent semantics. As shown in the upper half of Figure FIGREF3 , the Chinese character \u201c\u9a7e\" (drive) can be decomposed into a sequence of eight strokes, where the last three strokes together correspond to a root character \u201c\u9a6c\" (horse) similar to the root \u201cclar\" of English word \u201cdeclare\" and \u201cclarify\". Moreover, Chinese is a language originated from Oracle Bone Inscriptions (a kind of hieroglyphics). Its character glyphs have a spatial structure similar to graphs which can convey abundant semantics BIBREF4 . Additionally, the critical reason why Chinese characters are so rich in morphological information is that they are composed of basic strokes in a 2-D spatial order. However, different spatial configurations of strokes may lead to different semantics. As shown in the lower half of Figure 1, three Chinese characters \u201c\u5165\" (enter), \u201c\u516b\" (eight) and \u201c\u4eba\" (man) share exactly a common stroke sequence, but they have completely different semantics because of their", ", BIBREF4 , BIBREF19 , we propose to combine the representation of Chinese words with the representation of characters to obtain finer-grained semantics, so that unknown words can be identified and their relationship with other known Chinese characters can be found by distinguishing the common stroke sequences or character glyph they share. UTF8gbsn Our DWE model is shown in Figure FIGREF9 . For an arbitrary Chinese word INLINEFORM0 , e.g., \u201c\u9a7e\u8f66\", it will be firstly decomposed into several characters, e.g., \u201c\u9a7e\" and \u201c\u8f66\", and each of the characters will be further processed in a dual-channel character embedding sub-module to refine its morphological information. In sequential channel, each character can be decomposed into a stroke sequence according to the criteria of Chinese writing system as shown in Figure FIGREF3 . After retrieving the stroke sequence, we add special boundary symbols INLINEFORM1 and INLINEFORM2 at the beginning and end of it and adopt an efficient approach by", "between morphologically related words, which can be verified by the results of the similarity task. Meanwhile, in the word analogy task, those words expressing family relations in Chinese are mostly compositional in their character glyphs. For example, in an analogy pair \u201c\u5144\u5f1f\" (brother) : \u201c\u59d0\u59b9\" (sister) = \u201c\u513f\u5b50\" (son) : \u201c\u5973\u513f\" (daughter), we can easily find that \u201c\u5144\u5f1f\" and \u201c\u513f\u5b50\" share an exactly common part of glyph \u201c\u513f\" (male relative of a junior generation) while \u201c\u59d0\u59b9\" and \u201c\u5973\u513f\" share an exactly common part of glyph \u201c\u5973\" (female), and this kind of morphological pattern can be accurately captured by our model. However, most of the names of countries, capitals and cities are transliterated words, and the relationship between the morphology and semantics of words is minimal, which is consistent with the findings reported in BIBREF4 . For instance, in an analogy pair \u201c\u897f\u73ed\u7259\" (Spain) : \u201c\u9a6c\u5fb7\u91cc\" (Madrid) = \u201c\u6cd5\u56fd\" (France) : \u201c\u5df4\u9ece\" (Paris), we cannot infer any relevance among these four words literally because", "Chinese characters, Su and Lee su2017learning creatively proposed to enhance word representations by character glyphs. Lately, Cao et al. cao2018cw2vec proposed that a Chinese word can be decomposed into a sequence of strokes which correspond to subwords in English, and Wu et al. wu2019glyce designed a Tianzige-CNN to model the spatial structure of Chinese characters from the perspective of image processing. However, their methods are either somewhat loose for the stroke criteria or unable to capture the interactions between strokes and character glyphs. DWE Model. As we mentioned earlier, it is reasonable and imperative to learn Chinese word embeddings from two channels, i.e., a sequential stroke n-gram channel and a spatial glyph channel. Inspired by the previous works BIBREF14 , BIBREF18 , BIBREF4 , BIBREF19 , we propose to combine the representation of Chinese words with the representation of characters to obtain finer-grained semantics, so that unknown words can be identified and", "in root-like strokes order, and the spatial information hidden in graph-like character glyphs. Along this line, we propose a novel Dual-channel Word Embedding (DWE) model for Chinese to realize the joint learning of sequential and spatial information in characters. Finally, we evaluate DWE on two representative tasks, where the experimental results exactly validate the superiority of DWE in capturing the morphological information of Chinese. Morphological Word Representations. Traditional methods on getting word embeddings are mainly based on the distributional hypothesis BIBREF9 : words with similar contexts tend to have similar semantics. To explore more interpretable models, some scholars have gradually noticed the importance of the morphology of words in conveying semantics BIBREF10 , BIBREF11 , and some studies have proved that the morphology of words can indeed enrich the semantics of word embeddings BIBREF12 , BIBREF13 , BIBREF2 . More recently, Wieting et al.", "Introduction. Word embeddings are fixed-length vector representations for words BIBREF0 , BIBREF1 . In recent years, the morphology of words is drawing more and more attention BIBREF2 , especially for Chinese whose writing system is based on logograms. UTF8gbsn With the gradual exploration of the semantic features of Chinese, scholars have found that not only words and characters are important semantic carriers, but also stroke feature of Chinese characters is crucial for inferring semantics BIBREF3 . Actually, a Chinese word usually consists of several characters, and each character can be further decomposed into a stroke sequence which is certain and changeless, and this kind of stroke sequence is very similar to the construction of English words. In Chinese, a particular sequence of strokes can reflect the inherent semantics. As shown in the upper half of Figure FIGREF3 , the Chinese character \u201c\u9a7e\" (drive) can be decomposed into a sequence of eight strokes, where the last three"]}
{"question_id": "0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "predicted_answer": "The model does not have flexibility in OOV words, and it assigns a zero vector to unknown words, affecting testing results. In order to minimize this problem, they passed the unknown words through a FastText model to get a vector from their subwords.", "predicted_evidence": ["Introduction. In the research field of Natural Language Processing (NLP) there are several tasks that contribute to understanding natural text. These tasks can manipulate natural language, such as tokenization process, and consequently can be used in other implementations, in order to extract syntactic or semantic information. One such task for syntactic components is Part of Speech Tagging (POS Tagging). Part of Speech Tagging in corpus linguistics is a process where a word is assigned with a label of the grammatical term, given the context it appears in. In many languages, POS Tagging models achieve an accuracy of 96 to 97 percent BIBREF0. Part of Speech Tagging for highly inflective languages, such as Greek is quite a difficult task. In the Greek Language, words can have different morphological forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek", "parameters that extracted the Common Crawl vectors. As a result, 140.000 vectors with 300 dimension were exported. It must be mentioned that the tagset with the morphological features was used. The values of the metrics in this case were almost as good and comparable to the CC ones. However, the model trained with a larger vocabulary had higher results. Also, the model with the dataset vectors did not have the flexibility to classify unknown words. As a next step, the test set of the dataset was altered by replacing words with syntactical mistakes to test the tolerance of the model in OOV words. Suffixes of verbs were altered and vowels were replaced with others, affecting 20% of the tokens of the dataset. Using again the more complex tagset for training, the results can be found in Table 3. What can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its", "3. What can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class. One disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4. It was noticed that the model performed better when using the vectors from different FastText models. It was expected that the second experiment would have performed better, as the tokens were inserted into the same FastText model and the vectors", "Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7. Creating a Greek POS Tagger using spaCy ::: POS Tagger training. The articles from the newspaper were fed in spaCy library into the proper format for training. Different parameters were tested, in order to get the optimal result. The dataset was shuffled, using the same seed for all the experiments and was split into a train set (70%), a test set (20%) and a validation set (10%). Information was passed through the training algorithm in batches with an increasing batch size from 4 to 32 and a step of 1.001. Additionally, a dropout rate was configured in every batch, initialized to 0.6", "of a set of articles and their position in a sentence, the lemma and the part of speech of every token. The various values of POS tags were retrieved and incorporated into a tag map. The labels and morphology they describe are explained below. Creating a Greek POS Tagger using spaCy ::: Creation of the Tag Map with reference to Universal Dependencies. Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the", "speech with the performance of the recognizer was explored. In this experiment, both pipelines (part of speech, entity recognition) were used for training with 30 iterations and the model was trained twice: with and without the usage of the part of speech information for recognition. It is evident that the recognizer did not gain knowledge from the part of speech tags of the tokens. Conclusions. Natural Language Processing meets numerous problems in its applications, especially in uncommon languages such as Greek. This paper proposes a machine learning approach to part-of-speech tagging and named entity recognition for Greek, a highly inflected language using spaCy, a very robust and popular framework. Although significant work has been done, there are several more things that can be accomplished. The need of more datasets for the Greek language is evident, but the results are quite satisfying, comparable to other languages.", "was passed through the training algorithm in batches with an increasing batch size from 4 to 32 and a step of 1.001. Additionally, a dropout rate was configured in every batch, initialized to 0.6 which dropped during the training process to 0.4. Most of the experiments were trained using 30 epochs. The main area of study for the experiments focuses on three important components. At first, we investigate the difference in results between part of speech taggers that classify morphological features and taggers that detect only the part of speech. Moreover, we explore the significance of pretrained vectors used from a model and their effect on the extraction of better results. Most importantly, the usage of subwords of tokens from a tagger as embeddings is issued. For the experiments, precision, recall and f1 score are used as evaluation metrics. Creating a Greek POS Tagger using spaCy ::: Evaluation and comparison of results. In the first experiment the model was trained using pretrained", "forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek POS Tagging BIBREF1. Another main task for extracting semantic information is Named Entity Recognition (NER). Named Entity Recognition is a process where a word or a set of words reference to a world object. Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX) BIBREF2. The greek Part of Speech Tagging and Named Entity Recognition models presented in this paper were developed using the spaCy library BIBREF3. SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based"]}
{"question_id": "6cad6f074b0486210ffa4982c8d1632f5aa91d91", "predicted_answer": "No.", "predicted_evidence": ["rules that the model will use. Different parameters in the configuration of spaCy's model were tested while training and their results are presented in SECREF6. Creating a Greek POS Tagger using spaCy ::: Dataset evaluation and selection. The dataset comes from texts of the Greek newspaper \u201cMakedonia\u201d. The articles in the newspaper are categorized in different subjects, such as sports, health, economy and political news. Data retrieval was done from the website of the clarin project BIBREF5 and consist of a set of xml files with information at paragraph, sentence and word level. It must be underlined that this annotation was performed by the Institute for Language and Speech Processing and data is licenced under the CC - BY - NC - SA licence. Information about the dataset includes the tokens of a set of articles and their position in a sentence, the lemma and the part of speech of every token. The various values of POS tags were retrieved and incorporated into a tag map. The labels", "forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek POS Tagging BIBREF1. Another main task for extracting semantic information is Named Entity Recognition (NER). Named Entity Recognition is a process where a word or a set of words reference to a world object. Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX) BIBREF2. The greek Part of Speech Tagging and Named Entity Recognition models presented in this paper were developed using the spaCy library BIBREF3. SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based", "recall and f1 score are used as evaluation metrics. Creating a Greek POS Tagger using spaCy ::: Evaluation and comparison of results. In the first experiment the model was trained using pretrained vectors extracted from two different sources, Common Crawl and Wikipedia and can be found at the official FastText web page BIBREF8. Both sources were trained on the same algorithm called FastText BIBREF9, an extension of Word2Vec that treats tokens as an average sum of sub-words and finds similarities of words based on their n-grams. The configuration of the FastText model for Wikipedia vectors is according to BIBREF10, whilst the model for CC vectors is a position-weight CBOW 5 length n-grams with a window size of 5 tokens and 10 negative words. The file with the Common Crawl vectors consists of 2.000.000 tokens with 300 dimension, whereas the file with the Wikipedia vectors consists of 300.000 tokens with 300 dimension.The results can be viewed in the following table, with the first part", "models are implemented using Thinc, spaCy\u2019s machine learning library. Creating a Greek POS Tagger using spaCy. The Institute for Language and Speech Processing was the first to implement a Part of Speech Tagger with morphological features and has evaluated the experiments in terms of the error rate of the predicted classes BIBREF4. These models can be accessed from web services offered by the Institute . However, the creation of a compound Greek POS tagger using spaCy, a fast and accurate NLP python framework is new. For the creation of a Part of Speech Tagger in the Greek Language a number of steps was followed. The tags from the \u201cMakedonia\u201d dataset, which is described below, were extracted and matched to a set of morphological rules. The tokens in the dataset were adjusted to annotation rules that the model will use. Different parameters in the configuration of spaCy's model were tested while training and their results are presented in SECREF6. Creating a Greek POS Tagger using", "when using the vectors from different FastText models. It was expected that the second experiment would have performed better, as the tokens were inserted into the same FastText model and the vectors exported from both sources should match. Creating a state of the art Named Entity Recognizer using spaCy. In BIBREF11 the development of an entity recognizer with named entities that follow a proper set of rules is described with evaluation metrics that reach 86% for precision and 81% for recall. Our implementation follows these rules as well. Also, a pretrained model is offered from a library called polyglot for recognition BIBREF12, which has evaluated NER in Greek with statistical machine translation. For the creation of a Named Entity Recognizer in the Greek Language a number of steps was followed. The entities from the \u201cMakedonia\u201d dataset were extracted and annotated, forming a set of keywords that matched a specific set of rules the entities had to follow. These keywords were used", "Finally, different experiments were conducted for evaluating the accuracy of the models. SpaCy's deep learning model for POS tagging and Named Entity Recognition. SpaCy uses a deep learning formula for implementing NLP models, summarised as \u201cembed, encode, attend, predict\u201d. In spaCy's approach text is inserted in the model in the form of unique numerical values (ID) for every input that can represent a token of a corpus or a class of the NLP task (part of speech tag, named entity class). At the embedding stage, features such as the prefix, the suffix, the shape and the lowercase form of a word are used for the extraction of hashed values that reflect word similarities. At this stage a vocabulary with hashed values and their vectors exist in the model. For the exploitation of adjacent vectors in the state of encoding, values pass through the Convolutional Neural Network (CNN) and get merged with their context. The result of the encoding process is a matrix of vectors that represents", "followed. The entities from the \u201cMakedonia\u201d dataset were extracted and annotated, forming a set of keywords that matched a specific set of rules the entities had to follow. These keywords were used to reform the dataset and also to find entities from a larger dataset, like Wikipedia. The spaCy model was trained using both datasets and their results are compared to a test set. Additionally, the spaCy model was trained using as a feature the POS tags of the tokens. All results are presented in SECREF13. Creating a state of the art Named Entity Recognizer using spaCy ::: Dataset evaluation and selection. In the \u201cMakedonia\u201d dataset information about named entities is organized with the index of the character the named entity starts, the index of the character the named entity ends and the class of the named entity. The dataset was parsed and the named entities were added into the keyword list, with every record representing the token (or the set of tokens) and its class. Noise was removed", "source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based models for the implementation of Natural Language Processing components that achieve state-of-the-art results in many of these tasks. In the following chapters the process for implementing Part of Speech Tagging and Named Entity Recognition for the Greek Language is explained. A dataset with extended POS Tags was found and matched to a set of morphological rules, according to a treebank. The dataset was then processed, fed to the spaCy model and used for training. Similarly, for Named Entity Recognition, datasets from different sources were compared to a custom set of rules for named entities. Finally, different experiments were conducted for evaluating the accuracy of the models. SpaCy's deep learning model for POS tagging and Named Entity Recognition. SpaCy uses a deep learning formula"]}
{"question_id": "d38b3e0896b105d171e69ce34c689e4a7e934522", "predicted_answer": "gender, number, and case", "predicted_evidence": ["of a set of articles and their position in a sentence, the lemma and the part of speech of every token. The various values of POS tags were retrieved and incorporated into a tag map. The labels and morphology they describe are explained below. Creating a Greek POS Tagger using spaCy ::: Creation of the Tag Map with reference to Universal Dependencies. Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the", "Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7. Creating a Greek POS Tagger using spaCy ::: POS Tagger training. The articles from the newspaper were fed in spaCy library into the proper format for training. Different parameters were tested, in order to get the optimal result. The dataset was shuffled, using the same seed for all the experiments and was split into a train set (70%), a test set (20%) and a validation set (10%). Information was passed through the training algorithm in batches with an increasing batch size from 4 to 32 and a step of 1.001. Additionally, a dropout rate was configured in every batch, initialized to 0.6", "forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek POS Tagging BIBREF1. Another main task for extracting semantic information is Named Entity Recognition (NER). Named Entity Recognition is a process where a word or a set of words reference to a world object. Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX) BIBREF2. The greek Part of Speech Tagging and Named Entity Recognition models presented in this paper were developed using the spaCy library BIBREF3. SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based", "of 2.000.000 tokens with 300 dimension, whereas the file with the Wikipedia vectors consists of 300.000 tokens with 300 dimension.The results can be viewed in the following table, with the first part describing the Common Crawl results and the second one the Wikipedia results. At the results, POS and morph classes refer to the tag labels explained in SECREF4, whilst only POS classes relate to annotated labels that describe only the part of speech. It is evident that even though the CC vectors are noisy, coming from a web source, they lead to better results than Wikipedia, possibly because they have a larger variety of tokens. In the next experiment, the dataset was used for the composition of embeddings for the part of speech tagger. The dataset was trained on a FastText model with the same parameters that extracted the Common Crawl vectors. As a result, 140.000 vectors with 300 dimension were exported. It must be mentioned that the tagset with the morphological features was used. The", "models are implemented using Thinc, spaCy\u2019s machine learning library. Creating a Greek POS Tagger using spaCy. The Institute for Language and Speech Processing was the first to implement a Part of Speech Tagger with morphological features and has evaluated the experiments in terms of the error rate of the predicted classes BIBREF4. These models can be accessed from web services offered by the Institute . However, the creation of a compound Greek POS tagger using spaCy, a fast and accurate NLP python framework is new. For the creation of a Part of Speech Tagger in the Greek Language a number of steps was followed. The tags from the \u201cMakedonia\u201d dataset, which is described below, were extracted and matched to a set of morphological rules. The tokens in the dataset were adjusted to annotation rules that the model will use. Different parameters in the configuration of spaCy's model were tested while training and their results are presented in SECREF6. Creating a Greek POS Tagger using", "Introduction. In the research field of Natural Language Processing (NLP) there are several tasks that contribute to understanding natural text. These tasks can manipulate natural language, such as tokenization process, and consequently can be used in other implementations, in order to extract syntactic or semantic information. One such task for syntactic components is Part of Speech Tagging (POS Tagging). Part of Speech Tagging in corpus linguistics is a process where a word is assigned with a label of the grammatical term, given the context it appears in. In many languages, POS Tagging models achieve an accuracy of 96 to 97 percent BIBREF0. Part of Speech Tagging for highly inflective languages, such as Greek is quite a difficult task. In the Greek Language, words can have different morphological forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek", "source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based models for the implementation of Natural Language Processing components that achieve state-of-the-art results in many of these tasks. In the following chapters the process for implementing Part of Speech Tagging and Named Entity Recognition for the Greek Language is explained. A dataset with extended POS Tags was found and matched to a set of morphological rules, according to a treebank. The dataset was then processed, fed to the spaCy model and used for training. Similarly, for Named Entity Recognition, datasets from different sources were compared to a custom set of rules for named entities. Finally, different experiments were conducted for evaluating the accuracy of the models. SpaCy's deep learning model for POS tagging and Named Entity Recognition. SpaCy uses a deep learning formula", "was passed through the training algorithm in batches with an increasing batch size from 4 to 32 and a step of 1.001. Additionally, a dropout rate was configured in every batch, initialized to 0.6 which dropped during the training process to 0.4. Most of the experiments were trained using 30 epochs. The main area of study for the experiments focuses on three important components. At first, we investigate the difference in results between part of speech taggers that classify morphological features and taggers that detect only the part of speech. Moreover, we explore the significance of pretrained vectors used from a model and their effect on the extraction of better results. Most importantly, the usage of subwords of tokens from a tagger as embeddings is issued. For the experiments, precision, recall and f1 score are used as evaluation metrics. Creating a Greek POS Tagger using spaCy ::: Evaluation and comparison of results. In the first experiment the model was trained using pretrained"]}
{"question_id": "4379a3ece3fdb93b71db43f62833f5f724c49842", "predicted_answer": "Unanswerable", "predicted_evidence": ["1 indicates that each tweet is unique and has been posted only once. As Figure 1 illustrates, a subset of \"bot\" users (in the training set) have posted more of the same tweets than \"non-bot\" users. URL score. During manual annotation, we found that \"bot\" users' tweets frequently contain URLs (e.g., advertisements for health-related products, such as medications), so we use the ratio of the number of a user's tweets containing a URL to the total number of tweets posted by the user. Mean Daily Posts. Considering that \"bot\" users may post tweets more frequently than \"non-bot\" users, we measured the average and standard deviation of the number of tweets posted daily by a user. As Figure 1 illustrates, a subset of \"bot\" users post, on average, more tweets daily than \"non-bot\" users. Topics. Considering that \"bot\" users may post tweets about a limited number of targeted topics, we used topic modeling to the measure the heterogeneity of topics in a user's tweets. We used Latent Dirichlet", "Considering that \"bot\" users may post tweets about a limited number of targeted topics, we used topic modeling to the measure the heterogeneity of topics in a user's tweets. We used Latent Dirichlet Allocation (LDA)BIBREF25 to extract the top five topics from all of the users' 1000 most recent tweets (or all the tweets if a user has posted less than 1000 tweets), and used the mean of the weights of each topic across all of a user's tweets. Mean Post Length. Considering that the length of tweets may be different between \"bot\" and \"non-bot\" users, we used the mean word length and standard deviation of a user's tweets. Profile Picture. In addition to tweet-related features, we used features based on information in users' profiles. Considering that a \"non-bot\" user's profile picture may be more likely to contain a face, we used a publicly available system to detect the number of faces in a profile picture. As Figure 2, illustrates a face was not detected in the profile picture of the", "realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18, BIBREF19. The only system for which we found both operational code and data available, Botometer BIBREF20 (formerly BotOrNot), was chosen as the benchmark system for this study. To the best of our knowledge, this paper presents the first study on health-related bot detection. We have made the classification code and training set of annotated users available at (we will provide a URL with the camera-ready version of the paper). Methods ::: Corpus. To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various", "no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4. When conducting user-level studies from social media, one challenge is to ascertain the credibility of the information posted. Particularly, it is important to verify, when deriving statistical estimates from user cohorts, that the user accounts represent", "the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting personal information. These users are particularly challenging to automatically identify as \"bot\" users because, with humans posting on behalf of an online maternity store, or to a pregnancy forum, for example, their tweets resemble those posted by \"non-bot\" users. In future work, we will focus on deriving features for modeling the nuances that distinguish such \"bot\" users. Conclusion. As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while", "challenge is to ascertain the credibility of the information posted. Particularly, it is important to verify, when deriving statistical estimates from user cohorts, that the user accounts represent humans and not bots (accounts that can be controlled to automatically produce content and interact with other profiles)BIBREF5, BIBREF6. Bots may spread false information by automatically retweeting posts without a human verifying the facts or to influence public opinions on particular topics on purpose BIBREF5, BIBREF7, BIBREF8. For example, a recent study BIBREF9 showed that the highest proportion of anti-vaccine content is generated by accounts with unknown or intermediate bot scores, meaning that the existing methods were not able to fully determine if they were indeed bots. Automatic bot detection techniques mostly rely on extracting features from users' profiles and their social networks BIBREF10, BIBREF11. Some studies have used Honeypot profiles on Twitter to identify and analyze", "create artificial instances of \"bot\" users in the training set. We also performed 5-fold cross validation over the training set to optimize parameters for the classifier; we used exponential as the loss function, set the number of estimators to 200, and set the learning rate to $0.1$. Third, we used the classifier with an extended set of features that are not used by Botometer. Based on our manual annotation, we consider the following features to be potentially informative for distinguishing \"bot\" and \"non-bot\" users in health-related data: Tweet Diversity. Considering that \"bot\" users may re-post the same tweets, we used the ratio of a user's unique tweets to the total number of tweets posted by the user, where 0 indicates that the user has posted only the same tweet multiple times, and 1 indicates that each tweet is unique and has been posted only once. As Figure 1 illustrates, a subset of \"bot\" users (in the training set) have posted more of the same tweets than \"non-bot\" users.", "the system with simple features derived from health-related data significantly improves performance. An F$_1$-score of $0.700$ for the \"bot\" class represents a promising benchmark for automatic classification of highly imbalanced Twitter data and, in this case, for detecting users who are not reporting information about their own pregnancy on Twitter. Detecting such users is particularly important in the process of automatically selecting cohortsBIBREF26 from a population of social media users for user-level observational studiesBIBREF27. A brief error analysis of the 25 false negatives users (in the held-out test set of 1652 users) from the classifier with the extended feature set reveals that, while only one of the users is an account that automatically re-posts other users' tweets, the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting"]}
{"question_id": "138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "predicted_answer": "By using a machine learning algorithm on top of the existing system and introducing more features to significantly improve bot detection performance in health-related data.", "predicted_evidence": ["presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while existing systems have been successful in detecting bots in other domains, they do not perform as well for detecting health-related bots. Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. Introducing more features would likely contribute to further improving performance, which we will explore in future work. Acknowledgments. This study was funded in part by the National Library of Medicine (NLM) (grant number: R01LM011176) and the National Institute on Drug Abuse (NIDA) (grant number: R01DA046619) of the National Institutes of Health (NIH). The content is solely the responsibility of the authors and does not necessarily represent the", "realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18, BIBREF19. The only system for which we found both operational code and data available, Botometer BIBREF20 (formerly BotOrNot), was chosen as the benchmark system for this study. To the best of our knowledge, this paper presents the first study on health-related bot detection. We have made the classification code and training set of annotated users available at (we will provide a URL with the camera-ready version of the paper). Methods ::: Corpus. To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various", "detection techniques mostly rely on extracting features from users' profiles and their social networks BIBREF10, BIBREF11. Some studies have used Honeypot profiles on Twitter to identify and analyze bots BIBREF12, while other studies have analyzed social proximity BIBREF13 or both social and content proximities BIBREF10, tweet timing intervals BIBREF14, or user-level content-based and graph-based features BIBREF15. However, in response to efforts towards keeping Twitter bot-free, bots have evolved and changed to overcome the detection techniques BIBREF16. The objectives of this study are to (i) evaluate an existing bot detection system on user-level datasets selected for their health-related content, and (ii) extend the bot detection system for effective application within the health realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18,", "the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting personal information. These users are particularly challenging to automatically identify as \"bot\" users because, with humans posting on behalf of an online maternity store, or to a pregnancy forum, for example, their tweets resemble those posted by \"non-bot\" users. In future work, we will focus on deriving features for modeling the nuances that distinguish such \"bot\" users. Conclusion. As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while", "The F$_1$-score for the \"bot\" class indicates that Botometer ($0.361$), designed for political bot detection, does not generalize well for detecting \"bot\" users in health-related data. Although the classifier with only the Botometer score as a feature ($0.286$) performs even worse than the default Botometer system, our extended feature set significantly improves performance ($0.700$). For imbalanced data, a higher F$_1$-score for the majority class is typical; in this case, it reflects that we have modeled the detection of \"bot\" users based on their natural distribution in health-related data. Discussion. Our results demonstrate that (i) a publicly available bot detection system, designed for political bot detection, underperforms when applied to health-related data, and (ii) extending the system with simple features derived from health-related data significantly improves performance. An F$_1$-score of $0.700$ for the \"bot\" class represents a promising benchmark for automatic", "more likely to contain a face, we used a publicly available system to detect the number of faces in a profile picture. As Figure 2, illustrates a face was not detected in the profile picture of the majority of \"non-bot\" users (in the training set), whereas at least one face was detected in the profile picture of the majority of \"bot\" users. User Name. Finally, we used a publicly available lexicon to detect the presence or absence of a person's name in a user name. As Figure 2 illustrates, the name of a person is present (1) in approximately half of \"non-bot\" user names, whereas the name of a person is absent (0) in the majority of \"bot\" user names. Results. Table 1 presents the precision, recall, and F$_1$-scores for the three bot detection systems evaluated on the held-out test set. The F$_1$-score for the \"bot\" class indicates that Botometer ($0.361$), designed for political bot detection, does not generalize well for detecting \"bot\" users in health-related data. Although the", "challenge is to ascertain the credibility of the information posted. Particularly, it is important to verify, when deriving statistical estimates from user cohorts, that the user accounts represent humans and not bots (accounts that can be controlled to automatically produce content and interact with other profiles)BIBREF5, BIBREF6. Bots may spread false information by automatically retweeting posts without a human verifying the facts or to influence public opinions on particular topics on purpose BIBREF5, BIBREF7, BIBREF8. For example, a recent study BIBREF9 showed that the highest proportion of anti-vaccine content is generated by accounts with unknown or intermediate bot scores, meaning that the existing methods were not able to fully determine if they were indeed bots. Automatic bot detection techniques mostly rely on extracting features from users' profiles and their social networks BIBREF10, BIBREF11. Some studies have used Honeypot profiles on Twitter to identify and analyze", "the system with simple features derived from health-related data significantly improves performance. An F$_1$-score of $0.700$ for the \"bot\" class represents a promising benchmark for automatic classification of highly imbalanced Twitter data and, in this case, for detecting users who are not reporting information about their own pregnancy on Twitter. Detecting such users is particularly important in the process of automatically selecting cohortsBIBREF26 from a population of social media users for user-level observational studiesBIBREF27. A brief error analysis of the 25 false negatives users (in the held-out test set of 1652 users) from the classifier with the extended feature set reveals that, while only one of the users is an account that automatically re-posts other users' tweets, the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting"]}
{"question_id": "7e906dc00e92088a25df3719104d1750e5a27485", "predicted_answer": "adverse drug reaction detection, syndromic surveillance, subject recruitment for cancer trials, characterizing drug abuse and more.", "predicted_evidence": ["Introduction. In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject", "no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4. When conducting user-level studies from social media, one challenge is to ascertain the credibility of the information posted. Particularly, it is important to verify, when deriving statistical estimates from user cohorts, that the user accounts represent", "the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting personal information. These users are particularly challenging to automatically identify as \"bot\" users because, with humans posting on behalf of an online maternity store, or to a pregnancy forum, for example, their tweets resemble those posted by \"non-bot\" users. In future work, we will focus on deriving features for modeling the nuances that distinguish such \"bot\" users. Conclusion. As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while", "realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18, BIBREF19. The only system for which we found both operational code and data available, Botometer BIBREF20 (formerly BotOrNot), was chosen as the benchmark system for this study. To the best of our knowledge, this paper presents the first study on health-related bot detection. We have made the classification code and training set of annotated users available at (we will provide a URL with the camera-ready version of the paper). Methods ::: Corpus. To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various", "presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while existing systems have been successful in detecting bots in other domains, they do not perform as well for detecting health-related bots. Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. Introducing more features would likely contribute to further improving performance, which we will explore in future work. Acknowledgments. This study was funded in part by the National Library of Medicine (NLM) (grant number: R01LM011176) and the National Institute on Drug Abuse (NIDA) (grant number: R01DA046619) of the National Institutes of Health (NIH). The content is solely the responsibility of the authors and does not necessarily represent the", "detection techniques mostly rely on extracting features from users' profiles and their social networks BIBREF10, BIBREF11. Some studies have used Honeypot profiles on Twitter to identify and analyze bots BIBREF12, while other studies have analyzed social proximity BIBREF13 or both social and content proximities BIBREF10, tweet timing intervals BIBREF14, or user-level content-based and graph-based features BIBREF15. However, in response to efforts towards keeping Twitter bot-free, bots have evolved and changed to overcome the detection techniques BIBREF16. The objectives of this study are to (i) evaluate an existing bot detection system on user-level datasets selected for their health-related content, and (ii) extend the bot detection system for effective application within the health realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18,", "challenge is to ascertain the credibility of the information posted. Particularly, it is important to verify, when deriving statistical estimates from user cohorts, that the user accounts represent humans and not bots (accounts that can be controlled to automatically produce content and interact with other profiles)BIBREF5, BIBREF6. Bots may spread false information by automatically retweeting posts without a human verifying the facts or to influence public opinions on particular topics on purpose BIBREF5, BIBREF7, BIBREF8. For example, a recent study BIBREF9 showed that the highest proportion of anti-vaccine content is generated by accounts with unknown or intermediate bot scores, meaning that the existing methods were not able to fully determine if they were indeed bots. Automatic bot detection techniques mostly rely on extracting features from users' profiles and their social networks BIBREF10, BIBREF11. Some studies have used Honeypot profiles on Twitter to identify and analyze", "the system with simple features derived from health-related data significantly improves performance. An F$_1$-score of $0.700$ for the \"bot\" class represents a promising benchmark for automatic classification of highly imbalanced Twitter data and, in this case, for detecting users who are not reporting information about their own pregnancy on Twitter. Detecting such users is particularly important in the process of automatically selecting cohortsBIBREF26 from a population of social media users for user-level observational studiesBIBREF27. A brief error analysis of the 25 false negatives users (in the held-out test set of 1652 users) from the classifier with the extended feature set reveals that, while only one of the users is an account that automatically re-posts other users' tweets, the majority of the errors can be attributed to our broad definition of \"bot\" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting"]}
{"question_id": "0d9241e904bd2bbf5b9a6ed7b5fc929651d3e28e", "predicted_answer": "Yes, customer support on Twitter.", "predicted_evidence": ["conclusions we'll draw don't have to be limited to this field. Table TABREF1 gives an overview of frequently used and interesting conversational datasets. The lacuna of emoji-rich reference datasets was already mentioned in Section SECREF1 and is in our opinion one of the factors that emoji remain fairly underutilized. For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances. The tweets were filtered on hyper links and personal identifiers, but Unicode emoji characters were preserved. As emoji are frequently used on Twitter, this", "our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric. Note that 1-in-100 accuracy gives a summary of the model performance for a particular dataset. Since not all 99 negative responses are necessarily bad choices, the resulting score is in part dependent on the prior distribution of a dataset. For example, BIBREF26 compares models for three datasets, where the best performing model has a score of 30.6 for OpenSubtitles BIBREF22 and 84.2 for AmazonQA BIBREF21. Aside from the 1-of-100 accuracy, we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out", "we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out of $N=100$, can differentiate whether or not the model is still selecting good responses. For each input sentence, a rank of 1 means the positive response is ranked highest and is thus correctly selected and a rank of $N$ signifies the positive response was\u2014incorrectly\u2014considered the worst-matching candidate. Emoji provide additional context to response selection models. After training of the language model with additional tokens for all Unicode emoji, we achieved a final perplexity of 2.0281. For comparison, the BERT model with 16 heads achieved a perplexity of 3.23 BIBREF5, but this is on a general dataset. For the sentence prediction task, Table TABREF11 shows the results of the baseline and our model with additional emoji tokens. For each of the 600 utterance pairs of the held-out test set, we", "relatively often, 8.75% of all utterances. The tweets were filtered on hyper links and personal identifiers, but Unicode emoji characters were preserved. As emoji are frequently used on Twitter, this resulted in a dataset with 170 of the 2000 tuples containing at least one emoji character. Fine-tuning BERT with emoji support. We continue training of a multilingual BERT model BIBREF5 with new tokens for emoji and fine-tune this model and a baseline on the dataset discussed in Section SECREF4. This approach is explained in Subsection SECREF4 and the training itself is discussed in Subsection SECREF6. At last, the evaluation is then discussed in Subsection SECREF10. Fine-tuning BERT with emoji support ::: Tokenizing emoji. We add new tokens to the BERT tokenizer for 2740 emoji from the Unicode Full Emoji List BIBREF0, as well as some aliases (in the form of :happy: as is a common notation for emoji). In total, 3627 emoji tokens are added to the vocabulary. We converted all UTF-8 encoded", "settings and with binary cross entropy. In this case, the training was limited to 10 epochs. To mitigate the need for weighting and other class imbalance issues, we trained with pairs of positive and negative candidates. This is in contract to the evaluation, where 99 negative candidates are used. However, since each candidate is considered on its own merit during evaluation, this discrepancy won't affect the performance. For the formulation of the fine-tuning task, we use the same approach as BIBREF5. The first input sentence is joined with the second sentence, separated by a special [SEP] token, as can be seen in Figure FIGREF5. The model, with a specialized head for next sentence prediction, then outputs a correlation score. Fine-tuning BERT with emoji support ::: evaluation metrics. Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system", "For the sentence prediction task, Table TABREF11 shows the results of the baseline and our model with additional emoji tokens. For each of the 600 utterance pairs of the held-out test set, we added 99 randomly selected negative candidates, as described in Subsection SECREF10. The 1-out-of-100 accuracy measures how often the true candidate was correctly selected and the mean rank gives an indication of how the model performs if it fails to correctly select the positive candidate. The baseline correctly picks 12.7% of all candidate responses, out of 100. Given that the dataset is focussed on support questions and multiple responses are likely to be relevant, this baseline already performs admirable. For reference, a BERT model on the OpenSubtitles dataset BIBREF22 achieves a 1-of-100 accuracy between 12.2% and 17.5%, depending on the model size BIBREF26. Our model improves on this baseline with a 1-of-100 accuracy of 17.8%. The mean rank remains almost the same. This indicates that the", "free-form responses, generative models like GPT BIBREF30, GPT-2 BIBREF6 or seq2seq BIBREF31 are appropriate. When the conversational task is modeled as a response selection task to pick the correct response out of $N$ candidates BIBREF32, BIBREF26, BIBREF33, this can be a language model like BERT BIBREF5 with a dedicated head. Emoji-rich datasets are hard to find. Emoji are commonly used in casual settings, like on social media or in casual conversations. In conversations\u2014as opposed to relatively context-free social media posts\u2014an emoji alone can be an utterance by itself. And with a direct impact for some applications, like customer support, we focus on conversational datasets. We hope the conversational community has the most direct benefit from these emoji-enabled models. Of course, the conclusions we'll draw don't have to be limited to this field. Table TABREF1 gives an overview of frequently used and interesting conversational datasets. The lacuna of emoji-rich reference datasets", "AI makers have to choose between dataset size and emoji support, with some datasets at least containing a few textual emoticons. We argued that this duality results in systems that fail to capture some information encoded in those emoji and in turn fail to respond adequately. Based on this premise, we investigated how a response selection system based on BERT can be modified to support emoji. We proposed a format and tokenization method that's indifferent to current Unicode specifications, and thus also works for datasets containing custom emoji. Evaluation of this emoji-aware system increased the 1-of-100 accuracy from 12.7% for the baseline to 17.8%. Thus showing that supporting emoji correctly can help increasing performance for more casual systems, without having to rely on labeling or external descriptions for those emoji. However, the lack of high-quality, general datasets with emoji limits our conversational model. Working towards larger casual conversational datasets would"]}
{"question_id": "95646d0ac798dcfc15b43fa97a1908df9f7b9681", "predicted_answer": "The baseline model is not specified.", "predicted_evidence": ["For the sentence prediction task, Table TABREF11 shows the results of the baseline and our model with additional emoji tokens. For each of the 600 utterance pairs of the held-out test set, we added 99 randomly selected negative candidates, as described in Subsection SECREF10. The 1-out-of-100 accuracy measures how often the true candidate was correctly selected and the mean rank gives an indication of how the model performs if it fails to correctly select the positive candidate. The baseline correctly picks 12.7% of all candidate responses, out of 100. Given that the dataset is focussed on support questions and multiple responses are likely to be relevant, this baseline already performs admirable. For reference, a BERT model on the OpenSubtitles dataset BIBREF22 achieves a 1-of-100 accuracy between 12.2% and 17.5%, depending on the model size BIBREF26. Our model improves on this baseline with a 1-of-100 accuracy of 17.8%. The mean rank remains almost the same. This indicates that the", "settings and with binary cross entropy. In this case, the training was limited to 10 epochs. To mitigate the need for weighting and other class imbalance issues, we trained with pairs of positive and negative candidates. This is in contract to the evaluation, where 99 negative candidates are used. However, since each candidate is considered on its own merit during evaluation, this discrepancy won't affect the performance. For the formulation of the fine-tuning task, we use the same approach as BIBREF5. The first input sentence is joined with the second sentence, separated by a special [SEP] token, as can be seen in Figure FIGREF5. The model, with a specialized head for next sentence prediction, then outputs a correlation score. Fine-tuning BERT with emoji support ::: evaluation metrics. Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system", "our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric. Note that 1-in-100 accuracy gives a summary of the model performance for a particular dataset. Since not all 99 negative responses are necessarily bad choices, the resulting score is in part dependent on the prior distribution of a dataset. For example, BIBREF26 compares models for three datasets, where the best performing model has a score of 30.6 for OpenSubtitles BIBREF22 and 84.2 for AmazonQA BIBREF21. Aside from the 1-of-100 accuracy, we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out", "or external descriptions for those emoji. However, the lack of high-quality, general datasets with emoji limits our conversational model. Working towards larger casual conversational datasets would help both for our model, and for the conversational NLP community in general. We investigated the impact of emoji for conversational models and one could argue that these conclusions\u2014or even the BERT model\u2014can be generalized. We didn't investigate whether other tasks also benefited from our fine-tuned BERT model with the additional emoji tokens. During evaluation, we also observed utterances with only emoji characters. Even with our model that supports emoji, it could still be difficult to extract information like the subject of a conversation. Some of these utterances\u2014but not all\u2014were part of a larger conversation, so an interesting question could be how additional information affects the model.  Acknowledgements. This work was supported by the Research Foundation - Flanders under EOS No.", "end of the embeddings matrix. We then continue training on the language modeling task. We use the default configuration as is also used by BIBREF5 where randomly selected tokens are replaced by: a mask token: 80% chance, another random word: 10% chance, the original word: 10% chance. This model is trained for 100 epochs with the Adam BIBREF35 optimizer. The learning rate is set to the commonly used $lr=5\\cdot 10^{-5}$ and $\\epsilon = 10^{-8}$. No hyper-parameter tuning was done, as the results are acceptable on their own and are sufficient to allow conclusions for this paper. The loss is cross entropy BIBREF36. We then fine-tune both models, with and without emoji tokenization, on the sentence prediction task with a training set of 70%. We again use the Adam optimizer with the same settings and with binary cross entropy. In this case, the training was limited to 10 epochs. To mitigate the need for weighting and other class imbalance issues, we trained with pairs of positive and", "between 12.2% and 17.5%, depending on the model size BIBREF26. Our model improves on this baseline with a 1-of-100 accuracy of 17.8%. The mean rank remains almost the same. This indicates that the emoji tokens do help with with picking the correct response, but don't really aide when selecting alternative suitable candidates. One possible explanation is that when emoji are used (this is the case for 8.75% of all utterances), including those tokens helps matching those based on those emoji and their meaning. When there are no emoji present, our model might be just as clueless as the baseline. Conclusion and future work. In this paper we discussed the current state of emoji usage for conversational systems, which mainly lacks large baseline datasets. When looking at public datasets, conversational AI makers have to choose between dataset size and emoji support, with some datasets at least containing a few textual emoticons. We argued that this duality results in systems that fail to", "use\u2014or rather non-existing use\u2014of emoji in the tokenizing, training, and the datasets themselves is insufficient. Secondly, we attempt to quantify the significance of incorporating emoji-based features by presenting a fine-tuned model. We then compare this model to a baseline, but without special attention to emoji. Section SECREF2 will start with an overview of work on emoji representations, emoji-based models and analysis of emoji usage. A brief introduction in conversational systems will also be given. Section SECREF3 will then look into popular datasets with and without emoji and then introduce the dataset we used. Our model will then be discussed in Section SECREF4, including the tokenization in Subsection SECREF4, training setup in Subsection SECREF6 and evaluation in Subsection SECREF10. This brings us to the results of our experiment, which is discussed in Section SECREF5 and finally our conclusion and future work are presented in Section SECREF6. Related work. Inspired by the", "a larger conversation, so an interesting question could be how additional information affects the model.  Acknowledgements. This work was supported by the Research Foundation - Flanders under EOS No. 30992574."]}
{"question_id": "12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "predicted_answer": "Unanswerable", "predicted_evidence": ["Take for example the Ubuntu Dialog Corpus by BIBREF3, a commonly used corpus for multi-turn systems. This dataset was collected from an Internet Relay Chat (IRC) room casually discussing the operating system Ubuntu. IRC nodes usually support the ASCII text encoding, so there's no support for graphical emoji. However, in the 7,189,051 utterances, there are only 9946 happy emoticons (i.e. :-) and the cruelly denosed :) version) and 2125 sad emoticons. Word embeddings are also handling emoji poorly: Word2vec BIBREF4 with the commonly used pre-trained Google News vectors doesn't support the graphical emoji at all and vectors for textual emoticons are inconsistent. As another example with contextualized word embeddings, there are also no emoji or textual emoticons in the vocabulary list of BERT BIBREF5 by default and support for emoji is only recently added to the tokenizer. The same is true for GPT-2 BIBREF6. As all downstream systems, ranging from multilingual r\u00e9sum\u00e9 parsing to fallacy", "a larger conversation, so an interesting question could be how additional information affects the model.  Acknowledgements. This work was supported by the Research Foundation - Flanders under EOS No. 30992574.", "we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out of $N=100$, can differentiate whether or not the model is still selecting good responses. For each input sentence, a rank of 1 means the positive response is ranked highest and is thus correctly selected and a rank of $N$ signifies the positive response was\u2014incorrectly\u2014considered the worst-matching candidate. Emoji provide additional context to response selection models. After training of the language model with additional tokens for all Unicode emoji, we achieved a final perplexity of 2.0281. For comparison, the BERT model with 16 heads achieved a perplexity of 3.23 BIBREF5, but this is on a general dataset. For the sentence prediction task, Table TABREF11 shows the results of the baseline and our model with additional emoji tokens. For each of the 600 utterance pairs of the held-out test set, we", "Introduction. The prevalent use of emoji\u2014and their text-based precursors\u2014is mostly unaddressed in current natural language processing (NLP) tasks. The support of the Unicode Standard BIBREF0 for emoji characters in 2010 ushered in a wide-spread, international adoption of these graphical elements in casual contexts. Interpreting the meaning of these characters has been challenging however, since they take on multiple semantic roles BIBREF1. Whether or not emoji are used depends on the context of a text or conversation, with more formal settings generally being less tolerating. So is the popular aligned corpus Europarl BIBREF2 naturally devoid of emoji. Technical limitations, like no Unicode support, also limit its use. This in turn affects commonly used corpora, tokenizers, and pre-trained networks. Take for example the Ubuntu Dialog Corpus by BIBREF3, a commonly used corpus for multi-turn systems. This dataset was collected from an Internet Relay Chat (IRC) room casually discussing the", "or external descriptions for those emoji. However, the lack of high-quality, general datasets with emoji limits our conversational model. Working towards larger casual conversational datasets would help both for our model, and for the conversational NLP community in general. We investigated the impact of emoji for conversational models and one could argue that these conclusions\u2014or even the BERT model\u2014can be generalized. We didn't investigate whether other tasks also benefited from our fine-tuned BERT model with the additional emoji tokens. During evaluation, we also observed utterances with only emoji characters. Even with our model that supports emoji, it could still be difficult to extract information like the subject of a conversation. Some of these utterances\u2014but not all\u2014were part of a larger conversation, so an interesting question could be how additional information affects the model.  Acknowledgements. This work was supported by the Research Foundation - Flanders under EOS No.", "between 12.2% and 17.5%, depending on the model size BIBREF26. Our model improves on this baseline with a 1-of-100 accuracy of 17.8%. The mean rank remains almost the same. This indicates that the emoji tokens do help with with picking the correct response, but don't really aide when selecting alternative suitable candidates. One possible explanation is that when emoji are used (this is the case for 8.75% of all utterances), including those tokens helps matching those based on those emoji and their meaning. When there are no emoji present, our model might be just as clueless as the baseline. Conclusion and future work. In this paper we discussed the current state of emoji usage for conversational systems, which mainly lacks large baseline datasets. When looking at public datasets, conversational AI makers have to choose between dataset size and emoji support, with some datasets at least containing a few textual emoticons. We argued that this duality results in systems that fail to", "our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric. Note that 1-in-100 accuracy gives a summary of the model performance for a particular dataset. Since not all 99 negative responses are necessarily bad choices, the resulting score is in part dependent on the prior distribution of a dataset. For example, BIBREF26 compares models for three datasets, where the best performing model has a score of 30.6 for OpenSubtitles BIBREF22 and 84.2 for AmazonQA BIBREF21. Aside from the 1-of-100 accuracy, we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out", "settings and with binary cross entropy. In this case, the training was limited to 10 epochs. To mitigate the need for weighting and other class imbalance issues, we trained with pairs of positive and negative candidates. This is in contract to the evaluation, where 99 negative candidates are used. However, since each candidate is considered on its own merit during evaluation, this discrepancy won't affect the performance. For the formulation of the fine-tuning task, we use the same approach as BIBREF5. The first input sentence is joined with the second sentence, separated by a special [SEP] token, as can be seen in Figure FIGREF5. The model, with a specialized head for next sentence prediction, then outputs a correlation score. Fine-tuning BERT with emoji support ::: evaluation metrics. Finally, our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system"]}
{"question_id": "647f6e6b168ec38fcdb737d3b276f78402282f9d", "predicted_answer": "Ubuntu Dialog Corpus", "predicted_evidence": ["Take for example the Ubuntu Dialog Corpus by BIBREF3, a commonly used corpus for multi-turn systems. This dataset was collected from an Internet Relay Chat (IRC) room casually discussing the operating system Ubuntu. IRC nodes usually support the ASCII text encoding, so there's no support for graphical emoji. However, in the 7,189,051 utterances, there are only 9946 happy emoticons (i.e. :-) and the cruelly denosed :) version) and 2125 sad emoticons. Word embeddings are also handling emoji poorly: Word2vec BIBREF4 with the commonly used pre-trained Google News vectors doesn't support the graphical emoji at all and vectors for textual emoticons are inconsistent. As another example with contextualized word embeddings, there are also no emoji or textual emoticons in the vocabulary list of BERT BIBREF5 by default and support for emoji is only recently added to the tokenizer. The same is true for GPT-2 BIBREF6. As all downstream systems, ranging from multilingual r\u00e9sum\u00e9 parsing to fallacy", "Introduction. The prevalent use of emoji\u2014and their text-based precursors\u2014is mostly unaddressed in current natural language processing (NLP) tasks. The support of the Unicode Standard BIBREF0 for emoji characters in 2010 ushered in a wide-spread, international adoption of these graphical elements in casual contexts. Interpreting the meaning of these characters has been challenging however, since they take on multiple semantic roles BIBREF1. Whether or not emoji are used depends on the context of a text or conversation, with more formal settings generally being less tolerating. So is the popular aligned corpus Europarl BIBREF2 naturally devoid of emoji. Technical limitations, like no Unicode support, also limit its use. This in turn affects commonly used corpora, tokenizers, and pre-trained networks. Take for example the Ubuntu Dialog Corpus by BIBREF3, a commonly used corpus for multi-turn systems. This dataset was collected from an Internet Relay Chat (IRC) room casually discussing the", "we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out of $N=100$, can differentiate whether or not the model is still selecting good responses. For each input sentence, a rank of 1 means the positive response is ranked highest and is thus correctly selected and a rank of $N$ signifies the positive response was\u2014incorrectly\u2014considered the worst-matching candidate. Emoji provide additional context to response selection models. After training of the language model with additional tokens for all Unicode emoji, we achieved a final perplexity of 2.0281. For comparison, the BERT model with 16 heads achieved a perplexity of 3.23 BIBREF5, but this is on a general dataset. For the sentence prediction task, Table TABREF11 shows the results of the baseline and our model with additional emoji tokens. For each of the 600 utterance pairs of the held-out test set, we", "free-form responses, generative models like GPT BIBREF30, GPT-2 BIBREF6 or seq2seq BIBREF31 are appropriate. When the conversational task is modeled as a response selection task to pick the correct response out of $N$ candidates BIBREF32, BIBREF26, BIBREF33, this can be a language model like BERT BIBREF5 with a dedicated head. Emoji-rich datasets are hard to find. Emoji are commonly used in casual settings, like on social media or in casual conversations. In conversations\u2014as opposed to relatively context-free social media posts\u2014an emoji alone can be an utterance by itself. And with a direct impact for some applications, like customer support, we focus on conversational datasets. We hope the conversational community has the most direct benefit from these emoji-enabled models. Of course, the conclusions we'll draw don't have to be limited to this field. Table TABREF1 gives an overview of frequently used and interesting conversational datasets. The lacuna of emoji-rich reference datasets", "or external descriptions for those emoji. However, the lack of high-quality, general datasets with emoji limits our conversational model. Working towards larger casual conversational datasets would help both for our model, and for the conversational NLP community in general. We investigated the impact of emoji for conversational models and one could argue that these conclusions\u2014or even the BERT model\u2014can be generalized. We didn't investigate whether other tasks also benefited from our fine-tuned BERT model with the additional emoji tokens. During evaluation, we also observed utterances with only emoji characters. Even with our model that supports emoji, it could still be difficult to extract information like the subject of a conversation. Some of these utterances\u2014but not all\u2014were part of a larger conversation, so an interesting question could be how additional information affects the model.  Acknowledgements. This work was supported by the Research Foundation - Flanders under EOS No.", "a larger conversation, so an interesting question could be how additional information affects the model.  Acknowledgements. This work was supported by the Research Foundation - Flanders under EOS No. 30992574.", "our model is compared against the pre-trained version of BERT without special emoji tokens. We evaluate both this baseline and our model as a response selection task. In this case, the system has to select the most appropriate response out $N=100$ candidates. This is a more restricted problem, where the 1-of-100 accuracy BIBREF26 is a popular evaluation metric. Note that 1-in-100 accuracy gives a summary of the model performance for a particular dataset. Since not all 99 negative responses are necessarily bad choices, the resulting score is in part dependent on the prior distribution of a dataset. For example, BIBREF26 compares models for three datasets, where the best performing model has a score of 30.6 for OpenSubtitles BIBREF22 and 84.2 for AmazonQA BIBREF21. Aside from the 1-of-100 accuracy, we also present the mean rank of the correct response. Since the Twitter dataset is focussed on customer service, the correct response is sometimes similar to others. The mean rank, also out", "are inclined to use representing emoji for themselves. BIBREF19 reported that no negative sentiment was associated with specific skin tone modifiers. Related work ::: Conversational AI systems. The research on conversational AI has been focussing on various aspects, including building high-quality datasets BIBREF3, BIBREF25, BIBREF22, BIBREF23, BIBREF26, BIBREF27, adding customizable personalities BIBREF23, BIBREF28, BIBREF29 or conjoining the efforts with regard to different datasets, models and evaluation practices BIBREF26. With these combined efforts, businesses and the general public quickly began developing ambitious use-cases, like customer support agents on social networks. The proposed models in this field are diverse and largely depending on how the problem is formulated. When considering free-form responses, generative models like GPT BIBREF30, GPT-2 BIBREF6 or seq2seq BIBREF31 are appropriate. When the conversational task is modeled as a response selection task to pick the"]}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "predicted_answer": "Light Gradient Boosting Machine (LGBM) algorithm.", "predicted_evidence": ["the BioBERT embedding. In addition, the performance of the PIO classifier was measured by averaging the three Area Under Receiver Operating Characteristic Curve (ROC_AUC) scores for P, I, and O. The ROC_AUC score of 0.9951 was obtained by the model using the general BERT embedding. This score was improved to 0.9971 when using the BioBERT model pre-trained on medical context. The results are illustrated in Figure FIGREF9 . Model Boosting . We further applied ensemble methods to enhance the model. This approach consists of combining predictions from base classifiers with features of the input data to increase the accuracy of the model BIBREF17 . We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the", "dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on the excluded one and repeat the process for all five folds. The results of the LGBM classifier for the different boosting frameworks and the scores from the base classifiers are illustrated in Table TABREF14 . The highest average ROC_AUC score of 0.9998 is obtained in the case of combining the two base learners along with the TF-IDF and QIEF features. Discussion and Conclusion. In this paper, we presented an improved methodology to extract PIO elements, with reduced ambiguity, from abstracts of medical papers. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the", "which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM. We train the base classifier using the original training dataset, using INLINEFORM0 of the whole data as training dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on", "We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the classifier. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context. In order to enhance the accuracy of the model, we investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to optimize the classification by learning a linear combination of the predicted probabilities, for the three classes, with the TF-IDF and QIEF scores. The results indicate that these text features were adequate for boosting the contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in", "problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 . These models are shallow and limited in terms of modeling capacity. Furthermore, most of these classifiers are trained to extract PIO elements one by one which is sub-optimal since this approach does not allow the use of shared structure among the individual classifiers. Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To", ", BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 . Datasets. In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the", "structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment. Clinical questions are formed using the PIO framework, where clinical issues are broken down into four components: Population/Problem (P), Intervention (I), Comparator (C), and Outcome (O). We will refer to these categories as PIO elements, by using the common practice of merging the C and I categories. In BIBREF3 a literature screening performed in 10 systematic reviews was studied. It was found that using the PIO framework can significantly improve literature screening efficacy. Therefore, efficient extraction of PIO elements is a key feature of many EBM applications and could be thought of as a multi-label sentence classification problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 .", ". Using this framework, we trained the model using the two pretrained embedding models described in the previous section. It is worth to mention that the embedding is contextualized during the training phase. For both models, the pretrained embedding layer is frozen during the first epoch (the embedding vectors are not updated). After the first epoch, the embedding layer is unfrozen and the vectors are fine-tuned for the classification task during training. The advantage of this approach is that few parameters need to be learned from scratch BIBREF16 , BIBREF11 , BIBREF9 . Performance Comparison. In order to quantify the performance of the classification model, we computed the precision and recall scores. On average, it was found that the model leads to better results when trained using the BioBERT embedding. In addition, the performance of the PIO classifier was measured by averaging the three Area Under Receiver Operating Characteristic Curve (ROC_AUC) scores for P, I, and O. The"]}
{"question_id": "ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "predicted_answer": "Yes. They used BioBERT in addition to the original BERT release.", "predicted_evidence": ["abstract sections can be poor candidates for the corresponding label. For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by taking sentences from sections with headings such as aim. Furthermore, we cleaned the remaining data with various approaches including, but not limited to, language identification, removal of missing values, cleaning unicode characters, and filtering for sequences between 5 and 200 words, inclusive. Background. BERT (Bidirectional Encoder Representations from Transformers) is a deep bidirectional attention text embedding model. The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer BIBREF13 , BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks. Since its release, BERT has", ", BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks. Since its release, BERT has been pre-trained on a multitude of corpora. In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release BIBREF9 . This model is pre-trained on the BooksCorpus (800M words) BIBREF14 and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT BIBREF15 , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words). The Model. The classification model is built on top of the BERT representation by adding a dense layer corresponding to the multi-label classifier with three output neurons corresponding to PIO labels. In order to insure that independent probabilities are assigned to the labels, as a loss", "problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 . These models are shallow and limited in terms of modeling capacity. Furthermore, most of these classifiers are trained to extract PIO elements one by one which is sub-optimal since this approach does not allow the use of shared structure among the individual classifiers. Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To", "We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the classifier. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context. In order to enhance the accuracy of the model, we investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to optimize the classification by learning a linear combination of the predicted probabilities, for the three classes, with the TF-IDF and QIEF scores. The results indicate that these text features were adequate for boosting the contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in", "dense layer corresponding to the multi-label classifier with three output neurons corresponding to PIO labels. In order to insure that independent probabilities are assigned to the labels, as a loss function we have chosen the binary cross entropy with logits (BCEWithLogits) defined by DISPLAYFORM0  where t and y are the target and output vectors, respectively; n is the number of independent targets (n=3). The outputs are computed by applying the logistic function to the weighted sums of the last hidden layer activations, s, DISPLAYFORM0 DISPLAYFORM1  For the original BERT model, we have chosen the smallest uncased model, Bert-Base. The model has 12 attention layers and all texts are converted to lowercase by the tokenizer BIBREF9 . The architecture of the model is illustrated in Figure FIGREF7 . Using this framework, we trained the model using the two pretrained embedding models described in the previous section. It is worth to mention that the embedding is contextualized during the", "methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative. Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study", "Introduction. Evidence-based medicine (EBM) is of primary importance in the medical field. Its goal is to present statistical analyses of issues of clinical focus based on retrieving and analyzing numerous papers in the medical literature BIBREF0 . The PubMed database is one of the most commonly used databases in EBM BIBREF1 . Biomedical papers, describing randomized controlled trials in medical intervention, are published at a high rate every year. The volume of these publications makes it very challenging for physicians to find the best medical intervention for a given patient group and condition BIBREF2 . Computational methods and natural language processing (NLP) could be adopted in order to expedite the process of biomedical evidence synthesis. Specifically, NLP tasks applied to well structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment. Clinical questions are formed using", "which consists of an implementation of fast gradient boosting on decision trees. In this study, we use a library implemented by Microsoft BIBREF18 . In our model, we learn a linear combination of the prediction given by the base classifiers and the input text features to predict the labels. As features, we consider the average term frequency-inverse document frequency (TF-IDF) score for each instance and the frequency of occurrence of quantitative information elements (QIEF) (e.g. percentage, population, dose of medicine). Finally, the output of the binary cross entropy with logits layer (predicted probabilities for the three classes) and the feature information are fed to the LGBM. We train the base classifier using the original training dataset, using INLINEFORM0 of the whole data as training dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on"]}
{"question_id": "afd1c482c311e25fc42b9dd59cdc32ac542f5752", "predicted_answer": "Unanswerable", "predicted_evidence": [", BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 . Datasets. In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the", "would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label. For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by", "Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 . Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such", "dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on the excluded one and repeat the process for all five folds. The results of the LGBM classifier for the different boosting frameworks and the scores from the base classifiers are illustrated in Table TABREF14 . The highest average ROC_AUC score of 0.9998 is obtained in the case of combining the two base learners along with the TF-IDF and QIEF features. Discussion and Conclusion. In this paper, we presented an improved methodology to extract PIO elements, with reduced ambiguity, from abstracts of medical papers. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the", "the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in the latter case. The present work resulted in the creation of a PIO elements dataset, PICONET, and a classification tool. These constitute an important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.", "dense layer corresponding to the multi-label classifier with three output neurons corresponding to PIO labels. In order to insure that independent probabilities are assigned to the labels, as a loss function we have chosen the binary cross entropy with logits (BCEWithLogits) defined by DISPLAYFORM0  where t and y are the target and output vectors, respectively; n is the number of independent targets (n=3). The outputs are computed by applying the logistic function to the weighted sums of the last hidden layer activations, s, DISPLAYFORM0 DISPLAYFORM1  For the original BERT model, we have chosen the smallest uncased model, Bert-Base. The model has 12 attention layers and all texts are converted to lowercase by the tokenizer BIBREF9 . The architecture of the model is illustrated in Figure FIGREF7 . Using this framework, we trained the model using the two pretrained embedding models described in the previous section. It is worth to mention that the embedding is contextualized during the", ", BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks. Since its release, BERT has been pre-trained on a multitude of corpora. In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release BIBREF9 . This model is pre-trained on the BooksCorpus (800M words) BIBREF14 and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT BIBREF15 , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words). The Model. The classification model is built on top of the BERT representation by adding a dense layer corresponding to the multi-label classifier with three output neurons corresponding to PIO labels. In order to insure that independent probabilities are assigned to the labels, as a loss", "structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment. Clinical questions are formed using the PIO framework, where clinical issues are broken down into four components: Population/Problem (P), Intervention (I), Comparator (C), and Outcome (O). We will refer to these categories as PIO elements, by using the common practice of merging the C and I categories. In BIBREF3 a literature screening performed in 10 systematic reviews was studied. It was found that using the PIO framework can significantly improve literature screening efficacy. Therefore, efficient extraction of PIO elements is a key feature of many EBM applications and could be thought of as a multi-label sentence classification problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 ."]}
{"question_id": "ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "predicted_answer": "By collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories.", "predicted_evidence": [", BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 . Datasets. In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the", "Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 . Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such", "the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in the latter case. The present work resulted in the creation of a PIO elements dataset, PICONET, and a classification tool. These constitute an important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.", "would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label. For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by", "dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on the excluded one and repeat the process for all five folds. The results of the LGBM classifier for the different boosting frameworks and the scores from the base classifiers are illustrated in Table TABREF14 . The highest average ROC_AUC score of 0.9998 is obtained in the case of combining the two base learners along with the TF-IDF and QIEF features. Discussion and Conclusion. In this paper, we presented an improved methodology to extract PIO elements, with reduced ambiguity, from abstracts of medical papers. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the", "structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment. Clinical questions are formed using the PIO framework, where clinical issues are broken down into four components: Population/Problem (P), Intervention (I), Comparator (C), and Outcome (O). We will refer to these categories as PIO elements, by using the common practice of merging the C and I categories. In BIBREF3 a literature screening performed in 10 systematic reviews was studied. It was found that using the PIO framework can significantly improve literature screening efficacy. Therefore, efficient extraction of PIO elements is a key feature of many EBM applications and could be thought of as a multi-label sentence classification problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 .", "methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative. Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study", "problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 . These models are shallow and limited in terms of modeling capacity. Furthermore, most of these classifiers are trained to extract PIO elements one by one which is sub-optimal since this approach does not allow the use of shared structure among the individual classifiers. Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To"]}
{"question_id": "7066f33c373115b1ead905fe70a1e966f77ebeee", "predicted_answer": "Unanswerable", "predicted_evidence": ["Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 . Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such", ", BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 . Datasets. In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the", "the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in the latter case. The present work resulted in the creation of a PIO elements dataset, PICONET, and a classification tool. These constitute an important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.", "would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label. For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by", "dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on the excluded one and repeat the process for all five folds. The results of the LGBM classifier for the different boosting frameworks and the scores from the base classifiers are illustrated in Table TABREF14 . The highest average ROC_AUC score of 0.9998 is obtained in the case of combining the two base learners along with the TF-IDF and QIEF features. Discussion and Conclusion. In this paper, we presented an improved methodology to extract PIO elements, with reduced ambiguity, from abstracts of medical papers. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the", "methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative. Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study", ", BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks. Since its release, BERT has been pre-trained on a multitude of corpora. In the following, we describe different BERT embedding versions used for our classification problem. The first version is based on the original BERT release BIBREF9 . This model is pre-trained on the BooksCorpus (800M words) BIBREF14 and English Wikipedia (2,500M words). For Wikipedia, text passages were extracted while lists were ignored. The second version is BioBERT BIBREF15 , which was trained on biomedical corpora: PubMed (4.5B words) and PMC (13.5B words). The Model. The classification model is built on top of the BERT representation by adding a dense layer corresponding to the multi-label classifier with three output neurons corresponding to PIO labels. In order to insure that independent probabilities are assigned to the labels, as a loss", "abstract sections can be poor candidates for the corresponding label. For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by taking sentences from sections with headings such as aim. Furthermore, we cleaned the remaining data with various approaches including, but not limited to, language identification, removal of missing values, cleaning unicode characters, and filtering for sequences between 5 and 200 words, inclusive. Background. BERT (Bidirectional Encoder Representations from Transformers) is a deep bidirectional attention text embedding model. The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer BIBREF13 , BIBREF9 . Like any other language model, BERT can be pre-trained on different contexts. A contextualized representation is generally optimized for downstream NLP tasks. Since its release, BERT has"]}
{"question_id": "018b81f810a39b3f437a85573d24531efccd835f", "predicted_answer": "ambiguous and redundant labeling, lack of specificity in labeling certain categories such as P, I, and O", "predicted_evidence": ["would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label. For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by", ", BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 . More recently, jin2018pico proposed a bidirectional long short term memory (LSTM) model to simultaneously extract PIO components from PubMed abstracts. To our knowledge, that study was the first in which a deep learning framework was used to extract PIO elements from PubMed abstracts. In the present paper, we build a dataset of PIO elements by improving the methodology found in BIBREF12 . Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model BIBREF9 . Datasets. In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the", "Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 . Our aim was to perform automatic labeling while removing as much ambiguity as possible. We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such", "structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment. Clinical questions are formed using the PIO framework, where clinical issues are broken down into four components: Population/Problem (P), Intervention (I), Comparator (C), and Outcome (O). We will refer to these categories as PIO elements, by using the common practice of merging the C and I categories. In BIBREF3 a literature screening performed in 10 systematic reviews was studied. It was found that using the PIO framework can significantly improve literature screening efficacy. Therefore, efficient extraction of PIO elements is a key feature of many EBM applications and could be thought of as a multi-label sentence classification problem. Previous works on PIO element extraction focused on classical NLP methods, such as Naive Bayes (NB), Support Vector Machines (SVM) and Conditional Random Fields (CRF) BIBREF4 , BIBREF5 .", "dataset, and use a five-fold cross-validation framework to train the LGBM on the remaining INLINEFORM1 of the data to avoid any information leakage. We train the LGBM on four folds and test on the excluded one and repeat the process for all five folds. The results of the LGBM classifier for the different boosting frameworks and the scores from the base classifiers are illustrated in Table TABREF14 . The highest average ROC_AUC score of 0.9998 is obtained in the case of combining the two base learners along with the TF-IDF and QIEF features. Discussion and Conclusion. In this paper, we presented an improved methodology to extract PIO elements, with reduced ambiguity, from abstracts of medical papers. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. It has been shown that using the contextualized BioBERT embedding improved the accuracy of the", "the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the best performance in the latter case. The present work resulted in the creation of a PIO elements dataset, PICONET, and a classification tool. These constitute an important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.", "Introduction. Evidence-based medicine (EBM) is of primary importance in the medical field. Its goal is to present statistical analyses of issues of clinical focus based on retrieving and analyzing numerous papers in the medical literature BIBREF0 . The PubMed database is one of the most commonly used databases in EBM BIBREF1 . Biomedical papers, describing randomized controlled trials in medical intervention, are published at a high rate every year. The volume of these publications makes it very challenging for physicians to find the best medical intervention for a given patient group and condition BIBREF2 . Computational methods and natural language processing (NLP) could be adopted in order to expedite the process of biomedical evidence synthesis. Specifically, NLP tasks applied to well structured documents and queries can help physicians extract appropriate information to identify the best available evidence in the context of medical treatment. Clinical questions are formed using", "methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative. Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study"]}
{"question_id": "e2c8d7f3ef5913582503e50244ca7158d0a62c42", "predicted_answer": "Unanswerable", "predicted_evidence": ["using 1,468 words, compared with 2 in BIBREF17. Following BIBREF17, we use the pre-trained BERT models from the original authors, but through the PyTorch implementation. BIBREF17 showed that in his experiments the base BERT model performed better than the larger model, so we restrict our attention to the base model. For English, we use the model trained only on English data, whereas for all other languages we use the multilingual model. Results. Overall, BERT performs well on our experimental task, suggesting that it is able to model syntactic structure. BERT was correct in 94.3% of all cloze examples. This high performance is found across all four types of agreement relations. Figure FIGREF13 shows that BERT performed above 90% accuracy in each type. Performance is best on determiner and attributive agreement relations, while worst on subject-verb and predicate adjective. In figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of", "and attributive agreement relations, while worst on subject-verb and predicate adjective. In figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of languages, although some fare much worse than others. It is important to note that it is an unfair comparison because even though the datasets were curated using the same methodology, each language's dataset is different. It is possible, for example, that the examples we have for Basque are simply harder than they are for Portuguese. Finally, we ask how BERT's performance is affected by distance between the controller and the target, as well as the number of distractors. Figure FIGREF15 shows BERT's performance, aggregated over all languages and types, as a function of the distance involved in the agreement, while figure FIGREF16 shows the same for number of distractors. There is a slight but consistent decrease in performance as the distance and the number of distractors increase. The decline", "such structure-dependent phenomena under certain conditions BIBREF11, BIBREF12, BIBREF13, BIBREF14, in addition to their widespread success in practical tasks. The recently introduced BERT model BIBREF15, which is based on transformers, achieves state-of-the-art results on eleven natural language processing tasks. In this work, we assess BERT's ability to learn structure-dependent linguistic phenomena of agreement relations. To test whether BERT is sensitive to agreement relations, we use the cloze test BIBREF16, in which we mask out one of two words in an agreement relation and ask BERT to predict the masked word, one of the two tasks on which BERT is initially trained. BIBREF17 adapted the experimental setup of BIBREF13, BIBREF11 and BIBREF18 to use the cloze test to assess BERT's sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all", "in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by BIBREF17 showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntax-sensitive agreement relations decreases slightly as the dependency becomes longer range, and as the", "sensitivity to number agreement in English subject-verb agreement relations. The results showed that the single-language BERT model performed surprisingly well at this task (above 80% accuracy in all experiments), even when there were multiple \u201cdistractors\u201d in the sentence (other nouns that differed from the subject in number). This suggests that BERT is actually learning to approximate structure-dependent computation, and not simply relying on flawed heuristics. However, English subject-verb agreement is a rather restricted phenomenon, with the majority of verbs having only two inflected forms and only one morphosyntactic feature (number) involved. To what extent does Goldberg's BIBREF17 result hold for subject-verb agreement in other languages, including more morphologically rich ones, as well as for other types of agreement relations? Building on Goldberg's BIBREF17 work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging", "this, we need to know the feature values for each word in BERT's vocabulary. This is our second type of data. Throughout this paper, we refer to the first type of data as the cloze data, and the second as the feature data. In the design of our datasets, we followed two principles. First, we chose data sources that are available across multiple languages, because we are interested in cross-linguistic generality. The languages in this study are those with sufficiently large data sources that also appear in the multilingual BERT model. Second, we use naturally-occurring data (cf. BIBREF18). Data ::: Cloze data. We sourced our cloze data from version 2.4 of the Universal Dependencies treebanks BIBREF28. The UD treebanks use a consistent schema across all languages to annotate naturally occurring sentences at the word level with rich grammatical information. We used the part-of-speech and dependency information to identify potential agreement relations. Specifically, we identified all", "of each language to note which of the four types of agreement exist in the language. We removed all examples that are of the wrong type for the language (8% of harvested examples). Across the 26 languages, we curated almost one million cloze examples. Their breakdown across agreement type and language is shown in Tables 1 and 2. In all four types of agreement studied, the controller of the agreement is a noun or pronoun, while the target can be a determiner, adjective or verb. Because of this part-of-speech restriction, we chose to mask out the controller in every cloze example so that BERT is evaluated against the same vocabulary across all four types. This also means that we only need to collect feature data on nouns and pronouns. Data ::: Feature data. Our feature data comes from both the UD and the UniMorph projects BIBREF29. The UniMorph project also uses a consistent schema across all languages to annotate word types with morphological features. Although this schema is not the", "to evaluate whether entire syntax trees are embedded in a linear transformation of a model's word representation space, finding that BERT does capture such information. As a complementary approach, BIBREF27 studied the attention mechanism of BERT, finding clear correlates with interpretable linguistic structures such as direct objects, and suggest that BERT's success is due in part to its syntactic awareness. However, by subjecting it to existing psycholinguistic tasks, BIBREF32 found that BERT fails in its ability to understand negation. In concurrent work, BIBREF33 show that BERT does not consistently outperform LSTM-based models on English subject-verb agreement tasks. Conclusions & future work. Core linguistic phenomena depend on syntactic structure. Yet current state-of-the-art models in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by BIBREF17 showed that BERT captures English subject-verb number agreement well"]}
{"question_id": "654fe0109502f2ed2dc8dad359dbbce4393e03dc", "predicted_answer": "four types of agreement relations: subject-verb, noun-determiner, noun-adjective, subject-predicate adjective", "predicted_evidence": ["and morphology BIBREF19. Languages often express grammatical features, such as number and gender, through inflectional morphology. An agreement relation is a morphophonologically overt co-variance in feature values between two words in a syntactic relationship BIBREF20. In other words, agreement refers to when the morphosyntactic features of one word are reflected in its syntactic dependents. In this way, agreement relations are overt markers of covert syntactic structure. Thus, evaluating a model's ability to capture agreement relations is also an evaluation of its ability to capture syntactic structure. Following BIBREF21, we call the syntactically dependent word the \u201ctarget\u201d of the agreement relation, and the word with which it agrees we call the \u201ccontroller\u201d. An example of an agreement relation in English is given in (UNKREF2), in which the inflected form of the verb be (are) reflects the plural number of its syntactic head keys. In all examples in this section, the controller and", "as well as for other types of agreement relations? Building on Goldberg's BIBREF17 work, we expand the experiment to 26 languages and four types of agreement relations, which include more challenging examples. In Section 2, we define what is meant by agreement relations and outline the particular agreement relations under study. Section 3 introduces our newly curated cross-linguistic dataset of agreement relations, while section 4 discusses our experimental setup. We report the results of our experiments in section 5. All data and code are available at https://github.com/geoffbacon/does-bert-agree. Structure-dependent agreement relations. Agreement phenomena are an important and cross-linguistically common property of natural languages, and as such have been extensively studied in syntax and morphology BIBREF19. Languages often express grammatical features, such as number and gender, through inflectional morphology. An agreement relation is a morphophonologically overt co-variance in", "employs all four types of agreement relations. Examples are given in (UNKREF3)-(UNKREF6). The subject and verb in (UNKREF3) agree for number, while the noun and determiner in (UNKREF4), the noun and attributive adjective in (UNKREF5) and the subject and predicated adjective in (UNKREF6) agree for both number and gender. `The keys to the door are on the table.' `I can see the keys.' `I no longer want the completely broken keys.' `The keys to the door are broken.' Previous work using agreement relations to assess knowledge of syntactic structure in modern neural networks has focussed on subject-verb agreement in number BIBREF17, BIBREF11, BIBREF13. In our work, we study all four types of agreement relations and all four features discussed above. Moreover, previous work using any method to assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model BIBREF23, BIBREF17, BIBREF24, BIBREF25, BIBREF26, BIBREF27. We expand this line of work to", "highlights an important qualification of this result. We showed that BERT's ability to model syntax-sensitive agreement relations decreases slightly as the dependency becomes longer range, and as the number of distractors increases. We release our new curated cross-linguistic datasets and code in the hope that it is useful to future research that may probe why this pattern appears. The experimental setup we used has some known limitations. First, in certain languages some of the cloze examples we studied contain redundant information. Even when one word from an agreement relation is masked out, other cues remain in the sentence (e.g. when masking out the noun for a French attributive adjective agreement relation, number information is still available from the determiner). To counter this in future work, we plan to run our experiment twice, masking out the controller and then the target. Second, we used a different evaluation scheme than previous work BIBREF17 by averaging BERT's", "in language representations, such as BERT, do not have explicit syntactic structural representations. Previous work by BIBREF17 showed that BERT captures English subject-verb number agreement well despite this lack of explicit structural representation. We replicated this result using a different evaluation methodology that addresses shortcomings in the original methodology and expanded the study to 26 languages. Our study further broadened existing work by considering the most cross-linguistically common agreement types as well as the most common morphosyntactic features. The main result of this expansion into more languages, types and features is that BERT, without explicit syntactic structure, is still able to capture syntax-sensitive agreement patterns well. However, our analysis highlights an important qualification of this result. We showed that BERT's ability to model syntax-sensitive agreement relations decreases slightly as the dependency becomes longer range, and as the", "of each language to note which of the four types of agreement exist in the language. We removed all examples that are of the wrong type for the language (8% of harvested examples). Across the 26 languages, we curated almost one million cloze examples. Their breakdown across agreement type and language is shown in Tables 1 and 2. In all four types of agreement studied, the controller of the agreement is a noun or pronoun, while the target can be a determiner, adjective or verb. Because of this part-of-speech restriction, we chose to mask out the controller in every cloze example so that BERT is evaluated against the same vocabulary across all four types. This also means that we only need to collect feature data on nouns and pronouns. Data ::: Feature data. Our feature data comes from both the UD and the UniMorph projects BIBREF29. The UniMorph project also uses a consistent schema across all languages to annotate word types with morphological features. Although this schema is not the", "assess BERT's knowledge of syntactic structure has focussed exclusively on the single-language English model BIBREF23, BIBREF17, BIBREF24, BIBREF25, BIBREF26, BIBREF27. We expand this line of work to 26 languages. Not all languages in our sample exhibit all four types of agreement nor use all four features examined, but they all exhibit at least one of the agreement types involving at least one of the features. Data. Our study requires two types of data. First, we need sentences containing agreement relations. We mask out one of the words in the agreement relation and ask BERT to predict the masked word. We are interested in BERT's ability to predict words that respect the agreement relation, that is, words which share the morphosyntactic features of the word with which it agrees. To measure this, we need to know the feature values for each word in BERT's vocabulary. This is our second type of data. Throughout this paper, we refer to the first type of data as the cloze data, and the", "and attributive agreement relations, while worst on subject-verb and predicate adjective. In figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of languages, although some fare much worse than others. It is important to note that it is an unfair comparison because even though the datasets were curated using the same methodology, each language's dataset is different. It is possible, for example, that the examples we have for Basque are simply harder than they are for Portuguese. Finally, we ask how BERT's performance is affected by distance between the controller and the target, as well as the number of distractors. Figure FIGREF15 shows BERT's performance, aggregated over all languages and types, as a function of the distance involved in the agreement, while figure FIGREF16 shows the same for number of distractors. There is a slight but consistent decrease in performance as the distance and the number of distractors increase. The decline"]}
{"question_id": "da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "predicted_answer": "Amazon, Yelp, IMDB, MR, MPQA, Subj, TREC", "predicted_evidence": ["as a function of maximum $\\%$ perturbation (ratio of number of word replacements and insertions to the length of the original text) for the TREC dataset. From Figure , we clearly observe that the BAE attacks are consistently stronger than TextFooler. The classifier models are relatively robust to perturbations up to 20$\\%$, while the effectiveness saturates at 40-50$\\%$. Surprisingly, a 50$\\%$ perturbation for the TREC dataset translates to replacing or inserting just 3-4 words, due to the short text lengths. Qualitative Examples We present adversarial examples generated by the attacks on a sentence from the IMDB and Yelp datasets in Table . BAE produces more natural looking examples than TextFooler as tokens predicted by the BERT-MLM fit well in the sentence context. TextFooler tends to replace words with complex synonyms, which can be easily detected. Moreover, BAE's additional degree of freedom to insert tokens allows for a successful attack with fewer perturbations. Human", "we are the first to use a LM for adversarial example generation. We summarize our major contributions as follows:  We propose BAE, a novel strategy for generating natural looking adversarial examples using a masked language model. We introduce 4 BAE attack modes, all of which are almost always stronger than previous baselines on 7 text classification datasets. We show that, surprisingly, just a few replace/insert operations can reduce the accuracy of even a powerful BERT-based classifier by over $80\\%$ on some datasets.  Methodology. Problem Definition We are given a dataset $(S,Y) = \\lbrace (\\mathbb {S}_1,y_1),(\\mathbb {S}_2,y_2)\\dots (\\mathbb {S}_m,y_m)\\rbrace $ and a trained classification model $C:\\mathbb {S}\\rightarrow Y$. We assume the soft-label black-box setting where the attacker can only query the classifier for output probabilities on a given input, and does not have access to the model parameters, gradients or training data. For an input pair $(\\mathbb {S},y)$, we want to", "attacks are much stronger. We observe that the BERT-based classifier is more robust to the BAE and TextFooler attacks than the word-LSTM and word-CNN models which can be attributed to its large size and pre-training on a large corpus. The baseline attack is often stronger than the BAE-R and BAE-I attacks for the BERT based classifier. We attribute this to the shared parameter space between the BERT-MLM and the BERT classifier before fine-tuning. The predicted tokens from BERT-MLM may not drastically change the internal representations learned by the BERT classifier, hindering their ability to adversarially affect the classifier prediction. Effectiveness We study the effectiveness of BAE on limiting the number of R/I operations permitted on the original text. We plot the attack performance as a function of maximum $\\%$ perturbation (ratio of number of word replacements and insertions to the length of the original text) for the TREC dataset. From Figure , we clearly observe that the BAE", "probability $P(C(\\mathbb {S}_{adv}){=}y)$ the most. The perturbations are applied iteratively to the input tokens in decreasing order of importance, until either $C(\\mathbb {S}_{adv}){\\ne }y$ (successful attack) or all the tokens of $\\mathbb {S}$ have been perturbed (failed attack). We present 4 attack modes for BAE based on the R and I operations, where for each token $t$ in $\\mathbb {S}$: BAE-R: Replace token $t$ (See Algorithm ) BAE-I: Insert a token to the left or right of $t$ BAE-R/I: Either replace token $t$ or insert a token to the left or right of $t$ BAE-R+I: First replace token $t$, then insert a token to the left or right of $t$ Experiments. Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16", "In this paper, we present a simple yet novel technique: BAE (BERT-based Adversarial Examples), which uses a language model (LM) for token replacement to best fit the overall context. We perturb an input sentence by either replacing a token or inserting a new token in the sentence, by means of masking a part of the input and using a LM to fill in the mask (See Figure FIGREF1). BAE relies on the powerful BERT masked LM for ensuring grammatical correctness of the adversarial examples. Our attack beats the previous baselines by a large margin and confirms the inherent vulnerabilities of modern text classification models to adversarial attacks. Moreover, BAE produces more richer and natural looking adversarial examples as it uses the semantics learned by a LM. To the best of our knowledge, we are the first to use a LM for adversarial example generation. We summarize our major contributions as follows:  We propose BAE, a novel strategy for generating natural looking adversarial examples", "replacement using a fixed word embedding space BIBREF22. We only consider the top $K{=}50$ synonyms from the MLM predictions and set a threshold of 0.8 for the cosine similarity between USE based embeddings of the adversarial and input text. Results We perform the 4 modes of our attack and summarize the results in Table . Across datasets and models, our BAE attacks are almost always more effective than the baseline attack, achieving significant drops of 40-80% in test accuracies, with higher average semantic similarities as shown in parentheses. BAE-R+I is the strongest attack since it allows both replacement and insertion at the same token position, with just one exception. We observe a general trend that the BAE-R and BAE-I attacks often perform comparably, while the BAE-R/I and BAE-R+I attacks are much stronger. We observe that the BERT-based classifier is more robust to the BAE and TextFooler attacks than the word-LSTM and word-CNN models which can be attributed to its large size", "to replace words with complex synonyms, which can be easily detected. Moreover, BAE's additional degree of freedom to insert tokens allows for a successful attack with fewer perturbations. Human Evaluation We consider successful adversarial examples generated from the Amazon and IMDB datasets and verify their sentiment and grammatical correctness. Human evaluators annotated the sentiment and the grammar (Likert scale of 1-5) of randomly shuffled adversarial examples and original texts. From Table , BAE and TextFooler have inferior accuracies compared to the Original, showing they are not always perfect. However, BAE has much better grammar scores, suggesting more natural looking adversarial examples. Ablation Study We analyze the benefits of R/I operations in BAE in Table . From the table, the splits $\\mathbb {A}$ and $\\mathbb {B}$ are the $\\%$ of test points which compulsorily need I and R operations respectively for a successful attack. We can observe that the split $\\mathbb {A}$ is", "subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification. We use 3 popular text classification models: word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier. For each dataset we train the model on the training data and perform the adversarial attack on the test data. For complete model details refer to Appendix. As a baseline, we consider TextFooler BIBREF11 which performs synonym replacement using a fixed word embedding space BIBREF22. We only consider the top $K{=}50$ synonyms from the MLM predictions and set a threshold of 0.8 for the cosine similarity between USE based"]}
{"question_id": "363a24ecb8ab45215134935e7e8165fff72ff90f", "predicted_answer": "Word-LSTM and word-CNN models", "predicted_evidence": ["we are the first to use a LM for adversarial example generation. We summarize our major contributions as follows:  We propose BAE, a novel strategy for generating natural looking adversarial examples using a masked language model. We introduce 4 BAE attack modes, all of which are almost always stronger than previous baselines on 7 text classification datasets. We show that, surprisingly, just a few replace/insert operations can reduce the accuracy of even a powerful BERT-based classifier by over $80\\%$ on some datasets.  Methodology. Problem Definition We are given a dataset $(S,Y) = \\lbrace (\\mathbb {S}_1,y_1),(\\mathbb {S}_2,y_2)\\dots (\\mathbb {S}_m,y_m)\\rbrace $ and a trained classification model $C:\\mathbb {S}\\rightarrow Y$. We assume the soft-label black-box setting where the attacker can only query the classifier for output probabilities on a given input, and does not have access to the model parameters, gradients or training data. For an input pair $(\\mathbb {S},y)$, we want to", "BIBREF8, BIBREF9, BIBREF10, etc. for creating adversarial examples. These techniques often result in adversarial examples which are unnatural looking and lack grammatical correctness, and thus can be easily identified by humans. TextFooler BIBREF11 is a black-box attack, that uses rule based synonym replacement from a fixed word embedding space to generate adversarial examples. These adversarial examples do not account for the overall semantics of the sentence, and consider only the token level similarity using word embeddings. This can lead to out-of-context and unnaturally complex replacements (see Table ), which can be easily identifiable by humans. The recent advent of powerful language models BIBREF12, BIBREF13 in NLP has paved the way for using them in various downstream applications. In this paper, we present a simple yet novel technique: BAE (BERT-based Adversarial Examples), which uses a language model (LM) for token replacement to best fit the overall context. We perturb an", "as a function of maximum $\\%$ perturbation (ratio of number of word replacements and insertions to the length of the original text) for the TREC dataset. From Figure , we clearly observe that the BAE attacks are consistently stronger than TextFooler. The classifier models are relatively robust to perturbations up to 20$\\%$, while the effectiveness saturates at 40-50$\\%$. Surprisingly, a 50$\\%$ perturbation for the TREC dataset translates to replacing or inserting just 3-4 words, due to the short text lengths. Qualitative Examples We present adversarial examples generated by the attacks on a sentence from the IMDB and Yelp datasets in Table . BAE produces more natural looking examples than TextFooler as tokens predicted by the BERT-MLM fit well in the sentence context. TextFooler tends to replace words with complex synonyms, which can be easily detected. Moreover, BAE's additional degree of freedom to insert tokens allows for a successful attack with fewer perturbations. Human", "probability $P(C(\\mathbb {S}_{adv}){=}y)$ the most. The perturbations are applied iteratively to the input tokens in decreasing order of importance, until either $C(\\mathbb {S}_{adv}){\\ne }y$ (successful attack) or all the tokens of $\\mathbb {S}$ have been perturbed (failed attack). We present 4 attack modes for BAE based on the R and I operations, where for each token $t$ in $\\mathbb {S}$: BAE-R: Replace token $t$ (See Algorithm ) BAE-I: Insert a token to the left or right of $t$ BAE-R/I: Either replace token $t$ or insert a token to the left or right of $t$ BAE-R+I: First replace token $t$, then insert a token to the left or right of $t$ Experiments. Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16", "attacks are much stronger. We observe that the BERT-based classifier is more robust to the BAE and TextFooler attacks than the word-LSTM and word-CNN models which can be attributed to its large size and pre-training on a large corpus. The baseline attack is often stronger than the BAE-R and BAE-I attacks for the BERT based classifier. We attribute this to the shared parameter space between the BERT-MLM and the BERT classifier before fine-tuning. The predicted tokens from BERT-MLM may not drastically change the internal representations learned by the BERT classifier, hindering their ability to adversarially affect the classifier prediction. Effectiveness We study the effectiveness of BAE on limiting the number of R/I operations permitted on the original text. We plot the attack performance as a function of maximum $\\%$ perturbation (ratio of number of word replacements and insertions to the length of the original text) for the TREC dataset. From Figure , we clearly observe that the BAE", "replacement using a fixed word embedding space BIBREF22. We only consider the top $K{=}50$ synonyms from the MLM predictions and set a threshold of 0.8 for the cosine similarity between USE based embeddings of the adversarial and input text. Results We perform the 4 modes of our attack and summarize the results in Table . Across datasets and models, our BAE attacks are almost always more effective than the baseline attack, achieving significant drops of 40-80% in test accuracies, with higher average semantic similarities as shown in parentheses. BAE-R+I is the strongest attack since it allows both replacement and insertion at the same token position, with just one exception. We observe a general trend that the BAE-R and BAE-I attacks often perform comparably, while the BAE-R/I and BAE-R+I attacks are much stronger. We observe that the BERT-based classifier is more robust to the BAE and TextFooler attacks than the word-LSTM and word-CNN models which can be attributed to its large size", "can only query the classifier for output probabilities on a given input, and does not have access to the model parameters, gradients or training data. For an input pair $(\\mathbb {S},y)$, we want to generate an adversarial example $\\mathbb {S}_{adv}$ such that $C(\\mathbb {S}_{adv}){\\ne }y$ where $\\mathbb {S}_{adv}$ is natural looking, grammatically correct and semantically similar to $\\mathbb {S}$ (by some pre-defined definition of similarity). BAE For generating adversarial example $\\mathbb {S}_{adv}$, we define two perturbations on the input $\\mathbb {S}$: Replace a token $t \\in \\mathbb {S}$ with another Insert a new token $t^{\\prime }$ in $\\mathbb {S}$ Some tokens in the input are more attended to by $C$ than others, and therefore contribute more towards the final prediction. Replacing these tokens or inserting a new token adjacent to them can thus have a stronger effect on altering the classifier prediction. We estimate the token importance $I_i$ of each token $t_i \\in \\mathbb", "The pre-trained BERT masked language model (MLM) is used to predict the mask tokens (See Figure FIGREF1). BERT is a powerful LM trained on a large training corpus ($\\sim $ 2 billion words), and hence the predicted mask tokens fit well grammatically in $\\mathbb {S}$. The BERT-MLM does not however guarantee semantic coherence to the original text $\\mathbb {S}$ as demonstrated by the following simple example. Consider the sentence: `the food was good'. For replacing the token `good', BERT-MLM may predict the tokens `nice' and `bad', both of which fit well into the context of the sentence. However, replacing `good' with `bad' changes the original sentiment of the sentence. To ensure semantic similarity on introducing perturbations in the input text, we filter the set of top K masked tokens (K is a pre-defined constant) predicted by BERT-MLM using a Universal Sentence Encoder (USE) BIBREF14 based sentence similarity scorer. For the R operations we add an additional check for grammatical"]}
{"question_id": "8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "predicted_answer": "Logistic Regression", "predicted_evidence": ["case as Our Results, Same Classifier as Prior Best. Results. The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features. Discussion. When testing on Amazon reviews, the", "with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for a domain-general approach. [6]Part-of-speech is considered in MPQA; Amazon and Twitter data was tagged using Stanford CoreNLP BIBREF20 and the Twitter POS-tagger BIBREF21 , respectively. Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best. Results. The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the", "to the combined dataset performed quite well ( INLINEFORM0 =0.780). The classifier was able to take advantage of a wealth of additional Twitter samples that had led to terrible performance on their own ( INLINEFORM1 =0.276). Thus, the high performance demonstrated when the EasyAdapt algorithm is applied to the training data from the two domains is particularly impressive. It shows that more data is indeed better data\u2014provided that the proper features are selected and the classifier is properly guided in handling it. Overall, the system cut the error rate from .256 to .220, representing a 14% relative reduction in error over prior best results on the Amazon dataset. Our results testing on Twitter are not directly comparable to others, since prior work's datasets could not be released; however, our results ( INLINEFORM0 =0.583) are in line with those reported previously ( BIBREF4 RiloffSarcasm: INLINEFORM1 =0.51; BIBREF13 davidov-tsur-rappoport:2010:CONLL: INLINEFORM2 =0.545).", "average polarities, overall polarities, and largest polarity gap features from Table TABREF14 . General: Subjectivity includes the % strongly subjective positive words, % weakly subjective positive words, and their negative counterparts. We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic. Results are reported for models trained only on Twitter, only on Amazon, on both training sets, and on both training sets when Daum\u00e9's daumeiii:2007:ACLMain EasyAdapt technique is applied, employing Twitter as the algorithm's source domain and Amazon as its target domain. EasyAdapt works by modifying the feature space so that it contains three mappings of the original features: a general (source + target) version, a source-only version, and a target-only version. More specifically, assuming an input feature set INLINEFORM0 for some INLINEFORM1 , where INLINEFORM2 is the number", "and both datasets were randomly divided into training (80%) and testing (20%) sets. Further details are shown in Table TABREF6 . Features. Three feature sets were developed (one general, and two targeted toward Twitter and Amazon, respectively). Resources used to develop the features are described in Table TABREF9 . Five classifiers (Na\u00efve Bayes, J48, Bagging, DecisionTable, and SVM), all from the Weka library, were tested using five-fold cross-validation on the training sets, and the highest-scoring (Na\u00efve Bayes) was selected for use on the test set. The Twitter- (T) and Amazon-specific (A) features are shown in Table TABREF11 . Domain-specific features were still computed for instances from the other domain unless it was impossible to compute those features in that domain (i.e., Amazon Star Rating for Twitter instances), in which case they were left empty. Twitter-specific features are based on the work of BIBREF15 maynard2014cares and BIBREF4 RiloffSarcasm. Maynard and Greenwood", "finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features. Discussion. When testing on Amazon reviews, the worst-performing case was that in which the classifier was trained only on Twitter data (it did not manage to outperform either baseline). This underscores the inherent variations in the data across the two domains; despite the fact that many of the features were deliberately designed to be generalizable and robust to domain-specific idiosyncrasies, the different trends across domains still confused the classifier. However, combining all of that same Twitter data with a much smaller amount of Amazon data (3998 Twitter training instances relative to 1003 Amazon training instances) and applying EasyAdapt to the combined dataset performed quite well ( INLINEFORM0 =0.780). The classifier was able to take advantage of a wealth of additional Twitter samples that had led to terrible performance on", "(source + target) version, a source-only version, and a target-only version. More specifically, assuming an input feature set INLINEFORM0 for some INLINEFORM1 , where INLINEFORM2 is the number of features in the set, EasyAdapt transforms INLINEFORM3 to the augmented set, INLINEFORM4 . The mappings INLINEFORM5 for the source and target domain data, respectively, are defined as INLINEFORM6 and INLINEFORM7 , where INLINEFORM8 is the zero vector. Refer to Daum\u00e9 daumeiii:2007:ACLMain for an in-depth discussion of this technique. Each model was tested on the Amazon test data (the model trained only on Twitter was also tested on the Twitter test set). Amazon reviews were selected as the target domain since the Twitter dataset was much larger than the Amazon dataset; this scenario is more consistent with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for", "other domains has been limited, but recently a publicly-available corpus of sarcastic and non-sarcastic Amazon product reviews was released by Filatova FILATOVA12.661 to facilitate research. BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 test many feature combinations on this dataset, including those based on metadata (e.g., Amazon star rating), sentiment, grammar, the presence of interjections (e.g., \u201cwow\u201d) or laughter (e.g., through onomatopoeia or acronyms such as \u201clol\u201d), the presence of emoticons, and bag-of-words features. Their highest F1 (0.744) is achieved using all of these with a logistic regression classifier; however, using only the star rating, they still achieve an F1 of 0.717. This highlights the need for high-performing, general features for sarcasm detection; metadata features are highly domain-specific, and even bag-of-words trends may be unique to certain domains (\u201ctrump\u201d was one of the most common unigrams in our own Twitter training set, but only occurred once"]}
{"question_id": "ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "predicted_answer": "Twitter and Amazon product reviews.", "predicted_evidence": ["detection and show that the features generally perform well across text domains. Further, we validate that domain adaptation can be applied to sarcasm detection to leverage patterns in out-of-domain training data, even when results from training only on that source domain data are extremely bad (far below baseline results), to improve over training on only the target data or over training on the simply combined dataset. Finally, we make a new dataset of sarcastic and non-sarcastic tweets available online as a resource to other researchers. Related Work. The majority of work on automatic sarcasm detection has been done using Twitter, and to a smaller extent Amazon product reviews. Research outside of those domains has been scarce, but interesting. Notably, Burfoot and Baldwin Burfoot:2009:ASD:1667583.1667633 automatically detected satirical news articles using unigrams, lexical features, and semantic validity features, and Justo et al. Justo2014124 used n-gram, linguistic, and semantic", "with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for a domain-general approach. [6]Part-of-speech is considered in MPQA; Amazon and Twitter data was tagged using Stanford CoreNLP BIBREF20 and the Twitter POS-tagger BIBREF21 , respectively. Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best. Results. The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the", "(source + target) version, a source-only version, and a target-only version. More specifically, assuming an input feature set INLINEFORM0 for some INLINEFORM1 , where INLINEFORM2 is the number of features in the set, EasyAdapt transforms INLINEFORM3 to the augmented set, INLINEFORM4 . The mappings INLINEFORM5 for the source and target domain data, respectively, are defined as INLINEFORM6 and INLINEFORM7 , where INLINEFORM8 is the zero vector. Refer to Daum\u00e9 daumeiii:2007:ACLMain for an in-depth discussion of this technique. Each model was tested on the Amazon test data (the model trained only on Twitter was also tested on the Twitter test set). Amazon reviews were selected as the target domain since the Twitter dataset was much larger than the Amazon dataset; this scenario is more consistent with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for", "and both datasets were randomly divided into training (80%) and testing (20%) sets. Further details are shown in Table TABREF6 . Features. Three feature sets were developed (one general, and two targeted toward Twitter and Amazon, respectively). Resources used to develop the features are described in Table TABREF9 . Five classifiers (Na\u00efve Bayes, J48, Bagging, DecisionTable, and SVM), all from the Weka library, were tested using five-fold cross-validation on the training sets, and the highest-scoring (Na\u00efve Bayes) was selected for use on the test set. The Twitter- (T) and Amazon-specific (A) features are shown in Table TABREF11 . Domain-specific features were still computed for instances from the other domain unless it was impossible to compute those features in that domain (i.e., Amazon Star Rating for Twitter instances), in which case they were left empty. Twitter-specific features are based on the work of BIBREF15 maynard2014cares and BIBREF4 RiloffSarcasm. Maynard and Greenwood", "detection to date has focused on the Twitter domain, which boasts an ample source of publicly-available data, some of which is already self-labeled by users for the presence of sarcasm (e.g., with #sarcasm). However, Twitter is highly informal, space-restricted, and subject to frequent topic fluctuations from one post to the next due to the ebb and flow of current events\u2014in short, it is not broadly representative of most text domains. Thus, sarcasm detectors trained using features designed for maximum Twitter performance are not necessarily transferable to other domains. Despite this, it is desirable to develop approaches that can harness the more generalizable information present in the abundance of Twitter data. In this work, we develop a set of domain-independent features for sarcasm detection and show that the features generally perform well across text domains. Further, we validate that domain adaptation can be applied to sarcasm detection to leverage patterns in out-of-domain", "negatively impacted by common Twitter characteristics, such as the trend to join keywords together in hashtags, and the use of acronyms that are unconventional in other domains. These issues could be addressed to some extent in the future via word segmentation tools, spell-checkers, and acronym expansion. Conclusions. This work develops a set of domain-independent features and demonstrates their usefulness for general sarcasm detection. Moreover, it shows that by applying a domain adaptation step to the extracted features, even a surplus of \u201cbad\u201d training data can be used to improve the performance of the classifier on target domain data, reducing error by 14% relative to prior work. The Twitter corpus described in this paper is publicly available for research purposes,[2] and represents a substantial contribution to multiple NLP sub-communities. This shared corpus of tweets annotated for sarcasm will hasten the advancement of further research. In the future, we plan to extend our", "a substantial contribution to multiple NLP sub-communities. This shared corpus of tweets annotated for sarcasm will hasten the advancement of further research. In the future, we plan to extend our approach to detect sarcasm in a completely novel domain, literature, eventually integrating the work into an application to support reading comprehension. Acknowledgments. This material is based upon work supported by the NSF Graduate Research Fellowship Program under Grant 1144248, and the NSF under Grant 1262860. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.", "rather than on task-specific feature design BIBREF8 , BIBREF9 , BIBREF10 . Since neural networks offer little transparency, it is uncertain whether the features learned in these approaches would be easily transferable across text domains for this task (prior research on other tasks suggests that the features computed by deep neural networks grow increasingly specific to the training dataset\u2014and in turn, to the training domain\u2014with each layer BIBREF11 ). Although an interesting question, the focus herein is on uncovering the specific types of features capable of leveraging general patterns for sarcasm detection, and this can be more easily examined using shallower learning algorithms. Sarcasm Detection on Amazon Reviews. Research on automatic sarcasm detection in other domains has been limited, but recently a publicly-available corpus of sarcastic and non-sarcastic Amazon product reviews was released by Filatova FILATOVA12.661 to facilitate research. BIBREF12"]}
{"question_id": "38af3f25c36c3725a31304ab96e2c200c55792b4", "predicted_answer": "Amazon and Twitter data were used.", "predicted_evidence": ["case as Our Results, Same Classifier as Prior Best. Results. The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features. Discussion. When testing on Amazon reviews, the", "finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features. Discussion. When testing on Amazon reviews, the worst-performing case was that in which the classifier was trained only on Twitter data (it did not manage to outperform either baseline). This underscores the inherent variations in the data across the two domains; despite the fact that many of the features were deliberately designed to be generalizable and robust to domain-specific idiosyncrasies, the different trends across domains still confused the classifier. However, combining all of that same Twitter data with a much smaller amount of Amazon data (3998 Twitter training instances relative to 1003 Amazon training instances) and applying EasyAdapt to the combined dataset performed quite well ( INLINEFORM0 =0.780). The classifier was able to take advantage of a wealth of additional Twitter samples that had led to terrible performance on", "to the combined dataset performed quite well ( INLINEFORM0 =0.780). The classifier was able to take advantage of a wealth of additional Twitter samples that had led to terrible performance on their own ( INLINEFORM1 =0.276). Thus, the high performance demonstrated when the EasyAdapt algorithm is applied to the training data from the two domains is particularly impressive. It shows that more data is indeed better data\u2014provided that the proper features are selected and the classifier is properly guided in handling it. Overall, the system cut the error rate from .256 to .220, representing a 14% relative reduction in error over prior best results on the Amazon dataset. Our results testing on Twitter are not directly comparable to others, since prior work's datasets could not be released; however, our results ( INLINEFORM0 =0.583) are in line with those reported previously ( BIBREF4 RiloffSarcasm: INLINEFORM1 =0.51; BIBREF13 davidov-tsur-rappoport:2010:CONLL: INLINEFORM2 =0.545).", "and both datasets were randomly divided into training (80%) and testing (20%) sets. Further details are shown in Table TABREF6 . Features. Three feature sets were developed (one general, and two targeted toward Twitter and Amazon, respectively). Resources used to develop the features are described in Table TABREF9 . Five classifiers (Na\u00efve Bayes, J48, Bagging, DecisionTable, and SVM), all from the Weka library, were tested using five-fold cross-validation on the training sets, and the highest-scoring (Na\u00efve Bayes) was selected for use on the test set. The Twitter- (T) and Amazon-specific (A) features are shown in Table TABREF11 . Domain-specific features were still computed for instances from the other domain unless it was impossible to compute those features in that domain (i.e., Amazon Star Rating for Twitter instances), in which case they were left empty. Twitter-specific features are based on the work of BIBREF15 maynard2014cares and BIBREF4 RiloffSarcasm. Maynard and Greenwood", "with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for a domain-general approach. [6]Part-of-speech is considered in MPQA; Amazon and Twitter data was tagged using Stanford CoreNLP BIBREF20 and the Twitter POS-tagger BIBREF21 , respectively. Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best. Results. The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the", "(source + target) version, a source-only version, and a target-only version. More specifically, assuming an input feature set INLINEFORM0 for some INLINEFORM1 , where INLINEFORM2 is the number of features in the set, EasyAdapt transforms INLINEFORM3 to the augmented set, INLINEFORM4 . The mappings INLINEFORM5 for the source and target domain data, respectively, are defined as INLINEFORM6 and INLINEFORM7 , where INLINEFORM8 is the zero vector. Refer to Daum\u00e9 daumeiii:2007:ACLMain for an in-depth discussion of this technique. Each model was tested on the Amazon test data (the model trained only on Twitter was also tested on the Twitter test set). Amazon reviews were selected as the target domain since the Twitter dataset was much larger than the Amazon dataset; this scenario is more consistent with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for", "detection and show that the features generally perform well across text domains. Further, we validate that domain adaptation can be applied to sarcasm detection to leverage patterns in out-of-domain training data, even when results from training only on that source domain data are extremely bad (far below baseline results), to improve over training on only the target data or over training on the simply combined dataset. Finally, we make a new dataset of sarcastic and non-sarcastic tweets available online as a resource to other researchers. Related Work. The majority of work on automatic sarcasm detection has been done using Twitter, and to a smaller extent Amazon product reviews. Research outside of those domains has been scarce, but interesting. Notably, Burfoot and Baldwin Burfoot:2009:ASD:1667583.1667633 automatically detected satirical news articles using unigrams, lexical features, and semantic validity features, and Justo et al. Justo2014124 used n-gram, linguistic, and semantic", "(PMI) between the most positive or negative word in the instance and the n-grams that immediately proceed it to create a more general version of the feature. Other general features developed for this work rely on syntactic characteristics, or are bag-of-words-style features corresponding to the tokens most strongly correlated or most common in sarcastic and non-sarcastic instances from Twitter and Amazon training data. All general features are outlined in Table TABREF14 . Evaluation. The features used for each train/test scenario are shown in the first column of Table TABREF18 . Twitter Features refers to all features listed in Table TABREF11 preceded by the parenthetical (T), and Amazon Features to all features preceded by (A). General: Other Polarity includes the positive and negative percentages, average polarities, overall polarities, and largest polarity gap features from Table TABREF14 . General: Subjectivity includes the % strongly subjective positive words, % weakly subjective"]}
{"question_id": "9465d96a1368299fd3662d91aa94ba85347b4ccd", "predicted_answer": "0.89", "predicted_evidence": ["Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher. Models trained with TF/IDF bigram features performed worse, with scores of all evaluation metrics dropping with the exception of Multinomial Na\u00efve Bayes which improved in F1-score for the Not Offensive class. The full results are reported in table TABREF9 below. Three other approaches were opted for training the models with the implementation of POS and dependency relation tags via a transformation pipeline, also including TF/IDF unigram features, performing better than the addition of bigrams. Experiments with linguistic features were conducted, to inspect their efficiency for this task. For these experiments, the RBF SVM was not used due to data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all", "data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all three features are shown in table TABREF10 below. While the Linear SVM model improved the recall score on the previous model trained with bigrams, the other models show a significant drop in their performance. In the next experiment, POS tags were used in conjunction with TF/IDF unigram features. Surprisingly, the addition of POS tags in the Linear SVM yields the same F1-score as the first model trained on TF/IDF unigram features, yielding lower precision scores for both classes, while the recall score for the Offensive class improved marginally. The Na\u00efve Bayes models show a marginal decrease in their performance. On the other hand, the performance of SGDC significantly decreases with POS tags only and, interestingly enough, its recall score for the Offensive class is the worst among", "other classifiers in terms of macro-F1, which does not take label imbalance into account. The Linear SVM and SGDC perform almost identically, with the Linear SVM performing slightly better in recall score for the Not Offensive class and SGDC in recall score for the Offensive class. Bernoulli Na\u00efve Bayes performs better than all classifiers in recall score for the Offensive class but yields the lowest precision score of all classifiers. While the RBF SVM and Multinomial Na\u00efve Bayes yield better recall score for the Not Offensive class, their recall scores for the Offensive class are really low. For a binary text classification task like offensive language detection, a high recall score for both classes, especially for the Offensive class, is important for a model to be considered successful. Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher. Models trained with TF/IDF bigram features", "the Not Offensive class was already high, so this increase in recall score could slightly facilitate the offensive language detection task. Without improving upon the first SGDC presented, the SGDC rised in performance overall and as for the Na\u00efve Bayes representatives, the both the Multinomial and Bernoulli approaches performed better than in the second experiment. The complete results are shown in table TABREF12 below. The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be", "Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD. Methods ::: Discussion. The data annotated in OGTD proved to be facilitating in offensive language detection with a significant success for Greek, taking into consideration its size and label distribution, with the best model (LSTM and GRU with Attention) achieving a F1-macro of 0.89. Among the classical machine learning approaches, the linear SVM model achieved the best results, 0.80, whereas the the Stochastic Gradient Descent (SGD) learning classifier yielded the best recall score for the Offensive class, at 0.61. In terms of features used, TF/IDF matrices of word unigrams proved to work work well with multiple classical ML classifiers. Overall, it is clear that deep learning models with word embedding feature provide better results than the", "matrices of word unigrams proved to work work well with multiple classical ML classifiers. Overall, it is clear that deep learning models with word embedding feature provide better results than the classical machine learning models. Of the linguistic features, POS tags improved the performance of the Linear SVM marginally in terms of recall for the Offensive class, other classifiers deteriorated in their performance.It is not yet clear whether this is due to the accuracy of the Greek model available for spaCy in producing such tags or the tags themselves as features and is a subject that can be explored with further improvements of spaCy or other NLP tools developed for Greek. The dataset itself contains many instances with neologisms, creative uses of language or and even rare slang words, therefore training the existing model with such instances could improve both spaCy's accuracy for POS and dependency relation tags and the Linear SVM's performance in text classification for Greek.", "in their performance. On the other hand, the performance of SGDC significantly decreases with POS tags only and, interestingly enough, its recall score for the Offensive class is the worst among classifiers. The complete results are presented in table TABREF11 below. The experiment with linguistic features was the combination of dependency relation tags with TF/IDF unigrams. This experimented yielded the same F1-score of 80% as the other Linear SVM classifiers, performing almost identically with the previous model trained with POS tags, only bested in precision for the Offensive class. While the recall score for Offensive instances improves on the first model trained only on TF/IDF unigrams by 0.01%, the recall score for Not Offensive instances drops by the same amount. The recall score for the Not Offensive class was already high, so this increase in recall score could slightly facilitate the offensive language detection task. Without improving upon the first SGDC presented, the SGDC", "Greek. However there was no BERT model available for Greek language. The model that came closest our requirement was multilingual BERT model trained on 108 languages BIBREF24 including Greek. Since training BERT is a very computationaly expensive task we used the available multilingual BERT cased model for the sixth deep learning architecture. Methods ::: Models ::: Classical Machine Learning Models. Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient"]}
{"question_id": "e8c3f59313df20db0cdd49b84a37c44da849fe17", "predicted_answer": "Classical Machine Learning Models, Deep Learning Models.", "predicted_evidence": ["data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all three features are shown in table TABREF10 below. While the Linear SVM model improved the recall score on the previous model trained with bigrams, the other models show a significant drop in their performance. In the next experiment, POS tags were used in conjunction with TF/IDF unigram features. Surprisingly, the addition of POS tags in the Linear SVM yields the same F1-score as the first model trained on TF/IDF unigram features, yielding lower precision scores for both classes, while the recall score for the Offensive class improved marginally. The Na\u00efve Bayes models show a marginal decrease in their performance. On the other hand, the performance of SGDC significantly decreases with POS tags only and, interestingly enough, its recall score for the Offensive class is the worst among", "Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher. Models trained with TF/IDF bigram features performed worse, with scores of all evaluation metrics dropping with the exception of Multinomial Na\u00efve Bayes which improved in F1-score for the Not Offensive class. The full results are reported in table TABREF9 below. Three other approaches were opted for training the models with the implementation of POS and dependency relation tags via a transformation pipeline, also including TF/IDF unigram features, performing better than the addition of bigrams. Experiments with linguistic features were conducted, to inspect their efficiency for this task. For these experiments, the RBF SVM was not used due to data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all", "indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features. Methods ::: Models ::: Deep Learning Models. Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention", "Greek. However there was no BERT model available for Greek language. The model that came closest our requirement was multilingual BERT model trained on 108 languages BIBREF24 including Greek. Since training BERT is a very computationaly expensive task we used the available multilingual BERT cased model for the sixth deep learning architecture. Methods ::: Models ::: Classical Machine Learning Models. Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient", "the Not Offensive class was already high, so this increase in recall score could slightly facilitate the offensive language detection task. Without improving upon the first SGDC presented, the SGDC rised in performance overall and as for the Na\u00efve Bayes representatives, the both the Multinomial and Bernoulli approaches performed better than in the second experiment. The complete results are shown in table TABREF12 below. The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be", "measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5. Methods. Before experimenting with OGTD, an unique aspect of Greek which is the accentuation of characters for correct pronunciation needed to be normalized. When posting a tweet, many users omit accents due to their haste, resulting in a mixed dataset containing fully accented tweets, partially-accented tweets, and non-accented tweets. To achieve data uniformity and to avoid ambiguity, every word is lower-cased and then normalized to its non-accented equivalent. Several experiments were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning", "the best results were achieved by a LSTM and GRU with Attention model. Conclusion ::: Ongoing - OGTD v2.0 and OffensEval 2020. We have recently released OGTD v2.0 as training data for OffensEval 2020 (SemEval-2020 Task 12) BIBREF28. The reasoning behind the expansion of the dataset was to have a larger Greek dataset for the competition. New posts were collected in November 2019 following the same approach we used to compile v1.0 described in this paper. This second batch of tweets included tweets with trending hashtags, shows and topics from Greece at the time. Additionally, keywords that proved to retrieve interesting tweets in the first version were once again used in the search, along with new keywords like pejorative terms. When the collection was finished, 5,508 tweets were randomly sampled to be then annotated by a team of volunteers. The annotation guidelines were the same ones we used for v1.0. OGTD v2.0 combines the existing with the newly annotated tweets in a larger dataset", "were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning models. These features were selected based on previous methodology used by researchers and taking the dataset size into consideration. The TF-IDF weighted features are often used for text classification and are useful for determining how important a word is to a post in a corpus. The threshold for corpus specific words was set to 80%, ignoring terms appearing in more than 80% of the documents while the minimum document frequency was set to 6, and both unigrams and bigrams were tested. Given the consistent use of linguistic features for training machine learning models and results from previous work for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features. Using the spaCy pipeline for Greek, POS-tags and dependency relations were"]}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "predicted_answer": "LSTM and GRU with Attention", "predicted_evidence": ["BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish BIBREF2, Hindi BIBREF3, and German BIBREF4, to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1", "Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD. Methods ::: Discussion. The data annotated in OGTD proved to be facilitating in offensive language detection with a significant success for Greek, taking into consideration its size and label distribution, with the best model (LSTM and GRU with Attention) achieving a F1-macro of 0.89. Among the classical machine learning approaches, the linear SVM model achieved the best results, 0.80, whereas the the Stochastic Gradient Descent (SGD) learning classifier yielded the best recall score for the Offensive class, at 0.61. In terms of features used, TF/IDF matrices of word unigrams proved to work work well with multiple classical ML classifiers. Overall, it is clear that deep learning models with word embedding feature provide better results than the", "are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention BIBREF26 and BERT BIBREF24. These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks BIBREF26. Parameters described in BIBREF26 were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github . Methods ::: Results. The performance of individual classifiers for offensive language identification with TF/IDF unigram features is demonstrated in table TABREF8 below. We can see that both linear classifiers (SVM and SGDC) outperform the other classifiers in terms of macro-F1, which does not take label imbalance into account. The Linear SVM and SGDC perform almost identically, with the Linear SVM performing slightly better in recall", "therefore training the existing model with such instances could improve both spaCy's accuracy for POS and dependency relation tags and the Linear SVM's performance in text classification for Greek. Conclusion. This paper presented the Offensive Greek Tweet Dataset (OGTD), a manually annotated dataset for offensive language identification and the first Greek dataset of its kind. The OGTD v1.0 contains a total of 4,779 tweets, encompassing posts related to an array of topics popular among Greek people (e.g. political elections, TV shows, etc.). Tweets were manually annotated by a team volunteers through an annotation platform. We used the same guidelines used in the annotation of the English OLID dataset BIBREF5. Finally, we run several machine learning and deep learning classifiers and the best results were achieved by a LSTM and GRU with Attention model. Conclusion ::: Ongoing - OGTD v2.0 and OffensEval 2020. We have recently released OGTD v2.0 as training data for OffensEval 2020", "the Not Offensive class was already high, so this increase in recall score could slightly facilitate the offensive language detection task. Without improving upon the first SGDC presented, the SGDC rised in performance overall and as for the Na\u00efve Bayes representatives, the both the Multinomial and Bernoulli approaches performed better than in the second experiment. The complete results are shown in table TABREF12 below. The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be", "dataset of Greek tweets for offensive language detection, as well as Fotini and that helped review tweets with ambivalent labels. Additionally, we would like to express our sincere gratitude to the LightTag team and especially to Tal Perry for granting us free use for their annotation platform.", "for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features. Using the spaCy pipeline for Greek, POS-tags and dependency relations were extracted for every token in a tweet, which were then transformed to count matrices. A sentiment lexicon was considered, but one suitable for this project is as of yet unavailable for Greek. For the first six deep learning models we used Greek word embeddings trained on a large Greek web corpus BIBREF23. Each Greek word can be represented with a 300 dimensional vector using the trained model. The vector then can be used to feed in to the deep learning models which will be described in section SECREF16. For the last deep learning architecture we wanted to use a BERT BIBREF24 model trained on Greek. However there was no BERT model available for Greek language. The model that came closest our requirement was multilingual BERT model trained on 108 languages BIBREF24 including Greek. Since", "Introduction. In the age of social media, offensive content online has become prevalent in recent years. There are many types of offensive content online such as racist and sexist posts and insults and threats targeted at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types BIBREF0. BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in"]}
{"question_id": "de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "predicted_answer": "145", "predicted_evidence": ["to be then annotated by a team of volunteers. The annotation guidelines were the same ones we used for v1.0. OGTD v2.0 combines the existing with the newly annotated tweets in a larger dataset of 10,287 instances. Finally, both OGTD v1.0 and v2.0 provide the opportunity for researchers to test cross-lingual learning methods as it can be used in conjunction with the English OLID and other datasets annotated using the same guidelines such as the one by sigurbergsson2019offensive for Danish and by coltekikin2020 for Turkish while simultaneously facilitating the development of language resources for NLP in Greek. Acknowledgements. We would like to acknowledge Maria, Raphael and Anastasia, the team of volunteer annotators that provided their free time and efforts to help us produce v1.0 of the dataset of Greek tweets for offensive language detection, as well as Fotini and that helped review tweets with ambivalent labels. Additionally, we would like to express our sincere gratitude to the", "of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset. Inter-annotator agreement was subsequently calculated and labels with 100% agreement were deemed acceptable annotations. In cases of disagreement, labels with majority agreement above 66% were selected as the actual annotations of the tweets in question. For labels with complete disagreement between annotators, one of the authors of this paper reviewed the tweets with two extra human judges, to get the desired majority agreement above 66%. Figure FIGREF6 is a confusion matrix that shows the inter-annotator agreement or reliability, statistically measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive", "other users or famous people and TV personas, confirming that \u201c\u03b5\u03af\u03c3\u03b1\u03b9\u201d was a facilitating keyword for the task in question. The OGTD Dataset ::: Pre-processing and annotation. We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID BIBREF5. Duplicate punctuation such as question and exclamation marks was normalized. After removing duplicate tweets, the dataset was comprised of 46,218 tweets of which 5,000 were randomly sampled for annotation. We used LightTag to annotate the dataset due to its simple and straightforward user interface and limitless annotations, provided by the software creators. Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which", "therefore training the existing model with such instances could improve both spaCy's accuracy for POS and dependency relation tags and the Linear SVM's performance in text classification for Greek. Conclusion. This paper presented the Offensive Greek Tweet Dataset (OGTD), a manually annotated dataset for offensive language identification and the first Greek dataset of its kind. The OGTD v1.0 contains a total of 4,779 tweets, encompassing posts related to an array of topics popular among Greek people (e.g. political elections, TV shows, etc.). Tweets were manually annotated by a team volunteers through an annotation platform. We used the same guidelines used in the annotation of the English OLID dataset BIBREF5. Finally, we run several machine learning and deep learning classifiers and the best results were achieved by a LSTM and GRU with Attention model. Conclusion ::: Ongoing - OGTD v2.0 and OffensEval 2020. We have recently released OGTD v2.0 as training data for OffensEval 2020", "dataset of Greek tweets for offensive language detection, as well as Fotini and that helped review tweets with ambivalent labels. Additionally, we would like to express our sincere gratitude to the LightTag team and especially to Tal Perry for granting us free use for their annotation platform.", "BIBREF1 investigate the similarity between these sub-tasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish BIBREF2, Hindi BIBREF3, and German BIBREF4, to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English BIBREF5 used in the recent OffensEval (SemEval-2019 Task 6) BIBREF6. In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a high-quality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1", "the best results were achieved by a LSTM and GRU with Attention model. Conclusion ::: Ongoing - OGTD v2.0 and OffensEval 2020. We have recently released OGTD v2.0 as training data for OffensEval 2020 (SemEval-2020 Task 12) BIBREF28. The reasoning behind the expansion of the dataset was to have a larger Greek dataset for the competition. New posts were collected in November 2019 following the same approach we used to compile v1.0 described in this paper. This second batch of tweets included tweets with trending hashtags, shows and topics from Greece at the time. Additionally, keywords that proved to retrieve interesting tweets in the first version were once again used in the search, along with new keywords like pejorative terms. When the collection was finished, 5,508 tweets were randomly sampled to be then annotated by a team of volunteers. The annotation guidelines were the same ones we used for v1.0. OGTD v2.0 combines the existing with the newly annotated tweets in a larger dataset", "measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5. Methods. Before experimenting with OGTD, an unique aspect of Greek which is the accentuation of characters for correct pronunciation needed to be normalized. When posting a tweet, many users omit accents due to their haste, resulting in a mixed dataset containing fully accented tweets, partially-accented tweets, and non-accented tweets. To achieve data uniformity and to avoid ambiguity, every word is lower-cased and then normalized to its non-accented equivalent. Several experiments were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning"]}
{"question_id": "5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "predicted_answer": "No", "predicted_evidence": ["measured by Cohen's kappa coefficient. The benchmark annotated dataset produced contained 4,779 tweets, containing over 29% offensive content. The final distribution of labels in the new Offensive Greek Tweet Dataset (OGTD), along with the breakdown of the data into training and testing, is showing in Table TABREF5. Methods. Before experimenting with OGTD, an unique aspect of Greek which is the accentuation of characters for correct pronunciation needed to be normalized. When posting a tweet, many users omit accents due to their haste, resulting in a mixed dataset containing fully accented tweets, partially-accented tweets, and non-accented tweets. To achieve data uniformity and to avoid ambiguity, every word is lower-cased and then normalized to its non-accented equivalent. Several experiments were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning", "the best results were achieved by a LSTM and GRU with Attention model. Conclusion ::: Ongoing - OGTD v2.0 and OffensEval 2020. We have recently released OGTD v2.0 as training data for OffensEval 2020 (SemEval-2020 Task 12) BIBREF28. The reasoning behind the expansion of the dataset was to have a larger Greek dataset for the competition. New posts were collected in November 2019 following the same approach we used to compile v1.0 described in this paper. This second batch of tweets included tweets with trending hashtags, shows and topics from Greece at the time. Additionally, keywords that proved to retrieve interesting tweets in the first version were once again used in the search, along with new keywords like pejorative terms. When the collection was finished, 5,508 tweets were randomly sampled to be then annotated by a team of volunteers. The annotation guidelines were the same ones we used for v1.0. OGTD v2.0 combines the existing with the newly annotated tweets in a larger dataset", "therefore training the existing model with such instances could improve both spaCy's accuracy for POS and dependency relation tags and the Linear SVM's performance in text classification for Greek. Conclusion. This paper presented the Offensive Greek Tweet Dataset (OGTD), a manually annotated dataset for offensive language identification and the first Greek dataset of its kind. The OGTD v1.0 contains a total of 4,779 tweets, encompassing posts related to an array of topics popular among Greek people (e.g. political elections, TV shows, etc.). Tweets were manually annotated by a team volunteers through an annotation platform. We used the same guidelines used in the annotation of the English OLID dataset BIBREF5. Finally, we run several machine learning and deep learning classifiers and the best results were achieved by a LSTM and GRU with Attention model. Conclusion ::: Ongoing - OGTD v2.0 and OffensEval 2020. We have recently released OGTD v2.0 as training data for OffensEval 2020", "data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all three features are shown in table TABREF10 below. While the Linear SVM model improved the recall score on the previous model trained with bigrams, the other models show a significant drop in their performance. In the next experiment, POS tags were used in conjunction with TF/IDF unigram features. Surprisingly, the addition of POS tags in the Linear SVM yields the same F1-score as the first model trained on TF/IDF unigram features, yielding lower precision scores for both classes, while the recall score for the Offensive class improved marginally. The Na\u00efve Bayes models show a marginal decrease in their performance. On the other hand, the performance of SGDC significantly decreases with POS tags only and, interestingly enough, its recall score for the Offensive class is the worst among", "and women with a dataset from Twitter, in English and Spanish BIBREF22. The OGTD Dataset. The posts in OGTD v1.0 were collected between May and June, 2019. We used the Twitter API initially collecting tweets from popular and trending hashtags in Greece, including television programs such as series, reality and entertainment shows. Due to the municipal, regional as well as the European Parliament election taking place at the time, many hashtags included tweets discussing the elections. The intuition behind this approach is that Twitter as a microblogging service often gathers complaints and profane comments on widely viewed television and politics, and as such, this period was a good opportunity for data collection. Following the methodology described in BIBREF5 and others, including a recent comparable Danish dataset BIBREF20, we collected tweets using keywords such as sensitive or obscene language. Queries for tweets containing common curse words and expressions usually found in", "other users or famous people and TV personas, confirming that \u201c\u03b5\u03af\u03c3\u03b1\u03b9\u201d was a facilitating keyword for the task in question. The OGTD Dataset ::: Pre-processing and annotation. We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID BIBREF5. Duplicate punctuation such as question and exclamation marks was normalized. After removing duplicate tweets, the dataset was comprised of 46,218 tweets of which 5,000 were randomly sampled for annotation. We used LightTag to annotate the dataset due to its simple and straightforward user interface and limitless annotations, provided by the software creators. Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which", "work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or okay BIBREF8 with a Na\u00efve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures BIBREF9. A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using n-grams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features BIBREF10. The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods BIBREF11. The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and", "other classifiers in terms of macro-F1, which does not take label imbalance into account. The Linear SVM and SGDC perform almost identically, with the Linear SVM performing slightly better in recall score for the Not Offensive class and SGDC in recall score for the Offensive class. Bernoulli Na\u00efve Bayes performs better than all classifiers in recall score for the Offensive class but yields the lowest precision score of all classifiers. While the RBF SVM and Multinomial Na\u00efve Bayes yield better recall score for the Not Offensive class, their recall scores for the Offensive class are really low. For a binary text classification task like offensive language detection, a high recall score for both classes, especially for the Offensive class, is important for a model to be considered successful. Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher. Models trained with TF/IDF bigram features"]}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "predicted_answer": "Classical Machine Learning Models, Deep Learning Models, Linear SVM, Na\u00efve Bayes, RBF SVM, Stochastic Gradient Descent (SGDC)", "predicted_evidence": ["data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all three features are shown in table TABREF10 below. While the Linear SVM model improved the recall score on the previous model trained with bigrams, the other models show a significant drop in their performance. In the next experiment, POS tags were used in conjunction with TF/IDF unigram features. Surprisingly, the addition of POS tags in the Linear SVM yields the same F1-score as the first model trained on TF/IDF unigram features, yielding lower precision scores for both classes, while the recall score for the Offensive class improved marginally. The Na\u00efve Bayes models show a marginal decrease in their performance. On the other hand, the performance of SGDC significantly decreases with POS tags only and, interestingly enough, its recall score for the Offensive class is the worst among", "Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher. Models trained with TF/IDF bigram features performed worse, with scores of all evaluation metrics dropping with the exception of Multinomial Na\u00efve Bayes which improved in F1-score for the Not Offensive class. The full results are reported in table TABREF9 below. Three other approaches were opted for training the models with the implementation of POS and dependency relation tags via a transformation pipeline, also including TF/IDF unigram features, performing better than the addition of bigrams. Experiments with linguistic features were conducted, to inspect their efficiency for this task. For these experiments, the RBF SVM was not used due to data handling problems by the model in the scikit-learn library. In the first experiment, TF/IDF unigram features were combined with POS and dependency relation tags. The results of implementing all", "indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features. Methods ::: Models ::: Deep Learning Models. Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU BIBREF25, Stacked LSTM with Attention BIBREF25, LSTM and GRU with Attention BIBREF25, 2D Convolution with Pooling BIBREF26, GRU with Capsule BIBREF27, LSTM with Capsule and Attention", "the Not Offensive class was already high, so this increase in recall score could slightly facilitate the offensive language detection task. Without improving upon the first SGDC presented, the SGDC rised in performance overall and as for the Na\u00efve Bayes representatives, the both the Multinomial and Bernoulli approaches performed better than in the second experiment. The complete results are shown in table TABREF12 below. The performance of the deep learning models is presented in table TABREF18. As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be", "Greek. However there was no BERT model available for Greek language. The model that came closest our requirement was multilingual BERT model trained on 108 languages BIBREF24 including Greek. Since training BERT is a very computationaly expensive task we used the available multilingual BERT cased model for the sixth deep learning architecture. Methods ::: Models ::: Classical Machine Learning Models. Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient", "Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD. Methods ::: Discussion. The data annotated in OGTD proved to be facilitating in offensive language detection with a significant success for Greek, taking into consideration its size and label distribution, with the best model (LSTM and GRU with Attention) achieving a F1-macro of 0.89. Among the classical machine learning approaches, the linear SVM model achieved the best results, 0.80, whereas the the Stochastic Gradient Descent (SGD) learning classifier yielded the best recall score for the Offensive class, at 0.61. In terms of features used, TF/IDF matrices of word unigrams proved to work work well with multiple classical ML classifiers. Overall, it is clear that deep learning models with word embedding feature provide better results than the", "were conducted with the OGTD, each one utilizing a different combination from a pool of features (e.g. TF/IDF unigrams, bigrams, POS and dependency relation tags) to train machine learning models. These features were selected based on previous methodology used by researchers and taking the dataset size into consideration. The TF-IDF weighted features are often used for text classification and are useful for determining how important a word is to a post in a corpus. The threshold for corpus specific words was set to 80%, ignoring terms appearing in more than 80% of the documents while the minimum document frequency was set to 6, and both unigrams and bigrams were tested. Given the consistent use of linguistic features for training machine learning models and results from previous work for offensive language detection, part-of-speech (POS) and dependency relation tags were considered as additional features. Using the spaCy pipeline for Greek, POS-tags and dependency relations were", "in their performance. On the other hand, the performance of SGDC significantly decreases with POS tags only and, interestingly enough, its recall score for the Offensive class is the worst among classifiers. The complete results are presented in table TABREF11 below. The experiment with linguistic features was the combination of dependency relation tags with TF/IDF unigrams. This experimented yielded the same F1-score of 80% as the other Linear SVM classifiers, performing almost identically with the previous model trained with POS tags, only bested in precision for the Offensive class. While the recall score for Offensive instances improves on the first model trained only on TF/IDF unigrams by 0.01%, the recall score for Not Offensive instances drops by the same amount. The recall score for the Not Offensive class was already high, so this increase in recall score could slightly facilitate the offensive language detection task. Without improving upon the first SGDC presented, the SGDC"]}
{"question_id": "211c242c028b35bb9cbd5e303bb6c750f859fd34", "predicted_answer": "No", "predicted_evidence": ["languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level BIBREF17 , BIBREF18 , as well as at aspect-level BIBREF7 , BIBREF19 , BIBREF20 . The co-official languages, however, have almost none. The authors are aware of a small discourse-related sentiment corpus available in Basque BIBREF21 , as well as a stance corpus in Catalan BIBREF22 . These resources, however, are limited in size and scope. Data Collection. In order to improve the lack of data in low-resource languages, we introduce two aspect-level sentiment datasets to the community, available for Catalan and Basque. To collect suitable corpora, we crawl hotel reviews from www.booking.com. Booking.com allows you to search for reviews in Catalan, but it does not include Basque. Therefore, for Basque we crawled reviews from a number of other websites that allow users to comment on their stay Many of the reviews that we found through crawling are either 1) in Spanish, 2)", "Therefore, for Basque we crawled reviews from a number of other websites that allow users to comment on their stay Many of the reviews that we found through crawling are either 1) in Spanish, 2) include a mix of Spanish and the target language, or 3) do not contain any sentiment phrases. Therefore, we use a simple language identification method in order to remove any Spanish or mixed reviews and also remove any reviews that are shorter than 7 tokens. This finally gave us a total of 568 reviews in Catalan and 343 reviews in Basque, collected from November 2015 to January 2016. We preprocess them through a very light normalization, after which we perform tokenization, pos-tagging and lemmatization using Ixa-pipes Agerri2014. Our final documents are in KAF/NAF format BIBREF23 , BIBREF24 . This is a stand-off xml format originally from the Kyoto project BIBREF23 and allows us to enrich our documents with many layers of linguistic information, such as the pos tag of a word, its lemma,", "be overcome by focusing only on English data. The novelty of this work lies in creating corpora which cover both Basque and Catalan languages and are annotated in such a way that they are compatible with similarly compiled corpora available in a number of languages BIBREF7 . This allows for further research into cross-lingual sentiment analysis, as well as introducing the first resource for aspect-level sentiment analysis in Catalan and Basque. The corpus is available at http://hdl.handle.net/10230/33928 or https://jbarnesspain.github.io/resources/. Related Work. In English there are many datasets available for document- and sentence-level sentiment analysis across different domains and at different levels of annotation BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require", "score. The results of the benchmark experiment (shown in Table TABREF23 ) show that these simple baselines achieve results which are somewhat lower but still comparable to similar tasks in English BIBREF5 . The drop is not surprising given that we use a relatively simple baseline system and due to the fact that Catalan and Basque have richer morphological systems than English, which were not exploited. Conclusion. In this paper we have presented the MultiBooked corpus \u2013 a corpus of hotel reviews annotated for aspect-level sentiment analysis available in Basque and Catalan. The aim of this annotation project is to allow researchers to enable research on supervised aspect-level sentiment analysis in Basque and Catalan, as well as provide useful data for cross- and multi-lingual sentiment analysis. We also provide inter-annotator agreement scores and benchmarks, as well as making the corpus available to the community. Language Resource References. lrec lit", "any span for these labels and were not limited to the number of annotations they could make. This reflects the clarity of the guidelines used to guide the annotation process. The agreement score for opinion holders is somewhat lower and stems from the fact that there were relatively few instances of explicit opinion holders. Additionally, Catalan and Basque both have agreement features for verbs, which could be considered an implicit mention of the opinion holder. This is not always clear, however. Finally, the mean squared error of the polarity scores shows that annotators generally agree on where and which polarity score should be given. Again, the mean squared error in this annotation scheme requires both annotators to choose the same span and the same polarity to achieve perfect agreement. Difficult Examples. During annotation, there were certain sentences which presented a great deal of problems for the annotators. Many of these are difficult because of 1) nested opinions, 2)", "have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus BIBREF13 has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have concentrated on aspect-level sentiment analysis BIBREF14 , BIBREF15 , BIBREF16 . The Iberian peninsula contains two official languages (Portuguese and Spanish), as well as three co-official languages (Basque, Catalan, and Galician) and several smaller languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level BIBREF17 , BIBREF18 , as well as at aspect-level BIBREF7 , BIBREF19 , BIBREF20 . The", "Difficult Examples. During annotation, there were certain sentences which presented a great deal of problems for the annotators. Many of these are difficult because of 1) nested opinions, 2) implicit opinions reported only through the presence or absence of certain aspects, or 3) the difficulty to identify the span of an expression. Here, we give examples of each difficulty and detail how these were resolved during the annotation process. In the Basque sentence in Example UID18 , we can see that there are two distinct levels of aspects. First, the aspect `hotel', which has a positive polarity and then the sub-aspect `workers'. We avoid the problem of deciding which is the opinion target by treating these as two separate opinions, whose targets are `hotel' and `workers'. If there was an implicit opinion based on the presence or absence of a desirable aspect, such as the one seen in Example UID19 , we asked annotators to identify the phrase that indicates presence or absence, i.e.", "('BAAAAD!!!!!') in order to decide between the default polarity and the strong version. Process. We used the KafAnnotator Tool BIBREF7 to annotate each review. This tool allows the user to select a span of tokens and to annotate them as any of the four labels mentioned in Section SECREF3 . The annotation of each corpus was performed in three phases: first, each annotator annotated a small number of reviews (20-50), after which they compared annotations and discussed any differences. Second, the annotators annotated half of the remaining reviews and met again to discuss any new differences. Finally, they annotated the remaining reviews. For cases of conflict after the final iteration, a third annotator decided between the two. The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343. Dataset Characteristics. The reviews are typical hotel reviews, which often mention various aspects of the hotel or experience and the polarity towards these aspects. An"]}
{"question_id": "9b05d5f723a8a452522907778a084b52e27fd924", "predicted_answer": "567 annotated reviews for Catalan and 343 annotated reviews for Basque.", "predicted_evidence": ["be overcome by focusing only on English data. The novelty of this work lies in creating corpora which cover both Basque and Catalan languages and are annotated in such a way that they are compatible with similarly compiled corpora available in a number of languages BIBREF7 . This allows for further research into cross-lingual sentiment analysis, as well as introducing the first resource for aspect-level sentiment analysis in Catalan and Basque. The corpus is available at http://hdl.handle.net/10230/33928 or https://jbarnesspain.github.io/resources/. Related Work. In English there are many datasets available for document- and sentence-level sentiment analysis across different domains and at different levels of annotation BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 . These resources have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require", "the final Basque corpus 343. Dataset Characteristics. The reviews are typical hotel reviews, which often mention various aspects of the hotel or experience and the polarity towards these aspects. An example is shown in Example Statistics for the two corpora are shown in Table TABREF12 . Agreement Scores. Common metrics for determining inter-annotator agreement, e.g. Cohen's Kappa BIBREF25 or Fleiss' Kappa BIBREF26 , can not be applied when annotating sequences, as the annotators are free to choose which parts of a sequence to include. Therefore, we use the agr metric BIBREF13 , which is defined as: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are annotators and INLINEFORM2 and INLINEFORM3 are the set of annotations for each annotator. If we consider INLINEFORM4 to be the gold standard, INLINEFORM5 corresponds to the recall of the system, and precision if INLINEFORM6 is the gold standard. For each pair of annotations, we report the average of the INLINEFORM7 metric with both", "('BAAAAD!!!!!') in order to decide between the default polarity and the strong version. Process. We used the KafAnnotator Tool BIBREF7 to annotate each review. This tool allows the user to select a span of tokens and to annotate them as any of the four labels mentioned in Section SECREF3 . The annotation of each corpus was performed in three phases: first, each annotator annotated a small number of reviews (20-50), after which they compared annotations and discussed any differences. Second, the annotators annotated half of the remaining reviews and met again to discuss any new differences. Finally, they annotated the remaining reviews. For cases of conflict after the final iteration, a third annotator decided between the two. The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343. Dataset Characteristics. The reviews are typical hotel reviews, which often mention various aspects of the hotel or experience and the polarity towards these aspects. An", "have been built up over a period of more than a decade and are currently necessary to achieve state-of-the-art performance. Corpora annotated at fine-grained levels (opinion- or aspect-level) require more effort from annotators, but are able to capture information which is not present at document- or sentence-level, such as nested opinions or differing polarities of different aspects of a single entity. In English, the MPQA corpus BIBREF13 has been widely used in fine-grained opinion research. More recently, a number of SemEval tasks have concentrated on aspect-level sentiment analysis BIBREF14 , BIBREF15 , BIBREF16 . The Iberian peninsula contains two official languages (Portuguese and Spanish), as well as three co-official languages (Basque, Catalan, and Galician) and several smaller languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level BIBREF17 , BIBREF18 , as well as at aspect-level BIBREF7 , BIBREF19 , BIBREF20 . The", ", BIBREF5 , but for other languages there is a marked drop in accuracy. This is mainly due to the lack of annotations and resources in these languages. This is especially true of corpora annotated at aspect-level. Unlike document- or tweet-level annotation, aspect-level annotation requires a large amount of effort from the annotators, which further reduces the likelihood of finding an aspect-level sentiment corpus in under-resourced languages. We are, however, aware of one corpus annotated for aspects in German BIBREF6 , although German is not a particularly low-resource language. The movement towards multi-lingual datasets for sentiment analysis is important because many languages offer different challenges, such as complex morphology or highly productive word formation, which can not be overcome by focusing only on English data. The novelty of this work lies in creating corpora which cover both Basque and Catalan languages and are annotated in such a way that they are compatible", "languages (Aragonese, Gascon). The two official languages do have available resources for sentiment at tweet-level BIBREF17 , BIBREF18 , as well as at aspect-level BIBREF7 , BIBREF19 , BIBREF20 . The co-official languages, however, have almost none. The authors are aware of a small discourse-related sentiment corpus available in Basque BIBREF21 , as well as a stance corpus in Catalan BIBREF22 . These resources, however, are limited in size and scope. Data Collection. In order to improve the lack of data in low-resource languages, we introduce two aspect-level sentiment datasets to the community, available for Catalan and Basque. To collect suitable corpora, we crawl hotel reviews from www.booking.com. Booking.com allows you to search for reviews in Catalan, but it does not include Basque. Therefore, for Basque we crawled reviews from a number of other websites that allow users to comment on their stay Many of the reviews that we found through crawling are either 1) in Spanish, 2)", "score. The results of the benchmark experiment (shown in Table TABREF23 ) show that these simple baselines achieve results which are somewhat lower but still comparable to similar tasks in English BIBREF5 . The drop is not surprising given that we use a relatively simple baseline system and due to the fact that Catalan and Basque have richer morphological systems than English, which were not exploited. Conclusion. In this paper we have presented the MultiBooked corpus \u2013 a corpus of hotel reviews annotated for aspect-level sentiment analysis available in Basque and Catalan. The aim of this annotation project is to allow researchers to enable research on supervised aspect-level sentiment analysis in Basque and Catalan, as well as provide useful data for cross- and multi-lingual sentiment analysis. We also provide inter-annotator agreement scores and benchmarks, as well as making the corpus available to the community. Language Resource References. lrec lit", "an implicit opinion based on the presence or absence of a desirable aspect, such as the one seen in Example UID19 , we asked annotators to identify the phrase that indicates presence or absence, i.e. `there was', as the opinion phrase. Finally, in order to improve overlap in span selection, we instructed annotators to choose the smallest span possible that retains the necessary information. Even after several iterations, however, there were still discrepancies with difficult examples, such as the one shown in Example UID20 , where the opinion target could be either `attention', `the attention', or `the attention that the staff gave'. Benchmarks. In order to provide a simple baseline, we frame the extraction of opinion holders, targets, and phrases as a sequence labeling task and map the NAF tags to BIO tags for the opinions in each review. These tags serve as the gold labels which will need to be predicted at test time. We also perform classification of the polarity of opinion"]}
{"question_id": "21175d8853fd906266f884bced85c598c35b1cbc", "predicted_answer": "Unanswerable", "predicted_evidence": ["('BAAAAD!!!!!') in order to decide between the default polarity and the strong version. Process. We used the KafAnnotator Tool BIBREF7 to annotate each review. This tool allows the user to select a span of tokens and to annotate them as any of the four labels mentioned in Section SECREF3 . The annotation of each corpus was performed in three phases: first, each annotator annotated a small number of reviews (20-50), after which they compared annotations and discussed any differences. Second, the annotators annotated half of the remaining reviews and met again to discuss any new differences. Finally, they annotated the remaining reviews. For cases of conflict after the final iteration, a third annotator decided between the two. The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343. Dataset Characteristics. The reviews are typical hotel reviews, which often mention various aspects of the hotel or experience and the polarity towards these aspects. An", "any span for these labels and were not limited to the number of annotations they could make. This reflects the clarity of the guidelines used to guide the annotation process. The agreement score for opinion holders is somewhat lower and stems from the fact that there were relatively few instances of explicit opinion holders. Additionally, Catalan and Basque both have agreement features for verbs, which could be considered an implicit mention of the opinion holder. This is not always clear, however. Finally, the mean squared error of the polarity scores shows that annotators generally agree on where and which polarity score should be given. Again, the mean squared error in this annotation scheme requires both annotators to choose the same span and the same polarity to achieve perfect agreement. Difficult Examples. During annotation, there were certain sentences which presented a great deal of problems for the annotators. Many of these are difficult because of 1) nested opinions, 2)", ". This is a stand-off xml format originally from the Kyoto project BIBREF23 and allows us to enrich our documents with many layers of linguistic information, such as the pos tag of a word, its lemma, whether it is a polar word, and if so, if it has an opinion holder or target. The advantage of this format is that we do not have to change the original text in any way. Annotation. For annotation, we adopt the approach taken in the OpeNER project BIBREF7 , where annotators are free to choose both the span and label for any part of the text. Guidelines. In the OpeNER annotation scheme (see Table TABREF8 for a short summary), an annotator reads a review and must first decide if there is any positive or negative attitudes in the sentence. If there are, they then decide if the sentence is on topic. Since these reviews are about hotels, we constrain the opinion targets and opinion expressions to those that deal with aspects of the hotel. Annotators should annotate the span of text which", "topic. Since these reviews are about hotels, we constrain the opinion targets and opinion expressions to those that deal with aspects of the hotel. Annotators should annotate the span of text which refers to: opinion holders, opinion targets, and opinion expressions. If any opinion expression is found, the annotators must then also determine the polarity of the expression, which can be strong negative, negative, positive, or strong positive. As the opinion holder and targets are often implicit, we only require that each review has at least one annotated opinion expression. For the strong positive and strong negative labels, annotators must use clues such as adverbial modifiers ('very bad'), inherently strong adjectives ('horrible'), and any use of capitalization, repetition, or punctuation ('BAAAAD!!!!!') in order to decide between the default polarity and the strong version. Process. We used the KafAnnotator Tool BIBREF7 to annotate each review. This tool allows the user to select a", "INLINEFORM5 corresponds to the recall of the system, and precision if INLINEFORM6 is the gold standard. For each pair of annotations, we report the average of the INLINEFORM7 metric with both annotators as the temporary gold standard, DISPLAYFORM0  Perfect agreement, therefore, is 1.0 and no agreement whatsoever is 0.0. Similar annotation projects BIBREF13 report INLINEFORM0 scores that range between 0.6 and 0.8 in general. For polarity, we assign integers to each label (Strong Negative: 0, Negative: 1, Positive: 2, Strong Positive: 3). For each sentence of length INLINEFORM0 , we take the mean squared error (MSE), DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are the sets of annotations for the sentence in question. This approach punishes larger discrepancies in polarity more than small discrepancies, i.e. if annotator 1 decides an opinion expression is strong negative and annotator two that the same expression is positive, this will be reflected in a larger MSE score than if", "small discrepancies, i.e. if annotator 1 decides an opinion expression is strong negative and annotator two that the same expression is positive, this will be reflected in a larger MSE score than if annotator 2 had chosen negative. Perfect agreement between annotators would lead to a MSE of 0.0, with the maximum depending on the length of the phrase. For a phrase of ten words, the worst MSE possible (assuming annotator 1 labeled all words strong positive and annotator 2 labeled them strong negative) would be a 9.0. We take the mean of all the MSE scores in the corpus. Inter-annotator agreement is reported in Table TABREF17 . The inter-annotator agreement for target and expressions is high and in line with previous annotation efforts BIBREF13 , given the fact that annotators could choose any span for these labels and were not limited to the number of annotations they could make. This reflects the clarity of the guidelines used to guide the annotation process. The agreement score for", "Difficult Examples. During annotation, there were certain sentences which presented a great deal of problems for the annotators. Many of these are difficult because of 1) nested opinions, 2) implicit opinions reported only through the presence or absence of certain aspects, or 3) the difficulty to identify the span of an expression. Here, we give examples of each difficulty and detail how these were resolved during the annotation process. In the Basque sentence in Example UID18 , we can see that there are two distinct levels of aspects. First, the aspect `hotel', which has a positive polarity and then the sub-aspect `workers'. We avoid the problem of deciding which is the opinion target by treating these as two separate opinions, whose targets are `hotel' and `workers'. If there was an implicit opinion based on the presence or absence of a desirable aspect, such as the one seen in Example UID19 , we asked annotators to identify the phrase that indicates presence or absence, i.e.", "an implicit opinion based on the presence or absence of a desirable aspect, such as the one seen in Example UID19 , we asked annotators to identify the phrase that indicates presence or absence, i.e. `there was', as the opinion phrase. Finally, in order to improve overlap in span selection, we instructed annotators to choose the smallest span possible that retains the necessary information. Even after several iterations, however, there were still discrepancies with difficult examples, such as the one shown in Example UID20 , where the opinion target could be either `attention', `the attention', or `the attention that the staff gave'. Benchmarks. In order to provide a simple baseline, we frame the extraction of opinion holders, targets, and phrases as a sequence labeling task and map the NAF tags to BIO tags for the opinions in each review. These tags serve as the gold labels which will need to be predicted at test time. We also perform classification of the polarity of opinion"]}
{"question_id": "87c00edc497274ae6a972c3097818de85b1b384f", "predicted_answer": "It is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components.", "predicted_evidence": ["our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph. Method ::: Sentence Structure Recognition. The sentence structure recognition process involves 2 modules: natural language processing (NLP) module and logical reasoning on result from NLP module. In this paper, we make use of the Stanford Parser tools described in BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14 The NLP module tokenizes the input", "Method ::: Sentence Components Recognition. The goal of this step is to identify the relationship between elements of a sentence structure and chunks of words in a sentence from the POS tags and the dependency-based parse tree. For example, the sentence \u201cBill plays a game\u201d is encoded by a structure #2 and we expect that Bill, plays, and game correspond to the subject, verb, and object, respectively. We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\\Pi _2$ (Listing ). The first four rules of $\\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in \u201cCathy is gorgeous,\u201d the part after tobe is an adjective, but in \u201cCathy is a beautiful girl,\u201d the part after tobe is a noun, though, with adjective beautiful.", "$i$-value of the structure. For example, $structure(1,1)$ will be recognized if the nsubj relation is in the dependency-based parse tree; $structure(3,3)$ needs 3 dependency relations to be actived: nsubj, xcomp and dobj. We often use structure #$x$ to indicate a structure of type $x$. Together with the collection of the atoms encoding the relations in the dependency-based parse tree, $\\Pi _1$ generates several atoms of the form $structure(x,y)$ for a sentence. Among all these atoms, an atom with the highest $i$-value represents the structure constructed using the highest number of dependency relations. And hence, that structure is the most informative structure that is recoginized for the sentence. Observe that $structure(1,1)$ is the most simplified structure of any sentence. Method ::: Sentence Components Recognition. The goal of this step is to identify the relationship between elements of a sentence structure and chunks of words in a sentence from the POS tags and the", "provide encouraging results. They also point to possible improvements that we plan to introduce to the next version of the system. We will focus on processing relative clauses and enriching the set of sentence structures, especially for compound and complex sentences.", "the above GF rules, a GF grammar for each sentence can be constructed. However, this grammar can only be used to generate fairly simple sentences. For example, for the sentence \u201cBill plays a popular board game with his close friends.\u201d, a GF grammar for structure #2 can be constructed, which can only generate the sentence \u201cBill plays game.\u201d because it does not contain any complement components identified in Section SECREF15. Therefore, we assgin a set of GF rules for the construction of each parameter in the GF rules in Table TABREF19. The set of GF rules has to follow two conventions. The first one is after applying the set of rules to some components of the sentence, the type of the production is one of the type in Table TABREF19, e.g. $NP$, $VP$, $Cl$, $V2$, .... The second convention is that the GF encoder will select the rules as the order from top to bottom in Table TABREF20. Note that the encoder always has information of what type of input and output for the rule it is looking", "(in Italian) which is displayed in the second line. Figure FIGREF6 shows the meaning in the abstract syntax is represented in Japanese and in Italian, i.e. the two strings represent the same meaning. Method. To generate a sentence, we need a sentence structure and vocabularies. Our system is developed to emulate the process of a person learning a new language and has to make guesses to understand new sentences from time to time. For example, someone, who understands the sentence \u201cBill plays a game\u201d would not fully understand the sentence \u201cBill plays a popular board game\u201d without knowing the meaning of \u201cpopular\u201d and \u201cboard game\u201d but could infer that the latter sentence indicates that its subject plays a type of game. The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and", "be of different forms. For instance, in \u201cCathy is gorgeous,\u201d the part after tobe is an adjective, but in \u201cCathy is a beautiful girl,\u201d the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\\Pi _2$. The result of program $\\Pi _2$ is an one-to-one mapping of some of the words in the sentence into the importaint components of a sentence, called main components, i.e. subject, object and verb. The mapping is constructed by using the core arguments in Universal Dependency Relations . Since not every word in the sentence is in a core argument relation, there are some words in the sentence that are not in the domain of the mapping that $\\Pi _2$ produces. We denote these words are complement components. To identify these words, we encode the Non-core dependents and Nominal dependents from Universal Dependency Relations into the set of rules in program $\\Pi _3$. Program $\\Pi _3$ (Listing ), together with the atoms extracted from the", "the table below:  In GF, NP, VV, V2, VP, and Cl stand for noun phrase, verb-phrase-complement verb, two-place verb, verb phrase and clause, respectively. Note that although the set of GF grammatical rules can be used to construct a constituency-based parse tree , the reverse direction is not always true. To the best of our knowledge, there exists no algorithm for converting a constituency-based parse tree to a set GF grammar rules. We therefore need to identify the GF rules for each sentence structure. In our system, a GF rule is assigned to a structure initially (Table TABREF19). Each rule in Table TABREF19 represents the first level of the constituency-based parse tree. It acts as the coordinator for all other succeeding rules. Given the seed components identified in Section SECREF15 and the above GF rules, a GF grammar for each sentence can be constructed. However, this grammar can only be used to generate fairly simple sentences. For example, for the sentence \u201cBill plays a popular"]}
{"question_id": "de4e949c6917ff6933f5fa2a3062ba703aba014c", "predicted_answer": "Generating natural language descriptions for workflows created by the systems built in the Phylotastic project; generating natural language descriptions of computational results to non-expert users such as those developed in the Phylotastic project.", "predicted_evidence": ["require a large corpus for the generation task and can be used in different types of applications. In the first type of applications, the system can work with annotated ontologies to translate a set of atoms\u2014representing the answer to a query to the ontology\u2014to a set of sentences. To do so, the system extracts the annotations related to the atoms in the answer and creates a GF program that is then used to generate natural language description of the given set of atoms. In the second type of applications, the system receives a paragraph of text and generates an intermediate representation\u2014as a GF program\u2014for the paragraph, which can be used for different purpose such as cross-translation, addressing a need identified in BIBREF7 . Our use cases with different ontologies and Wikipedia portals provide encouraging results. They also point to possible improvements that we plan to introduce to the next version of the system. We will focus on processing relative clauses and enriching the set", "Introduction. Natural language generation (NLG) has been one of the key topics of research in natural language processing, which was highlighted by the huge body of work on NLG surveyed in BIBREF0, BIBREF1. With the advances of several devices capable of understanding spoken language and conducting conversation with human (e.g., Google Home, Amazon Echo) and the shrinking gap created by the digital devices, it is not difficult to foresee that the market and application areas of NLG systems will continue to grow, especially in applications whose users are non-experts. In such application, a user often asks for certain information and waits for the answer and a NLG module would return the answer in spoken language instead of text such as in question-answering systems or recommendation systems. The NLG system in these two applications uses templates to generate the answers in natural language for the users. A more advanced NLG system in this direction is described in BIBREF2, which works", "system discussed in the Phylotastic project BIBREF2. Specifically, we assume that the annotations given in an ontology are natural language sentences. This is a reasonable assumption given that the developers of an ontology are usually those who have intimate knowledge about entities described in the ontology and often have some sort of comments about classes, objects, and instances of the ontology. We then show that the system is very flexible and can be used for the same purpose with new ontologies. The rest of the paper is organized as follows. Section SECREF2 briefly reviews the basics of Grammatical Framework (GF)BIBREF6. Section SECREF3 describes the main modules of the system. Section SECREF4 includes two use cases of the system using an available ontologies against in the context of reasoning about ontologies. Specifically, it compares with the system used in the Phylotastic project and an ontology about people. This section also contains a use case that highlights the", "use ASP to develop a system answering questions in the do-it-yourself domain. These papers use templates to generate answers. The generated GF program generated by our system, that is used for the NLG task, is automatically created from a provided input. The sophisticated system presented by BIBREF5 translates both question and the given natural language text to logical representation, and uses logical reasoning to produce the answer. Our system is similar to their system in that both employ recent developments of NLP into solving NLG problems. Conclusions and Future Work. We propose a system implemented using answer set programming (ASP) and Grammatical Framework (GF), for automatic generation of natural language descriptions in applications targeting mainstream users. The system does not require a large corpus for the generation task and can be used in different types of applications. In the first type of applications, the system can work with annotated ontologies to translate a set", "The NLG system in these two applications uses templates to generate the answers in natural language for the users. A more advanced NLG system in this direction is described in BIBREF2, which works with ontologies annotated using the Attempto language and can generate a natural language description for workflows created by the systems built in the Phylotastic project. The applications targeted by these systems are significantly different from NLG systems, whose main purpose is to generate high-quality natural language description of objects or reports, such as those reported in the recent AAAI conference BIBREF3, BIBREF4, BIBREF5. The present paper is motivated by the need to generate natural language description of computational results to non-expert users such as those developed in the Phylotastic project. In this project, the users are experts in evolutionary biology but are none experts in ontologies and web services. When a user places a request, he/she will receive a workflow", "the sentences of the paragraph. By taking the union of all respective elements of each grammar for each sentence, i.e., categories, functions, linearizations and operators, the Grammar Exporter will group them into the set of categories (respectively, categories, functions, linearizations, operators) of the final grammar. Experiments. We describe our method of generating natural language in two applications. The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2. Instead of requiring that the ontologies are annotated using Attempto, we use natural language sentences to annotate the ontologies. To test the feasibility of the approach, we also conduct another use case with the second ontology, that is entirely different from the ontologies used in the Phylotastic project. The ontology is about people and includes descriptions for certain class. The second application targets the", "(in Italian) which is displayed in the second line. Figure FIGREF6 shows the meaning in the abstract syntax is represented in Japanese and in Italian, i.e. the two strings represent the same meaning. Method. To generate a sentence, we need a sentence structure and vocabularies. Our system is developed to emulate the process of a person learning a new language and has to make guesses to understand new sentences from time to time. For example, someone, who understands the sentence \u201cBill plays a game\u201d would not fully understand the sentence \u201cBill plays a popular board game\u201d without knowing the meaning of \u201cpopular\u201d and \u201cboard game\u201d but could infer that the latter sentence indicates that its subject plays a type of game. The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and", "representation of a paragraph. This intermediate representation could be used by GF for automatic translation as GF is well-suited for cross-languages translation. On the other hand, we need to assess whether the intermediate representation is meaningful. This use case aims at checking the adequacy of the representation. To do so, we generate the English sentences from the GF program and evaluate the quality of these sentences against the original ones. We randomly select 5 articles from 3 Wikipedia portals: People, Mathematics and Food & Drink. With the small set of rules introducing in this paper to recognize sentence structure, there would be very limited 4-gram in the generated text appearing in original Wikipedia corpus. Therefore, we use BLEU-3 with equal weight distribution instead of BLEU-4 to assess the generated content. Table TABREF27 shows the summary of the number of assessable sentences from our system. Out of 62 sentences from 3 portals, the system cannot determine the"]}
{"question_id": "4cf05da602669a4c09c91ff5a1baae6e30adefdf", "predicted_answer": "Yes", "predicted_evidence": ["stable across models, but for very similar languages (such as Danish and the two varieties of Norwegian) the hierarchy might differ, and the some holds for languages or groups that are significantly different from the major groups. An example from fig:germanic is English, which is traditionally classified as a West Germanic language with strong influences from North Germanic as well as Romance languages. In the figure English is (weakly) grouped with the West Germanic languages, but in other experiments it is instead weakly grouped with North Germanic. Generating Text. Since our language model is conditioned on a language vector, we can gain some intuitive understanding of the language space by generating text from different points in it. These points could be either one of the vectors learned during training, or some arbitrary other point. tab:interpolation shows text samples from different points along the line between Modern English [eng] and Middle English [enm]. Consistent with", "during training, or some arbitrary other point. tab:interpolation shows text samples from different points along the line between Modern English [eng] and Middle English [enm]. Consistent with the results of Johnson2016zeroshot, it appears that the interesting region lies rather close to 0.5. Compare also to our fig:eng-deu, which shows that up until about a third of the way between English and German, the language model is nearly perfectly tuned to English. Mixing and Interpolating Between Languages. By means of cross-entropy, we can also visualize the relation between languages in the multilingual space. Figure FIGREF12 plots the interpolation results for two relatively dissimilar languages, English and German. As expected, once the language vector moves too close to the German one, model performance drops drastically. More interesting results can be obtained if we interpolate between two language variants and compute cross-entropy of a text that represents an intermediate form.", "held-out sentences. In this experiment, we used 32 sentences from the King James Version of the Bible. Using the resulting language vector, test set cross-entropy improved from 1.39 (using the Modern English language vector as initial value) to 1.35. This is comparable to the result obtained in sec:interpolation, except that here we do not restrict the search space to points on a straight line between two language vectors. Conclusions. We have shown that language vectors, dense vector representations of natural languages, can be learned efficiently from raw text and possess several interesting properties. First, they capture language similarity to the extent that language family trees can be reconstructed by clustering the vectors. Second, they allow us to interpolate between languages in a sensible way, and even allow adopting the model using a very small set of text, simply by optimizing the language vector.", "a simple interpolation experiment (as in our sec:generating) they did not further explore the language vectors, which would have been challenging to do given the small number of languages used in their study. Ammar2016manylanguages used one-hot language identifiers as input to a multilingual word-based dependency parser, based on multilingual word embeddings. Given that they report this resulting in higher accuracy than using features from a typological database, it is a reasonable guess that their system learned language vectors which were able to encode syntactic properties relevant to the task. Unfortunately, they also did not look closer at the language vector space, which would have been interesting given the relatively large and diverse sample of languages represented in the Universal Dependencies treebanks. Our evaluation in sec:clustering calls to mind previous work on automatic language classification, by Wichmann2010evaluating among others. However, our purpose is not to", "and test sets by using Bible verse numbers, which allows us to control for semantic similarity between languages in a way that would have been difficult in a corpus that is not multi-parallel. Altogether we have 1,303 translations in 990 languages that we can use for our purposes. These were chosen so that the model alphabet size is below 1000 symbols, which was satisfied by choosing only translations in Latin, Cyrillic or Greek script. Certainly, there are disadvantages as well, such as the limited size (roughly 500 million tokens in total, with most languages having only one translation of the New Testament each, with roughly 200 thousand tokens), the narrow domain and the high overlap of named entities. The latter can lead to some unexpected effects when using nonsensical language vectors, as the model will then generate a sequence of random names. The corpus deviates in some ways from an ideal multi-parallel corpus. Most translations are of the complete New Testament, whereas", "Introduction. Neural language models BIBREF0 , BIBREF1 , BIBREF2 have become an essential component in several areas of natural language processing (NLP), such as machine translation, speech recognition and image captioning. They have also become a common benchmarking application in machine learning research on recurrent neural networks (RNN), because producing an accurate probabilistic model of human language is a very challenging task which requires all levels of linguistic analysis, from pragmatics to phonology, to be taken into account. A typical language model is trained on text in a single language, and if one needs to model multiple languages the standard solution is to train a separate model for each language. This presupposes large quantities of monolingual data in each of the languages that needs to be covered and each model with its parameters is completely independent of any of the other models. We propose instead to use a single model with real-valued vectors to indicate", "model performance drops drastically. More interesting results can be obtained if we interpolate between two language variants and compute cross-entropy of a text that represents an intermediate form. fig:eng-enm shows the cross-entropy of the King James Version of the Bible (published 1611), when interpolating between Modern English (1500\u2013) and Middle English (1050\u20131500). The optimal point turns out to be close to the midway point between them. Language identification. If we have a sample of an unknown language or language variant, it is possible to estimate its language vector by backpropagating through the language model with all parameters except the language vector fixed. We found that a very small set of sentences is enough to give a considerable improvement in cross-entropy on held-out sentences. In this experiment, we used 32 sentences from the King James Version of the Bible. Using the resulting language vector, test set cross-entropy improved from 1.39 (using the Modern", "the computational resources for such an experiment increases with the number of languages and would not be practical to carry out with our current equipment. Structure of the language space. We now take a look at the language vectors found during training with the full model of 990 languages. fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family. While our experiments indicate that finding more remote relationships (say, connecting the Germanic languages to the Celtic) is difficult for the model, it is clear that the language vectors preserves similarity properties between languages. In additional experiments we found the overall structure of these clusterings to be relatively stable across models, but for very similar languages (such as Danish and the two varieties of Norwegian) the hierarchy might differ, and the some holds for languages or groups that are significantly"]}
{"question_id": "7380e62edcb11f728f6d617ee332dc8b5752b185", "predicted_answer": "Neural language models BIBREF0 , BIBREF1 , BIBREF2.", "predicted_evidence": ["Introduction. Neural language models BIBREF0 , BIBREF1 , BIBREF2 have become an essential component in several areas of natural language processing (NLP), such as machine translation, speech recognition and image captioning. They have also become a common benchmarking application in machine learning research on recurrent neural networks (RNN), because producing an accurate probabilistic model of human language is a very challenging task which requires all levels of linguistic analysis, from pragmatics to phonology, to be taken into account. A typical language model is trained on text in a single language, and if one needs to model multiple languages the standard solution is to train a separate model for each language. This presupposes large quantities of monolingual data in each of the languages that needs to be covered and each model with its parameters is completely independent of any of the other models. We propose instead to use a single model with real-valued vectors to indicate", "that needs to be covered and each model with its parameters is completely independent of any of the other models. We propose instead to use a single model with real-valued vectors to indicate the language used, and to train this model with a large number of languages. We thus get a language model whose predictive distribution INLINEFORM0 is a continuous function of the language vector INLINEFORM1 , a property that is trivially extended to other neural NLP models. In this paper, we explore the \u201clanguage space\u201d containing these vectors, and in particular explore what happens when we move beyond the points representing the languages of the training corpus. The motivation of combining languages into one single model is at least two-fold: First of all, languages are related and share many features and properties, a fact that is ignored when using independent models. The second motivation is data sparseness, an issue that heavily influences the reliability of data-driven models. Resources", "data. The only supervision that is giving during training is a language identifier as a one-hot encoding. From that and the actual training examples, the system learns dense vector representations for each language included in our data set along with the character-level RNN parameters of the language model itself. Related Work. Multilingual language models is not a new idea BIBREF3 , the novelty of our work lies primarily in the use of language vectors and the empirical evaluation using nearly a thousand languages. Concurrent with this work, Johnson2016zeroshot conducted a study using neural machine translation (NMT), where a sub-word decoder is told which language to generate by means of a special language identifier token in the source sentence. This is close to our model, although beyond a simple interpolation experiment (as in our sec:generating) they did not further explore the language vectors, which would have been challenging to do given the small number of languages used in", "a final output layer with softmax activations. The only modification made to accommodate the fact that we train the model with text in nearly a thousand languages, rather than one, is that language embedding vectors are concatenated to the inputs of the LSTMs at each time step and the hidden layer before the softmax. We used three separate embeddings for these levels, in an attempt to capture different types of information about languages. The model structure is summarized in fig:model. In our experiments we use 1024-dimensional LSTMs, 128-dimensional character embeddings, and 64-dimensional language embeddings. Layer normalization BIBREF5 is used, but no dropout or other regularization since the amount of data is very large (about 3 billion characters) and training examples are seen at most twice. For smaller models early stopping is used. We use Adam BIBREF6 for optimization. Training takes between an hour and a few days on a K40 GPU, depending on the data size. Results. In this", "and properties, a fact that is ignored when using independent models. The second motivation is data sparseness, an issue that heavily influences the reliability of data-driven models. Resources are scarce for most languages in the world (and also for most domains in otherwise well-supported languages), which makes it hard to train reasonable parameters. By combining data from many languages, we hope to mitigate this issue. In contrast to related work, we focus on massively multilingual data sets to cover for the first time a substantial amount of the linguistic diversity in the world in a project related to data-driven language modeling. We do not presuppose any prior knowledge about language similarities and evolution and let the model discover relations on its own purely by looking at the data. The only supervision that is giving during training is a language identifier as a one-hot encoding. From that and the actual training examples, the system learns dense vector representations", "most twice. For smaller models early stopping is used. We use Adam BIBREF6 for optimization. Training takes between an hour and a few days on a K40 GPU, depending on the data size. Results. In this section, we present several experiments with the model described. For exploring the language vector space, we use hierarchical agglomerative clustering for visualization. For measuring performance, we use cross-entropy on held out-data. For this, we use a set of the 128 most commonly translated Bible verses, to ensure that the held-out set is as large and overlapping as possible among languages. Model capacity. Our first experiment tries to answer what happens when more and more languages are added to the model. There are two settings: adding languages in a random order, or adding the most closely related languages first. Cross-entropy plots for these settings are shown in fig:random and fig:swe. In both cases, the model degrades gracefully (or even improves) for a number of languages, but", "the computational resources for such an experiment increases with the number of languages and would not be practical to carry out with our current equipment. Structure of the language space. We now take a look at the language vectors found during training with the full model of 990 languages. fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family. While our experiments indicate that finding more remote relationships (say, connecting the Germanic languages to the Celtic) is difficult for the model, it is clear that the language vectors preserves similarity properties between languages. In additional experiments we found the overall structure of these clusterings to be relatively stable across models, but for very similar languages (such as Danish and the two varieties of Norwegian) the hierarchy might differ, and the some holds for languages or groups that are significantly", "a simple interpolation experiment (as in our sec:generating) they did not further explore the language vectors, which would have been challenging to do given the small number of languages used in their study. Ammar2016manylanguages used one-hot language identifiers as input to a multilingual word-based dependency parser, based on multilingual word embeddings. Given that they report this resulting in higher accuracy than using features from a typological database, it is a reasonable guess that their system learned language vectors which were able to encode syntactic properties relevant to the task. Unfortunately, they also did not look closer at the language vector space, which would have been interesting given the relatively large and diverse sample of languages represented in the Universal Dependencies treebanks. Our evaluation in sec:clustering calls to mind previous work on automatic language classification, by Wichmann2010evaluating among others. However, our purpose is not to"]}
{"question_id": "f37b01e0c366507308fca44c20d3f69621b94a6e", "predicted_answer": "fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family.", "predicted_evidence": ["the computational resources for such an experiment increases with the number of languages and would not be practical to carry out with our current equipment. Structure of the language space. We now take a look at the language vectors found during training with the full model of 990 languages. fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family. While our experiments indicate that finding more remote relationships (say, connecting the Germanic languages to the Celtic) is difficult for the model, it is clear that the language vectors preserves similarity properties between languages. In additional experiments we found the overall structure of these clusterings to be relatively stable across models, but for very similar languages (such as Danish and the two varieties of Norwegian) the hierarchy might differ, and the some holds for languages or groups that are significantly", "stable across models, but for very similar languages (such as Danish and the two varieties of Norwegian) the hierarchy might differ, and the some holds for languages or groups that are significantly different from the major groups. An example from fig:germanic is English, which is traditionally classified as a West Germanic language with strong influences from North Germanic as well as Romance languages. In the figure English is (weakly) grouped with the West Germanic languages, but in other experiments it is instead weakly grouped with North Germanic. Generating Text. Since our language model is conditioned on a language vector, we can gain some intuitive understanding of the language space by generating text from different points in it. These points could be either one of the vectors learned during training, or some arbitrary other point. tab:interpolation shows text samples from different points along the line between Modern English [eng] and Middle English [enm]. Consistent with", "Dependencies treebanks. Our evaluation in sec:clustering calls to mind previous work on automatic language classification, by Wichmann2010evaluating among others. However, our purpose is not to detect genealogical relationships, even though we use the strong correlation between such classifications and our language vectors as evidence that the vector space captures sensible information about languages. Data. We base our experiments on a large collection of Bible translations crawled from the web, coming from various sources and periods of times. Any other multilingual data collection would work as well, but with the selected corpus we have the advantage that we cover the same genre and roughly the same coverage for each language involved. It is also easy to divide the data into training and test sets by using Bible verse numbers, which allows us to control for semantic similarity between languages in a way that would have been difficult in a corpus that is not multi-parallel.", "during training, or some arbitrary other point. tab:interpolation shows text samples from different points along the line between Modern English [eng] and Middle English [enm]. Consistent with the results of Johnson2016zeroshot, it appears that the interesting region lies rather close to 0.5. Compare also to our fig:eng-deu, which shows that up until about a third of the way between English and German, the language model is nearly perfectly tuned to English. Mixing and Interpolating Between Languages. By means of cross-entropy, we can also visualize the relation between languages in the multilingual space. Figure FIGREF12 plots the interpolation results for two relatively dissimilar languages, English and German. As expected, once the language vector moves too close to the German one, model performance drops drastically. More interesting results can be obtained if we interpolate between two language variants and compute cross-entropy of a text that represents an intermediate form.", "related languages first. Cross-entropy plots for these settings are shown in fig:random and fig:swe. In both cases, the model degrades gracefully (or even improves) for a number of languages, but then degrades linearly (i.e. exponential growth of perplexity) with exponentially increasing number of languages. For comparison, fig:swesize compares this to the effect of decreasing the number of parameters in the LSTM by successively halving the hidden state size. Here the behavior is similar, but unlike the Swedish model which got somewhat better when closely related languages were added, the increase in cross-entropy is monotone. It would be interesting to investigate how the number of model parameters needs to be scaled up in order to accommodate the additional languages, but unfortunately the computational resources for such an experiment increases with the number of languages and would not be practical to carry out with our current equipment. Structure of the language space. We now", "model performance drops drastically. More interesting results can be obtained if we interpolate between two language variants and compute cross-entropy of a text that represents an intermediate form. fig:eng-enm shows the cross-entropy of the King James Version of the Bible (published 1611), when interpolating between Modern English (1500\u2013) and Middle English (1050\u20131500). The optimal point turns out to be close to the midway point between them. Language identification. If we have a sample of an unknown language or language variant, it is possible to estimate its language vector by backpropagating through the language model with all parameters except the language vector fixed. We found that a very small set of sentences is enough to give a considerable improvement in cross-entropy on held-out sentences. In this experiment, we used 32 sentences from the King James Version of the Bible. Using the resulting language vector, test set cross-entropy improved from 1.39 (using the Modern", "and properties, a fact that is ignored when using independent models. The second motivation is data sparseness, an issue that heavily influences the reliability of data-driven models. Resources are scarce for most languages in the world (and also for most domains in otherwise well-supported languages), which makes it hard to train reasonable parameters. By combining data from many languages, we hope to mitigate this issue. In contrast to related work, we focus on massively multilingual data sets to cover for the first time a substantial amount of the linguistic diversity in the world in a project related to data-driven language modeling. We do not presuppose any prior knowledge about language similarities and evolution and let the model discover relations on its own purely by looking at the data. The only supervision that is giving during training is a language identifier as a one-hot encoding. From that and the actual training examples, the system learns dense vector representations", "held-out sentences. In this experiment, we used 32 sentences from the King James Version of the Bible. Using the resulting language vector, test set cross-entropy improved from 1.39 (using the Modern English language vector as initial value) to 1.35. This is comparable to the result obtained in sec:interpolation, except that here we do not restrict the search space to points on a straight line between two language vectors. Conclusions. We have shown that language vectors, dense vector representations of natural languages, can be learned efficiently from raw text and possess several interesting properties. First, they capture language similarity to the extent that language family trees can be reconstructed by clustering the vectors. Second, they allow us to interpolate between languages in a sensible way, and even allow adopting the model using a very small set of text, simply by optimizing the language vector."]}
{"question_id": "95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "predicted_answer": "Unanswerable", "predicted_evidence": ["of the dictionary with auxiliary synonymous (see Section SECREF66 ) or the naive ad hoc management of particular cases, can limit the efficiency and the desired of the algorithm. For these reasons, we carefully designed INLINEFORM0 , even through a side-by-side collaboration between pharmacologists and computer scientists, in order to yield an efficient tool, capable to really support pharmacovigilance activities. In literature, several NLP algorithms already exist, and several interesting approaches (such as the so called morpho-analysis of natural language) have been studied and proposed BIBREF24 , BIBREF6 , BIBREF25 . According to the described pharmacovigilance domain, we considered algorithms for the morpho-analysis and the part-of-speech (PoS) extraction techniques BIBREF24 , BIBREF6 too powerful and general purpose for the solution of our problem. Indeed, in most cases ADR descriptions are written in a very succinct way, without using verbs, punctuation, or other lexical items,", "In BIBREF16 the authors propose to use association mining and Proportional Reporting Ratio (PRR, a well-know pharmacovigilance statistical index) to mine the associations between drugs and adverse reactions from the user contributed content in social media. In order to extract adverse reactions from on-line text (from health care communities), the authors apply the Consumer Health Vocabulary to generate ADR lexicon. ADR lexicon is a computerized collection of health expressions derived from actual consumer utterances, linked to professional concepts and reviewed and validated by professionals and consumers. Narrative text is preprocessed following standard NLP techniques (such as stop word removal, see Section SECREF12 ). An experiment using ten drugs and five adverse drug reactions is proposed. The Food and Drug Administration alerts are used as the gold standard, to test the performance of the proposed techniques. The authors developed algorithms to identify ADRs from threads of", "revised and validated ADR reports. For this purpose, we exploited VigiSegn, a data warehouse and OLAP system that has been developed for the Italian Pharmacovigilance National Center BIBREF3 . This system is based on the open source business intelligence suite Pentaho. VigiSegn offers a large number of encoded ADRs. The encoding has been manually performed and validated by experts working at pharmacovigilance centres. Encoding results have then been sent to the national regulatory authority, AIFA. We performed a test composed by the following steps. We launch an ETL procedure through Pentaho Data Integration. Reports are transferred from VigiSegn to an ad hoc database TestDB. The dataset covers all the 4445 reports received, revised and validated during the year 2014 for the Italian region Veneto. The ETL procedure extracts the narrative descriptions from reports stored in TestDB. For each description, the procedure calls MagiCoder from VigiFarmaco; the output, i.e., a list of MedDRA", "from text has recently received an increasing interest in pharmacovigilance research. Narrative descriptions of ADRs come from heterogeneous sources: spontaneous reporting, Electronic Health Records, Clinical Reports, and social media. In BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 some NLP approaches have been proposed for the extraction of ADRs from text. In BIBREF13 , the authors collect narrative discharge summaries from the Clinical Information System at New York Presbyterian Hospital. MedLEE, an NLP system, is applied to this collection, for identifing medication events and entities, which could be potential adverse drug events. Co-occurrence statistics with adjusted volume tests were used to detect associations between the two types of entities, to calculate the strengths of the associations, and to determine their cutoff thresholds. In BIBREF14 , the authors report on the adaptation of a machine learning-based system for the identification and extraction of ADRs in case", "the associations, and to determine their cutoff thresholds. In BIBREF14 , the authors report on the adaptation of a machine learning-based system for the identification and extraction of ADRs in case reports. The role of NLP approaches in optimised machine learning algorithms is also explored in BIBREF15 , where the authors address the problem of automatic detection of ADR assertive text segments from several sources, focusing on data posted by users on social media (Twitter and DailyStrenght, a health care oriented social media). Existing methodologies for NLP are discussed and an experimental comparison between NLP-based machine learning algorithms over data sets from different sources is proposed. Moreover, the authors address the issue of data imbalance for ADR description task. In BIBREF16 the authors propose to use association mining and Proportional Reporting Ratio (PRR, a well-know pharmacovigilance statistical index) to mine the associations between drugs and adverse", "precision and recall. We provide formulas both in terms of relevant/retrieved solutions and false positives, true positives and false negatives. It is worth noting that the binary classification of solutions as relevant or non-relevant is referred to as the gold standard judgment of relevance. In our case, the gold standard has to be represented by a human encoding of a narrative description, i.e., a set of MedDRA terms choosen by a pharmacovigilance expert. Such a set is assumed to be definitively correct (only correct solutions are returned) and complete (all correct solutions have been returned). Experiment about MagiCoder performances. To evaluate MagiCoder performances, we developed a benchmark, which automatically compares MagiCoder behavior with human encoding on already manually revised and validated ADR reports. For this purpose, we exploited VigiSegn, a data warehouse and OLAP system that has been developed for the Italian Pharmacovigilance National Center BIBREF3 . This", "as work in progress, such as changes in stemming algorithm, considering synonyms, term filtering heuristics. The paper is organized as follows. In Section SECREF2 we provide some background notions and we discuss related work. In Section SECREF3 we present the algorithm MagiCoder, by providing both a qualitative description and the pseudocode. In Section SECREF4 we spend some words about the user interface of the related software tool. In Section SECREF5 we explain the benchmark we developed to test INLINEFORM0 performances and its results. Section SECREF6 is devoted to some discussions. Finally, in Section SECREF7 we summarize the main features of our work and sketch some future research lines. Natural language processing and text mining in medicine. Automatic detection of adverse drug reactions from text has recently received an increasing interest in pharmacovigilance research. Narrative descriptions of ADRs come from heterogeneous sources: spontaneous reporting, Electronic Health", "proposed. The Food and Drug Administration alerts are used as the gold standard, to test the performance of the proposed techniques. The authors developed algorithms to identify ADRs from threads of drugs, and implemented association mining to calculate leverage and lift for each possible pair of drugs and adverse reactions in the dataset. At the same time, PRR is also calculated. Other related papers about pharmacovigilance and machine learning or data mining are BIBREF17 , BIBREF18 . In BIBREF19 , a text extraction tool is implemented on the .NET platform for preprocessing text (removal of stop words, Porter stemming BIBREF20 and use of synonyms) and matching medical terms using permutations of words and spelling variations (Soundex, Levenshtein distance and Longest common subsequence distance BIBREF21 ). Its performance has been evaluated on both manually extracted medical terms from summaries of product characteristics and unstructured adverse effect texts from Martindale (a"]}
{"question_id": "ab37ae82e38f64d3fa95782f2c791488f26cd43f", "predicted_answer": "Yes. MagiCoder behaves very well on very short descriptions (class 1) and on short ones (class 2).", "predicted_evidence": ["to be as small as possible, while common PT, recall, and precision have to be as large as possible. MagiCoder behaves very well on very short descriptions (class 1) and on short ones (class 2). Recall and precision remain greater than 50% up to class 4. Notice that very long descriptions (class 5), on which performances drastically decrease, represent a negligible percentage of the whole set (less than 0.3%). Some remarks are mandatory. It is worth noting that this test simply estimates how much, for each report, the MagiCoder behavior is similar to the manual work, without considering the effective quality of the manual encoding. Clearly, as a set of official reports, revised and sent to RNF, we assume to deal with an high-quality encoding: notwithstanding, some errors in the human encoding possibly occur. Moreover, the query we perform to compare manual and automatic encoding is, obviously, quantitative. For each VigiSegn report, the query is able to detect common retrieved terms", "Veneto. The ETL procedure extracts the narrative descriptions from reports stored in TestDB. For each description, the procedure calls MagiCoder from VigiFarmaco; the output, i.e., a list of MedDRA terms, is stored in a table of TestDB. Manual and automatic encodings of each report are finally compared through an SQL query. In order to have two uniform data sets, we compared only those reports where MagiCoder recognized at most six terms, i.e., the maximum number of terms that human experts are allowed to select through the VigiFarmaco user interface. Moreover, we map each LLT term recognized by both the human experts and MagiCoder to its corresponding preferred term. Results are discussed below in Section UID57 . Table TABREF58 shows the results of this first performance test. We group narrative descriptions by increasing length (in terms of characters). We note that reported results are computed considering terms at PT level. By moving to PT level, instead of using the LLT level, we", "descriptions by increasing length (in terms of characters). We note that reported results are computed considering terms at PT level. By moving to PT level, instead of using the LLT level, we group together terms that represent the same medical concept (i.e., the same adverse reaction). In this way, we do not consider an error when MagiCoder and the human expert use two different LLTs for representing the same adverse event. The use of the LLT level for reporting purpose and the PT level for analysis purpose is suggested also by MedDRA BIBREF5 . With common PT we mean the percentage of preferred terms retrieved by human reviewers that have been recognized also by MagiCoder. Reported performances are summarized also in FIGREF59 . Note that, false positive and false negative errors are required to be as small as possible, while common PT, recall, and precision have to be as large as possible. MagiCoder behaves very well on very short descriptions (class 1) and on short ones (class 2).", "in the field \u201cDescrizione\" (in English, \u201cDescription\u201d). Up to six solutions are proposed as the best MedDRA term candidates returned by MagiCoder: the responsible can refuse a term (through the trash icon), change one or more terms (by an option menu), or simply validate the automatic encoding and switch to the next section \u201cFarmaci\u201d (in English, \u201cDrugs\u201d). The maximum number of six terms to be shown has been chosen in order to supply pharmacovigilance experts with a set of terms extended enough to represent the described adverse drug reaction but not so large to be redundant or excessive. We are testing MagiCoder performances in the daily pharmacovigilance activities. Preliminary qualitative results show that MagiCoder drastically reduces the amount of work required for the revision of a report, allowing the pharmacovigilance stakeholders to provide high quality data about suspected ADRs. Testing MagiCoder performances. In this section we describe the experiments we performed to", "of a report, allowing the pharmacovigilance stakeholders to provide high quality data about suspected ADRs. Testing MagiCoder performances. In this section we describe the experiments we performed to evaluate MagiCoder performances. The test exploits a large amount of manually revised reports we obtained from VigiSegn BIBREF3 . We briefly recall two metrics we used to evaluate MagiCoder: precision and recall. In statistical hypothesis and in particular in binary classification BIBREF28 , two main kinds of errors are pointed out: false positive errors (FP) and false negative errors (FN). In our setting, these errors can be viewed as follows: a false positive error is the inopportune retrieval of a \u201cwrong\u201d LLT, i.e., a term which does not correctly encode the textual description; a false negative error is the failure in the recognition of a \u201cgood\u201d LLT, i.e., a term which effectively encode (a part of) the narrative description and that would have been selected by a human expert. As dual", "from the words in ADR description corresponding to indexes in INLINEFORM4 . For example, INLINEFORM0 (i.e., the concatenation of the voters and the term are equal) and INLINEFORM1 .  We want to estimate how an LLT has been covered. INLINEFORM0  The intuitive meaning of the criterion is to quantify the \u201cquality\u201d of the coverage. If an LLT has been covered by nearby words, it will be considered a good candidate for the solution. This criterion has to be carefully implemented, taking into account possible duplicated voted words. After computing (and storing) the weights related to the above criteria, for each voted term INLINEFORM0 we have the data structure INLINEFORM1 , containing the weights corresponding to the four criteria. These weights will be used, after a first heuristic selection, to sort a subset of the syntactically retrieved terms. Continuing the example introduced before, we have that INLINEFORM0 while INLINEFORM1 . Thus, concluding, we obtain that INLINEFORM2 while", "of problems related to drug-based treatments. Thanks to MedDRA it is possible to group similar/analogous cases described in different ways (e.g., by synonyms) or with different details/levels of abstraction. Table TABREF8 shows an example of the hierarchy: reaction Itch is described starting from Skin disorders (SOC), Epidermal conditions (HLGT), Dermatitis and Eczema (HLT), and Asteatotic Eczema (PT). Preferred Terms are Low Level Terms chosen to be representative of a group of terms. It should be stressed that the hierarchy is multiaxial: for example, a PT can be grouped into one or more HLT, but it belongs to only one primary SOC term. MagiCoder: an NLP software for ADR automatic encoding. A natural language ADR description is a completely free text. The user has no limitations, she/he can potentially write everything: a number of online ADR descriptions actually contain information not directly related to drug effects. Thus, an NLP software has to face and solve many issues:", "proposal following several directions. First of all, we refined the procedure: MagiCoder has been equipped with some heuristic criteria and we started to address the problem of including auxiliary dictionaries (e.g., in order to deal with synonyms). MagiCoder computational complexity has been carefully studied and we will show that it is linear in the size of the dictionary (in this case, the number of LLTs in MedDRA) and the text description. We performed an accurate test of MagiCoder performances: by means of well-known statistical measures, we collected a significant set of quantitative information about the effective behavior of the procedure. We largely discuss some crucial key-points we met in the development of this version of MagiCoder, proposing short-time solutions we are addressing as work in progress, such as changes in stemming algorithm, considering synonyms, term filtering heuristics. The paper is organized as follows. In Section SECREF2 we provide some background"]}
{"question_id": "6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "predicted_answer": "tokenization, stop-word removal, stemming, voting procedure to find best matching terms.", "predicted_evidence": ["encoding task, previously completely manual, becomes semi-automatic, reducing errors and the required time for accomplishing it. In spite of its first goal, MagiCoder has now evolved in an autonomous algorithm and software usable in all contexts where terms from a dictionary have to be recognized in a free narrative text. With respect to other solutions already available in literature and market, MagiCoder has been designed to be efficient and less computationally expensive, unsupervised, and with no need of training. MagiCoder uses stemming to be independent from singular/plural and masculine/feminine forms. Moreover, it uses string distance and other techniques to find best matching terms, discarding similar and non optimal terms. With respect to the first version BIBREF7 , we extended our proposal following several directions. First of all, we refined the procedure: MagiCoder has been equipped with some heuristic criteria and we started to address the problem of including auxiliary", "the following shape: name of the pathology+\u201clocation\u201d or adjective. Intuitively, we privilege terms for which the recognized words are probably the ones describing the pathology. The addition of INLINEFORM3 (with the discard of condition INLINEFORM4 in the final selection) could improve the quality of the solution if a larger set of winning terms is admissible or in the case in which the complete ordered list of voted terms is returned. Conclusions and future work. In this paper we proposed MagiCoder, a simple and efficient NLP software, able to provide a concrete support to the pharmacovigilance task, in the revision of ADR spontaneous reports. MagiCoder takes in input a narrative description of a suspected ADR and produces as outcome a list of MedDRA terms that \u201ccovers\u201d the medical meaning of the free-text description. Differently from other BioNLP software proposed in literature, we developed an original text processing procedure. Preliminary results about MagiCoder performances", "recognized in INLINEFORM23 and if INLINEFORM24 is not prefix of another already selected terms INLINEFORM25 AND (( INLINEFORM26 = false OR (mark(adr_clear(index))=0)) AND t INLINEFORM27 AND prefix( INLINEFORM28 ,t)=false) mark(adr_clear(index))=1 remove from the selected term set all terms which are prefix of INLINEFORM29 INLINEFORM30 = remove_prefix( INLINEFORM31 ,t) INLINEFORM32 = INLINEFORM33 filtering of the finally selected terms by the second heuristic criterium INLINEFORM34 INLINEFORM35 INLINEFORM36 Pseudocode of MagiCoder MagiCoder complexity analysis. Let us now conclude this section by sketching the analysis of the computational complexity of MagiCoder. Let INLINEFORM0 be the input size (the length, in terms of words, of the narrative description). Let INLINEFORM1 be the cardinality of the dictionary (i.e., the number of terms). Moreover, let INLINEFORM2 be the number of distinct words occurring in the dictionary and let INLINEFORM3 be the length of the longest term in the", "work is expensive in terms of time and attention required, a problem about the accuracy of the encoding may occur given the continuous growing of the number of reports. According to the described scenario, in this paper we propose INLINEFORM0 , an original Natural Language Processing (NLP) BIBREF6 algorithm and related software tool, which automatically assigns one or more terms from a dictionary to a narrative text. A preliminary version of INLINEFORM1 has been proposed in BIBREF7 . MagiCoder has been first developed for supporting pharmacovigilance supervisors in using VigiFarmaco, providing them with an initial automatic MedDRA encoding of the ADR descriptions in the online reports collected by VigiFarmaco, that the supervisors check and may correct or accept as it is. In this way, the encoding task, previously completely manual, becomes semi-automatic, reducing errors and the required time for accomplishing it. In spite of its first goal, MagiCoder has now evolved in an autonomous", "the weights of the four criteria). Since the number of the criteria-related weights involved in the multi-sorting is constant, it can be neglected. Thus, the complexity of multi-value sorting can be considered to be INLINEFORM1 . Finally, to derive the best solutions actually requires INLINEFORM0 steps. The ordered-phrases criterium requires INLINEFORM1 ; the maximal set of voters criterium takes INLINEFORM2 time units. Thus, we conclude that MagiCoder requires in the worst case INLINEFORM0 computational steps. We again highlight that this is a (very) worst case scenario, while in average it performs quite better. Moreover, we did not take into account that each phase works on a subset of terms of the previous phase, and the size of these subset rapidly decreases in common application. the selection phase works only on voted terms, thus, in common applications, on a subset of the original dictionary. Software implementation: the user interface. MagiCoder has been implemented as a", "proposal following several directions. First of all, we refined the procedure: MagiCoder has been equipped with some heuristic criteria and we started to address the problem of including auxiliary dictionaries (e.g., in order to deal with synonyms). MagiCoder computational complexity has been carefully studied and we will show that it is linear in the size of the dictionary (in this case, the number of LLTs in MedDRA) and the text description. We performed an accurate test of MagiCoder performances: by means of well-known statistical measures, we collected a significant set of quantitative information about the effective behavior of the procedure. We largely discuss some crucial key-points we met in the development of this version of MagiCoder, proposing short-time solutions we are addressing as work in progress, such as changes in stemming algorithm, considering synonyms, term filtering heuristics. The paper is organized as follows. In Section SECREF2 we provide some background", "in spoken language the order of words can change w.r.t. the formal structure of the sentences. Moreover, some words can be omitted, while the sentence still retains the same meaning. These aspects come for free from our voting procedure: after the scan, we retrieve the information that a set of words covers a term INLINEFORM1 , but the order between words does not necessarily matter. MagiCoder: structure of the algorithm. Figure SECREF34 depicts the pseudocode of MagiCoder. We represent dictionaries either as sets of words or as sets of functions. We describe the main procedures and functions used in the pseudocode. Procedure INLINEFORM0 takes the narrative description, performs tokenization and stop-word removal and puts it into an array of words. Procedures INLINEFORM0 and INLINEFORM1 get LLTs and create a dictionary of words and of their stemmed versions, respectively, which belong to LLTs, retaining the information about the set of terms containing each word. By the functional", "of the free-text description. Differently from other BioNLP software proposed in literature, we developed an original text processing procedure. Preliminary results about MagiCoder performances are encouraging. Let us sketch here some ongoing and future work. We are addressing the task to include ad hoc knowledges, as the MedDRA-thesaurus described in Section SECREF66 . We are also proving that MagiCoder is robust with respect to language (and dictionary) changes. The way the algorithm has been developed suggests that MagiCoder can be a valid tool also for narrative descriptions written in English. Indeed, the algorithm retrieves a set of words, which covers an LLT INLINEFORM0 , from a free-text description, only slightly considering the order between words or the structure of the sentence. This way, we avoid the problem of \u201cspecializing\u201d MagiCoder for any given language. We plan to test MagiCoder on the English MedDRA and, moreover, we aim to test our procedure on different"]}
{"question_id": "71413505d7d6579e2a453a1f09f4efd20197ab4b", "predicted_answer": "Unanswerable", "predicted_evidence": ["INLINEFORM4 both for Class 4 and Class 5. Class 5, which enjoys a particular advantage from the introduction of the pseudo-LLTs, represents a small slice of the set of reports. Notwithstanding, these cases are very arduous to address, and we have, at least, a good evidence of the validity of our approach. Connectives in the narrative descriptions. As previously said, in MagiCoder we do not take into account the structure of written sentences. In this sense, our procedure is radically different from those based on the so called part-of-speech (PoS) BIBREF29 , powerful methodologies able to perform the morpho-syntactical analysis of texts, labeling each lexical item with its grammatical properties. PoS-based text analyzers are also able to detect and deal with logical connectives such as conjunctions, disjunctions and negations. Even if connectives generally play a central role in the logical foundation of natural languages, they have a minor relevance in the problem we are addressing:", "been covered (with or without stemming) and it holds 0 otherwise. We assume that before starting the final phase of building the solution (i.e., the returned set of LLTs), INLINEFORM5 for any word INLINEFORM6 belonging to the description. Procedures INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 is a set of terms, implement ordered-phrases and maximal-set-of-voters criteria (defined in Section UID28 ), respectively. Function INLINEFORM0 , returns the first INLINEFORM1 elements of an ordered set INLINEFORM2 . If INLINEFORM3 , the function returns the complete list of ordered terms and INLINEFORM4 nil values. [!t] MagiCoder( INLINEFORM0 text, INLINEFORM1 dictionary, INLINEFORM2 integer)  INLINEFORM0 : the narrative description;  INLINEFORM0 : a data structure containing the MedDRA INLINEFORM1 s;  INLINEFORM0 : the maximum number of winning terms that have to be released by the procedure an ordered set of LLTs INLINEFORM1 = CreateMetaDict( INLINEFORM2 ) INLINEFORM3 = CreateStemMetaDict(", "INLINEFORM0 ( INLINEFORM1 ) are the weights related to the criteria defined in Section UID23 . Procedure INLINEFORM0 performs the multi-value sorting of the array INLINEFORM1 based on the values of the properties INLINEFORM2 of its elements. Procedure INLINEFORM0 , where INLINEFORM1 is a set of terms and INLINEFORM2 is a term, tests whether INLINEFORM3 (considered as a string) is prefix of a term in INLINEFORM4 . Dually, procedure INLINEFORM5 tests if in INLINEFORM6 there are one or more prefixes of INLINEFORM7 , and eventually remove them from INLINEFORM8 . Function INLINEFORM0 specifies whether a word INLINEFORM1 has been already covered (i.e., a term voted by INLINEFORM2 has been selected) in the (partial) solution during the term release: INLINEFORM3 holds 1 if INLINEFORM4 has been covered (with or without stemming) and it holds 0 otherwise. We assume that before starting the final phase of building the solution (i.e., the returned set of LLTs), INLINEFORM5 for any word", "from the words in ADR description corresponding to indexes in INLINEFORM4 . For example, INLINEFORM0 (i.e., the concatenation of the voters and the term are equal) and INLINEFORM1 .  We want to estimate how an LLT has been covered. INLINEFORM0  The intuitive meaning of the criterion is to quantify the \u201cquality\u201d of the coverage. If an LLT has been covered by nearby words, it will be considered a good candidate for the solution. This criterion has to be carefully implemented, taking into account possible duplicated voted words. After computing (and storing) the weights related to the above criteria, for each voted term INLINEFORM0 we have the data structure INLINEFORM1 , containing the weights corresponding to the four criteria. These weights will be used, after a first heuristic selection, to sort a subset of the syntactically retrieved terms. Continuing the example introduced before, we have that INLINEFORM0 while INLINEFORM1 . Thus, concluding, we obtain that INLINEFORM2 while", "will be discussed in Section UID28 ). For this purpose, we define four criteria to assign \u201cweights\u201d to voted terms accordingly. In the following, INLINEFORM0 is a normalization factor (w.r.t. the length, in terms of words, of the LLT INLINEFORM1 ). First three criteria have 0 as optimum value and 1 as worst value, while the fourth criterion has optimum value to 1 and it grows in worst cases.  First, we consider how much part of the words of each voted LLT have not been recognized. INLINEFORM0  In the example we introduced before, we have that INLINEFORM0 (i.e., all words of the terms have been recognized in the description) while INLINEFORM1 (i.e., one word out of three has not been recognized in the description).  The algorithm considers whether a perfect matching has been performed using or not stemmed words. INLINEFORM0 is simply a flag. INLINEFORM1 holds if stemming has been used at least once in the voting procedure of INLINEFORM2 , and it is valued 1, otherwise it is valued 0.", "or not stemmed words. INLINEFORM0 is simply a flag. INLINEFORM1 holds if stemming has been used at least once in the voting procedure of INLINEFORM2 , and it is valued 1, otherwise it is valued 0. For example, INLINEFORM0 and INLINEFORM1 .  The use of stemming allows one to find a number of (otherwise lost) matches. As side effect, we often obtain a quite large set of joint winner candidate terms. In this phase, we introduce a string distance comparison between recognized words in the original text and voted LLTs. Among the possible string metrics, we use the so called pair distance BIBREF27 , which is robust with respect to word permutation. Thus, INLINEFORM0  where INLINEFORM0 is the pair distance function (between strings INLINEFORM1 and INLINEFORM2 ) and INLINEFORM3 is the term \u201crebuilt\u201d from the words in ADR description corresponding to indexes in INLINEFORM4 . For example, INLINEFORM0 (i.e., the concatenation of the voters and the term are equal) and INLINEFORM1 .  We want to", "; we call INLINEFORM1 this sequence of indexes. Moreover, we endow each voted term INLINEFORM0 with a third structure that will contain the sorting criteria we define below; we will call it INLINEFORM1 . Let us now introduce some notations we will use in the following. We denote as INLINEFORM0 the function that, given an LLT INLINEFORM1 , returns the number of words contained in INLINEFORM2 (excluding the stop words). We denote as INLINEFORM3 (resp. INLINEFORM4 ) the function that returns the number of indexes belonging to INLINEFORM5 (resp. INLINEFORM6 ). We denote as INLINEFORM7 and INLINEFORM8 the functions that return the maximum and the minimum indexes in INLINEFORM9 , respectively. From now on, sometimes we explicitly list the complete denomination of a terms: we will use the notation \u201cname\u201d(id), where \u201cname\u201d is the MedDRA description and id is its identifier, that is possibly used to refer to the term. Let us exemplify these notions by introducing an example. Consider the", "get LLTs and create a dictionary of words and of their stemmed versions, respectively, which belong to LLTs, retaining the information about the set of terms containing each word. By the functional notation INLINEFORM0 (resp., INLINEFORM1 ), we refer to the set of LLTs containing the word INLINEFORM2 (resp., the stem of INLINEFORM3 ). Function INLINEFORM0 returns the stemmed version of word INLINEFORM1 . Function INLINEFORM0 returns the position of word INLINEFORM1 in term INLINEFORM2 .  INLINEFORM0 is a flag, initially set to 0, which holds 1 if at least a stemmed matching with the MedDRA term INLINEFORM1 is found.  INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are arrays and INLINEFORM3 appends INLINEFORM4 to array INLINEFORM5 , where INLINEFORM6 may be an element or a sequence of elements.  INLINEFORM0 ( INLINEFORM1 ) are the weights related to the criteria defined in Section UID23 . Procedure INLINEFORM0 performs the multi-value sorting of the array INLINEFORM1 based on the values of"]}
{"question_id": "3e6b6820e7843209495b4f9a72177573afaa4bc3", "predicted_answer": "Unanswerable", "predicted_evidence": ["caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental", "research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 . A substantial body of qualitative and quantitative research has shown the importance of personal recovery for individuals diagnosed with BD BIBREF22 , BIBREF25 , BIBREF30 , BIBREF31 , BIBREF23 . Qualitative evidence mainly comes from (semi-)structured interviews and focus groups and has been criticised for small numbers of participants BIBREF10 , lacking complementary quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused", "Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained period of time BIBREF11 , is considered difficult to achieve BIBREF12 , BIBREF13 , BIBREF14 . Moreover, clinically recovered individuals often do not regain full social and educational/vocational functioning BIBREF15 , BIBREF16 . Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 ,", "quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused cognitive-behavioural therapy BIBREF31 . Critically, previous research has taken place only in structured settings. What is more, the recovery concept emerged from research primarily conducted in English-speaking countries, mainly involving researchers and participants of Western ethnicity. This might have led to a lack of non-Western notions of wellbeing in the concept, such as those found in indigenous peoples BIBREF32 , limiting its the applicability to a general population. Indeed, the variation in BD prevalence rates from 0.1% in India to 4.4% in the US is striking. It has been shown that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 ,", "short timeline of a PhD project and it might be impossible to share the used portions of the data with other researchers. Therefore, we will follow up the possibilities of obtaining access to these datasets, but in parallel also collect our own datasets to avoid dependence on external data providers. Methodology and Resources. As explained in the introduction, the overarching aim of this project is to investigate in how far information conveyed in social media posts can complement more traditional research methods in clinical psychology to get insights into the recovery experience of individuals with a BD diagnosis. Therefore, we will first conduct a systematic literature review of qualitative evidence to establish a solid base of what is already known about personal recovery experiences in BD for the subsequent social media studies. Our research questions, which regard the experiences of different populations, lend themselves to several subprojects. First, we will collect and analyse", "to an advisory board of individuals with a BD diagnosis and was received positively. The advisory board will be consulted at several stages of the project to inform the research design, analysis, and publication of results. We believe that board members can help to address several of the raised ethical problems, e.g., shaping the research questions to avoid feeding into existing biases or overexposing certain groups and highlighting potentially harmful interpretations and uses of our results. Impact and conclusion. The importance of the recovery concept in the design of mental health services has recently been prominently reinforced, suggesting \u2018recovery-oriented social enterprises as key component of the integrated service\u2019 BIBREF20 . We think that a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals as diverse as those it is supposed to serve. Therefore, we expect the proposed investigations of views", "that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 , BIBREF35 . While approaches to mental health classification from texts have long ignored the cultural dimension BIBREF36 , first studies show that online language of individuals affected by depression or related mental health difficulties differs significantly across cultures BIBREF37 , BIBREF36 . Hence, it seems timely to take into account the wealth of accounts of mental health difficulties and recovery stories from individuals of diverse ethnic and cultural backgrounds that are available in a multitude of languages on the internet. Corpus and computational linguistic methods are explicitly designed for processing large amounts of linguistic data BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , and as discussed above, recent advances have made it feasible to apply them to noisy user-generated", "for analyses on how mental health is talked about online. Due to this lack of appropriate existing publicly accessible datasets, we will create such resources and make them available to subsequent researchers. We plan to collect data relevant for BD in general as well as for personal recovery in BD from three sources varying in their available amount versus depth of the accounts we expect to find: 1) Twitter, 2) Reddit (focusing on mental health-related content unlike previous work), 3) blogs authored by affected individuals. Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'. To do so, we will extend on the diagnosis patterns and terms for BD provided by BIBREF47 . Implicit consent is assumed from users on these platforms to use their public tweets and posts. SECREF3 Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent"]}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "predicted_answer": "topic modelling, emotion and sentiment analysis, semantic tagging, distributional semantics, Wmatrix", "predicted_evidence": ["extent LIWC concepts can be transferred to other languages and cultures by mere translation. We therefore aim to apply and develop methods that require less manual labour and are applicable to many languages and cultures. One option constitute unsupervised methods, such as topic modelling, which has been applied to explore cultural differences in mental-health related online data already BIBREF37 , BIBREF36 . The Differential Language Analysis ToolKit (DLATK) BIBREF70 facilitates social-scientific language analyses, including tools for preprocessing, such as emoticon-aware tokenisers, filtering according to meta data, and analysis, e.g. via robust topic modelling methods. Furthermore, emotion and sentiment analysis constitute useful tools to investigate the emotions involved in talking about recovery and identify factors that facilitate or hinder it. There are many annotated datasets to train supervised classifiers BIBREF71 , BIBREF3 for these actively researched NLP tasks. Machine", "about recovery and identify factors that facilitate or hinder it. There are many annotated datasets to train supervised classifiers BIBREF71 , BIBREF3 for these actively researched NLP tasks. Machine learning methods were found to usually outperform rule-based approaches based on look-ups in dictionaries such as LIWC. Again, most annotated resources are English, but state of the art approaches based on multilingual embeddings allow transferring models between languages BIBREF4 . Ethical considerations. Ethical considerations are established as essential part in planning mental health research and most research projects undergo approval by an ethics committee. On the contrary, the computational linguistics community has started only recently to consider ethical questions BIBREF72 , BIBREF73 . Likely, this is because computational linguistics was traditionally concerned with publicly available, impersonal texts such as newspapers or texts published with some temporal distance, which", "that frequently co-occur with words from our keyword dictionary. Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects. Linguistic Inquiry and Word Count (LIWC) BIBREF67 is a frequently used tool in social-science text analysis to analyse emotional and cognitive components of texts and derive features for classification models BIBREF47 , BIBREF46 , BIBREF68 , BIBREF69 . LIWC counts target words organised in a manually constructed hierarchical dictionary without contextual disambiguation in the texts under analysis and has been psychometrically validated and developed for English exclusively. While translations for several languages exist, e.g., Dutch BIBREF9 , and it is questionable to what extent LIWC concepts can be transferred to other languages and cultures by mere translation. We therefore aim to apply and develop methods that require less manual labour and are applicable to many", ", we will compile an initial dictionary of recovery-related terms. Next, we will examine a small portion of the dataset manually, which will be partly randomly sampled and partly selected to contain recovery-related terms. Based on this, we will be able to expand the dictionary and additionally automatically annotate semantic concepts of the identified relevant text passages using a semantic tagging approach such as the UCREL Semantic Analysis System (USAS) BIBREF64 . Crucially for the multilingual aspect of the project, USAS can tag semantic categories in eight languages BIBREF8 . Then, semantic tagging will be applied to the full corpus to retrieve all text passages mentioning relevant concepts. Furthermore, distributional semantics methods BIBREF65 , BIBREF66 can be used to find terms that frequently co-occur with words from our keyword dictionary. Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related", "Introduction and background. Recent years have witnessed increased performance in many computational linguistics tasks such as syntactic and semantic parsing BIBREF0 , BIBREF1 , emotion classification BIBREF2 , and sentiment analysis BIBREF3 , BIBREF4 , BIBREF5 , especially concerning the applicability of such tools to noisy online data. Moreover, the field has made substantial progress in developing multilingual models and extending semantic annotation resources to languages beyond English BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . Concurrently, it has been argued for mental health research that it would constitute a `valuable critical step' BIBREF10 to analyse first-hand accounts by individuals with lived experience of severe mental health issues in blog posts, tweets, and discussion forums. Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained", "for processing large amounts of linguistic data BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , and as discussed above, recent advances have made it feasible to apply them to noisy user-generated texts from diverse domains, including mental health BIBREF42 , BIBREF43 . Computer-aided analysis of public social media data enables us to address several shortcomings in the scientific underpinning of personal recovery in BD by overcoming the small sample sizes of lab-collected data and including accounts from a more heterogeneous population. In sum, our research questions are as follows: (1) How is personal recovery discussed online by individuals meeting criteria for BD? (2) What new insights do we get about personal recovery and factors that facilitate or hinder it? We will investigate these questions in two parts, looking at English-language data by westerners and at multilingual data by individuals of diverse ethnicities. Data. Previous work in computational linguistics and clinical", ". Likely, this is because computational linguistics was traditionally concerned with publicly available, impersonal texts such as newspapers or texts published with some temporal distance, which left a distance between the text and author. Conversely, recent social media research often deals with highly personal information of living individuals, who can be directly affected by the outcomes BIBREF72 .  BIBREF72 discuss issues that can arise when constructing datasets from social media and conducting analyses or developing predictive models based on these data, which we review here in relation to our project: Demographic bias in sampling the data can lead to exclusion of minority groups, resulting in overgeneralisation of models based on these data. As discussed in the introduction, personal recovery research suffers from a bias towards English-speaking Western individuals of white ethnicity. By studying multilingual accounts of ethnically diverse populations we explicitly address the", "settings, even less so when looking at a more diverse and non-English speaking population, we will not derive hypotheses from existing recovery models for testing on the online data. Instead, we will start off with exploratory quantitative research using comparative analysis tools such as Wmatrix BIBREF62 to uncover important linguistic features, e.g., on keywords and key concepts that occur with unexpected frequency in our collected datasets relative to reference corpora. The underlying assumption is that keywords and key concepts are indicative of certain aspects of personal recovery, such as those specified in the CHIME model BIBREF24 , other previous research BIBREF22 , BIBREF23 , BIBREF60 , or novel ones. Comparing online sources with transcripts of structured interviews or subcorpora originating from different cultural backgrounds might uncover aspects that were not prominently represented in the accounts studied in prior research. A specific challenge will be to narrow down the"]}
{"question_id": "4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "predicted_answer": "Inconclusive", "predicted_evidence": ["for analyses on how mental health is talked about online. Due to this lack of appropriate existing publicly accessible datasets, we will create such resources and make them available to subsequent researchers. We plan to collect data relevant for BD in general as well as for personal recovery in BD from three sources varying in their available amount versus depth of the accounts we expect to find: 1) Twitter, 2) Reddit (focusing on mental health-related content unlike previous work), 3) blogs authored by affected individuals. Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'. To do so, we will extend on the diagnosis patterns and terms for BD provided by BIBREF47 . Implicit consent is assumed from users on these platforms to use their public tweets and posts. SECREF3 Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent", "quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused cognitive-behavioural therapy BIBREF31 . Critically, previous research has taken place only in structured settings. What is more, the recovery concept emerged from research primarily conducted in English-speaking countries, mainly involving researchers and participants of Western ethnicity. This might have led to a lack of non-Western notions of wellbeing in the concept, such as those found in indigenous peoples BIBREF32 , limiting its the applicability to a general population. Indeed, the variation in BD prevalence rates from 0.1% in India to 4.4% in the US is striking. It has been shown that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 ,", "research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 . A substantial body of qualitative and quantitative research has shown the importance of personal recovery for individuals diagnosed with BD BIBREF22 , BIBREF25 , BIBREF30 , BIBREF31 , BIBREF23 . Qualitative evidence mainly comes from (semi-)structured interviews and focus groups and has been criticised for small numbers of participants BIBREF10 , lacking complementary quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused", "linguistics. While this comes with the challenges of cross-disciplinary research, it has the potential to apply and develop state-of-the-art NLP methods in a way that is psychologically and ethically sound as well as informed and approved by affected people to increase our knowledge of severe mental illnesses such as BD. Acknowledgments. I would like to thank my supervisors Steven Jones, Fiona Lobban, and Paul Rayson for their guidance in this project. My heartfelt thanks go also to Chris Lodge, service user researcher at the Spectrum Centre, and the members of the advisory panel he coordinates that offer feedback on this project based on their lived experience of BD. Further, I would like to thank Masoud Rouhizadeh for his helpful comments during pre-submission mentoring and the anonymous reviewers. This project is funded by the Faculty of Health and Medicine at Lancaster University as part of a doctoral scholarship.", "from their tweets BIBREF49 or the (albeit noisy) location field in their user profiles BIBREF50 . Only one attempt to classify the location of Reddit users has been published so far BIBREF51 showing meagre results, indicating that the development of robust location classification approaches on this platform would constitute a valuable contribution. Some companies collect mental health-related online data and make them available to researchers subject to approval of their internal review boards, e.g., OurDataHelps by Qntfy or the peer-support forum provider 7 Cups. Unlike `raw' social media data, these datasets have richer user-provided metadata and explicit consent for research usage. On the other hand, less data is available, the process to obtain access might be tedious within the short timeline of a PhD project and it might be impossible to share the used portions of the data with other researchers. Therefore, we will follow up the possibilities of obtaining access to these", "caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental", "Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained period of time BIBREF11 , is considered difficult to achieve BIBREF12 , BIBREF13 , BIBREF14 . Moreover, clinically recovered individuals often do not regain full social and educational/vocational functioning BIBREF15 , BIBREF16 . Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 ,", "minority groups, we will pay special attention to this in the dissemination of our results. Lastly, most research, even when conducted with the best intentions, suffers from the dual-use problem BIBREF74 , in that it can be misused or have consequences that affect people's life negatively. For this reason, we refrain from publishing mental health classification methods, which could be used, for example, by health insurance companies for the risk assessment of applicants based on their social media profiles. If and how informed consent needs to be obtained for research on social media data is a debated issue BIBREF75 , BIBREF76 , BIBREF77 , mainly because it is not straightforward to determine if posts are made in a public or private context. From a legal point of view, the privacy policies of Twitter and Reddit, explicitly allow analysis of the user contents by third party, but it is unclear to what extent users are aware of this when posting to these platforms BIBREF78 . However, in"]}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "predicted_answer": "Unanswerable", "predicted_evidence": ["research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 . A substantial body of qualitative and quantitative research has shown the importance of personal recovery for individuals diagnosed with BD BIBREF22 , BIBREF25 , BIBREF30 , BIBREF31 , BIBREF23 . Qualitative evidence mainly comes from (semi-)structured interviews and focus groups and has been criticised for small numbers of participants BIBREF10 , lacking complementary quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused", "caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental", "quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused cognitive-behavioural therapy BIBREF31 . Critically, previous research has taken place only in structured settings. What is more, the recovery concept emerged from research primarily conducted in English-speaking countries, mainly involving researchers and participants of Western ethnicity. This might have led to a lack of non-Western notions of wellbeing in the concept, such as those found in indigenous peoples BIBREF32 , limiting its the applicability to a general population. Indeed, the variation in BD prevalence rates from 0.1% in India to 4.4% in the US is striking. It has been shown that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 ,", "linguistics. While this comes with the challenges of cross-disciplinary research, it has the potential to apply and develop state-of-the-art NLP methods in a way that is psychologically and ethically sound as well as informed and approved by affected people to increase our knowledge of severe mental illnesses such as BD. Acknowledgments. I would like to thank my supervisors Steven Jones, Fiona Lobban, and Paul Rayson for their guidance in this project. My heartfelt thanks go also to Chris Lodge, service user researcher at the Spectrum Centre, and the members of the advisory panel he coordinates that offer feedback on this project based on their lived experience of BD. Further, I would like to thank Masoud Rouhizadeh for his helpful comments during pre-submission mentoring and the anonymous reviewers. This project is funded by the Faculty of Health and Medicine at Lancaster University as part of a doctoral scholarship.", "to an advisory board of individuals with a BD diagnosis and was received positively. The advisory board will be consulted at several stages of the project to inform the research design, analysis, and publication of results. We believe that board members can help to address several of the raised ethical problems, e.g., shaping the research questions to avoid feeding into existing biases or overexposing certain groups and highlighting potentially harmful interpretations and uses of our results. Impact and conclusion. The importance of the recovery concept in the design of mental health services has recently been prominently reinforced, suggesting \u2018recovery-oriented social enterprises as key component of the integrated service\u2019 BIBREF20 . We think that a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals as diverse as those it is supposed to serve. Therefore, we expect the proposed investigations of views", "that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 , BIBREF35 . While approaches to mental health classification from texts have long ignored the cultural dimension BIBREF36 , first studies show that online language of individuals affected by depression or related mental health difficulties differs significantly across cultures BIBREF37 , BIBREF36 . Hence, it seems timely to take into account the wealth of accounts of mental health difficulties and recovery stories from individuals of diverse ethnic and cultural backgrounds that are available in a multitude of languages on the internet. Corpus and computational linguistic methods are explicitly designed for processing large amounts of linguistic data BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , and as discussed above, recent advances have made it feasible to apply them to noisy user-generated", "recovery research suffers from a bias towards English-speaking Western individuals of white ethnicity. By studying multilingual accounts of ethnically diverse populations we explicitly address the demographic bias of previous research. Topic overexposure is tricky to address, where certain groups are perceived as abnormal when research repeatedly finds that their language is different or more difficult to process. Unlike previous research BIBREF45 , BIBREF47 , BIBREF46 our goal is not to reveal particularities in the language of individuals affected by mental health problems. Instead, we will compare accounts of individuals with BD from different settings (structured interviews versus informal online discourse) and of different backgrounds. While the latter bears the risk to overexpose certain minority groups, we will pay special attention to this in the dissemination of our results. Lastly, most research, even when conducted with the best intentions, suffers from the dual-use problem", "for national or global health service strategies, should be informed by voices of individuals as diverse as those it is supposed to serve. Therefore, we expect the proposed investigations of views on recovery by previously under-researched ethnic, language, and cultural groups to yield valuable insights on the appropriateness of the recovery approach for a wider population. The datasets collected in this project can serve as useful resources for future research. More generally, our social-media data-driven approach could be applied to investigate other areas of mental health if it proves successful in leading to relevant new insights. Finally, this project is an interdisciplinary endeavour, combining clinical psychology, input from individuals with lived experience of BD, and computational linguistics. While this comes with the challenges of cross-disciplinary research, it has the potential to apply and develop state-of-the-art NLP methods in a way that is psychologically and"]}
{"question_id": "c2ce25878a17760c79031a426b6f38931cd854b2", "predicted_answer": "Unanswerable", "predicted_evidence": ["with relatively long body length. Second, their formulation format for training samples can be supplemented, for example, the stanza structure of CI is missing. Third, using contemporary Chinese news corpus to pre-train the model may not be necessary, owing to distinctive differences in both meaning and form between contemporary Chinese and Chinese classical poetry language. For the above considerations, we give up the pre-training on the news corpus and add a separation label to indicate the stanza structure of CI. Then we make use of GPT-2 to train the model. Furthermore, we propose a form-stressed weighting method in GPT-2 to strengthen the control in particular to the form of CI. Model ::: Pre-processing. We present a unified format for formulating all types of training samples of SHI and CI by extending the format given in liao2019gpt. First, we change various punctuations between lines into the comma \u2018,\u2019, serving as a uniform separation label between two lines. Second, we", "the unified format of training samples is beneficial in some degree for meeting the form requirement. For instance, we input the same title to the enhanced model and to a model trained under the same condition except without the stanza separation, asking them to generate a number of CIs with Cipai of Busuanzi, a task similar to that in Figure 4. We find that about 20% of CIs generated by the latter suffer from some errors in form, as illustrated in Figure 5, meanwhile all the CIs generated by the former ideally match the expected form. Experiment ::: Case Observation. According to our observation, the enhanced model is likely to generate poems with both high quality and diversity. We present two examples generated by the model and give some comments on the meaning of each poem. UTF8gbsn\u4e03\u5f8b \u00b7 \u8fdc\u671b UTF8gbsn\u6c5f\u4e0a\u5fae\u832b\u4e00\u53f6\u821f\uff0c\u5929\u6daf\u82b3\u8349\u6ee1\u6c40\u6d32 UTF8gbsn\u6570\u58f0\u6e14\u5531\u9694\u8239\u8fc7\uff0c\u51e0\u70b9\u4eba\u5bb6\u843d\u5e06\u6e38 UTF8gbsn\u6625\u8272\u4e0d\u4ece\u83ba\u8bed\u5230\uff0c\u5915\u9633\u7a7a\u5ea6\u5ba2\u5fc3\u6101 UTF8gbsn\u4f55\u65f6\u91cd\u5411\u957f\u6865\u996e\uff0c\u540c\u6cdb\u6eaa\u5149\u5171\u767d\u5934 The example above is a Qiyan Lvshi. The title of this poem means \u201clook far around\u201d. In this poem,", "from 33 to 114 characters and with relatively sufficient training samples in CPCC, as our observation target. We generate 300 poems with the two models accordingly. Table 1 summarizes the correct rates of the two models under these 6 Cipais (a generated poem is considered to be correct in form if and only if its form fully matches the expected form). As can be seen, a tendency is the longer the body of CI, the worse the performance of the two models in form and, the more significant the gain in the form correct rate for the enhanced model (an extreme is in the case of Qinyuanchun where the correct rate is raised from 12.0% to 55.0%). Experiment ::: Effect of the Stanza Separation. The preliminary observation on the generated poems suggests that the inclusion of the stanza separation into the unified format of training samples is beneficial in some degree for meeting the form requirement. For instance, we input the same title to the enhanced model and to a model trained under the same", "where $x[i]$ is the vector of $i$th token, $j$ is over all possible token types. To address the form problem, we simply add a weighting factor into the loss function with particular stress on the aforementioned three types of form-related tokens, i.e., the line separation label \u2018,\u2019, the stanza separation label \u2018$\\&$\u2019, and $[EOF]$, as in: where $weight[i]$ is set as 1 for any Chinese character, 2 for \u2018,\u2019 and \u2018$\\&$\u2019, and 3 for $[EOF]$. This simple method (we thus call it the form-stressed weighting method) enhances the model\u2019s capability to form control quite significantly. Figure 4(b) shows an example that contrasts the case in Figure 4(a). Experiment ::: Experiment Setup. We implement the GPT-2 model based on the transformers library BIBREF8. The model configuration is 8 attention heads per layer, 8 layers, 512 embedding dimensions, and 1024 feed-forward layer dimensions. We employ the OpenAIAdam optimizer and train the model with 400,000 steps in total on 4 NVIDIA 1080Ti GPUs. The", "the effectiveness of our method. Nevertheless, we also find that enabling GPT-2 to have a strong capability in form manipulation for the generated texts remains a difficult challenge, particularly for those forms with longer body length and fewer training samples. We plan to figure out a more sophisticated way to make the model better learn the form structure and hope to enrich the general GPT-2 from this special perspective. Acknowledgements. We would like to thank Zhipeng Guo, Xiaoyuan Yi, Xinran Gu and anonymous reviewers for their insightful comments. This work is supported by the project Text Analysis and Studies on Chinese Classical Literary Canons with Big Data Technology under grant number 18ZDA238 from the Major Program of the National Social Science Fund of China. Hu is also supported by the Initiative Scientific Research Program and Academic Training Program of the Department of Computer Science and Technology, Tsinghua University.", "Lvshi, 20,000 CIs, 700,000 pairs of couplets. A key point is they defined a unified format to formulate different types of training samples, as [form, identifier 1, theme, identifier 2, body], where \u201cbody\u201d accommodates the full content of an SHI, CI, or couplet in corresponding \u201cform\u201d with \u201ctheme\u201d as its title. Experiments demonstrated GPT-based poem generation gained promising performance, meanwhile still faced some limitations, for instance, only 70% of the generated CIs for the Cipai Shuidiaogetou, a sort of CI with quite long body, are correct in form. Regarding this, we think the work of liao2019gpt could be improved in the following three respects. First, there is a large improving room for better fitting the form requirement of CI in the process of generation, especially for those with relatively long body length. Second, their formulation format for training samples can be supplemented, for example, the stanza structure of CI is missing. Third, using contemporary Chinese news", "per layer, 8 layers, 512 embedding dimensions, and 1024 feed-forward layer dimensions. We employ the OpenAIAdam optimizer and train the model with 400,000 steps in total on 4 NVIDIA 1080Ti GPUs. The characters with frequency less than 3 in CCPC1.0 are treated as UNK and a vocabulary with 11259 tokens (characters) is finally built up. Experiment ::: Performance Comparison of the Two Models in Form. For Jueju and Lvshi of SHI, because of their simplicity in form, the two models hardly make form errors. We generate 500 poems for each type using the two models accordingly. All of these poems are in the right form. This demonstrates that both models are all very powerful in generating Jueju and Lvshi with almost perfect performance in form. For CI, we select 6 Cipais, with the body length varying from 33 to 114 characters and with relatively sufficient training samples in CPCC, as our observation target. We generate 300 poems with the two models accordingly. Table 1 summarizes the correct", ". 1.1em  ::: . 1.1.1em  :::  ::: . 1.1.1.1em Jinyi Hu, Maosong Sun$^{*}$ $*$ Corresponding author Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China hujy17@mails.tsinghua.edu.cn, sms@tsinghua.edu.cn Poetry generation is an interesting research topic in the field of text generation. As one of the most valuable literary and cultural heritages of China, Chinese classical poetry is very familiar and loved by Chinese people from generation to generation. It has many particular characteristics in its language structure, ranging from form, sound to meaning, thus is regarded as an ideal testing task for text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training"]}
{"question_id": "1d263356692ed8cdee2a13f103a82d98f43d66eb", "predicted_answer": "SHI and CI", "predicted_evidence": ["text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training samples by integrating detailed form information, then present a simple form-stressed weighting method in GPT-2 to strengthen the control to the form of the generated poems, with special emphasis on those forms with longer body length. Preliminary experimental results show this enhanced model can generate Chinese classical poems of major types with high quality in both form and content, validating the effectiveness of the proposed strategy. The model has been incorporated into Jiuge, the most influential Chinese classical poetry generation system developed by Tsinghua University BIBREF0. Introduction. Chinese poetry is a rich treasure in Chinese traditional culture. For thousands of years, poetry is always considered as the crystallization of human wisdom and erudition by Chinese people and", "Chinese poetry is a rich treasure in Chinese traditional culture. For thousands of years, poetry is always considered as the crystallization of human wisdom and erudition by Chinese people and deeply influences the Chinese history from the mental and cultural perspective. In general, a Chinese classical poem is a perfect combination of three aspects, i.e., form, sound, and meaning. Firstly, it must strictly obey a particular form which specifies the number of lines (i.e., sentences) in the poem and the number of characters in each line. Secondly, it must strictly obey a particular sound pattern which specifies the sound requirement for each character in every position of the poem. Lastly, it must be meaningful, i.e., with grammatical and semantic well-formedness for each line and, with thematic coherence and integrity throughout the poem. These three points form the universal principles for human poets to create Chinese classical poems. Chinese Classical poetry can be classified into", ". 1.1em  ::: . 1.1.1em  :::  ::: . 1.1.1.1em Jinyi Hu, Maosong Sun$^{*}$ $*$ Corresponding author Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China hujy17@mails.tsinghua.edu.cn, sms@tsinghua.edu.cn Poetry generation is an interesting research topic in the field of text generation. As one of the most valuable literary and cultural heritages of China, Chinese classical poetry is very familiar and loved by Chinese people from generation to generation. It has many particular characteristics in its language structure, ranging from form, sound to meaning, thus is regarded as an ideal testing task for text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training", "of flutes. The poem is saturated with nostalgia, solitude and desolate feelings of life, which is not only embodied in the bleak scenery but also overtly revealed in the last three sentences. The combination of visual and audio feelings and of reality and imagination is tactfully employed in the poem and makes it even more impressive and resonating. Conclusion and Future Works. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems, including SHI and CI. To this end, we at first define a unified format for formulating all types of training samples by integrating more detailed form information, then present a simple form-stressed weighting method in GPT-2 to strengthen the control to the form of CI. Preliminary experiments validate the effectiveness of our method. Nevertheless, we also find that enabling GPT-2 to have a strong capability in form manipulation for the generated texts remains a difficult challenge, particularly", "methods in Chinese classical poems). In addition, for Lvshi, the pairs of $<$the third line, the fourth line$>$ and $<$the fifth line, the sixth line$>$ must satisfy the requirement of Duizhang, a correspondence in both part-of-speech(POS) and word sense between two parallel lines. This point is perfectly reflected in the generated poem, as shown in Table 2. UTF8gbsn\u6ee1\u6c5f\u7ea2 \u00b7 \u585e\u5916 UTF8gbsn\u98ce\u6025\u79cb\u7a7a\uff0c\u5929\u6b32\u66ae\uff0c\u9ec4\u4e91\u98de\u5904\u3002 UTF8gbsn\u4eba\u4e0d\u89c1\uff0c\u6c99\u5824\u91ce\u620d\uff0c\u4e71\u9e26\u557c\u82e6\u3002 UTF8gbsn\u4e07\u91cc\u80e1\u7b33\u5439\u96c1\u65ad\uff0c\u4e09\u66f4\u7f8c\u7b1b\u6101\u5982\u8bb8\u3002 UTF8gbsn\u751a\u5173\u6cb3\u3001\u5f81\u5987\u6cea\u75d5\u591a\uff0c\u65e0\u884c\u8def\u3002 UTF8gbsn\u9752\u72fc\u706b\uff0c\u8352\u70df\u6811\u3002 UTF8gbsn\u767d\u9732\u8349\uff0c\u6b8b\u9633\u5ea6\u3002 UTF8gbsn\u4f46\u5bd2\u5c71\u8fdc\u8fd1\uff0c\u6545\u4e61\u5343\u53e4\u3002 UTF8gbsn\u4e00\u89d2\u659c\u6656\u5f52\u68a6\u7ed5\uff0c\u6ee1\u6c5f\u7ea2\u53f6\u897f\u9675\u53bb\u3002 UTF8gbsn\u5f85\u660e\u5e74\uff0c\u53c8\u5230\u6c49\u5bb6\u57ce\uff0c\u91cd\u56de\u987e\u3002  The example above is a CI in the form of Manjianghong and the title means \u201cbeyond the Great Wall\u201d. It vividly depicts a typical view of the Northwestern China howling wind, clouds of dust, crying crows and lugubrious sound of flutes. The poem is saturated with nostalgia, solitude and desolate feelings of life, which is not only embodied in the bleak scenery but also overtly revealed in the last three sentences. The", "major forms of SHI are Jueju and Lvshi with four lines and eight lines accordingly. Jueju and Lvshi are further divided into Wuyan Jueju and Qiyan Jueju as well as Wuyan Lvshi and Qiyan Lvshi where Wuyan means five characters each line and Qiyan means seven characters. Figure 1 is a famous classical poem of Wuyan Jueju. In addition, Lvshi has a strict requirement for the two-sentence pairs composed of $<$the third line, the fourth line$>$ and $<$the fifth line, the sixth line$>$: they must satisfy the requirement of Duizhang, this is, a strict parallel matching for both part of speech and sense of every character in two lines. This obviously increases the difficulty of poem composition. According to CCPC1.0, Wuyan Jueju, Qiyan Jueju, Wuyan Lvshi, and Qiyan Lvshi constitute 67.96% of SHI, with 4.26%, 22.57%, 15.99%, and 25.14% respectively. Introduction ::: CI. CI is another primary type of Chinese poetry. In contrast to SHI, CI has nearly one thousand forms. Each form of CI (it is", "to http://jiuge.thunlp.cn/). Related Work. With the development of deep learning, the mainstream of poem generation research has been shifted from traditional statistical models to neural network methods in recent years. Most existing works are based on the Encoder-Decoder architecture BIBREF2. In Chinese classical poetry generation, yan2013poet proposed a model using the Encoder-Decoder architecture and wang2016chinese further used attention-based sequence-to-sequence model. The key factor in designing the model architecture is how to treat the generated context so far in the process of generating a poem. The input to the encoder could be as short as a single poetic line or all the previously generated lines (whole history). Theoretically, considering the whole history is more appropriate for keeping the thematic coherence and integrity of the generated poem than considering the short history, at the expense that may hurt the fluency of the generated sentences due to the data", "\u00b7 \u8fdc\u671b UTF8gbsn\u6c5f\u4e0a\u5fae\u832b\u4e00\u53f6\u821f\uff0c\u5929\u6daf\u82b3\u8349\u6ee1\u6c40\u6d32 UTF8gbsn\u6570\u58f0\u6e14\u5531\u9694\u8239\u8fc7\uff0c\u51e0\u70b9\u4eba\u5bb6\u843d\u5e06\u6e38 UTF8gbsn\u6625\u8272\u4e0d\u4ece\u83ba\u8bed\u5230\uff0c\u5915\u9633\u7a7a\u5ea6\u5ba2\u5fc3\u6101 UTF8gbsn\u4f55\u65f6\u91cd\u5411\u957f\u6865\u996e\uff0c\u540c\u6cdb\u6eaa\u5149\u5171\u767d\u5934 The example above is a Qiyan Lvshi. The title of this poem means \u201clook far around\u201d. In this poem, the first four lines depict a view seen from the river bank-misty and rolling waters, a drifting boat, lush vanillas, melodies from passing boats and cottages on the bank, creating a tranquil and halcyon atmosphere. However, the poet is still overcome by solitude and nostalgia because of the lonely trip, which is vividly revealed in the second four sentences. The poem adopts a typical semantic structure of Qiyan Lvshi with its first-half delineating a view and then conveying the poet\u2019s feeling in the second-half (the contrast between the view and the feeling is one of the appreciated artistic methods in Chinese classical poems). In addition, for Lvshi, the pairs of $<$the third line, the fourth line$>$ and $<$the fifth line, the sixth line$>$ must satisfy the requirement of Duizhang, a"]}
{"question_id": "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "predicted_answer": "Subword-based and fully character-based methods.", "predicted_evidence": ["e-commerce domain, the training data is large. We use a distributed training framework for both the baseline system and our system. Training data are split into several parts, each being trained on a single worker node. A parameter server averages the model parameters from each worker node after every 100 training batchs and then synchronizes the averaged model to every worker node. Each worker continues with the training process based on the averaged model. Results and Analysis. We use BLEU BIBREF26 as our evaluation metric. The performance of different systems are shown in Table TABREF34 and TABREF35 . On both the news and e-commerce domains, our system performs better than baseline systems. On news domain, the average improvement of our method is 1.75 and 0.97 BLEU score when implemented on RNN-based NMT, compared with subword BIBREF3 method and fully character-based BIBREF4 method, respectively. When implemented on Transformer BIBREF15 , average improvement is 1.47 BLEU compared", "the highest scores are pushed to the stack. Then for each stem, we predict the top INLINEFORM1 suffixes, which will result in INLINEFORM2 complete candidates. The candidates will be inserted to a priority queue, which keeps records of the top INLINEFORM3 complete candidates. After all the stems are expanded, the final n-best candidates are obtained. Experiments. We run our experiments on English to Russian (En-RU) data under two significantly different domain, namely the news domain and the e-commerce domain. We verify our method on both RNN based NMT architecture and Transformer based NMT architecture. Data. News We select 5.3M sentences from the bilingual training corpus released by WMT2017 shared task on the news translation domain as our training data. We use 3 test set, which are published by WMT2017 news translation task, namely \u201cNews2014\u201d, \u201cNews2015\u201d, \u201cNews2016\u201d. E-commerce We collect 50M bilingual sentences as our training corpus: 10M sentences are crawled and automatic", "on RNN-based NMT, compared with subword BIBREF3 method and fully character-based BIBREF4 method, respectively. When implemented on Transformer BIBREF15 , average improvement is 1.47 BLEU compared with subword method. On the e-commerce domain, which use 50M sentences as training corpus, the average improvement of our method is 0.68 BLEU compared with the subword method. We evaluate stem accuracies and suffix accuracies separately. For stem, we use BLEU as evaluation metric, Table TABREF34 shows stem BLEU of different methods on \u201cNews2014\u201d test set, our method can gain significant improvement compared with baselines, since our method can reduce data sparsity better than baselines. Our method can effectively reduce suffix error, Figure FIGREF43 gives some examples both on e-commerce and news domains: For the first sample, the suffix of the translation words (tagged by 1 and 2) from two different baseline systems means a reflexive verb, whose direct object is the same as its subject. In", "published by WMT2017 news translation task, namely \u201cNews2014\u201d, \u201cNews2015\u201d, \u201cNews2016\u201d. E-commerce We collect 50M bilingual sentences as our training corpus: 10M sentences are crawled and automatic aligned from some international brand's English and Russian websites. 20M are back translated corpus: First we crawled the Russian sentences from websites of certain Russian's Brands. Then translated them to English through a machine translation system trained on limited RU-EN corpus BIBREF24 . The last 20M bilingual sentences are crawled from the web, and are not domain specific. We typically use the following 3 types of data as test set, which are named title, description and comment, these sentences are all extracted from e-commerce websites. Title are the goods' titles showed on a listing page when some buyers type in some keywords in a searching bar under an e-commerce website. Description refers to the information in a commodities' detail page. Comment include the review or feedback", "types can be as small as several hundreds. Another advantage of this structure is that during the prediction of suffix, the previously generated stem sequence can be considered, which can further improve the accuracy of suffix prediction. We empirically study this method and compare it with previous work on reducing OOV rates ( BIBREF3 , BIBREF3 ; BIBREF4 , BIBREF4 ). Results show that our method gives significant improvement on the English to Russian translation task on two different domains and two popular NMT architectures. We also verify our method on training data consisting of 50M bilingual sentences, which proves that this method works effectively on large-scale corpora. Translation Granularity. Subword based BIBREF3 and character-based ( BIBREF4 , BIBREF4 ; BIBREF5 , BIBREF5 ) NMT are the two directions of adjusting translation granularity, which can be helpful to our problem. In BIBREF3 ( BIBREF3 )'s work, commonly appearing words remain unchanged, while others are segmented", "page when some buyers type in some keywords in a searching bar under an e-commerce website. Description refers to the information in a commodities' detail page. Comment include the review or feedback from some buyers. Example sentences are shown in Table TABREF33 . For each kind of test set, we randomly select 1K English sentences and translate it by human. Pre-Processing Both the training set and the test set are lowercased, and some entity words appeared in the data are generalized into specific symbols, such as \u201c_date_\u201d, \u201c_time_\u201d, \u201c_number_\u201d. When selecting our training data, we keep the sentences which has length between 1 to 30. We use a bilingual sentence scorer to discard some low-quality bilingual sentences. The scorer is simply trained under algorithm of IBM Model 1 BIBREF25 on a very large bilingual corpus. Target Side Word Stemming We use snowball to create stems from words. Because stem created from snowball is always a substring of the original word, we can obtain", "because the corresponding source word \u201cpositive\u201d is singular form. Character-based system can correctly translate source word \u201cstars\u201d into a Russian word with plural form. However, the translation of \u201cpositive\u201d (tagged by 2) is still with wrong form. Both the translation of \u201cpositive\u201d and \u201cstars\u201d from our system are with the correct forms. In the third sample, the translation word tagged by 3 represents past tense; However, the translation words tagged by 1 and 2 represent present tense. Our system successfully predicted the tense moods. Conclusion. We proposed a simple but effective method to improve English-Russian NMT, for which a morphologically rich language is on the target side. We take a two-step approach in the decoder. At each step, a stem is first generated, then its suffix is generated. We empirically compared our method with two previous methods (namely subword and fully character-based), which can also to some extent address our problem. Our method gives an improvement", "baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work, and is available on github. We limit the source and target vocabularies to the most frequent 30K tokens for both English and Russian. For news domain, about 99.7% tokens are covered by the source side vocabulary, about 97.0% target tokens are covered by the target side vocabulary. Our System. For our system, the source token coverage is the same as the baselines. On the other hand, 100% target tokens are covered by the target-side vocabulary, which consists of \u201csub-stem\u201d units generated from target side stem sequence by applying BPE algorithm. There are totally 752 types of suffixes, which are calculated from the suffix sequences generated from target side sentences. Distributed Training. For the experiments on the e-commerce domain, the training data is large. We use a distributed training framework for both the baseline system and our system. Training data are split into several parts, each being trained on a"]}
{"question_id": "c7f43c95db3d0c870407cd0e7becdd802463683b", "predicted_answer": "Yes", "predicted_evidence": ["linear structure of vector space and the highly increased memory requirement when confronting huge vocabularies. Both problems have been solved by a popular model called Word2Vec BIBREF12, which utilizes two shallow neural networks, i.e., skip-gram and continuous bag-of-words, to learn word vectors from large corpora. The model is also capable of capturing interesting linear relationships between word vectors. While Word2Vec makes a breakthrough in word representation, it has not been fully understood and its theoretical exploitation is still in demand. One aspect, which has always been ignored, is the choice of noise distribution for negative sampling. Word2Vec employs a smoothed unigram distribution with a power rate of 3/4 as the noise distribution. The decision is made by empirical trials but has been widely adopted in subsequent work BIBREF13, BIBREF4, BIBREF14, BIBREF15. However, the quality of learned word vectors is sensitive to the choice of noise distribution BIBREF16,", "contrastive estimation by extensive experiments. (3) We propose a semantics weighted model for the MSR sentence completion task, resulting in considerable improvements. Word2Vec ::: Architectures. Firstly, we briefly introduce the two architectures, i.e., skip-gram (SG) and continuous bag-of-words (CBOW) in Word2Vec BIBREF12. For a corpus with a word sequence $w_{1}, w_{2}, \\cdots , w_{T}$, skip-gram predicts the context word $w_{t+j}$ given the center word $w_t$, and maximizes the average log probability, where $c$ is the size of context window, and $p(w_{t+j}|w_{t})$ is defined by the full softmax function, where $v_{w}$ and $v_{w}^{\\prime }$ are the vectors of the \u201cinput\u201d and \u201coutput\u201d words, and $|V|$ is the size of vocabulary. As for CBOW, it predicts the center word based on the context words. The input vector is usually the average of the context words' vectors, i.e., $v_{w_{I}} = \\frac{1}{2c} \\sum _{-c \\le j \\le c, j \\ne 0} v_{w_{t+j}}$. Word2Vec ::: Negative Sampling. For", "context words. The input vector is usually the average of the context words' vectors, i.e., $v_{w_{I}} = \\frac{1}{2c} \\sum _{-c \\le j \\le c, j \\ne 0} v_{w_{t+j}}$. Word2Vec ::: Negative Sampling. For large vocabularies, it is inefficient to compute the full softmax function in Eq. (DISPLAY_FORM3). To tackle this problem, Word2Vec utilizes negative sampling to distinguish the real output word from $k$ noise words, where $\\sigma (x) = \\frac{1}{1 + \\exp (-x)}$, and $P_n$ is the so-called noise distribution, representing the probability for a word to be sampled as a noise word. The smoothed unigram distribution used in Word2Vec is expressed as, where $f(w_i)$ is the frequency of word $w_i$. Word2Vec ::: Sub-sampling. Sub-sampling is a process in Word2Vec for randomly deleting the most frequent words during training, since they are usually stop words with less information than infrequent ones. During sub-sampling, the probability that a word $w_i$ should be kept is defined as, where", "Introduction. The recent decade has witnessed the great success achieved by word representation in natural language processing (NLP). It proves to be an integral part of most other NLP tasks, in which words have to be vectorized before input to the models. High quality word vectors have boosted the performance of many tasks, such as named entity recognition BIBREF0, BIBREF1, sentence completion BIBREF2, BIBREF3, part-of-speech tagging BIBREF4, BIBREF5, sentiment analysis BIBREF6, BIBREF7, and machine translation BIBREF8, BIBREF9. In a conventional way, word vectors are obtained from word-context co-occurrence matrices by either cascading the row and column vectors BIBREF10 or applying singular value decomposition (SVD) BIBREF11. However, these approaches are limited by their sub-optimal linear structure of vector space and the highly increased memory requirement when confronting huge vocabularies. Both problems have been solved by a popular model called Word2Vec BIBREF12, which", "the performance of models is limited due to the inadequate training of infrequent words BIBREF22, BIBREF23. Smoothed Unigram. The smoothed unigram distribution in Word2Vec BIBREF12 solves this problem because it gives more chances for infrequent words to be sampled. However, the required power rate is decided empirically, and may need adjustment for different scenarios BIBREF24, BIBREF25. BIBREF23 even propose to use a bigram distribution after studying the power rate, but it is infeasible for large corpora. Besides, the smoothed unigram distribution also changes the lexical structure of infrequent words, which could be a reason for the limited quality of word vectors. Sub-sampled Unigram Distribution. We believe a sub-sampled unigram distribution is better for negative sampling since it reduces the amount of frequent words and also maintains the lexical structure of infrequent words. To our best knowledge, we are the first to employ such a noise distribution for negative sampling.", "is 20 in both models; we train SG for 5 and 10 epochs when the size of word vectors is 100 and 300, while the number of epochs is 10 and 20 in CBOW; we use all the rest words in a sentence to form $\\mathcal {S}$. Our focus here is to popularize SWM rather than to compare the noise distributions. We show the results of this task by previous word presentation models and our approach in Table TABREF60. The bottom three previous models follow the conventional method. Accordingly, we draw two conclusions: (1) sub-sampling on the words in sentences results in significant improvements to the conventional method; and (2) SWM further improves CM$^s$ and beats previous word representation models with a vector size of 300, indicating the success of semantics quantification. Conclusions. We propose to employ a sub-sampled unigram distribution for better negative sampling, and design an approach to derive the required sub-sampling rate. Experimental results show that our noise distribution", "of $x$ such that $\\left\\langle x \\right\\rangle = \\sum _{r=1}^{|V|}\\frac{x}{r} / \\sum _{r=1}^{|V|}\\frac{1}{r}$. (2) wLSE-2. We use wLSE with a condition that the fitting line passes through the point $(\\log 1, \\log f_1)$. This method engages the most frequent word to further control the trend of the line. As a result, $\\hat{\\gamma }= f_1$ and Now, we can write down the expression of the sub-sampled unigram distribution where $\\alpha _i$ satisfies Note that we use $P_n^{sub}$ to distinguish from the original noise distribution $P_n$ in Word2Vec. Sub-sampled Unigram Distribution ::: Discussions. In semantics quantification, the modeling of word distribution is not limited to zipf's law. We adopt it because of its popularity and conciseness. There could be other choices BIBREF28, BIBREF29, and the expression of $t_c$ needs modification accordingly. Besides, one can either use the chosen law to decide the critical word or just search through the unigram distribution to find it.", "trials but has been widely adopted in subsequent work BIBREF13, BIBREF4, BIBREF14, BIBREF15. However, the quality of learned word vectors is sensitive to the choice of noise distribution BIBREF16, BIBREF13 when using a moderate number (5 to 15) of negative samples, which is a common strategy for the tradeoff between vector quality and computation costs. In this paper, we propose to employ a sub-sampled unigram distribution for negative sampling and demonstrate its capability of improving the linear relationships between word vectors. Our contributions include three aspects: (1) We propose the concept of semantics quantification and derive a suitable sub-sampling rate for the proposed distribution. (2) We demonstrate the advantages of our noise distribution in both negative sampling and noise contrastive estimation by extensive experiments. (3) We propose a semantics weighted model for the MSR sentence completion task, resulting in considerable improvements. Word2Vec ::: Architectures."]}
{"question_id": "4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "predicted_answer": "Word similarity task, synonym selection task, named entity recognition, sentence completion, part-of-speech tagging, sentiment analysis, and machine translation.", "predicted_evidence": ["linear structure of vector space and the highly increased memory requirement when confronting huge vocabularies. Both problems have been solved by a popular model called Word2Vec BIBREF12, which utilizes two shallow neural networks, i.e., skip-gram and continuous bag-of-words, to learn word vectors from large corpora. The model is also capable of capturing interesting linear relationships between word vectors. While Word2Vec makes a breakthrough in word representation, it has not been fully understood and its theoretical exploitation is still in demand. One aspect, which has always been ignored, is the choice of noise distribution for negative sampling. Word2Vec employs a smoothed unigram distribution with a power rate of 3/4 as the noise distribution. The decision is made by empirical trials but has been widely adopted in subsequent work BIBREF13, BIBREF4, BIBREF14, BIBREF15. However, the quality of learned word vectors is sensitive to the choice of noise distribution BIBREF16,", "Introduction. The recent decade has witnessed the great success achieved by word representation in natural language processing (NLP). It proves to be an integral part of most other NLP tasks, in which words have to be vectorized before input to the models. High quality word vectors have boosted the performance of many tasks, such as named entity recognition BIBREF0, BIBREF1, sentence completion BIBREF2, BIBREF3, part-of-speech tagging BIBREF4, BIBREF5, sentiment analysis BIBREF6, BIBREF7, and machine translation BIBREF8, BIBREF9. In a conventional way, word vectors are obtained from word-context co-occurrence matrices by either cascading the row and column vectors BIBREF10 or applying singular value decomposition (SVD) BIBREF11. However, these approaches are limited by their sub-optimal linear structure of vector space and the highly increased memory requirement when confronting huge vocabularies. Both problems have been solved by a popular model called Word2Vec BIBREF12, which", "trials but has been widely adopted in subsequent work BIBREF13, BIBREF4, BIBREF14, BIBREF15. However, the quality of learned word vectors is sensitive to the choice of noise distribution BIBREF16, BIBREF13 when using a moderate number (5 to 15) of negative samples, which is a common strategy for the tradeoff between vector quality and computation costs. In this paper, we propose to employ a sub-sampled unigram distribution for negative sampling and demonstrate its capability of improving the linear relationships between word vectors. Our contributions include three aspects: (1) We propose the concept of semantics quantification and derive a suitable sub-sampling rate for the proposed distribution. (2) We demonstrate the advantages of our noise distribution in both negative sampling and noise contrastive estimation by extensive experiments. (3) We propose a semantics weighted model for the MSR sentence completion task, resulting in considerable improvements. Word2Vec ::: Architectures.", "to employ a sub-sampled unigram distribution for better negative sampling, and design an approach to derive the required sub-sampling rate. Experimental results show that our noise distribution captures better linear relationships between words than the baselines. It adapts to different corpora and is scalable to NCE related work. The proposed semantics weighted model also achieves a success on the MSR sentence completion task. In summary, our work not only improves the quality of word vectors, but also sheds light on the understanding of Word2Vec.", "is 20 in both models; we train SG for 5 and 10 epochs when the size of word vectors is 100 and 300, while the number of epochs is 10 and 20 in CBOW; we use all the rest words in a sentence to form $\\mathcal {S}$. Our focus here is to popularize SWM rather than to compare the noise distributions. We show the results of this task by previous word presentation models and our approach in Table TABREF60. The bottom three previous models follow the conventional method. Accordingly, we draw two conclusions: (1) sub-sampling on the words in sentences results in significant improvements to the conventional method; and (2) SWM further improves CM$^s$ and beats previous word representation models with a vector size of 300, indicating the success of semantics quantification. Conclusions. We propose to employ a sub-sampled unigram distribution for better negative sampling, and design an approach to derive the required sub-sampling rate. Experimental results show that our noise distribution", "sizes of the resulted vocabularies. The rightmost two columns are the sub-sampling rates for our noise distribution by the wLSE-1 and wLSE-2 estimations, respectively. The values are $10^6$ times of the true ones for readability. Experiments ::: Experimental Setup ::: Training details. We implement the training of word vectors with the word2vec tool, in which the part of noise distribution is modified to support several choices. For SG and CBOW, we set the vector dimensionality to 100, and the size of the context window to 5. We choose 10 negative samples for each training sample in the models. The models are trained using the stochastic gradient decent (SGD) algorithm with a linear decaying learning rate with an initial value of 0.025 in SG and 0.05 in CBOW. We train the models on the three large corpora for 2 epochs, and for MSR's Holmes novels the value may vary. Results in this paper are shown in percentages and each of them is the average result of 4 repeated experiments, unless", "context words. The input vector is usually the average of the context words' vectors, i.e., $v_{w_{I}} = \\frac{1}{2c} \\sum _{-c \\le j \\le c, j \\ne 0} v_{w_{t+j}}$. Word2Vec ::: Negative Sampling. For large vocabularies, it is inefficient to compute the full softmax function in Eq. (DISPLAY_FORM3). To tackle this problem, Word2Vec utilizes negative sampling to distinguish the real output word from $k$ noise words, where $\\sigma (x) = \\frac{1}{1 + \\exp (-x)}$, and $P_n$ is the so-called noise distribution, representing the probability for a word to be sampled as a noise word. The smoothed unigram distribution used in Word2Vec is expressed as, where $f(w_i)$ is the frequency of word $w_i$. Word2Vec ::: Sub-sampling. Sub-sampling is a process in Word2Vec for randomly deleting the most frequent words during training, since they are usually stop words with less information than infrequent ones. During sub-sampling, the probability that a word $w_i$ should be kept is defined as, where", "MEN BIBREF34, and RW BIBREF35. Experiments ::: Task 1: Word Similarity Task ::: Results. We implement the task on the mentioned 5 datasets and show the results in the column Word Similarity of Table TABREF42. At the first glance, our noise distributions Sub$^{L1}$ and Sub$^{L2}$ perform slightly better than Uni$^{3/4}$. Significant improvements can be achieved on two small datasets RG and MC, because they are more sensitive to the vector quality. Another observation is that CBOW is more affected by Sub$^{L1}$ and Sub$^{L2}$ than SG, if comparing results on RG and MC with Wiki10 corpus. These results show that our noise distributions have the potential as high as or even higher than the smoothed unigram distribution in learning good word vectors. Experiments ::: Task 2: Synonym Selection Task ::: Task Description. This task attempts to select the semantically closest word, from the candidate answers, to the stem word. For example, given the stem word \u201ccostly\u201d and the candidate answers"]}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "predicted_answer": "KPTimes dataset", "predicted_evidence": ["and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations. Building the KPTimes dataset. To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites. We applied heuristics to identify the content (title, headline and body) of each article and regarded the keyphrases provided in the HTML metadata as the gold standard. A cherry-picked sample document is showcased in Figure , it allows to show present and absent keyphrases, as well as keyphrase variants (in this example News media and journalism). We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm. We first retrieved", "the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that comes with editor-assigned keyphrases. Online news are particularly relevant to keyphrase generation since they are a natural fit for faceted navigation BIBREF6 or topic detection and tracking BIBREF7. Also, and not less importantly, they are available in large quantities and are sometimes accompanied by metadata containing human-assigned keyphrases initially intended for search engines. Here, we divert these annotations from their primary purpose, and use them as gold-standard labels to automatically build our dataset. More precisely, we collect data by crawling selected news websites and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from", "way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm. We first retrieved the URLs of the free-to-read articles from 2006 to 2017, and collected the corresponding archived HTML pages using the Internet Archive. Doing so allows the distribution of our dataset using a thin, URL-only list. We then extracted the HTML body content using beautifulsoup and devised heuristics to extract the main content and title of each article while excluding extraneous HTML markup and inline ads. Gold standard keyphrases are obtained from the metadata (field types news_keywords and keywords) available in the HTML page of each article. Surface form variants of gold keyphrases (e.g. \u201cAIDS; HIV\u201d, \u201cDriverless Cars; Self-Driving Cars\u201d or \u201cFatalities; Casualties\u201d), which are sometimes present in the metadata, are kept to be used for evaluation purposes. We further cleansed and filtered the dataset by", "pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset. Although in this study we concentrate only on the textual content of the news articles, it is worth noting that the HTML pages also provide additional information that can be helpful in generating keyphrases such as text style properties (e.g. bold, italic), links to related articles, or news categorization (e.g. politics, science, technology). Data analysis. We explored the KPTimes dataset to better understand how it stands out from the existing ones. First, we looked at how editors tag news articles. Figure illustrates the difference between the annotation behaviour of readers, authors and editors through the number of times that each unique keyphrase is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have.", "paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents: DUC-2001 BIBREF9 and KPCrowd BIBREF10. Originally created for the DUC evaluation campaign on text summarization BIBREF11, the former is composed of 308 news annotated by graduate students. The latter includes 500 news annotated by crowdsourcing. Both datasets are very small and contain newswire articles from various online sources labelled by non-expert annotators, in this case readers, which is not without issues. Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive", "and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from author-assigned keyphrases found in scholarly documents. Finally, we analyse the performance of state-of-the-art keyphrase generation models and investigate their transferability to the news domain and the impact of domain shift. Existing datasets. Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents:", "one observed for KP20k while the number of missing keyphrases is higher. This indicates that editors are more likely to generalize and assign keyphrases that do not occur in the document ($\\approx 55\\%$). It is therefore this ability to generalize that models should mimic in order to perform well on KPTimes. We also note that keyphrases are on average shorter in news datasets ($1.5$ words) than those in scientific paper datasets ($2.4$ words). This may be due to the abundant use of longer, more specific phrases in scholarly documents BIBREF14. Variants of keyphrases recovered from the metadata occur in 8% of the documents and represent 810 sets of variants in the KPTimes test split. These variants often refer to the same concept (e.g. \u201cMarijuana; Pot; Weed\u201c), but can sometimes be simply semantically related (e.g. \u201cBridges; Tunnels\u201c). Thereafter, keyphrase variants will be used during model evaluation for reducing the number of mismatches associated with commonly used lexical overlap", "Cars; Self-Driving Cars\u201d or \u201cFatalities; Casualties\u201d), which are sometimes present in the metadata, are kept to be used for evaluation purposes. We further cleansed and filtered the dataset by removing duplicates, articles without content and those with too few (less than 2) or too many (more than 10) keyphrases. This process resulted in a set of 279 923 article-keyphrase pairs. We randomly divided this dataset into training (92.8%), development (3.6%) and test (3.6%) splits. Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset. Although in this study we concentrate only on the textual"]}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "predicted_answer": "CopyRNN neural model.", "predicted_evidence": ["this model is bound to the content of the document and cannot generate missing keyphrases. We use the implementation of MultipartiteRank available in pke BIBREF18. Performance of existing models ::: Models ::: State-of-the-art, supervised: CopyRNN. The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by BIBREF3 to include correlation constraints among keyphrases which we do not include here as it yields comparable results. Two models were trained to bring evidence on the necessity to have datasets from multiple domains. CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes), the two models use the same architecture. Performance of existing", "generation. The dataset and the code are available at https://github.com/ygorg/KPTimes. Large datasets have driven rapid improvement in other natural language generation tasks, such as machine translation or summarization. We hope that KPTimes will play this role and help the community in devising more robust and generalizable neural keyphrase generation models.", "Introduction. Keyphrases are single or multi-word lexical units that best summarise a document BIBREF0. As such, they are of great importance for indexing, categorising and browsing digital libraries BIBREF1. Yet, very few documents have keyphrases assigned, thus raising the need for automatic keyphrase generation systems. This task falls under the task of automatic keyphrase extraction which can also be the subtask of finding keyphrases that only appear in the input document. Generating keyphrases can be seen as a particular instantiation of text summarization, where the goal is not to produce a well-formed piece of text, but a coherent set of phrases that convey the most salient information. Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good", "the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that comes with editor-assigned keyphrases. Online news are particularly relevant to keyphrase generation since they are a natural fit for faceted navigation BIBREF6 or topic detection and tracking BIBREF7. Also, and not less importantly, they are available in large quantities and are sometimes accompanied by metadata containing human-assigned keyphrases initially intended for search engines. Here, we divert these annotations from their primary purpose, and use them as gold-standard labels to automatically build our dataset. More precisely, we collect data by crawling selected news websites and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from", "semantically related (e.g. \u201cBridges; Tunnels\u201c). Thereafter, keyphrase variants will be used during model evaluation for reducing the number of mismatches associated with commonly used lexical overlap metrics. Performance of existing models. We train and evaluate several keyphrase generation models to understand the challenges of KPTimes and its usefulness for training models. Performance of existing models ::: Evaluation metrics. We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases. Performance of existing models ::: Models ::: Baseline: FirstPhrase. Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline \u2013that is, the", "to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance. There is therefore a great need for annotated data from different sources, that is both sufficiently large to support the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that", "and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from author-assigned keyphrases found in scholarly documents. Finally, we analyse the performance of state-of-the-art keyphrase generation models and investigate their transferability to the news domain and the impact of domain shift. Existing datasets. Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents:", "In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts. Here, we fill this gap and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations. Building the KPTimes dataset. To create the KPTimes dataset, we collected over half"]}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "predicted_answer": "larger, less controlled indexing vocabulary", "predicted_evidence": ["paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents: DUC-2001 BIBREF9 and KPCrowd BIBREF10. Originally created for the DUC evaluation campaign on text summarization BIBREF11, the former is composed of 308 news annotated by graduate students. The latter includes 500 news annotated by crowdsourcing. Both datasets are very small and contain newswire articles from various online sources labelled by non-expert annotators, in this case readers, which is not without issues. Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive", "and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from author-assigned keyphrases found in scholarly documents. Finally, we analyse the performance of state-of-the-art keyphrase generation models and investigate their transferability to the news domain and the impact of domain shift. Existing datasets. Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents:", "pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset. Although in this study we concentrate only on the textual content of the news articles, it is worth noting that the HTML pages also provide additional information that can be helpful in generating keyphrases such as text style properties (e.g. bold, italic), links to related articles, or news categorization (e.g. politics, science, technology). Data analysis. We explored the KPTimes dataset to better understand how it stands out from the existing ones. First, we looked at how editors tag news articles. Figure illustrates the difference between the annotation behaviour of readers, authors and editors through the number of times that each unique keyphrase is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have.", "and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations. Building the KPTimes dataset. To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites. We applied heuristics to identify the content (title, headline and body) of each article and regarded the keyphrases provided in the HTML metadata as the gold standard. A cherry-picked sample document is showcased in Figure , it allows to show present and absent keyphrases, as well as keyphrase variants (in this example News media and journalism). We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm. We first retrieved", "In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts. Here, we fill this gap and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations. Building the KPTimes dataset. To create the KPTimes dataset, we collected over half", "is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have. For example, we observe that frequent keyphrases in KPTimes are close to topic descriptors (e.g. \u201cBaseball\u201c, \u201cPolitics and Government\u201c) while those appearing only once are very precise (e.g. \u201cMarley's Cafe\u201c, \u201cCatherine E. Connelly\u201c). Annotations in KPTimes are arguably more uniform and consistent, through the use of tag suggestions, which, as we will soon discuss in \u00a7SECREF12, makes it easier for supervised approaches to learn a good model. Next, we further looked at the characteristics of the gold keyphrases in KPTimes. Table shows that the number of gold keyphrases per document is similar to the one observed for KP20k while the number of missing keyphrases is higher. This indicates that editors are more likely to generalize and assign keyphrases that do not occur in the document ($\\approx", "to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance. There is therefore a great need for annotated data from different sources, that is both sufficiently large to support the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that", "the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that comes with editor-assigned keyphrases. Online news are particularly relevant to keyphrase generation since they are a natural fit for faceted navigation BIBREF6 or topic detection and tracking BIBREF7. Also, and not less importantly, they are available in large quantities and are sometimes accompanied by metadata containing human-assigned keyphrases initially intended for search engines. Here, we divert these annotations from their primary purpose, and use them as gold-standard labels to automatically build our dataset. More precisely, we collect data by crawling selected news websites and use heuristics to draw texts paired with gold keyphrases. We then explore the resulting dataset to better understand how editors tag documents, and how these expert annotations differ from"]}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_answer": "Unanswerable", "predicted_evidence": ["shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results were notably less than 72.5%. In the third section of the table, we report our changes to the hyper-parameters. Specifically, we see the importance of using GloVe as pre-trained word embeddings, how word dropout improves generalization, and how context size plays an important role in the final classification result (showing one of our experiments). Discussion. From the results of Table TABREF19 , we notice our single WSD network, despite eliminating the problem of having a large number of WSD classifiers, still falls short when is compared with the state-of-the-art WSD algorithms. Based on our intuition and supported by some of our preliminary experiments, this deficiency stems from an important factor in our BLSTM network. Since no sense embedding is made publicly available for use, the", "were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed. Results. Between-all-models comparisons - When SensEval-3 task was launched 47 submissions (supervised and unsupervised algorithms) were received addressing this task. Afterward, some other papers tried to work on this data and reported their results in separate articles as well. We compare the result of our model with the top-performing and low-performing algorithms (supervised). We show our single model sits among the 5 top-performing algorithms, considering that in other algorithms for each ambiguous word one separate classifier is", "shows the best result of the network that we described above (and depicted in Fig. FIGREF4 ). Each of the other rows shows one change that we applied to the network to see the behavior of the network in terms of F-measure. In the middle part, we are specifically concerned about the importance of the presence of a BLSTM layer in our network. So, we introduced some fundamental changes in the input or in the structure of the network. Generally, it is expected that the cosine similarities of closer words (in the context) to the true sense be larger than the incorrect senses' BIBREF17 ; however, if a series of cosine similarities can be encoded through an LSTM (or BLSTM) network should be experimented. We observe if reverse the sequential follow of information into our Bidirectional LSTM, we shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results", "discussed direction in order to resolve the inadequacy of the network regarding having two non-overlapping vector spaces of the embeddings, we plan to examine the network on technical domains such as biomedicine as well. In this case, our model will be evaluated on MSH WSD dataset prepared by National Library of Medicine (NLM). Also, construction of sense embeddings using (extended) definitions of senses BIBREF25 BIBREF26 can be tested. Moreover, considering that for many senses we have at least one (lexically) unambiguous word representing that sense, we also aim to experiment with unsupervised (pre-)training of our network which benefits form quarry management by which more training data will be automatically collected from the web.", "that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'. We outline the organization of the rest of the paper as follows. In Section 2, we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings. Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3. We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable. Finally, in Section 5, we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine. Background and Related Work. Generally, there are three categories of WSD algorithms: supervised, knowledge-based, and unsupervised. Supervised algorithms consist", "embeddings, recently, computation of sense embeddings has gained the attention of numerous studies as well. For example, Chen et al. BIBREF11 adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval-2007 data BIBREF12 . Bidirectional LSTM. Long Short-Term Memory (LSTM), introduced by Hochreiter and Schmidhuber (1997) BIBREF13 , is a gated recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. A Bidirectional LSTM is made up of two reversed unidirectional LSTMs BIBREF14 . For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense. One Single BLSTM network for WSD. Given a document and the", "specified along with the sense set {S1, S2, S3, S4} and our goal is to identify the correct sense S4 (as the closest meaning) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context. In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'. We outline the organization of the rest of the paper as follows. In Section 2,", "intuition and supported by some of our preliminary experiments, this deficiency stems from an important factor in our BLSTM network. Since no sense embedding is made publicly available for use, the sense embeddings are initialized randomly; yet, word embeddings are initialized by pre-trained GloVe vectors in order to benefit from the semantic and syntactic properties of the context words conveyed by these embeddings. That is to say, the separate spaces that the sense embeddings and the (context) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data. Furthermore, this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful. Our first attempt to deal with such problem was to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the (informative) definition content words of senses, or taking the average of the GloVe"]}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "predicted_answer": "6", "predicted_evidence": ["information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense. One Single BLSTM network for WSD. Given a document and the position of a target word, our model computes a probability distribution over possible senses related to that word. The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom). In contrast to other supervised neural WSD networks in which generally a softmax layer - with a cross entropy or hinge loss - is parameterized by the context words and selects the corresponding weight matrix and bias vector for each ambiguous word's senses BIBREF15 BIBREF16 , our network shares parameters over all words' senses. While remaining computationally efficient, this structure aims to encode statistical information", "that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'. We outline the organization of the rest of the paper as follows. In Section 2, we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings. Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3. We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable. Finally, in Section 5, we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine. Background and Related Work. Generally, there are three categories of WSD algorithms: supervised, knowledge-based, and unsupervised. Supervised algorithms consist", "specified along with the sense set {S1, S2, S3, S4} and our goal is to identify the correct sense S4 (as the closest meaning) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context. In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'. We outline the organization of the rest of the paper as follows. In Section 2,", "low-performing algorithms (supervised). We show our single model sits among the 5 top-performing algorithms, considering that in other algorithms for each ambiguous word one separate classifier is trained (i.e. in the same number of ambiguous words in a language there have to be classifiers; which means 57 classifiers for this specific task). Table TABREF19 shows the results of the top-performing and low-performing supervised algorithms. The first two algorithms represent the state-of-the-art models of supervised WSD when evaluated on SensEval-3. Multi-classifier BLSTM BIBREF15 consists of deep neural networks which make use of pre-trained word embeddings. While the lower layers of these networks are shared, upper layers of each network are responsible to individually classify the ambiguous that word the network is associated with. IMS+adapted CW BIBREF16 is another WSD model that considers deep neural networks and also uses pre-trained word embeddings as inputs. In contrast to", "other domains such as biomedicine. Background and Related Work. Generally, there are three categories of WSD algorithms: supervised, knowledge-based, and unsupervised. Supervised algorithms consist of automatically inducing classification models or rules from labeled examples BIBREF4 . Knowledge-based WSD approaches are dependent on manually created lexical resources such as WordNet BIBREF5 and the Unified Medical Language System (UMLS) BIBREF6 . Unsupervised algorithms may employ topic modeling-based methods to disambiguate when the senses are known ahead of time BIBREF7 . For a thorough survey of WSD algorithms refer to Navigli BIBREF8 . Neural Embeddings for WSD. In the past few years, there has been an increasing interest in training neural word embeddings from large unlabeled corpora using neural networks BIBREF9 BIBREF10 . Word embeddings are typically represented as a dense real-valued low dimensional matrix INLINEFORM0 (i.e. a lookup table) of size INLINEFORM1 , where", "embeddings, recently, computation of sense embeddings has gained the attention of numerous studies as well. For example, Chen et al. BIBREF11 adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval-2007 data BIBREF12 . Bidirectional LSTM. Long Short-Term Memory (LSTM), introduced by Hochreiter and Schmidhuber (1997) BIBREF13 , is a gated recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. A Bidirectional LSTM is made up of two reversed unidirectional LSTMs BIBREF14 . For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense. One Single BLSTM network for WSD. Given a document and the", "output of the network for a sense that gives the highest value of INLINEFORM1 will be considered as the true sense of the ambiguous term, in other words, the correct sense would be: DISPLAYFORM0   By applying softmax to the result of estimated classification values, INLINEFORM0 , we can show them as probabilities; this facilitates interpretation of the results. Further, the hidden layer INLINEFORM0 is computed as DISPLAYFORM0   where INLINEFORM0 means rectified linear unit; INLINEFORM1 is the concatenated outputs of the right and left traversing LSTMs of the BLSTM when the last context components are met. INLINEFORM2 and INLINEFORM3 are the weights and bias for the hidden layer. Validation for Selection of Hyper-parameters. SensEval-3 data BIBREF3 on which the network is evaluated, consist of separate training and test samples. In order to find hyper-parameters of the network 5% of the training samples were used for the validation in advance. Once the hyper-parameters are selected,", "shows the best result of the network that we described above (and depicted in Fig. FIGREF4 ). Each of the other rows shows one change that we applied to the network to see the behavior of the network in terms of F-measure. In the middle part, we are specifically concerned about the importance of the presence of a BLSTM layer in our network. So, we introduced some fundamental changes in the input or in the structure of the network. Generally, it is expected that the cosine similarities of closer words (in the context) to the true sense be larger than the incorrect senses' BIBREF17 ; however, if a series of cosine similarities can be encoded through an LSTM (or BLSTM) network should be experimented. We observe if reverse the sequential follow of information into our Bidirectional LSTM, we shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results"]}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "predicted_answer": "F-measure", "predicted_evidence": ["shows the best result of the network that we described above (and depicted in Fig. FIGREF4 ). Each of the other rows shows one change that we applied to the network to see the behavior of the network in terms of F-measure. In the middle part, we are specifically concerned about the importance of the presence of a BLSTM layer in our network. So, we introduced some fundamental changes in the input or in the structure of the network. Generally, it is expected that the cosine similarities of closer words (in the context) to the true sense be larger than the incorrect senses' BIBREF17 ; however, if a series of cosine similarities can be encoded through an LSTM (or BLSTM) network should be experimented. We observe if reverse the sequential follow of information into our Bidirectional LSTM, we shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results", "that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'. We outline the organization of the rest of the paper as follows. In Section 2, we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings. Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3. We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable. Finally, in Section 5, we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine. Background and Related Work. Generally, there are three categories of WSD algorithms: supervised, knowledge-based, and unsupervised. Supervised algorithms consist", "shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results were notably less than 72.5%. In the third section of the table, we report our changes to the hyper-parameters. Specifically, we see the importance of using GloVe as pre-trained word embeddings, how word dropout improves generalization, and how context size plays an important role in the final classification result (showing one of our experiments). Discussion. From the results of Table TABREF19 , we notice our single WSD network, despite eliminating the problem of having a large number of WSD classifiers, still falls short when is compared with the state-of-the-art WSD algorithms. Based on our intuition and supported by some of our preliminary experiments, this deficiency stems from an important factor in our BLSTM network. Since no sense embedding is made publicly available for use, the", "to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the (informative) definition content words of senses, or taking the average of the GloVe embeddings of the (informative) context words in their training samples - did not give us a better result than our random initialization. Our preliminary experiments though in which we replaced all GloVe embeddings in the network with sense embeddings (using a method proposed by Chen et al. BIBREF11 ), showed considerable improvements in the results of some ambiguous words. That means both senses and context words (while they can be ambiguous by themselves) come from one vector space. In other words, the context would also be represented by the possible senses that its words can take. This idea not only can help to improve the results of the current model, it can also avoid the need for a large amount of training data since senses can be seen in both places, center and context, to be", "embeddings, recently, computation of sense embeddings has gained the attention of numerous studies as well. For example, Chen et al. BIBREF11 adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval-2007 data BIBREF12 . Bidirectional LSTM. Long Short-Term Memory (LSTM), introduced by Hochreiter and Schmidhuber (1997) BIBREF13 , is a gated recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. A Bidirectional LSTM is made up of two reversed unidirectional LSTMs BIBREF14 . For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense. One Single BLSTM network for WSD. Given a document and the", "discussed direction in order to resolve the inadequacy of the network regarding having two non-overlapping vector spaces of the embeddings, we plan to examine the network on technical domains such as biomedicine as well. In this case, our model will be evaluated on MSH WSD dataset prepared by National Library of Medicine (NLM). Also, construction of sense embeddings using (extended) definitions of senses BIBREF25 BIBREF26 can be tested. Moreover, considering that for many senses we have at least one (lexically) unambiguous word representing that sense, we also aim to experiment with unsupervised (pre-)training of our network which benefits form quarry management by which more training data will be automatically collected from the web.", "output of the network for a sense that gives the highest value of INLINEFORM1 will be considered as the true sense of the ambiguous term, in other words, the correct sense would be: DISPLAYFORM0   By applying softmax to the result of estimated classification values, INLINEFORM0 , we can show them as probabilities; this facilitates interpretation of the results. Further, the hidden layer INLINEFORM0 is computed as DISPLAYFORM0   where INLINEFORM0 means rectified linear unit; INLINEFORM1 is the concatenated outputs of the right and left traversing LSTMs of the BLSTM when the last context components are met. INLINEFORM2 and INLINEFORM3 are the weights and bias for the hidden layer. Validation for Selection of Hyper-parameters. SensEval-3 data BIBREF3 on which the network is evaluated, consist of separate training and test samples. In order to find hyper-parameters of the network 5% of the training samples were used for the validation in advance. Once the hyper-parameters are selected,", "intuition and supported by some of our preliminary experiments, this deficiency stems from an important factor in our BLSTM network. Since no sense embedding is made publicly available for use, the sense embeddings are initialized randomly; yet, word embeddings are initialized by pre-trained GloVe vectors in order to benefit from the semantic and syntactic properties of the context words conveyed by these embeddings. That is to say, the separate spaces that the sense embeddings and the (context) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data. Furthermore, this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful. Our first attempt to deal with such problem was to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the (informative) definition content words of senses, or taking the average of the GloVe"]}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "predicted_answer": "DUC-2001, Inspec, NUS Keyphrase Corpus, ICSI Meeting Corpus", "predicted_evidence": ["Work. We proposed an unsupervised graph-based keyphrase extraction method WikiRank. This method connects the text with concepts in Wikipedia, thus incorporate the background information into the semantic graph and finally construct a set of keyphrase that has optimal coverage of the concepts of the document. Experiment results show the method outperforms two related keyphrase extraction methods. We suggest that future work could incorporate more other semantic approaches to investigate keyphrase extraction task. Introducing the results of dependency parsing or semantic parsing (e.g., OntoUSP) in intermediate steps could be helpful.", "keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams. Result. For comparing with our system, we reimplemented SingleRank and Topical PageRank. Table shows the result of our reimplementation of SingleRank and Topical PageRank, as well as the result of our system. Note that we predict the same number of phrase ( INLINEFORM0 ) for each document while testing all three methods. The result shows our result has guaranteed improvement over SingleRank and Topical PageRank on all four corpora. Conclusion and Future Work. We proposed an unsupervised graph-based keyphrase extraction method WikiRank. This method connects the text with concepts in Wikipedia, thus incorporate the background information into the", "PageRank are: First, The topics are too general. Second, since they are using LDA, they only classify the words to several topics, but don't know what the topics exactly are. However, the topical information we need for keyphrase extraction should be precise. As shown in Figure , the difference between a correct keyphrase sheep disease and an incorrect keyphrase incurable disease could be small, which is hard to be captured by rough topical categorization approach. To overcome the limitations of aforementioned approaches, we propose WikiRank, an unsupervised automatic keyphrase extraction approach that links semantic meaning to text The key contribution of this paper could be summarized as follows: Existing Error Illustration with Example. Figure shows part of an example document. In this figure, the gold keyphrases are marked with bold, and the keyphrases extracted by the TextRank system are marked with parentheses. We are going to illustrate the errors exist in most of present", "concept. Combining these two logic, a candidate satisfying the constrains of Step 3 is not likely to be picked in the best keyphrase set INLINEFORM8 , so we can prune it before the optimalization process. Corpora. The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 . The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 . The NUS Keyphrase Corpus BIBREF10 includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al.", "our example, bovine spongiform encephalopathy and bse refer to the same concept. If a system predicts both of them as keyphrases, it commits a redundancy error. Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document. Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document. In the Mad cow disease example, the keyphrase extractor fails to identify export and scrapie as keyphrases, resulting in infrequency errors. Proposed Model. The WikiRank algorithm includes three steps: (1) Construct the semantic graph including concepts and candidate keyphrases; (2)(optional) Prune the graph with heuristic to filter out candidates which are likely to be erroneously produced; (3) Generate the best set of keyphrases as output. Graph Construction. This is one of the crucial steps in our paper that connects the plain text with", "corresponds to a node. The node corresponds to a concept INLINEFORM8 and the node corresponds to a candidate keyphrase INLINEFORM9 are connected by an edge INLINEFORM10 , if the candidate keyphrase INLINEFORM11 contains concept INLINEFORM12 according to the annotation of TAGME. Part of the semantic graph of the sample document is shown in Figure . Concepts corresponding to are shown in Table . WikiRank. According to BIBREF1 , good keyphrases should be relevant to the major topics of the given document, at the same time should also have good coverage of the major topics of the document. Since we represent the topical information with concepts annotated with TAGME, the goal of our approach is to find the set INLINEFORM0 consisting of INLINEFORM1 keyphrases, to cover concepts (1) as important as possible (2) as much as possible. Let INLINEFORM0 denote the weight of concept INLINEFORM1 . We compute INLINEFORM2 as the frequency INLINEFORM3 exists in the whole document INLINEFORM4 . To", "focus on unsupervised methods. Traditional methods of unsupervised keyphrase extraction mostly focus on getting information of document from word frequency and document structure BIBREF0 , however, after years of attempting, the performance seems very hard to be improved any more. Based on this observation, it is reasonable to suspect that the document itself possibly cannot provide enough information for keyphrase extraction task. To get good coverage of the main topics of the document, Topical PageRank BIBREF1 started to adopt topical information in automatic keyphrase extraction. The main idea of Topical PageRank is to extract the top topics of the document using LDA, then sum over the scores of a candidate phrase under each topic to be the final score. The main problems with Topical PageRank are: First, The topics are too general. Second, since they are using LDA, they only classify the words to several topics, but don't know what the topics exactly are. However, the topical", "which are likely to be erroneously produced; (3) Generate the best set of keyphrases as output. Graph Construction. This is one of the crucial steps in our paper that connects the plain text with human knowledge, facilitating the understanding of semantics. In this step, we adopt TAGME BIBREF2 to obtain the underlying concepts in documents. TAGME is a powerful topic annotator. It identifies meaningful sequences of words in a short text and link them to a pertinent Wikipedia page, as shown in Figure . These links add a new topical dimension to the text that enable us to relate, classify or cluster short texts. This step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. As reported in BIBREF3 , most manually assigned keyphrases turn out to be noun groups. We follow BIBREF4 and select candidates lexical unit with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford"]}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "predicted_answer": "Unanswerable", "predicted_evidence": ["in the subgraph INLINEFORM1 as following: DISPLAYFORM0  where INLINEFORM0 is the weight of INLINEFORM1 as we defined before, and INLINEFORM2 is the degree of INLINEFORM3 in the subgraph INLINEFORM4 . Essentially, INLINEFORM5 is equal to the frequency that concept INLINEFORM6 is annotated in the keyphrase set INLINEFORM7 . The optimization problem is defined as: The goal of the optimization problem is to find the candidate keyphrase set INLINEFORM0 , such that the sum of the scores of the concepts annotated from the phrases in INLINEFORM1 is maximized. We propose an algorithm to solve the optimization problem, as shown in Algorithm . In each iteration, we compute the score INLINEFORM0 for all candidate keyphrases INLINEFORM1 and include the INLINEFORM2 with highest score into INLINEFORM3 , in which INLINEFORM4 evaluates the score of concepts added to the new set INLINEFORM5 by adding INLINEFORM6 into INLINEFORM7 . Approximation Approach with Pre-pruning. In practice, computing score", ", in which INLINEFORM4 evaluates the score of concepts added to the new set INLINEFORM5 by adding INLINEFORM6 into INLINEFORM7 . Approximation Approach with Pre-pruning. In practice, computing score for all the candidate keyphrases is not always necessary, because some of the candidates are very unlikely to be gold keyphrase that we can remove them from our graph before applying the algorithm to reduce the complexity. In this section, we introduce three heuristic pruning steps that significantly reduces the complexity of the optimization problem without reducing much of the accuracy. Step 1. Remove the candidate keyphrase INLINEFORM0 from original graph INLINEFORM1 , if it is not connected to any concept. The intuition behind this heuristic is straightforward. Since our objective function is constructed over concepts, if a candidate keyphrase INLINEFORM0 doesn't contain any concept, adding it to INLINEFORM1 doesn't bring any improvement to the objective function, so INLINEFORM2 is", "keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams. Result. For comparing with our system, we reimplemented SingleRank and Topical PageRank. Table shows the result of our reimplementation of SingleRank and Topical PageRank, as well as the result of our system. Note that we predict the same number of phrase ( INLINEFORM0 ) for each document while testing all three methods. The result shows our result has guaranteed improvement over SingleRank and Topical PageRank on all four corpora. Conclusion and Future Work. We proposed an unsupervised graph-based keyphrase extraction method WikiRank. This method connects the text with concepts in Wikipedia, thus incorporate the background information into the", "concept. Combining these two logic, a candidate satisfying the constrains of Step 3 is not likely to be picked in the best keyphrase set INLINEFORM8 , so we can prune it before the optimalization process. Corpora. The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 . The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 . The NUS Keyphrase Corpus BIBREF10 includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al.", "focus on unsupervised methods. Traditional methods of unsupervised keyphrase extraction mostly focus on getting information of document from word frequency and document structure BIBREF0 , however, after years of attempting, the performance seems very hard to be improved any more. Based on this observation, it is reasonable to suspect that the document itself possibly cannot provide enough information for keyphrase extraction task. To get good coverage of the main topics of the document, Topical PageRank BIBREF1 started to adopt topical information in automatic keyphrase extraction. The main idea of Topical PageRank is to extract the top topics of the document using LDA, then sum over the scores of a candidate phrase under each topic to be the final score. The main problems with Topical PageRank are: First, The topics are too general. Second, since they are using LDA, they only classify the words to several topics, but don't know what the topics exactly are. However, the topical", "Work. We proposed an unsupervised graph-based keyphrase extraction method WikiRank. This method connects the text with concepts in Wikipedia, thus incorporate the background information into the semantic graph and finally construct a set of keyphrase that has optimal coverage of the concepts of the document. Experiment results show the method outperforms two related keyphrase extraction methods. We suggest that future work could incorporate more other semantic approaches to investigate keyphrase extraction task. Introducing the results of dependency parsing or semantic parsing (e.g., OntoUSP) in intermediate steps could be helpful.", "as possible (2) as much as possible. Let INLINEFORM0 denote the weight of concept INLINEFORM1 . We compute INLINEFORM2 as the frequency INLINEFORM3 exists in the whole document INLINEFORM4 . To quantify how good the coverage of a keyphrase set INLINEFORM5 is, we compute the overall score of the concepts that INLINEFORM6 contains. Consider a subgraph of INLINEFORM0 , INLINEFORM1 , which captures all the concepts connected to INLINEFORM2 . In INLINEFORM3 , the set of vertices INLINEFORM4 is the union of the candidate keyphrase set INLINEFORM5 , and the set INLINEFORM6 of concepts that nodes in INLINEFORM7 connect to. The set of edges INLINEFORM8 of INLINEFORM9 is constructed with the edges connect nodes in INLINEFORM10 with nodes in INLINEFORM11 . We set up the score of a concept INLINEFORM0 in the subgraph INLINEFORM1 as following: DISPLAYFORM0  where INLINEFORM0 is the weight of INLINEFORM1 as we defined before, and INLINEFORM2 is the degree of INLINEFORM3 in the subgraph INLINEFORM4", "major topic of the document. Step 3. For a concept INLINEFORM0 connecting to more than INLINEFORM1 candidate keyphrases, remove any candidate keyphrase INLINEFORM2 which (1)Does not connect to any other concept. AND (2)The ranking is lower than INLINEFORM3 th among all candidate keyphrases connect to INLINEFORM4 .(In practice, INLINEFORM5 is usually 3 or 4.) According to equation EQREF10 , if there are already INLINEFORM0 instances of concept INLINEFORM1 in the INLINEFORM2 , adding the INLINEFORM3 th instance of INLINEFORM4 will only contribute INLINEFORM5 to INLINEFORM6 . At the same time, among all the candidate keyphrases connected to concept INLINEFORM7 , our optimization process always chooses the ones that connect to other concepts as well over the ones that do not connect to any other concept. Combining these two logic, a candidate satisfying the constrains of Step 3 is not likely to be picked in the best keyphrase set INLINEFORM8 , so we can prune it before the optimalization"]}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "predicted_answer": "ResNet, MobileNet, Faster R-CNN, Mask R-CNN, Simple Pose, textCNN and BERT are the state of the art models.", "predicted_evidence": ["concept. Combining these two logic, a candidate satisfying the constrains of Step 3 is not likely to be picked in the best keyphrase set INLINEFORM8 , so we can prune it before the optimalization process. Corpora. The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 . The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 . The NUS Keyphrase Corpus BIBREF10 includes 211 scientific conference papers with lengths between 4 to 12 pages. Each paper has one or more sets of keyphrases assigned by its authors and other annotators. The number of candidate keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al.", "which are likely to be erroneously produced; (3) Generate the best set of keyphrases as output. Graph Construction. This is one of the crucial steps in our paper that connects the plain text with human knowledge, facilitating the understanding of semantics. In this step, we adopt TAGME BIBREF2 to obtain the underlying concepts in documents. TAGME is a powerful topic annotator. It identifies meaningful sequences of words in a short text and link them to a pertinent Wikipedia page, as shown in Figure . These links add a new topical dimension to the text that enable us to relate, classify or cluster short texts. This step is to filter out unnecessary word tokens from the input document and generate a list of potential keywords using heuristics. As reported in BIBREF3 , most manually assigned keyphrases turn out to be noun groups. We follow BIBREF4 and select candidates lexical unit with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford", "keyphrases that can be extracted is potentially large, making this corpus the most challenging of the four. Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams. Result. For comparing with our system, we reimplemented SingleRank and Topical PageRank. Table shows the result of our reimplementation of SingleRank and Topical PageRank, as well as the result of our system. Note that we predict the same number of phrase ( INLINEFORM0 ) for each document while testing all three methods. The result shows our result has guaranteed improvement over SingleRank and Topical PageRank on all four corpora. Conclusion and Future Work. We proposed an unsupervised graph-based keyphrase extraction method WikiRank. This method connects the text with concepts in Wikipedia, thus incorporate the background information into the", "keyphrases turn out to be noun groups. We follow BIBREF4 and select candidates lexical unit with the following Penn Treebank tags: NN, NNS, NNP, NNPS, and JJ, which are obtained using the Stanford POS tagger BIBREF5 , and then extract the noun groups whose pattern is zero or more adjectives followed by one or more nouns. The pattern can be represented using regular expressions as follows INLINEFORM0  where JJ indicates adjectives and various forms of nouns are represented using NN, NNS and NNP . We build a semantic graph INLINEFORM0 in which the set of vertices INLINEFORM1 is the union of the concept set INLINEFORM2 and the candidate keyphrase set INLINEFORM3 \u2014i.e., INLINEFORM4 . In the graph, each unique concept INLINEFORM5 or candidate keyphrase INLINEFORM6 for document INLINEFORM7 corresponds to a node. The node corresponds to a concept INLINEFORM8 and the node corresponds to a candidate keyphrase INLINEFORM9 are connected by an edge INLINEFORM10 , if the candidate keyphrase", "this figure, the gold keyphrases are marked with bold, and the keyphrases extracted by the TextRank system are marked with parentheses. We are going to illustrate the errors exist in most of present keyphrase extraction systems using this example. Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that frequently appears in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word BIBREF0 . It is not easy to reject a non-keyphrase containing a word with a high term frequency: many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate. To be more concrete, consider the news article in Figure . The word Cattle has a significant presence in the document. Consequently, the system not only correctly predict British cattle as a keyphrase,", "our example, bovine spongiform encephalopathy and bse refer to the same concept. If a system predicts both of them as keyphrases, it commits a redundancy error. Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document. Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document. In the Mad cow disease example, the keyphrase extractor fails to identify export and scrapie as keyphrases, resulting in infrequency errors. Proposed Model. The WikiRank algorithm includes three steps: (1) Construct the semantic graph including concepts and candidate keyphrases; (2)(optional) Prune the graph with heuristic to filter out candidates which are likely to be erroneously produced; (3) Generate the best set of keyphrases as output. Graph Construction. This is one of the crucial steps in our paper that connects the plain text with", "be more concrete, consider the news article in Figure . The word Cattle has a significant presence in the document. Consequently, the system not only correctly predict British cattle as a keyphrase, but also erroneously predict cattle industry, cattle feed, and cattle brain as keyphrases, yielding overgeneration errors. Redundancy errors occur when a system correctly identifies a candidate as a keyphrase, but at the same time outputs a semantically equivalent candidate (e.g., its alias) as a keyphrase. This type of error can be attributed to the failure of a system to determine that two candidates are semantically equivalent. Nevertheless, some researchers may argue that a system should not be penalized for redundancy errors because the extracted candidates are in fact keyphrases. In our example, bovine spongiform encephalopathy and bse refer to the same concept. If a system predicts both of them as keyphrases, it commits a redundancy error. Infrequency errors occur when a system", "PageRank are: First, The topics are too general. Second, since they are using LDA, they only classify the words to several topics, but don't know what the topics exactly are. However, the topical information we need for keyphrase extraction should be precise. As shown in Figure , the difference between a correct keyphrase sheep disease and an incorrect keyphrase incurable disease could be small, which is hard to be captured by rough topical categorization approach. To overcome the limitations of aforementioned approaches, we propose WikiRank, an unsupervised automatic keyphrase extraction approach that links semantic meaning to text The key contribution of this paper could be summarized as follows: Existing Error Illustration with Example. Figure shows part of an example document. In this figure, the gold keyphrases are marked with bold, and the keyphrases extracted by the TextRank system are marked with parentheses. We are going to illustrate the errors exist in most of present"]}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "predicted_answer": "Unanswerable", "predicted_evidence": ["multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used to further improve the performance. Once we have multiple translation results, we additionally use our ART teacher model to evaluate each result and select the one that achieves the highest probability. As the evaluation is fully parallelizable (since it is identical to the parallel training of the ART model), this rescoring operation will not hurt the non-autoregressive property of the NART model. Experiments ::: Experimental Results. We compare our model with several baselines, including three ART models, the fertility based (FT) NART model BIBREF5, the deterministic iterative refinement based (IR) NART model BIBREF6, and the Latent Transformer BIBREF7 which is not fully non-autoregressive by incorporating an autoregressive sub-module in the NART model architecture. The results are shown in", "NART model BIBREF6, and the Latent Transformer BIBREF7 which is not fully non-autoregressive by incorporating an autoregressive sub-module in the NART model architecture. The results are shown in the Table TABREF15. Across different datasets, our method achieves significant improvements over previous non-autoregressive models. Specifically, our method outperforms fertility based NART model with 6.54/7.11 BLEU score improvements on WMT En-De and De-En tasks in similar settings and achieves comparable results with state-of-the-art LSTM-based model on WMT En-De task. Furthermore, our model achieves a speedup of 30.2 (output a single sentence) or 17.8 (teacher rescoring) times over the ART counterparts. Note that our speedups significantly outperform all previous works, because of our lighter design of the NART model: without any computationally expensive module trying to improve the expressiveness. We also visualize the hidden state cosine similarities and attention distributions for the", "NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models. Acknowledgment. This work is supported by National Key R&D Program of China (2018YFB1402600), NSFC (61573026) and BJNSF (L172037) and a grant from Microsoft Research Asia. We would like to thank the anonymous reviewers for their valuable comments on our paper.", "overhead of new components will hurt the inference speed, contradicting with the goal of the NART models: to parallelize and speed up neural machine translation models. To tackle this, we proposed a novel hint-based method for NART model training. We first investigate the causes of the poor performance of the NART model. Comparing with the ART model, we find that: (1) the positions where the NART model outputs incoherent tokens will have very high hidden states similarity; (2) the attention distributions of the NART model are more ambiguous than those of ART model. Therefore, we design two kinds of hints from the hidden states and attention distributions of the ART model to help the training of the NART model. The experimental results show that our model achieves significant improvement over the NART baseline models and is even comparable to a strong ART baseline in BIBREF4. Approach. In this section, we first describe the observations on the ART and NART models, and then discuss what", "the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4. In order to speed up the inference process, a line of works begin to develop non-autoregressive translation models. These models break the autoregressive dependency by decomposing the joint probability with The lost of autoregressive dependency largely hurt the consistency of the output sentences, increase the difficulty in the learning process and thus lead to a low quality translation. Previous works mainly focus on adding different components into the NART model to improve the expressiveness of the network structure to overcome the loss of autoregressive dependency BIBREF5, BIBREF6, BIBREF7. However, the computational overhead of new components will hurt the inference speed, contradicting with the goal of the NART models: to parallelize and speed up neural machine translation models. To tackle this, we proposed", "which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix. Finally, we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48). Conclusion. In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable", "Introduction. Neural machine translation has attracted much attention in recent years BIBREF0, BIBREF1, BIBREF2, BIBREF3. Given a sentence $x=(x_1, \\dots ,x_{T_x})$ from the source language, the straight-forward way for translation is to generate the target words $y=(y_1, \\dots , y_{T_y})$ one by one from left to right. This is also known as the AutoRegressive Translation (ART) models, in which the joint probability is decomposed into a chain of conditional probabilities: While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4. In order to speed", "repetitive words) and miss meaningful tokens on the source side, while these patterns do not commonly appear in ART models. After some empirical study, we find two non-obvious facts that lead to this phenomenon. First, we visualize the cosine similarities between decoder hidden states of a certain layer in both ART and NART models for sampled cases. Mathematically, for a set of hidden states $r_1, \\ldots , r_T$, the pairwise cosine similarity can be derived by $\\cos _{ij} = {\\left<r_i, r_j\\right>}/{(\\Vert r_i\\Vert \\cdot \\Vert r_j\\Vert )}.$ We then plot the heatmap of the resulting matrix $\\cos $. A typical example is shown in Figure FIGREF4, where the cosine similarities in the NART model are larger than those of the ART model, indicating that the hidden states across positions in the NART model are \u201csimilar\u201d. Positions with highly-correlated hidden states tend to generate the same word and make the NART model output repetitive tokens, e.g., the yellow area on the top-left of Figure"]}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "predicted_answer": "BLEU", "predicted_evidence": ["Introduction. Neural machine translation has attracted much attention in recent years BIBREF0, BIBREF1, BIBREF2, BIBREF3. Given a sentence $x=(x_1, \\dots ,x_{T_x})$ from the source language, the straight-forward way for translation is to generate the target words $y=(y_1, \\dots , y_{T_y})$ one by one from left to right. This is also known as the AutoRegressive Translation (ART) models, in which the joint probability is decomposed into a chain of conditional probabilities: While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4. In order to speed", "have similar word alignments to the teacher model, i.e. Our final training loss $\\mathcal {L}$ is a weighted sum of two parts stated above and the negative log-likelihood loss $\\mathcal {L}_\\mathit {nll}$ defined on bilingual sentence pair $(x, y)$, i.e. where $\\lambda $ and $\\mu $ are hyperparameters controlling the weight of different loss terms. Experiments ::: Experimental Settings. The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset. We pretrain Transformer BIBREF8 as the teacher model on each dataset, which achieves 33.26/27.30/31.29 in terms of BLEU BIBREF11 in IWSLT14 De-En, WMT14 En-De and De-En test sets. The student model shares the same number of layers in encoder/decoder, size of hidden states/embeddings and number of", "open-sourced tensor2tensor BIBREF14. More settings can be found in Appendix. Experiments ::: Inference. During training, $T_y$ does not need to be predicted as the target sentence is given. During testing, we have to predict the length of the target sentence for each source sentence. In many languages, the length of the target sentence can be roughly estimated from the length of the source sentence. We choose a simple method to avoid the computational overhead, which uses input length to determine target sentence length: $T_y = T_x + C$, where $C$ is a constant bias determined by the average length differences between the source and target training sentences. We can also predict the target length ranging from $[(T_x+C)-B, (T_x+C)+B]$, where $B$ is the halfwidth. By doing this, we can obtain multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used", "model are \u201csimilar\u201d. Positions with highly-correlated hidden states tend to generate the same word and make the NART model output repetitive tokens, e.g., the yellow area on the top-left of Figure FIGREF4(b), while this does not happen in the ART model (Figure FIGREF4(a)). According to our statistics, 70% of the cosine similarities between hidden states in the ART model are less than 0.25, and 95% are less than 0.5. Second, we visualize the encoder-decoder attentions for sampled cases, shown in Figure FIGREF6. Good attentions between the source and target sentences are usually considered to lead to accurate translation while poor ones may cause wrong output tokens BIBREF0. In Figure FIGREF6(b), the attentions of the ART model almost covers all source tokens, while the attentions of the NART model do not cover \u201cfarm\u201d but with two \u201cmorning\u201d. This directly makes the translation result worse in the NART model. These phenomena inspire us to use the intermediate hidden information in the", "multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used to further improve the performance. Once we have multiple translation results, we additionally use our ART teacher model to evaluate each result and select the one that achieves the highest probability. As the evaluation is fully parallelizable (since it is identical to the parallel training of the ART model), this rescoring operation will not hurt the non-autoregressive property of the NART model. Experiments ::: Experimental Results. We compare our model with several baselines, including three ART models, the fertility based (FT) NART model BIBREF5, the deterministic iterative refinement based (IR) NART model BIBREF6, and the Latent Transformer BIBREF7 which is not fully non-autoregressive by incorporating an autoregressive sub-module in the NART model architecture. The results are shown in", "which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix. Finally, we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48). Conclusion. In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable", "the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4. In order to speed up the inference process, a line of works begin to develop non-autoregressive translation models. These models break the autoregressive dependency by decomposing the joint probability with The lost of autoregressive dependency largely hurt the consistency of the output sentences, increase the difficulty in the learning process and thus lead to a low quality translation. Previous works mainly focus on adding different components into the NART model to improve the expressiveness of the network structure to overcome the loss of autoregressive dependency BIBREF5, BIBREF6, BIBREF7. However, the computational overhead of new components will hurt the inference speed, contradicting with the goal of the NART models: to parallelize and speed up neural machine translation models. To tackle this, we proposed", "design of the NART model: without any computationally expensive module trying to improve the expressiveness. We also visualize the hidden state cosine similarities and attention distributions for the NART model with hint-based training, as shown in Figure FIGREF4(c) and FIGREF6(c). With hints from hidden states, the hidden states similarities of the NART model decrease in general, and especially for the positions where the original NART model outputs incoherent phrases. The attention distribution of the NART model after hint-based training is more similar to the ART teacher model and less ambiguous comparing to the NART model without hints. According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix. Finally, we conduct an"]}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "predicted_answer": "Unanswerable.", "predicted_evidence": ["have similar word alignments to the teacher model, i.e. Our final training loss $\\mathcal {L}$ is a weighted sum of two parts stated above and the negative log-likelihood loss $\\mathcal {L}_\\mathit {nll}$ defined on bilingual sentence pair $(x, y)$, i.e. where $\\lambda $ and $\\mu $ are hyperparameters controlling the weight of different loss terms. Experiments ::: Experimental Settings. The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset. We pretrain Transformer BIBREF8 as the teacher model on each dataset, which achieves 33.26/27.30/31.29 in terms of BLEU BIBREF11 in IWSLT14 De-En, WMT14 En-De and De-En test sets. The student model shares the same number of layers in encoder/decoder, size of hidden states/embeddings and number of", "Introduction. Neural machine translation has attracted much attention in recent years BIBREF0, BIBREF1, BIBREF2, BIBREF3. Given a sentence $x=(x_1, \\dots ,x_{T_x})$ from the source language, the straight-forward way for translation is to generate the target words $y=(y_1, \\dots , y_{T_y})$ one by one from left to right. This is also known as the AutoRegressive Translation (ART) models, in which the joint probability is decomposed into a chain of conditional probabilities: While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4. In order to speed", "open-sourced tensor2tensor BIBREF14. More settings can be found in Appendix. Experiments ::: Inference. During training, $T_y$ does not need to be predicted as the target sentence is given. During testing, we have to predict the length of the target sentence for each source sentence. In many languages, the length of the target sentence can be roughly estimated from the length of the source sentence. We choose a simple method to avoid the computational overhead, which uses input length to determine target sentence length: $T_y = T_x + C$, where $C$ is a constant bias determined by the average length differences between the source and target training sentences. We can also predict the target length ranging from $[(T_x+C)-B, (T_x+C)+B]$, where $B$ is the halfwidth. By doing this, we can obtain multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used", "which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix. Finally, we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48). Conclusion. In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable", "model are \u201csimilar\u201d. Positions with highly-correlated hidden states tend to generate the same word and make the NART model output repetitive tokens, e.g., the yellow area on the top-left of Figure FIGREF4(b), while this does not happen in the ART model (Figure FIGREF4(a)). According to our statistics, 70% of the cosine similarities between hidden states in the ART model are less than 0.25, and 95% are less than 0.5. Second, we visualize the encoder-decoder attentions for sampled cases, shown in Figure FIGREF6. Good attentions between the source and target sentences are usually considered to lead to accurate translation while poor ones may cause wrong output tokens BIBREF0. In Figure FIGREF6(b), the attentions of the ART model almost covers all source tokens, while the attentions of the NART model do not cover \u201cfarm\u201d but with two \u201cmorning\u201d. This directly makes the translation result worse in the NART model. These phenomena inspire us to use the intermediate hidden information in the", "design of the NART model: without any computationally expensive module trying to improve the expressiveness. We also visualize the hidden state cosine similarities and attention distributions for the NART model with hint-based training, as shown in Figure FIGREF4(c) and FIGREF6(c). With hints from hidden states, the hidden states similarities of the NART model decrease in general, and especially for the positions where the original NART model outputs incoherent phrases. The attention distribution of the NART model after hint-based training is more similar to the ART teacher model and less ambiguous comparing to the NART model without hints. According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix. Finally, we conduct an", "the NART baseline models and is even comparable to a strong ART baseline in BIBREF4. Approach. In this section, we first describe the observations on the ART and NART models, and then discuss what kinds of information can be used as hints to help the training of the NART model. We follow the network structure in BIBREF8, use a copy of the source sentence as decoder input, remove the attention masks in decoder self-attention layers and add a positional attention layer as suggested in BIBREF5. We provide a visualization of ART and NART models we used in Figure FIGREF11 and a detailed description of the model structure in Appendix. Approach ::: Observation: Illed States and Attentions. According to the case study in BIBREF5, the translations of the NART models contain incoherent phrases (e.g. repetitive words) and miss meaningful tokens on the source side, while these patterns do not commonly appear in ART models. After some empirical study, we find two non-obvious facts that lead to", "since we only want to penalize hidden states that are highly similar in the NART model, but not similar in the ART model. We have tested several choices of $-\\log (1-d_\\mathit {st})$, e.g., $\\exp (d_\\mathit {st})$, from which we find similar experimental results. Approach ::: Hints from the ART teacher Model ::: Hints from word alignments. We observe that meaningful words in the source sentence are sometimes untranslated by the NART model, and the corresponding positions often suffer from ambiguous attention distributions. Therefore, we use the word alignment information from the ART model to help the training of the NART model. In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model, i.e. Our final training loss $\\mathcal {L}$ is a weighted sum of two parts stated above and the negative log-likelihood loss $\\mathcal {L}_\\mathit"]}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "predicted_answer": "average, maximum and minimum, and standard deviation", "predicted_evidence": ["American ones \u2014 evolved in a more similar manner that the Spanish variety. It is also logical that even more language variety similarities are shared across neighbour countries, e.g. Chilean compared with Peruvian and Argentinian. In Figure 3 we show the precision and recall values for the identification of each variety. As can be seen, Spain and Chile have the highest recall so that texts written in these varieties may have less probability to be misclassified as other varieties. Nevertheless, the highest precisions are obtained for Mexico and Peru, implying that texts written in such varieties may be easier to discriminate. Most discriminating features. In Table 8 we show the most discriminant features. The features are sorted by their information gain (IG). As can be seen, the highest gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain. We experimented with different sets of", "gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain. We experimented with different sets of features and show the results in Figure 4 . As may be expected, average-based features obtain high accuracies (67.0%). However, although features based on standard deviation have not the highest information gain, they obtained the highest results individually (69.2%), as well as their combination with average ones (70,8%). Features based on minimum and maximum obtain low results (48.3% and 54.7% respectively), but in combination they obtain a significant increase (61.1%). The combination of the previous features obtains almost the highest accuracy (71.0%), equivalent to the accuracy obtained with probability and proportionality features (71.1%). Cost analysis. We analyse the cost from two perspectives: i) the complexity to the features; and ii) the number of features needed to represent a", "important in social media. Despite the vastness and accessibility of the Internet destroyed frontiers among regions or traits, companies are still very interested in author profiling segmentation. For example, when a new product is launched to the market, knowing the geographical distribution of opinions may help to improve marketing campaigns. Or given a security threat, knowing the possible cultural idiosyncrasies of the author may help to better understand who could have written the message. Language variety identification is a popular research topic of natural language processing. In the last years, several tasks and workshops have been organized: the Workshop on Language Technology for Closely Related Languages and Language Variants @ EMNLP 2014; the VarDial Workshop @ COLING 2014 - Applying NLP Tools to Similar Languages, Varieties and Dialects; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect @ RANLP BIBREF0 BIBREF1", "NLP Tools to Similar Languages, Varieties and Dialects; and the LT4VarDial - Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialect @ RANLP BIBREF0 BIBREF1 . We can find also several works focused on the task. In BIBREF2 the authors addressed the problem of identifying Arabic varieties in blogs and social fora. They used character $n$ -gram features to discriminate between six different varieties and obtained accuracies between 70%-80%. Similarly, BIBREF3 collected 1,000 news articles of two varieties of Portuguese. They applied different features such as word and character $n$ -grams and reported accuracies over 90%. With respect to the Spanish language, BIBREF4 focused on varieties from Argentina, Chile, Colombia, Mexico and Spain in Twitter. They used meta-learning and combined four types of features: i) character $n$ -gram frequency profiles, ii) character $n$ -gram language models, iii) Lempel-Ziv-Welch compression and iv) syllable-based", "approaches that employed the popular continuous Skip-gram model. The dimensionality reduction obtained by means of LDR is from thousands to only 6 features per language variety. This allows to deal with large collections in big data environments such as social media. Recently, we have applied LDR to the age and gender identification task obtaining competitive results with the best performing teams in the author profiling task at the PAN Lab at CLEF. As a future work, we plan to apply LDR to other author profiling tasks such as personality recognition.", "used meta-learning and combined four types of features: i) character $n$ -gram frequency profiles, ii) character $n$ -gram language models, iii) Lempel-Ziv-Welch compression and iv) syllable-based language models. They obtained an interesting 60%-70% accuracy of classification. We are interested in discovering which kind of features capture higher differences among varieties. Our hypothesis is that language varieties differ mainly in lexicographic clues. We show an example in Table 1 .  In this work we focus on the Spanish language variety identification. We differentiate from the previous works as follows: i) instead of $n$ -gram based representations, we propose a low dimensionality representation that is helpful when dealing with big data in social media; ii) in order to reduce the possible over-fitting, our training and test partitions do not share any author of instance between them; and iii) in contrast to the Twitter dataset of BIBREF4 , we will make available our dataset to", "O(l\\cdot {n})$ . Since the number of terms in the vocabulary will always be equal or greater than the number of coincident terms ( $n \\ge m$ ), and as the number of terms in the document will always be much higher than the number of language varieties ( $l<<n$ ), we can determine the cost as lineal with respect to the number of terms in the document $O(n)$ . With respect to the number of features needed to represent a document, we showed in Table 6 the considerable reduction of the proposed low dimensionality representation. Robustness. In order to analyse the robustness of the low dimensionality representation to different languages, we experimented with the development set of the DSLCC corpus from the Discriminating between Similar Languages task BIBREF1 . The corpus consists of 2,000 sentences per language or variety, with between 20 and 100 tokens per sentence, obtained from news headers. In Table 9 we show the results obtained with the proposed representation and the two", "Classifier. As can be appreciated, the proposed low dimensionality representation improves more than 35% the results obtained with the state-of-the-art representations. BOW obtains slightly better results than character 4-grams, and both of them improve significantly the ones obtained with tf-idf 2-grams. Instead of selecting the most frequent $n$ -grams, our approach takes advantage from the whole vocabulary and assigns higher weights to the most discriminative words for the different language varieties as shown in Equation 10 .  We highlight that our LDR obtains competitive results compared with the use of distributed representations. Concretely, there is no significant difference among them (Skip-gram $z_{0.05} = 0,5457 < 1,960$ and SenVec $z_{0.05} = 0,7095 < 1,960$ ). In addition, our proposal reduces considerably the dimensionality of one order of magnitude as shown in Table 6 . Error analysis. We aim at analysing the error of LDR to better understand which varieties are the"]}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "predicted_answer": "Unanswerable.", "predicted_evidence": ["over-fitting, our training and test partitions do not share any author of instance between them; and iii) in contrast to the Twitter dataset of BIBREF4 , we will make available our dataset to the research community. Low Dimensionality Representation. The key aspect of the low dimensionality representation (LDR) is the use of weights to represent the probability of each term to belong to each one of the different language varieties. We assume that the distribution of weights for a given document should be closer to the weights of its corresponding language variety. Formally, the LDR is estimated as follows: Evaluation Framework. In this section, we describe the corpus and the alternative representations that we employ in this work. HispaBlogs Corpus. We have created the HispaBlogs dataset by collecting posts from Spanish blogs from five different countries: Argentina, Chile, Mexico, Peru and Spain. For each country, there are 450 and 200 blogs respectively for training and test,", "by collecting posts from Spanish blogs from five different countries: Argentina, Chile, Mexico, Peru and Spain. For each country, there are 450 and 200 blogs respectively for training and test, ensuring that each author appears only in one set. Each blog contains at least 10 posts. The total number of blogs is 2,250 and 1,000 respectively. Statistics of the number of words are shown in Table 3 . Alternative representations. We are interested in investigating the impact of the proposed representation and compare its performance with state-of-the-art representations based on $n$ -grams and with two approaches based on the recent and popular distributed representations of words by means of the continuous Skip-gram model BIBREF6 . State-of-the-art representations are mainly based on $n$ -grams models, hence we tested character and word based ones, besides word with tf-idf weights. For each of them, we iterated $n$ from 1 to 10 and selected 1,000, 5,000 and 10,000 most frequent grams. The", "gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain. We experimented with different sets of features and show the results in Figure 4 . As may be expected, average-based features obtain high accuracies (67.0%). However, although features based on standard deviation have not the highest information gain, they obtained the highest results individually (69.2%), as well as their combination with average ones (70,8%). Features based on minimum and maximum obtain low results (48.3% and 54.7% respectively), but in combination they obtain a significant increase (61.1%). The combination of the previous features obtains almost the highest accuracy (71.0%), equivalent to the accuracy obtained with probability and proportionality features (71.1%). Cost analysis. We analyse the cost from two perspectives: i) the complexity to the features; and ii) the number of features needed to represent a", "2,000 sentences per language or variety, with between 20 and 100 tokens per sentence, obtained from news headers. In Table 9 we show the results obtained with the proposed representation and the two distributed representations, Skip-gram and SenVec. It is important to notice that, in general, when a particular representation improves for one language is at cost of the other one. We can conclude that the three representations obtained comparative results and support the robustness of the low dimensionality representation. Conclusions. In this work, we proposed the LDR low dimensionality representation for language variety identification. Experimental results outperformed traditional state-of-the-art representations and obtained competitive results compared with two distributed representation-based approaches that employed the popular continuous Skip-gram model. The dimensionality reduction obtained by means of LDR is from thousands to only 6 features per language variety. This allows", "Classifier. As can be appreciated, the proposed low dimensionality representation improves more than 35% the results obtained with the state-of-the-art representations. BOW obtains slightly better results than character 4-grams, and both of them improve significantly the ones obtained with tf-idf 2-grams. Instead of selecting the most frequent $n$ -grams, our approach takes advantage from the whole vocabulary and assigns higher weights to the most discriminative words for the different language varieties as shown in Equation 10 .  We highlight that our LDR obtains competitive results compared with the use of distributed representations. Concretely, there is no significant difference among them (Skip-gram $z_{0.05} = 0,5457 < 1,960$ and SenVec $z_{0.05} = 0,7095 < 1,960$ ). In addition, our proposal reduces considerably the dimensionality of one order of magnitude as shown in Table 6 . Error analysis. We aim at analysing the error of LDR to better understand which varieties are the", "representations, the error analysis that provides useful insights to better understand differences among languages, a depth analysis on the contribution of the different features and a cost analysis that highlights the suitability of LDR for a big data scenario. Machine learning algorithms comparison. We tested several machine learning algorithms with the aim at selecting the one that best solves the task. As can be seen in Table 4 , Multiclass Classifier obtains the best result (results in the rest of the paper refer to Multiclass Classifier). We carried out a statistical test of significance with respect to the next two systems with the highest performance: SVM ( $z_{0.05} 0, 880 < 1, 960$ ) and LogitBoost ( $z_{0.05} = 1, 983 > 1, 960$ ). Preprocessing impact. The proposed representation aims at using the whole vocabulary to obtain the weights of its terms. Social media texts may have noise and inadequately written words. Moreover, some of these words may be used only by few", "models, hence we tested character and word based ones, besides word with tf-idf weights. For each of them, we iterated $n$ from 1 to 10 and selected 1,000, 5,000 and 10,000 most frequent grams. The best results were obtained with the 10,000 most frequent BOW, character 4-grams and tf-idf 2-grams. Therefore, we will use them in the evaluation. Due to the increasing popularity of the distributed representations BIBREF7 , we used the continuous Skip-gram model to generate distributed representations of words (e.g. $n$ -dimensional vectors), with further refinements in order to use them with documents. The continuous Skip-gram model BIBREF8 , BIBREF9 is an iterative algorithm which attempts to maximize the classification of the context surrounding a word. Formally, given a word $w(t)$ , and its surrounding words $w(t-c),~w(t-c+1),...,~w(t+c)$ inside a window of size $2c+1$ , the training objective is to maximize the average of the log probability shown in Equation 23 :  $$\\frac{1}{T}", "approaches that employed the popular continuous Skip-gram model. The dimensionality reduction obtained by means of LDR is from thousands to only 6 features per language variety. This allows to deal with large collections in big data environments such as social media. Recently, we have applied LDR to the age and gender identification task obtaining competitive results with the best performing teams in the author profiling task at the PAN Lab at CLEF. As a future work, we plan to apply LDR to other author profiling tasks such as personality recognition."]}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "predicted_answer": "Yes", "predicted_evidence": ["Introduction. BioASQ is a semantic indexing, question answering (QA) and information extraction challenge BIBREF0 . We participated in Task B of the challenge which is concerned with biomedical QA. More specifically, our system participated in Task B, Phase B: Given a question and gold-standard snippets (i.e., pieces of text that contain the answer(s) to the question), the system is asked to return a list of answer candidates. The fifth BioASQ challenge is taking place at the time of writing. Five batches of 100 questions each were released every two weeks. Participating systems have 24 hours to submit their results. At the time of writing, all batches had been released. The questions are categorized into different question types: factoid, list, summary and yes/no. Our work concentrates on answering factoid and list questions. For factoid questions, the system's responses are interpreted as a ranked list of answer candidates. They are evaluated using mean-reciprocal rank (MRR). For", "Note that this approach is not perfect as it can produce false positives (e.g., the answer is mentioned in a sentence which does not answer the question) and false negatives (e.g., a sentence answers the question, but the exact string used is not in the synonym list). Because BioASQ usually contains multiple snippets for a given question, we process all snippets independently and then aggregate the answer spans, sorting globally according to their probability $p_{span}^{i, j}$ . During the inference phase, we retrieve the top 20 answers span via beam search with beam size 20. From this sorted list of answer strings, we remove all duplicate strings. For factoid questions, we output the top five answer strings as our ranked list of answer candidates. For list questions, we use a probability cutoff threshold $t$ , such that $\\lbrace (i, j)|p_{span}^{i, j} \\ge t\\rbrace $ is the set of answers. We set $t$ to be the threshold for which the list F1 score on the development set is optimized.", "however, the relative performance varies significantly. We expect our system to perform better on factoid questions than list questions, because our pre-training dataset (SQuAD) does not contain any list questions. Starting with batch 3, we also submitted responses to yes/no questions by always answering yes. Because of a very skewed class distribution in the BioASQ dataset, this is a strong baseline. Because this is done merely to have baseline performance for this question type and because of the naivety of the method, we do not list or discuss the results here. Conclusion. In this paper, we summarized the system design of our BioASQ 5B submission for factoid and list questions. We use a neural architecture which is trained end-to-end on the QA task. This approach has not been applied to BioASQ questions in previous challenges. Our results show that our approach achieves state-of-the art results on factoid questions and competitive results on list questions.", "questions. Training & decoding. We define our loss as the cross-entropy of the correct start and end indices. In the case of multiple occurrences of the same answer, we only minimize the span of the lowest loss. We train the network in two steps: First, the network is trained on SQuAD, following the procedure by weissenborn2017fastqa (pre-training phase). Second, we fine-tune the network parameters on BioASQ (fine-tuning phase). For both phases, we use the Adam optimizer BIBREF6 with an exponentially decaying learning rate. We start with learning rates of $10^{-3}$ and $10^{-4}$ for the pre-training and fine-tuning phases, respectively. During fine-tuning, we extract answer spans from the BioASQ training data by looking for occurrences of the gold standard answer in the provided snippets. Note that this approach is not perfect as it can produce false positives (e.g., the answer is mentioned in a sentence which does not answer the question) and false negatives (e.g., a sentence answers", "of resources and feature engineering that is specific to the biomedical domain. For example, OAQA BIBREF1 , which has been very successful in last year's challenge, uses a biomedical parser, entity tagger and a thesaurus to retrieve synonyms. Our system, on the other hand, is based on a neural network QA architecture that is trained end-to-end on the target task. We build upon FastQA BIBREF2 , an extractive factoid QA system which achieves state-of-the-art results on QA benchmarks that provide large amounts of training data. For example, SQuAD BIBREF3 provides a dataset of $\\approx 100,000$ questions on Wikipedia articles. Our approach is to train FastQA (with some extensions) on the SQuAD dataset and then fine-tune the model parameters on the BioASQ training set. Note that by using an extractive QA network as our central component, we restrict our system's responses to substrings in the provided snippets. This also implies that the network will not be able to answer yes/no questions.", "test batches of BioASQ 5 (Task 5b, Phase B) in Table 1 . Note that the performance numbers are not final, as the provided synonyms in the gold-standard answers will be updated as a manual step, in order to reflect valid responses by the participating systems. This has not been done by the time of writing. Note also that \u2013 in contrast to previous BioASQ challenges \u2013 systems are no longer allowed to provide an own list of synonyms in this year's challenge. In general, the single and ensemble system are performing very similar relative to the rest of field: Their ranks are almost always right next to each other. Between the two, the ensemble model performed slightly better on average. On factoid questions, our system has been very successful, winning three out of five batches. On list questions, however, the relative performance varies significantly. We expect our system to perform better on factoid questions than list questions, because our pre-training dataset (SQuAD) does not contain", "on answering factoid and list questions. For factoid questions, the system's responses are interpreted as a ranked list of answer candidates. They are evaluated using mean-reciprocal rank (MRR). For list questions, the system's responses are interpreted as a set of answers to the list question. Precision and recall are computed by comparing the given answers to the gold-standard answers. F1 score, i.e., the harmonic mean of precision and recall, is used as the official evaluation measure . Most existing biomedical QA systems employ a traditional QA pipeline, similar in structure to the baseline system by weissenborn2013answering. They consist of several discrete steps, e.g., named-entity recognition, question classification, and candidate answer scoring. These systems require a large amount of resources and feature engineering that is specific to the biomedical domain. For example, OAQA BIBREF1 , which has been very successful in last year's challenge, uses a biomedical parser, entity", "cutoff threshold $t$ , such that $\\lbrace (i, j)|p_{span}^{i, j} \\ge t\\rbrace $ is the set of answers. We set $t$ to be the threshold for which the list F1 score on the development set is optimized. In order to further tweak the performance of our systems, we built a model ensemble. For this, we trained five single models using 5-fold cross-validation on the entire training set. These models are combined by averaging their start and end scores before computing the span probabilities (Equations 8 - 10 ). As a result, we submit two systems to the challenge: The best single model (according to its development set) and the model ensemble. We implemented our system using TensorFlow BIBREF7 . It was trained on an NVidia GForce Titan X GPU. Results & discussion. We report the results for all five test batches of BioASQ 5 (Task 5b, Phase B) in Table 1 . Note that the performance numbers are not final, as the provided synonyms in the gold-standard answers will be updated as a manual step, in"]}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "predicted_answer": "unanswerable", "predicted_evidence": ["questions. Training & decoding. We define our loss as the cross-entropy of the correct start and end indices. In the case of multiple occurrences of the same answer, we only minimize the span of the lowest loss. We train the network in two steps: First, the network is trained on SQuAD, following the procedure by weissenborn2017fastqa (pre-training phase). Second, we fine-tune the network parameters on BioASQ (fine-tuning phase). For both phases, we use the Adam optimizer BIBREF6 with an exponentially decaying learning rate. We start with learning rates of $10^{-3}$ and $10^{-4}$ for the pre-training and fine-tuning phases, respectively. During fine-tuning, we extract answer spans from the BioASQ training data by looking for occurrences of the gold standard answer in the provided snippets. Note that this approach is not perfect as it can produce false positives (e.g., the answer is mentioned in a sentence which does not answer the question) and false negatives (e.g., a sentence answers", "test batches of BioASQ 5 (Task 5b, Phase B) in Table 1 . Note that the performance numbers are not final, as the provided synonyms in the gold-standard answers will be updated as a manual step, in order to reflect valid responses by the participating systems. This has not been done by the time of writing. Note also that \u2013 in contrast to previous BioASQ challenges \u2013 systems are no longer allowed to provide an own list of synonyms in this year's challenge. In general, the single and ensemble system are performing very similar relative to the rest of field: Their ranks are almost always right next to each other. Between the two, the ensemble model performed slightly better on average. On factoid questions, our system has been very successful, winning three out of five batches. On list questions, however, the relative performance varies significantly. We expect our system to perform better on factoid questions than list questions, because our pre-training dataset (SQuAD) does not contain", "however, the relative performance varies significantly. We expect our system to perform better on factoid questions than list questions, because our pre-training dataset (SQuAD) does not contain any list questions. Starting with batch 3, we also submitted responses to yes/no questions by always answering yes. Because of a very skewed class distribution in the BioASQ dataset, this is a strong baseline. Because this is done merely to have baseline performance for this question type and because of the naivety of the method, we do not list or discuss the results here. Conclusion. In this paper, we summarized the system design of our BioASQ 5B submission for factoid and list questions. We use a neural architecture which is trained end-to-end on the QA task. This approach has not been applied to BioASQ questions in previous challenges. Our results show that our approach achieves state-of-the art results on factoid questions and competitive results on list questions.", "of resources and feature engineering that is specific to the biomedical domain. For example, OAQA BIBREF1 , which has been very successful in last year's challenge, uses a biomedical parser, entity tagger and a thesaurus to retrieve synonyms. Our system, on the other hand, is based on a neural network QA architecture that is trained end-to-end on the target task. We build upon FastQA BIBREF2 , an extractive factoid QA system which achieves state-of-the-art results on QA benchmarks that provide large amounts of training data. For example, SQuAD BIBREF3 provides a dataset of $\\approx 100,000$ questions on Wikipedia articles. Our approach is to train FastQA (with some extensions) on the SQuAD dataset and then fine-tune the model parameters on the BioASQ training set. Note that by using an extractive QA network as our central component, we restrict our system's responses to substrings in the provided snippets. This also implies that the network will not be able to answer yes/no questions.", "cutoff threshold $t$ , such that $\\lbrace (i, j)|p_{span}^{i, j} \\ge t\\rbrace $ is the set of answers. We set $t$ to be the threshold for which the list F1 score on the development set is optimized. In order to further tweak the performance of our systems, we built a model ensemble. For this, we trained five single models using 5-fold cross-validation on the entire training set. These models are combined by averaging their start and end scores before computing the span probabilities (Equations 8 - 10 ). As a result, we submit two systems to the challenge: The best single model (according to its development set) and the model ensemble. We implemented our system using TensorFlow BIBREF7 . It was trained on an NVidia GForce Titan X GPU. Results & discussion. We report the results for all five test batches of BioASQ 5 (Task 5b, Phase B) in Table 1 . Note that the performance numbers are not final, as the provided synonyms in the gold-standard answers will be updated as a manual step, in", "on answering factoid and list questions. For factoid questions, the system's responses are interpreted as a ranked list of answer candidates. They are evaluated using mean-reciprocal rank (MRR). For list questions, the system's responses are interpreted as a set of answers to the list question. Precision and recall are computed by comparing the given answers to the gold-standard answers. F1 score, i.e., the harmonic mean of precision and recall, is used as the official evaluation measure . Most existing biomedical QA systems employ a traditional QA pipeline, similar in structure to the baseline system by weissenborn2013answering. They consist of several discrete steps, e.g., named-entity recognition, question classification, and candidate answer scoring. These systems require a large amount of resources and feature engineering that is specific to the biomedical domain. For example, OAQA BIBREF1 , which has been very successful in last year's challenge, uses a biomedical parser, entity", "Introduction. BioASQ is a semantic indexing, question answering (QA) and information extraction challenge BIBREF0 . We participated in Task B of the challenge which is concerned with biomedical QA. More specifically, our system participated in Task B, Phase B: Given a question and gold-standard snippets (i.e., pieces of text that contain the answer(s) to the question), the system is asked to return a list of answer candidates. The fifth BioASQ challenge is taking place at the time of writing. Five batches of 100 questions each were released every two weeks. Participating systems have 24 hours to submit their results. At the time of writing, all batches had been released. The questions are categorized into different question types: factoid, list, summary and yes/no. Our work concentrates on answering factoid and list questions. For factoid questions, the system's responses are interpreted as a ranked list of answer candidates. They are evaluated using mean-reciprocal rank (MRR). For", "QA network as our central component, we restrict our system's responses to substrings in the provided snippets. This also implies that the network will not be able to answer yes/no questions. We do, however, generalize the FastQA output layer in order to be able to answer list questions in addition to factoid questions. Model. Our system is a neural network which takes as input a question and a context (i.e., the snippets) and outputs start and end pointers to tokens in the context. At its core, we use FastQA BIBREF2 , a state-of-the-art neural QA system. In the following, we describe our changes to the architecture and how the network is trained. Network architecture. In the input layer, the context and question tokens are mapped to high-dimensional word vectors. Our word vectors consists of three components, which are concatenated to form a single vector: GloVe embedding: We use 300-dimensional GloVe embeddings BIBREF4 which have been trained on a large collection of web documents."]}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "predicted_answer": "Neural Tensor Network.", "predicted_evidence": ["BIBREF13 (BIBREF13). The backward LSTM component follows the same recurrent state transition process as the forward LSTM component. Starting from an initial state $\\overleftarrow{\\mathbf {h}}^{n+1}$, which is a model parameter, it reads the input $\\mathbf {x}_n,\\mathbf {x}_{n-1},\\dots ,\\mathbf {x}_0$, changing its value to $\\overleftarrow{\\mathbf {h}}^n,\\overleftarrow{\\mathbf {h}}^{n-1},\\dots ,\\overleftarrow{\\mathbf {h}}^0$, respectively. The BiLSTM model uses the concatenated value of $\\overrightarrow{\\mathbf {h}}^t$ and $\\overleftarrow{\\mathbf {h}}^t$ as the hidden vector for $w_t$: A single hidden vector representation $\\mathbf {v}_i$ of the input intent can be obtained by concatenating the last hidden states of the two LSTMs: In the training process, we calculate the similarity between a given event vector $\\mathbf {v}_e$ and its related intent vector $\\mathbf {v}_i$. For effectively training the model, we devise a ranking type loss function as follows: where $\\mathbf {v}^{\\prime", "in standard neural tensor network, we make low-rank approximation that represents each matrix by two low-rank matrices plus diagonal, as illustrated in Figure FIGREF7. Formally, the parameter of the $i$-th slice is $T_{appr}^{[i]}=T^{[i_1]}\\times T^{[i_2]}+diag(t^{[i]})$, where $T^{[i_1]}\\in \\mathbb {R}^{d\\times n}$, $T^{[i_2]}\\in \\mathbb {R}^{n\\times d}$, $t^{[i]}\\in \\mathbb {R}^d$, $n$ is a hyper-parameter, which is used for adjusting the degree of tensor decomposition. The output of neural tensor layer is formalized as follows. where $[T_{appr}]_1^{[1:k]}$ is the low-rank tensor that defines multiple low-rank bilinear layers. $k$ is the slice number of neural tensor network which is also equal to the output length of $S_1$. We assume that event tuples in the training data should be scored higher than corrupted tuples, in which one of the event arguments is replaced with a random argument. Formally, the corrupted event tuple is $E^r=(A^r, P, O)$, which is derived by replacing each", "prediction. Experiments ::: Baselines. We compare the performance of our approach against a variety of event embedding models developed in recent years. These models can be categorized into three groups: Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings BIBREF8. Compositional Neural Network (Comp. NN) The event representation in this model is computed by feeding the concatenation of the subject, predicate, and object embedding into a two layer neural network BIBREF17, BIBREF3, BIBREF2. Element-wise Multiplicative Composition (EM Comp.) This method simply concatenates the element-wise multiplications between the verb and its subject/object. Neural Tensor Network This line of work use tensors to learn the interactions between the predicate and its subject/object BIBREF4, BIBREF5. According to the different usage of tensors, we have three baseline methods: Role Factor Tensor BIBREF5 which represents the", "Introduction. Events are a kind of important objective information of the world. Structuralizing and representing such information as machine-readable knowledge are crucial to artificial intelligence BIBREF0, BIBREF1. The main idea is to learn distributed representations for structured events (i.e. event embeddings) from text, and use them as the basis to induce textual features for downstream applications, such as script event prediction and stock market prediction. Parameterized additive models are among the most widely used for learning distributed event representations in prior work BIBREF2, BIBREF3, which passes the concatenation or addition of event arguments' word embeddings to a parameterized function. The function maps the summed vectors into an event embedding space. Furthermore, BIBREF4 ding2015deep and BIBREF5 weber2018event propose using neural tensor networks to perform semantic composition of event arguments, which can better capture the interactions between event", "embeddings and feed them to a neural network to generate an event embedding. Event embeddings are further concatenated and fed through another neural network to predict the coherence between the events. Modi modi2016event encodes a set of events in a similar way and use that to incrementally predict the next event \u2013 first the argument, then the predicate and then next argument. BIBREF25 pichotta2016learning treat event prediction as a sequence to sequence problem and use RNN based models conditioned on event sequences in order to predict the next event. These three works all model narrative chains, that is, event sequences in which a single entity (the protagonist) participates in every event. BIBREF26 hu2017happens also apply an RNN approach, applying a new hierarchical LSTM model in order to predict events by generating descriptive word sequences. This line of work combines the words in these phrases by the passing the concatenation or addition of their word embeddings to a", "BIBREF12. We use BiLSTM model to learn intent representations. BiLSTM consists of two LSTM components, which process the input in the forward left-to-right and the backward right-to-left directions, respectively. In each direction, the reading of input words is modelled as a recurrent process with a single hidden state. Given an initial value, the state changes its value recurrently, each time consuming an incoming word. Take the forward LSTM component for example. Denoting the initial state as $\\overrightarrow{\\mathbf {h}}^0$, which is a model parameter, it reads the input word representations $\\mathbf {x}_0,\\mathbf {x}_1,\\dots ,\\mathbf {x}_n$, and the recurrent state transition step for calculating $\\overrightarrow{\\mathbf {h}}^1,\\dots ,\\overrightarrow{\\mathbf {h}}^{n+1}$ is defined as BIBREF13 (BIBREF13). The backward LSTM component follows the same recurrent state transition process as the forward LSTM component. Starting from an initial state $\\overleftarrow{\\mathbf {h}}^{n+1}$,", "Who \u201cbroke the record\u201d is likely to be happy, while, who \u201cbroke a vase\u201d may be sad. Hence, intent and sentiment can be used to learn more fine-grained semantic features for event embeddings. Such commonsense knowledge is not explicitly expressed but can be found in a knowledge base such as Event2Mind BIBREF6 and ATOMIC BIBREF7. Thus, we aim to incorporate the external commonsense knowledge, i.e., intent and sentiment, into the learning process to generate better event representations. Specifically, we propose a simple and effective model to jointly embed events, intents and emotions into the same vector space. A neural tensor network is used to learn baseline event embeddings, and we define a corresponding loss function to incorporate intent and sentiment information. Extensive experiments show that incorporating external commonsense knowledge brings promising improvements to event embeddings, achieving 78% and 200% improvements on hard similarity small and big dataset, respectively.", "outperformed parameterized additive models (Comp. NN and EM Comp.), which shows that tensor is capable of learning the semantic composition of event arguments. (3) Our commonsense knowledge enhanced event representation learning approach outperformed all baseline methods across all datasets (achieving 78% and 200% improvements on hard similarity small and big dataset, respectively, compared to previous SOTA method), which indicates that commonsense knowledge is useful for distinguishing distinct events. Experiments ::: Event Similarity Evaluation ::: Case Study. To further analyse the effects of intents and emotions on the event representation learning, we present case studies in Table TABREF29, which directly shows the changes of similarity scores before and after incorporating intent and sentiment. For example, the original similarity score of two events \u201cchef cooked pasta\u201d and \u201cchef cooked books\u201d is very high (0.89) as they have high lexical overlap. However, their intents differ"]}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "predicted_answer": "Multiple choice narrative cloze (MCNC) dataset", "predicted_evidence": ["for numerous applications. Script event prediction BIBREF20 is a challenging event-based commonsense reasoning task, which is defined as giving an existing event context, one needs to choose the most reasonable subsequent event from a candidate list. Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings. BIBREF22 (BIBREF22) and BIBREF21 (BIBREF21) showed that script event prediction is a challenging problem, and even 1% of accuracy improvement is very difficult. Experimental results shown in Table TABREF31 demonstrate that we can achieve more than 1.5% improvements in single model comparison and more than 1.4% improvements in multi-model integration comparison, just by replacing the input embeddings,", "sentiment. For example, the original similarity score of two events \u201cchef cooked pasta\u201d and \u201cchef cooked books\u201d is very high (0.89) as they have high lexical overlap. However, their intents differ greatly. The intent of \u201cchef cooked pasta\u201d is \u201cto hope his customer enjoying the delicious food\u201d, while the intent of \u201cchef cooked books\u201d is \u201cto falsify their financial statements\u201d. Enhanced with the intents, the similarity score of the above two events dramatically drops to 0.45. For another example, as the event pair \u201cman clears test\u201d and \u201che passed exam\u201d share the same sentiment polarity, their similarity score is boosted from -0.08 to 0.40. Experiments ::: Script Event Prediction. Event is a kind of important real-world knowledge. Learning effective event representations can be benefit for numerous applications. Script event prediction BIBREF20 is a challenging event-based commonsense reasoning task, which is defined as giving an existing event context, one needs to choose the most", "in order to predict events by generating descriptive word sequences. This line of work combines the words in these phrases by the passing the concatenation or addition of their word embeddings to a parameterized function that maps the summed vector into event embedding space. The additive nature of these models makes it difficult to model subtle differences in an event\u2019s surface form. To address this issue, BIBREF4 ding2015deep, and BIBREF5 weber2018event propose tensor-based composition models, which combine the subject, predicate and object to produce the final event representation. The models capture multiplicative interactions between these elements and are thus able to make large shifts in event semantics with only small changes to the arguments. However, previous work mainly focuses on the nature of the event and lose sight of external commonsense knowledge, such as the intent and sentiment of event participants. This paper proposes to encode intent and sentiment into event", "embeddings and feed them to a neural network to generate an event embedding. Event embeddings are further concatenated and fed through another neural network to predict the coherence between the events. Modi modi2016event encodes a set of events in a similar way and use that to incrementally predict the next event \u2013 first the argument, then the predicate and then next argument. BIBREF25 pichotta2016learning treat event prediction as a sequence to sequence problem and use RNN based models conditioned on event sequences in order to predict the next event. These three works all model narrative chains, that is, event sequences in which a single entity (the protagonist) participates in every event. BIBREF26 hu2017happens also apply an RNN approach, applying a new hierarchical LSTM model in order to predict events by generating descriptive word sequences. This line of work combines the words in these phrases by the passing the concatenation or addition of their word embeddings to a", "of the event above can be (Actor = Microsoft, Action = sues, Object = Barnes & Noble). They report improvements on stock market prediction using their structured representation instead of words as features. One disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. BIBREF4 ding2015deep propose to address this issue by representing structured events using event embeddings, which are dense vectors. The goal of event representation learning is that similar events should be embedded close to each other in the same vector space, and distinct events should be farther from each other. Previous work investigated compositional models for event embeddings. BIBREF2 granroth2016happens concatenate predicate and argument embeddings and feed them to a neural network to generate an event embedding. Event embeddings are further concatenated and fed through another neural network to predict the coherence between the", "between the predicate and its subject/object BIBREF4, BIBREF5. According to the different usage of tensors, we have three baseline methods: Role Factor Tensor BIBREF5 which represents the predicate as a tensor, Predicate Tensor BIBREF5 which uses two tensors learning the interactions between the predicate and its subject, and the predicate and its object, respectively, NTN BIBREF4, which we used as the baseline event embedding model in this paper, and KGEB BIBREF18, which incorporates knowledge graph information in NTN. Experiments ::: Event Similarity Evaluation ::: Hard Similarity Task. We first follow BIBREF5 (BIBREF5) evaluating our proposed approach on the hard similarity task. The goal of this task is that similar events should be close to each other in the same vector space, while dissimilar events should be far away with each other. To this end, BIBREF5 (BIBREF5) created two types of event pairs, one with events that should be close to each other but have very little lexical", "demonstrate that we can achieve more than 1.5% improvements in single model comparison and more than 1.4% improvements in multi-model integration comparison, just by replacing the input embeddings, which confirms that better event understanding can lead to better inference results. An interesting result is that the event embeddings only incorporated with intents achieved the best result against other baselines. This confirms that capturing people's intents is helpful to infer their next plan. In addition, we notice that the event embeddings only incorporated with sentiment also achieve better performance than SGNN. This is mainly because the emotional consistency does also contribute to predicate the subsequent event. Experiments ::: Stock Market Prediction. It has been shown that news events influence the trends of stock price movements BIBREF23. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that", "Introduction. Events are a kind of important objective information of the world. Structuralizing and representing such information as machine-readable knowledge are crucial to artificial intelligence BIBREF0, BIBREF1. The main idea is to learn distributed representations for structured events (i.e. event embeddings) from text, and use them as the basis to induce textual features for downstream applications, such as script event prediction and stock market prediction. Parameterized additive models are among the most widely used for learning distributed event representations in prior work BIBREF2, BIBREF3, which passes the concatenation or addition of event arguments' word embeddings to a parameterized function. The function maps the summed vectors into an event embedding space. Furthermore, BIBREF4 ding2015deep and BIBREF5 weber2018event propose using neural tensor networks to perform semantic composition of event arguments, which can better capture the interactions between event"]}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "predicted_answer": "ATOMIC, New York Times Gigaword Corpus", "predicted_evidence": ["a large-scale labeled dataset, which annotated the event and its actor's intents. Recently, BIBREF6 P18-1043 and BIBREF7 sap2018atomic released such valuable commonsense knowledge dataset (ATOMIC), which consists of 25,000 event phrases covering a diverse range of daily-life events and situations. For example, given an event \u201cPersonX drinks coffee in the morning\u201d, the dataset labels PersonX's likely intent is \u201cPersonX wants to stay awake\u201d. We notice that the intents labeled in ATOMIC is a sentence. Hence, intent embedding is actually a sentence representation learning task. Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM) BIBREF10 have been a dominant method, giving state-of-the-art results in language modelling BIBREF11 and syntactic parsing BIBREF12. We use BiLSTM model to learn intent representations. BiLSTM consists of two LSTM components, which process the input in the forward left-to-right and the backward right-to-left directions,", "dissimilar events should be far away with each other. To this end, BIBREF5 (BIBREF5) created two types of event pairs, one with events that should be close to each other but have very little lexical overlap (e.g., police catch robber / authorities apprehend suspect), and another with events that should be farther apart but have high overlap (e.g., police catch robber / police catch disease). The labeled dataset contains 230 event pairs (115 pairs each of similar and dissimilar types). Three different annotators were asked to give the similarity/dissimilarity rankings, of which only those the annotators agreed upon completely were kept. For each event representation learning method, we obtain the cosine similarity score of the pairs, and report the fraction of cases where the similar pair receives a higher cosine value than the dissimilar pair (we use Accuracy $\\in [0,1]$ denoting it). To evaluate the robustness of our approach, we extend this dataset to 1,000 event pairs (similar and", "outperformed parameterized additive models (Comp. NN and EM Comp.), which shows that tensor is capable of learning the semantic composition of event arguments. (3) Our commonsense knowledge enhanced event representation learning approach outperformed all baseline methods across all datasets (achieving 78% and 200% improvements on hard similarity small and big dataset, respectively, compared to previous SOTA method), which indicates that commonsense knowledge is useful for distinguishing distinct events. Experiments ::: Event Similarity Evaluation ::: Case Study. To further analyse the effects of intents and emotions on the event representation learning, we present case studies in Table TABREF29, which directly shows the changes of similarity scores before and after incorporating intent and sentiment. For example, the original similarity score of two events \u201cchef cooked pasta\u201d and \u201cchef cooked books\u201d is very high (0.89) as they have high lexical overlap. However, their intents differ", "receives a higher cosine value than the dissimilar pair (we use Accuracy $\\in [0,1]$ denoting it). To evaluate the robustness of our approach, we extend this dataset to 1,000 event pairs (similar and dissimilar events each account for 50%), and we will release this dataset to the public. Experiments ::: Event Similarity Evaluation ::: Transitive Sentence Similarity. Except for the hard similarity task, we also evaluate our approach on the transitive sentence similarity dataset BIBREF19, which contains 108 pairs of transitive sentences: short phrases containing a single subject, object and verb (e.g., agent sell property). It also has another dataset which consists of 200 sentence pairs. In this dataset, the sentences to be compared are constructed using the same subject and object and semantically correlated verbs, such as `spell\u2019 and `write\u2019; for example, `pupils write letters\u2019 is compared with `pupils spell letters\u2019. As this dataset is not suitable for our task, we only evaluate our", "Evaluation ::: Results. Experimental results of hard similarity and transitive sentence similarity are shown in Table TABREF23. We find that: (1) Simple averaging achieved competitive performance in the task of transitive sentence similarity, while performed very badly in the task of hard similarity. This is mainly because hard similarity dataset is specially created for evaluating the event pairs that should be close to each other but have little lexical overlap and that should be farther apart but have high lexical overlap. Obviously, on such dataset, simply averaging word vectors which is incapable of capturing the semantic interactions between event arguments, cannot achieve a sound performance. (2) Tensor-based compositional methods (NTN, KGEB, Role Factor Tensor and Predicate Tensor) outperformed parameterized additive models (Comp. NN and EM Comp.), which shows that tensor is capable of learning the semantic composition of event arguments. (3) Our commonsense knowledge enhanced", "of the loss functions on events, intents and sentiment: where $\\alpha , \\beta , \\gamma \\in [0,1]$ are model parameters to weight the three loss functions. We use the New York Times Gigaword Corpus (LDC2007T07) for pre-training event embeddings. Event triples are extracted based on the Open Information Extraction technology BIBREF15. We initialize the word embedding layer with 100 dimensional pre-trained GloVe vectors BIBREF8, and fine-tune initialized word vectors during our model training. We use Adagrad BIBREF16 for optimizing the parameters with initial learning rate 0.001 and batch size 128. Experiments. We compare the performance of intent and sentiment powered event embedding model with state-of-the-art baselines on three tasks: event similarity, script event prediction and stock prediction. Experiments ::: Baselines. We compare the performance of our approach against a variety of event embedding models developed in recent years. These models can be categorized into three", "of the event above can be (Actor = Microsoft, Action = sues, Object = Barnes & Noble). They report improvements on stock market prediction using their structured representation instead of words as features. One disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. BIBREF4 ding2015deep propose to address this issue by representing structured events using event embeddings, which are dense vectors. The goal of event representation learning is that similar events should be embedded close to each other in the same vector space, and distinct events should be farther from each other. Previous work investigated compositional models for event embeddings. BIBREF2 granroth2016happens concatenate predicate and argument embeddings and feed them to a neural network to generate an event embedding. Event embeddings are further concatenated and fed through another neural network to predict the coherence between the", "prediction. Experiments ::: Baselines. We compare the performance of our approach against a variety of event embedding models developed in recent years. These models can be categorized into three groups: Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings BIBREF8. Compositional Neural Network (Comp. NN) The event representation in this model is computed by feeding the concatenation of the subject, predicate, and object embedding into a two layer neural network BIBREF17, BIBREF3, BIBREF2. Element-wise Multiplicative Composition (EM Comp.) This method simply concatenates the element-wise multiplications between the verb and its subject/object. Neural Tensor Network This line of work use tensors to learn the interactions between the predicate and its subject/object BIBREF4, BIBREF5. According to the different usage of tensors, we have three baseline methods: Role Factor Tensor BIBREF5 which represents the"]}
{"question_id": "d38745a3910c380e6df97c7056a5dd9643fd365b", "predicted_answer": "Unanswerable", "predicted_evidence": ["morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes. We show some model analyses against the gold standard in Table 2 . Capturing semantic similarity. Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments", "form \u201cA is to B as C is to X\u201d, we find the word $w$ which satisfies  $$w = \\operatornamewithlimits{argmax}_{w \\in V - \\lbrace a, b, c\\rbrace } \\cos (w, b - a + c)$$   (Eq. 28)  where $a,\\, b,\\, c$ are the word vectors for the words A, B and C respectively. We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models. Discussion. We only report results for English. However, English is a morphologically", "where we demonstrate that character-based models do worse at the semantic similarity task than word-level models. Discussion. We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes. This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than", "We test the splitting capabilities of our model in \u00a7 \"Morphological awareness\" . Experiments. We evaluate our model on three tasks: morphological analysis (\u00a7 \"Morphological awareness\" ), semantic similarity (\u00a7 \"Capturing semantic similarity\" ), and analogy retrieval (\u00a7 \"Capturing syntactic and semantic regularity\" ). We trained all of the models once, and then use the same trained model for all three tasks \u2013 we do not perform hyperparameter tuning to optimize performance on each task. We trained our Char2Vec model on the Text8 corpus, consisting of the first 100MB of a 2006 cleaned-up dump of Wikipedia. We only trained on words which appeared more than 5 times in our corpus. We used a context window size of 3 words either side of the target word, and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer BIBREF23 . All experiments were carried out using Keras BIBREF24 and", "sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer BIBREF23 . All experiments were carried out using Keras BIBREF24 and Theano BIBREF25 , BIBREF26 . We initialized the context lookup table using word2vec, and kept it fixed during training. In all character-level models, the character embeddings have dimension $d_C = 64$ , while the forward and backward LSTMs have dimension $d_{LSTM} = 256$ . The concatenation of both therefore has dimensionality $d = 512$ . The concatenated LSTM hidden states are then compressed down to $d_{word} = 256$ by a feed-forward layer. As baselines, we trained a SGNS model on the same dataset with the same parameters. To test how much the attention model helps the character-level model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters. In this model, the word embeddings are just the concatenation of the final forward and", "of the form \u201cA is to B as C is to X\u201d. We split this collection into semantic and syntactic sections, based on whether the analogies between the words are driven by morphological changes or deeper semantic shifts. Example semantic questions are on capital-country relationships (\u201cParis is to France as Berlin is to X) and currency-country relationships. Example syntactic questions are adjective-adverb relationships (\u201camazing is to amazingly as apparent is to X\u201d) and opposites formed by prefixing a negation particle (\u201cacceptable is to unacceptable as aware is to X\u201d). This results in 5537 semantic analogies and 10411 syntactic analogies. We use the method of Mikolov et al. word2vec1 to answer these questions. We first $\\ell _2$ -normalize all of our word vectors. Then, to answer a question of the form \u201cA is to B as C is to X\u201d, we find the word $w$ which satisfies  $$w = \\operatornamewithlimits{argmax}_{w \\in V - \\lbrace a, b, c\\rbrace } \\cos (w, b - a + c)$$   (Eq. 28)  where $a,\\, b,\\, c$", "syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German. Conclusion. In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space. We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features. Finally, we show that character-level models, while outperformed by word-level models generally at the task of", "to linear shifts, demonstrating that our model can learn morphological features. Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better."]}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "predicted_answer": "Yes", "predicted_evidence": ["where we demonstrate that character-based models do worse at the semantic similarity task than word-level models. Discussion. We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes. This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than", "unit of language, they are in fact themselves compositional. The smallest unit of semantics is the morpheme, while the smallest unit of orthography is the grapheme, or character. Both have been used as a method to go beyond word-level models. Morphemic analysis and semantics. As word semantics is compositional, one might ask whether it is possible to learn morpheme representations, and compose them to obtain good word representations. Lazaridou et al. lazaridou demonstrated precisely this: one can derive good representations of morphemes distributionally, and apply tools from compositional distributional semantics to obtain good word representations. Luong et al. luong also trained a morphological composition model based on recursive neural networks. Botha and Blunsom Botha2014 built a language model incorporating morphemes, and demonstrated improvements in language modelling and in machine translation. All of these approaches incorporated external morphological knowledge, either in", "syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German. Conclusion. In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space. We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features. Finally, we show that character-level models, while outperformed by word-level models generally at the task of", "model incorporating morphemes, and demonstrated improvements in language modelling and in machine translation. All of these approaches incorporated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX BIBREF12 or an external morphological analyzer such as Morfessor BIBREF13 . Unsupervised morphology induction aims to decide whether two words are morphologically related or to generate a morphological analysis for a word BIBREF14 , BIBREF15 . While they may use semantic insights to perform the morphological analysis BIBREF16 , they typically are not concerned with obtaining a semantic representation for morphemes, nor of the resulting word. Character-level models. Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS", "morphological analyses. We obtained morphological analyses for all the words in our training vocabulary which were in the English Lexicon Project BIBREF27 . We then converted these into surface-level segmentations using heuristic affix-matching, and used this as a gold-standard morphemic analysis. We ended up with 14682 words, of which 7867 have at least two morphemes and 1138 have at least three. Evaluating morphological segmentation is a long-debated issue BIBREF28 . Traditional hard morphological analyzers are normally evaluated on border $F_1$ \u2013 that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above", "the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps this process. We also present some word nearest neighbours for our Char2Vec model in Table 5 , both on the whole vocabulary and then filtering the nearest neighbours to only include words which appear 100 times or more in our corpus. This corresponds to keeping the top 10k words, which is common among language models BIBREF8 , BIBREF9 . We note that nearest neighbour predictions include words that are orthographically distant but semantically similar, showing that our model has the capability to learn to compose characters into word meanings. We also note that word nearest neighbours seem to be more semantically coherent when rarely-observed words are filtered out of the vocabulary, and more based on", "morphological knowledge directly into a character-level model, one can improve the ability of character-level models to learn compositional word semantics. In addition, we hypothesize that incorporating morphological knowledge helps structure the embedding space in such a way that affixation corresponds to a regular shift in the embedding space. We test both hypotheses directly in \u00a7 \"Capturing semantic similarity\" and \u00a7 \"Capturing syntactic and semantic regularity\" respectively. The starting point for our model is the skip-gram with negative sampling (SGNS) objective of Mikolov et al. word2vec2. For a vocabulary $V$ of size $|V|$ and embedding size $N$ , SGNS learns two embedding tables $W, C \\in \\mathbb {R}^{N \\times |V|}$ , the target and context vectors. Every time a word $w$ is seen in the corpus with a context word $c$ , the tables are updated to maximize  $$\\log \\sigma (w \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-w \\cdot \\tilde{c}_i)]$$", "network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS tagging have been promising, with reductions in word perplexity for language modelling and state-of-the-art English POS tagging accuracy BIBREF8 , BIBREF9 . Ballesteros et al. ballesteros train a character-level model for parsing. Zhang et al. zhang do away with words completely, and train a convolutional neural network to do text classification directly from characters. Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one"]}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "predicted_answer": "No specific language is mentioned in the documents. Unanswerable.", "predicted_evidence": ["previous work on the morpheme level has all used external morphological analyzers. These require a separate pre-processing step, and cannot be adapted to suit the problem at hand. Another is to operate on the smallest orthographic unit, the character BIBREF8 , BIBREF9 . However, the link between shape and meaning is often complicated BIBREF10 , as alphabetic characters carry no inherent semantic meaning. To account for this, the model has to learn complicated dependencies between strings of characters to accurately capture word meaning. We hypothesize that explicitly introducing morphology into character-level models can help them learn morphological features, and hence word meaning. In this paper, we introduce a word embedding model that jointly learns word morphology and word embeddings. To the best of our knowledge, this is the first word embedding model that learns morphology as part of the model. Our guiding intuition is that the words with the same stem have similar contexts.", "To the best of our knowledge, this is the first word embedding model that learns morphology as part of the model. Our guiding intuition is that the words with the same stem have similar contexts. Thus, when considering word segments in terms of context-predictive power, the segment corresponding to the stem will have the most weight. Our model `reads' the word and outputs a sequence of word segments. We weight each segment, and then combine the segments to obtain the final word representation. These representations are trained to predict context words, as this has been shown to give word representations which capture word semantics well BIBREF11 . As the root morpheme has the most context-predictive power, we expect our model to assign high weight to this segment, thereby learning to separate root+affix structures. One exciting feature of character-level models is their ability to represent open-vocabulary words. After training, they can predict a vector for any word, not just words", "syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German. Conclusion. In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space. We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features. Finally, we show that character-level models, while outperformed by word-level models generally at the task of", "Introduction. Word embedding models associate each word in a corpus with a vector in a semantic space. These vectors can either be learnt to optimize performance in a downstream task BIBREF0 , BIBREF1 or learnt via the distributional hypothesis: words with similar contexts have similar meanings BIBREF2 , BIBREF3 . Current word embedding models treat words as atomic. However, words follow a power law distribution BIBREF4 , and word embedding models suffer from the problem of sparsity: a word like `unbelievableness' does not appear at all in the first 17 million words of Wikipedia, even though it is derived from common morphemes. This leads to three problems: One approach to smooth word distributions is to operate on the smallest meaningful semantic unit, the morpheme BIBREF6 , BIBREF7 . However, previous work on the morpheme level has all used external morphological analyzers. These require a separate pre-processing step, and cannot be adapted to suit the problem at hand. Another is to", "model incorporating morphemes, and demonstrated improvements in language modelling and in machine translation. All of these approaches incorporated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX BIBREF12 or an external morphological analyzer such as Morfessor BIBREF13 . Unsupervised morphology induction aims to decide whether two words are morphologically related or to generate a morphological analysis for a word BIBREF14 , BIBREF15 . While they may use semantic insights to perform the morphological analysis BIBREF16 , they typically are not concerned with obtaining a semantic representation for morphemes, nor of the resulting word. Character-level models. Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS", "morphological analyses. We obtained morphological analyses for all the words in our training vocabulary which were in the English Lexicon Project BIBREF27 . We then converted these into surface-level segmentations using heuristic affix-matching, and used this as a gold-standard morphemic analysis. We ended up with 14682 words, of which 7867 have at least two morphemes and 1138 have at least three. Evaluating morphological segmentation is a long-debated issue BIBREF28 . Traditional hard morphological analyzers are normally evaluated on border $F_1$ \u2013 that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above", "Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks. What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators. The Char2Vec model. We hypothesize that by incorporating morphological knowledge directly into a character-level model, one can improve the ability of character-level models to learn compositional word semantics. In addition, we hypothesize that", "compose characters into word meanings. We also note that word nearest neighbours seem to be more semantically coherent when rarely-observed words are filtered out of the vocabulary, and more based on orthographic overlap when the entire vocabulary is included. This suggests that for rarely-observed words, the model is basing its predictions on orthographic analysis, whereas for more commonly observed words it can `memorize' the mapping between the orthography and word semantics. Capturing syntactic and semantic regularity. Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space. To do this, we use the Google analogy dataset BIBREF3 . This consists of 19544 questions of the form \u201cA is to B as C is to X\u201d. We split this collection into semantic and syntactic sections, based on whether the analogies between the words are driven by morphological changes or deeper"]}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "predicted_answer": "Cosine similarity ranking", "predicted_evidence": ["morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes. We show some model analyses against the gold standard in Table 2 . Capturing semantic similarity. Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments", "results for in-corpus word pairs in Table 3 , and for all word pairs for those models able to predict vectors for unseen words in Table 4 . Overall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen", "scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 . We use the WordSim353 dataset BIBREF29 , the test split of the MEN dataset BIBREF30 , and the Rare Word (RW) dataset BIBREF31 . The word pairs in the WordSim353 and MEN datasets are typically simple, commonly occurring words denoting basic concepts, whereas the RW dataset contains many morphologically derived words which have low corpus frequencies. This is reflected by how many of the test pairs in each dataset contain out of vocabulary (OOV) items: 3/353 and 6/1000 of the word pairs in WordSim353 and MEN, compared with 1083/2034 for the RW dataset. We report results for in-corpus word pairs in Table 3 , and for all word pairs for those models able to predict vectors for unseen words in Table 4 . Overall, word-based embedding models learn vectors that", "Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks. What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators. The Char2Vec model. We hypothesize that by incorporating morphological knowledge directly into a character-level model, one can improve the ability of character-level models to learn compositional word semantics. In addition, we hypothesize that", "morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above non-morpheme boundaries. We use mean average precision (MAP) as our evaluation metric. We first calculate precision at $N$ for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over $N$ to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model. We report results of a random baseline as a point of comparison, which randomly places morpheme boundaries inside the word. We also report the results of the Porter stemmer, where we place a morpheme boundary at the end of the stem, then randomly thereafter. Finally, we trained Morfessor 2.0 BIBREF13 on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs", "sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer BIBREF23 . All experiments were carried out using Keras BIBREF24 and Theano BIBREF25 , BIBREF26 . We initialized the context lookup table using word2vec, and kept it fixed during training. In all character-level models, the character embeddings have dimension $d_C = 64$ , while the forward and backward LSTMs have dimension $d_{LSTM} = 256$ . The concatenation of both therefore has dimensionality $d = 512$ . The concatenated LSTM hidden states are then compressed down to $d_{word} = 256$ by a feed-forward layer. As baselines, we trained a SGNS model on the same dataset with the same parameters. To test how much the attention model helps the character-level model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters. In this model, the word embeddings are just the concatenation of the final forward and", "syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German. Conclusion. In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space. We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features. Finally, we show that character-level models, while outperformed by word-level models generally at the task of", "randomly thereafter. Finally, we trained Morfessor 2.0 BIBREF13 on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs is less than 0.1% of the total loss. We then used our trained Morfessor model to predict morpheme boundaries, and randomly permuted the morpheme boundaries and ranked them ahead of randomly permuted non-morpheme boundaries to calculate MAP. As the test set is dominated by words with simple morphology, we also extracted all the morphologically rich words with 3 or more morphemes, and created a separate evaluation on this subsection. We report the results in Table 1 . As the results show, our model performs the best out of all the methods at analysing morphologically rich words with multiple morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human"]}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "predicted_answer": "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500", "predicted_evidence": ["given a question, and usually different snippets are required by different questions. Hence, the same document should have different representations based on what the question is. To this end, attentions are incorporated into the hierarchical CNN to guide the learning of dynamic document representations which closely match the information requirements by questions. (iii) Document representations at sentence and snippet levels both are informative for the question, a highway network is developed to combine them, enabling our system to make a flexible tradeoff. Overall, we make three contributions. (i) We present a hierarchical attention-based CNN system \u201cHABCNN\u201d. It is, to our knowledge, the first deep learning based system for this MCTest task. (ii) Prior document modeling systems based on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our", "deep neural networks in QA tasks. HABCNN Variants. In addition to the main architectures described above, we also explore two variants of ABCHNN, inspired by BIBREF21 and BIBREF2 , respectively. Variant-I: As RNNs are widely recognized as a competitor of CNNs in sentence modeling, similar with BIBREF21 , we replace the sentence-CNN in Figure FIGREF3 by a GRU while keeping other parts unchanged. Variant-II: How to model attention at the granularity of words was shown in BIBREF2 ; see their paper for details. We develop their attention idea and model attention at the granularity of sentence and snippet. Our attention gives different weights to sentences/snippets (not words), then computes the document representation as a weighted average of all sentence/snippet representations. Results. Table TABREF16 lists the performance of baselines, HABCNN-TE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for top-performing HABCNN-TE).", "Table TABREF16 lists the performance of baselines, HABCNN-TE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for top-performing HABCNN-TE). Consistently, our HABCNN systems outperform all baselines, especially surpass the two competitive deep learning based systems AR and NR. The margin between our best-performing ABHCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. This demonstrates the promise of our architecture in this task. As said before, both AR and NR systems aim to generate answers in entity form. Their designs might not suit this machine comprehension task, in which the answers are openly-formed based on summarizing or abstracting the clues. To be more specific, AR models D always at word level, attentions are also paid to corresponding word representations, which is applicable for entity-style answers, but is less suitable for comprehension at sentence level or even snippet level. NR", "to answer. From its left part, we can see that \u201cGrandpa answered the door with a smile and welcomed Jimmy inside\u201d has the highest attention weight. This meets the intuition that this sentence has semantic overlap with the statement. And yet this sentence does not contain the answer. Look further the right part, in which the CNN layer over sentence-level representations is supposed to extract high-level features of snippets. In this level, the highest attention weight is cast to the best snippet \u201cFinally, Jimmy arrived...knocked. Grandpa answered the door...\u201d. And the neighboring snippets also get relatively higher attentions than other regions. Recall that our system chooses the one sentence with top attention at left part and choose top-3 snippets at right part (referring to INLINEFORM0 value in Table TABREF15 ) to form D representations at different granularity, then uses a highway network to combine both representations as an overall D representation. This visualization hints that", "on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other deep learning competitors by big margins. Related Work. Existing systems for MCTest task are mostly based on manually engineered features. Representative work includes BIBREF7 , BIBREF3 , BIBREF8 , BIBREF9 . In these works, a common route is first to define a regularized loss function based on assumed feature vectors, then the effort focuses on designing effective features based on various rules. Even though these researches are groundbreaking for this task, their flexibility and their capacity for generalization are limited. Deep learning based approaches appeal to increasing interest in analogous tasks. Weston et al., weston2014memory introduce memory networks for factoid QA. Memory network framework is extended in BIBREF1 , BIBREF10 for Facebook bAbI", "that this lies in the fundamental function of CNN and GRU. The CNN models a sentence without caring about the global word order information, and max-pooling is supposed to extract the features of key phrases in the sentence no matter where the phrases are located. This property should be useful for answer detection, as answers are usually formed by discovering some key phrases, not all words in a sentence should be considered. However, a GRU models a sentence by reading the words sequentially, the importance of phrases is less determined by the question requirement. The second variant, using a more complicated attention scheme to model biased D representations than simple cosine similarity based attention used in our model, is less effective to detect truly informative sentences or snippet. We doubt such kind of attention scheme when used in sentence sequences of large size. In training, the attention weights after softmax normalization have actually small difference across sentences,", "level, attentions are also paid to corresponding word representations, which is applicable for entity-style answers, but is less suitable for comprehension at sentence level or even snippet level. NR contrarily models D in sentence level always, neglecting the discovering of key phrases which however compose most of answers. In addition, the attention of AR system and the question-fact interaction in NR system both bring large numbers of parameters, this potentially constrains their power in a dataset of limited size. For Variant-I and Variant-II (second block of Table TABREF16 ), we can see that both modifications do harm to the original HABCNN-TE performance. The first variant, i.e, replacing the sentence-CNN in Figure FIGREF3 as GRU module is not helpful for this task. We suspect that this lies in the fundamental function of CNN and GRU. The CNN models a sentence without caring about the global word order information, and max-pooling is supposed to extract the features of key", "big limitation lies in \u201cwhy\u201d questions. This question type requires complex inference and long-distance dependencies. We observed that all deep lerning systems, including the two baselines, suffered somewhat from it. Conclusion. This work takes the lead in presenting a CNN based neural network system for open-domain machine comprehension task. Our systems tried to solve this task in a document projection way as well as a textual entailment way. The latter one demonstrates slightly better performance. Overall, our architecture, modeling dynamic document representation by attention scheme from sentence level to snippet level, shows promising results in this task. In the future, more fine-grained representation learning approaches are expected to model complex answer types and question types."]}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "predicted_answer": "Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model, manually engineered features based systems like BIBREF7 , BIBREF3 , BIBREF8 , BIBREF9 , and memory network frameworks like BIBREF1 , BIBREF10 for Facebook bAbI.", "predicted_evidence": ["on deep neural networks mostly generate generic representation, this work is the first to incorporate attention so that document representation is biased towards the question requirement. (iii) Our HABCNN systems outperform other deep learning competitors by big margins. Related Work. Existing systems for MCTest task are mostly based on manually engineered features. Representative work includes BIBREF7 , BIBREF3 , BIBREF8 , BIBREF9 . In these works, a common route is first to define a regularized loss function based on assumed feature vectors, then the effort focuses on designing effective features based on various rules. Even though these researches are groundbreaking for this task, their flexibility and their capacity for generalization are limited. Deep learning based approaches appeal to increasing interest in analogous tasks. Weston et al., weston2014memory introduce memory networks for factoid QA. Memory network framework is extended in BIBREF1 , BIBREF10 for Facebook bAbI", "Q and A directly, but they use Q and A to filter the document differently, extracting what is critical for the Q/A match by attention-pooling. Then they match the two document representations in the new space. For ease of exposition, we have used the symbol INLINEFORM0 so far, but in HABCNN-QP/QAP we compute two different document representations: INLINEFORM1 , for which attention is computed with respect to Q; and INLINEFORM2 for which attention is computed with respect to A. INLINEFORM3 also has two versions, one for Q: INLINEFORM4 , one for A: INLINEFORM5 . HABCNN-QP and HABCNN-QAP make different use of INLINEFORM0 . HABCNN-QP compares INLINEFORM1 with answer representation INLINEFORM2 . HABCNN-QAP compares INLINEFORM3 with INLINEFORM4 . HABCNN-QAP projects D twice, once based on attention from Q, once based on attention from A and compares the two projected representations, shown in Figure FIGREF2 (top). HABCNN-QP only utilizes the Q-based projection of D and then compares the", "accompanied with three negative answers. One is treating ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 ) as a training example, then our loss function can have three \u201cmax()\u201d terms, each for a positive-negative pair; the other one is treating ( INLINEFORM5 , INLINEFORM6 , INLINEFORM7 ) as an individual training example. In practice, we find the second way works better. We conjecture that the second way has more training examples, and positive answers are repeatedly used to balance the amounts of positive and negative answers. Multitask learning: Question typing is commonly used and proved to be very helpful in QA tasks BIBREF3 . Inspired, we stack a logistic regression layer over question representation INLINEFORM0 , with the purpose that this subtask can favor the parameter tuning of the whole system, and finally the question is better recognized and is able to find the answer more accurately. To be specific, we classify questions into 12 classes: \u201chow\u201d, \u201chow", "accuracy (proportion of questions correctly answered) and NDCG INLINEFORM0 BIBREF20 . Unlike accuracy which evaluates if the question is correctly answered or not, NDCG INLINEFORM1 , being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking. Baseline Systems. This work focuses on the comparison with systems about distributed representation learning and deep learning: Addition. Directly compare question and answers without considering the D. Sentence representations are computed by element-wise addition over word representations. Addition-proj. First compute sentence representations for Q, A and all D sentences as the same way as Addition, then match the two sentences in D which have highest similarity with Q and A respectively. NR. The Neural Reasoner BIBREF21 has an encoding layer, multiple reasoning layers and a final answer layer. The input for the encoding layer is a question and the sentences of the document (called facts); each", "This architecture we name HABCNN-QAP. (ii) We compute a representation of D based on Q attention (as before), but now we compare it directly with a representation of A. We name this architecture HABCNN-QP. (iii) We treat this QA task as textual entailment (TE), first reformatting Q-A pair into a statement (S), then matching S and D directly. This architecture we name HABCNN-TE. All three approaches are implemented in the common framework HABCNN. HABCNN. Recall that we use the abbreviations A (answer), Q (question), S (statement), D (document). HABCNN performs representation learning for triple (Q, A, D) in HABCNN-QP and HABCNN-QAP, for tuple (S, D) in HABCNN-TE. For convenience, we use \u201cquery\u201d to refer to Q, A, or S uniformly. HABCNN, depicted in Figure FIGREF3 , has the following phases. Input Layer. The input is (query,D). Query is two individual sentences (for Q, A) or one single sentence (for S), D is a sequence of sentences. Words are initialized by INLINEFORM0 -dimensional", "deep neural networks in QA tasks. HABCNN Variants. In addition to the main architectures described above, we also explore two variants of ABCHNN, inspired by BIBREF21 and BIBREF2 , respectively. Variant-I: As RNNs are widely recognized as a competitor of CNNs in sentence modeling, similar with BIBREF21 , we replace the sentence-CNN in Figure FIGREF3 by a GRU while keeping other parts unchanged. Variant-II: How to model attention at the granularity of words was shown in BIBREF2 ; see their paper for details. We develop their attention idea and model attention at the granularity of sentence and snippet. Our attention gives different weights to sentences/snippets (not words), then computes the document representation as a weighted average of all sentence/snippet representations. Results. Table TABREF16 lists the performance of baselines, HABCNN-TE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for top-performing HABCNN-TE).", "as an overall representation INLINEFORM2 of D: DISPLAYFORM0  where highway network weights INLINEFORM0 are learned by DISPLAYFORM0  where INLINEFORM0 . With the same highway network, we can generate the overall query representation, INLINEFORM1 in Figure FIGREF3 , by combining the two representations of the query at sentence and snippet levels. HABCNN-QP & HABCNN-QAP. HABCNN-QP/QAP computes the representation of D as a projection of D, either based on attention from Q or based on attention from A. We hope that these two projections of the document are close for a correct A and less close for an incorrect A. As we said in related work, machine comprehension can be viewed as an answer selection task using the document D as critical background information. Here, HABCNN-QP/QAP do not compare Q and A directly, but they use Q and A to filter the document differently, extracting what is critical for the Q/A match by attention-pooling. Then they match the two document representations in the", "other deep learning systems are developed for answer selection task BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 . Differently, this kind of question answering task does not involve document comprehension. They only try to match the question and answer candidate without any background information. Instead, we treat machine comprehension in this work as a question-answer matching problem under background guidance. Overall, for open-domain MCTest machine comprehension task, this work is the first to resort to deep neural networks. Model. We investigate this task by three approaches, illustrated in Figure FIGREF2 . (i) We can compute two different document (D) representations in a common space, one based on question (Q) attention, one based on answer (A) attention, and compare them. This architecture we name HABCNN-QAP. (ii) We compute a representation of D based on Q attention (as before), but now we compare it directly with a representation of A. We name this architecture"]}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_answer": "a one-stage RNN system, the baseline we use for comparison is a one-stage RNN system.", "predicted_evidence": ["(Table TABREF22 ), adding the nodes of BLSTM does not work, but adding another layer makes sense in short-duration task. Moreover, with the same layers and nodes, BLSTM outperforms BGRU in the two sub-tasks. We believe that sound related tasks do not need a very deep network as image related tasks, that is also the reason why we use a shallow ResNet14 as the CNN part. We evaluate the three-stage system with the same experiments, and the results (Table TABREF23 ) demonstrate that the three-stage system can achieve high accuracy in long duration task by larger BLSTM layers and the BGRU structure outperforms BLSTM on the whole. But adding the third RNN layer also does not work in these experiments. As Table TABREF24 shows, training networks in the first stage (with CTC loss) needs more time for convergence than training networks in the second or third stage (with cross-entropy loss). We can observe that the two-stage system spends less time while having a slightly higher accuracy", "setup. We convert the raw audio to 40-dimensional log Mel-filterbank coefficients with a frame-length of 25 ms, mean-normalized over the whole utterance. Then we stack all the log-mel filterbank features and feed it into the neural network, which is implemented in PyTorch. No voice activity detection (VAD) or other data augmentation approaches are applied. During the training process, we use Adam as the optimization method and set different learning rates and weight decay in different stages. We do not set dropout while training AM but set the dropout value=0.5 while training the LID network (the last stage). The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system. Comparison of different stage systems. First of all,", "can be divided to the CNN part and the RNN part, as described in Table TABREF7 . Given the input data of shape INLINEFORM0 , where INLINEFORM1 is the frame length of an utterance, we finally get 512-dimensional frame-level representation and INLINEFORM2 is the number of phonemes or dialect categories. Compared with other DNN based systems, we design the CNN part based on ResNet-18 BIBREF25 structure, named ResNet14, as the main part, which decreases the parameters a lot. The first conv layer is with kernel size INLINEFORM0 and stride size INLINEFORM1 , followed by a maxpool layer with stride size INLINEFORM2 for downsampling. Then the residual blocks extract high-dim features from the input sequences and keep the low-rank information. There are 6 res-blocks in all for decreasing parameters, the kernel size of each block is INLINEFORM3 and the features are downsampled while adding channels. We use 2-layer bidirectional long-short term memory (BLSTM) BIBREF26 as the RNN part following", "In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system. Comparison of different stage systems. First of all, we compare the two-stage system and the three-stage system trained with phonetic sequence annotation and dialect category label with the baseline trained only with dialect category label. The two multi-stage system have the same ResNet14 architecture and use 2-layer BLSTM as the RNN part with 256 nodes. From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task. By analyzing the confusing matrices (Figure FIGREF19 ) of", "the kernel size of each block is INLINEFORM3 and the features are downsampled while adding channels. We use 2-layer bidirectional long-short term memory (BLSTM) BIBREF26 as the RNN part following ResNet14. BLSTM extends original LSTM by introducing a backward direction layer so it considers the future context. The output of the network will be linked to different loss functions and different labels in different stages. Loss function. CTC is an objective function that allows an RNN to be trained for sequence transcription tasks without requiring any prior alignment between the input and target sequences. The label sequence INLINEFORM0 can be mapped to its corresponding CTC paths. We denote the set of CTC paths for INLINEFORM1 as INLINEFORM2 . Thus the likelihood of INLINEFORM3 can be evaluated as a sum of the probabilities of its CTC paths: DISPLAYFORM0  where INLINEFORM0 is the utterance and INLINEFORM1 is a CTC path. Then the network can be trained to optimize the CTC function", "sequence so it keeps all the information from the ResNet14 part. The network of second stage is 2-layer BLSTM. The final pooling strategy is average pooling on time-dimension so we can get the utterance-level category results from frame-level, and the output is the prediction of dialect category. We use CTC loss to train the AM so the network outputs can align with the phoneme sequences automatically and use cross-entropy loss to discriminate between dialects. Compared with multi-task training BIBREF27 , BIBREF28 in SV tasks, it should be emphasized that these stages should be trained step by step instead of multi-task learning with shared layers, that is to say we backpropagate the whole network while training AM, and only backpropagate the RNN part in the second stage, or the network will be degenerated and lost the information of acoustic knowledge. Three-stage system. The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an", "Data description. We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task). The phonetic sequence annotation of the corresponding text to each speech is also provided in the training set. There are 27 initials and 39 finals with 148 tones in the whole database. Experimental setup. We convert the raw audio to 40-dimensional log Mel-filterbank coefficients with a frame-length of 25 ms, mean-normalized over the whole utterance. Then we stack all the log-mel filterbank", "be degenerated and lost the information of acoustic knowledge. Three-stage system. The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an AM whose architecture is the same as the first-stage in the two-stage system. This AM is used to generate temporal locations of each phoneme through CTC loss, so that we can train an another AM by using cross-entropy loss as the second-stage to predict the corresponding phonetic labels of the input frames, in which we only use ResNet14 without an RNN because we have the precise locations of each phoneme from the first stage. The third stage is similar, we use the intermediate features from the second stage to train an RNN network for LID task, also the loss in this stage is cross-entropy loss. Data description. We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect"]}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "predicted_answer": "Unanswerable", "predicted_evidence": ["( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task. By analyzing the confusing matrices (Figure FIGREF19 ) of predicted results, we can find that the accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi. The results accord with regional distribution of the dialects. For example, Minnan and Kekka are both in Fujian Province and have lots of cognate words, so it is hard to recognize them in reality. Comparison of different RNN structures. We further explore the impact of different RNN structures with bidirectional gated recurrent unit (BGRU) and BLSTM. For the two-stage system (Table TABREF22 ), adding the nodes of BLSTM does not work, but adding another layer makes sense in short-duration task. Moreover, with the same layers and nodes, BLSTM outperforms BGRU in the two", "In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system. Comparison of different stage systems. First of all, we compare the two-stage system and the three-stage system trained with phonetic sequence annotation and dialect category label with the baseline trained only with dialect category label. The two multi-stage system have the same ResNet14 architecture and use 2-layer BLSTM as the RNN part with 256 nodes. From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task. By analyzing the confusing matrices (Figure FIGREF19 ) of", "(Table TABREF22 ), adding the nodes of BLSTM does not work, but adding another layer makes sense in short-duration task. Moreover, with the same layers and nodes, BLSTM outperforms BGRU in the two sub-tasks. We believe that sound related tasks do not need a very deep network as image related tasks, that is also the reason why we use a shallow ResNet14 as the CNN part. We evaluate the three-stage system with the same experiments, and the results (Table TABREF23 ) demonstrate that the three-stage system can achieve high accuracy in long duration task by larger BLSTM layers and the BGRU structure outperforms BLSTM on the whole. But adding the third RNN layer also does not work in these experiments. As Table TABREF24 shows, training networks in the first stage (with CTC loss) needs more time for convergence than training networks in the second or third stage (with cross-entropy loss). We can observe that the two-stage system spends less time while having a slightly higher accuracy", "sequence so it keeps all the information from the ResNet14 part. The network of second stage is 2-layer BLSTM. The final pooling strategy is average pooling on time-dimension so we can get the utterance-level category results from frame-level, and the output is the prediction of dialect category. We use CTC loss to train the AM so the network outputs can align with the phoneme sequences automatically and use cross-entropy loss to discriminate between dialects. Compared with multi-task training BIBREF27 , BIBREF28 in SV tasks, it should be emphasized that these stages should be trained step by step instead of multi-task learning with shared layers, that is to say we backpropagate the whole network while training AM, and only backpropagate the RNN part in the second stage, or the network will be degenerated and lost the information of acoustic knowledge. Three-stage system. The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an", "DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely. On the other hand, in many utterance analyzing tasks such as acoustic speech recognition (ASR), speaker verification (SV) and our LID, only a simple task or a specific aim is focused on. However, an utterance always has multi-dimensional information such as content, emotion, speaker and language and there are some certain correlations between them. Although the LID task is", "the kernel size of each block is INLINEFORM3 and the features are downsampled while adding channels. We use 2-layer bidirectional long-short term memory (BLSTM) BIBREF26 as the RNN part following ResNet14. BLSTM extends original LSTM by introducing a backward direction layer so it considers the future context. The output of the network will be linked to different loss functions and different labels in different stages. Loss function. CTC is an objective function that allows an RNN to be trained for sequence transcription tasks without requiring any prior alignment between the input and target sequences. The label sequence INLINEFORM0 can be mapped to its corresponding CTC paths. We denote the set of CTC paths for INLINEFORM1 as INLINEFORM2 . Thus the likelihood of INLINEFORM3 can be evaluated as a sum of the probabilities of its CTC paths: DISPLAYFORM0  where INLINEFORM0 is the utterance and INLINEFORM1 is a CTC path. Then the network can be trained to optimize the CTC function", "network will contain information of pronunciation and language category, so we combine the ASR method with our LID task and here are our main contributions: The remainder of the paper is organized as follows. Section 2 introduces some related works about ASR and section 3 introduces the ResNet14 structure and gives processing of the two multi-stage systems. We present details of the database and initialization methods of the network in Section 4. The results of experiments and analysis are shown in Section 5. Lastly we give the conclusion and some future work in Section 6. Related works. ASR BIBREF17 task enables the recognition and translation of spoken language into text. Traditionally, we can train an AM based on frame-wise cross-entropy loss to recognize phoneme, which requires tedious label alignment procedure such as Hidden Markov Model and Gaussian Mixture Model (HMM-GMM) paradigm. Then we can use a pronunciation model and a language model to transfer into text. In the latter", "evaluated as a sum of the probabilities of its CTC paths: DISPLAYFORM0  where INLINEFORM0 is the utterance and INLINEFORM1 is a CTC path. Then the network can be trained to optimize the CTC function INLINEFORM2 by the given sequence labeling. For the LID task, we use the multi-class cross-entropy loss for classification: DISPLAYFORM0  where INLINEFORM0 is the ground truth label and INLINEFORM1 is the output probability distribution. Two-stage system. Figure FIGREF12 shows the architecture of the two-stage system. The input is the sound feature of each utterance. We firstly train the AM with the ResNet14 followed by an RNN architecture, then the intermediate results computed by res-blocks are feed to the second stage as the input. The framework does not need to compress the feature sequence so it keeps all the information from the ResNet14 part. The network of second stage is 2-layer BLSTM. The final pooling strategy is average pooling on time-dimension so we can get the"]}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "predicted_answer": "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian", "predicted_evidence": ["Data description. We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task). The phonetic sequence annotation of the corresponding text to each speech is also provided in the training set. There are 27 initials and 39 finals with 148 tones in the whole database. Experimental setup. We convert the raw audio to 40-dimensional log Mel-filterbank coefficients with a frame-length of 25 ms, mean-normalized over the whole utterance. Then we stack all the log-mel filterbank", "Conclusions. In this work, we propose an acoustic model based on ResNet14 followed by an RNN to recognize phoneme sequence directly with CTC loss and train a simple RNN lastly to get posteriors for recognizing dialect category, forming a two-stage LID system. The system links the different stages by using intermediate features extracted by a shallow ResNet14 architecture. Compared with a simple network or the three-stage system, the two-stage system achieves the state-of-the-art in the Chinese dialect recognition task. We believe this idea of two-stage training can provide inspirations for learning different classes knowledge and can extend to other fields.", "Introduction. The aim of language identification (LID) is to determine the language of an utterance and can be defined as a variable-length sequence classification task on the utterance-level. The task introduced in this paper is more challenging than general LID tasks cause we use a dialect database which contains 10 dialects in China. The dialects' regions are close to each other and they all belong to Chinese, so they have the same characters and similar pronunciations. Recently, the use of deep neural network (DNN) has been explored in LID tasks. The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network", "be degenerated and lost the information of acoustic knowledge. Three-stage system. The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an AM whose architecture is the same as the first-stage in the two-stage system. This AM is used to generate temporal locations of each phoneme through CTC loss, so that we can train an another AM by using cross-entropy loss as the second-stage to predict the corresponding phonetic labels of the input frames, in which we only use ResNet14 without an RNN because we have the precise locations of each phoneme from the first stage. The third stage is similar, we use the intermediate features from the second stage to train an RNN network for LID task, also the loss in this stage is cross-entropy loss. Data description. We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect", "( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task. By analyzing the confusing matrices (Figure FIGREF19 ) of predicted results, we can find that the accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi. The results accord with regional distribution of the dialects. For example, Minnan and Kekka are both in Fujian Province and have lots of cognate words, so it is hard to recognize them in reality. Comparison of different RNN structures. We further explore the impact of different RNN structures with bidirectional gated recurrent unit (BGRU) and BLSTM. For the two-stage system (Table TABREF22 ), adding the nodes of BLSTM does not work, but adding another layer makes sense in short-duration task. Moreover, with the same layers and nodes, BLSTM outperforms BGRU in the two", "on. However, an utterance always has multi-dimensional information such as content, emotion, speaker and language and there are some certain correlations between them. Although the LID task is text-independent, which means the content of each utterance is totally different, different languages may have its own pronunciations or tones. Thus acoustic and language are two components in the LID task, BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 use the bottleneck features from an ASR system and feed to another neural network for recognition. Nevertheless, these ASR DNNs constituted by fully connected layers adds significant computational complexity and also require labels of physical states of a tied-state triphone. Inspired by all this, we assume that the high-dim features extracted from the network will contain information of pronunciation and language category, so we combine the ASR method with our LID task and here are our main contributions: The remainder of the paper is organized as", "sequence so it keeps all the information from the ResNet14 part. The network of second stage is 2-layer BLSTM. The final pooling strategy is average pooling on time-dimension so we can get the utterance-level category results from frame-level, and the output is the prediction of dialect category. We use CTC loss to train the AM so the network outputs can align with the phoneme sequences automatically and use cross-entropy loss to discriminate between dialects. Compared with multi-task training BIBREF27 , BIBREF28 in SV tasks, it should be emphasized that these stages should be trained step by step instead of multi-task learning with shared layers, that is to say we backpropagate the whole network while training AM, and only backpropagate the RNN part in the second stage, or the network will be degenerated and lost the information of acoustic knowledge. Three-stage system. The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an", "DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely. On the other hand, in many utterance analyzing tasks such as acoustic speech recognition (ASR), speaker verification (SV) and our LID, only a simple task or a specific aim is focused on. However, an utterance always has multi-dimensional information such as content, emotion, speaker and language and there are some certain correlations between them. Although the LID task is"]}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "predicted_answer": "No", "predicted_evidence": ["the detector (modular) or a BERT encoder (concurrent) provide additional benefits. The proposed models retain the strong effects of systems from the literature while also producing target-level fluency on average. Our results suggest there is no clear winner between our two proposed systems. modular is better at reducing bias and has higher accuracy, while concurrent produces more fluent responses, preserves meaning better, and has higher BLEU. Table TABREF39 indicates that BLEU is more correlated with fluency but accuracy is more correlated with subjective bias reduction. The weak association between BLEU and human evaluation scores is corroborated by other research BIBREF35, BIBREF36. We conclude that neither automatic metric is a true substitute for human judgment. Experiments ::: Real-world Media. To demonstrate the efficacy of the proposed methods on subjective bias in the wild, we perform inference on three out-of-domain datasets (Table TABREF45). We prepared each dataset", "n}$ is the BERT-embedded source and $\\mathbf {W}^H \\in \\mathcal {R}^{h \\times b}$ is a matrix of learned parameters. Initial states $\\mathbf {c}_0 = \\mathbf {W}^{c0}\\ \\sum \\mathbf {b}_i / n$ and $\\mathbf {h_0} = \\mathbf {W}^{h0}\\ \\sum \\mathbf {b}_i / n$. $\\mathbf {W}^{c0} \\in \\mathcal {R}^{h \\times b}$ and $\\mathbf {W}^{h0} \\in \\mathcal {R}^{h \\times b}$ are learned matrices. Model training. The concurrent model is pre-trained with the same autoencoding procedure described in Section SECREF28. It is then fine-tuned as a subjective-to-neutral translation system with the same loss function described in Section SECREF30. Experiments ::: Experimental Protocol. Implementation. We implemented nonlinear models with Pytorch BIBREF29 and optimized using Adam BIBREF30 as configured in BIBREF18 with a learning rate of 5e-5. We used a batch size of 16. All vectors were of length $h = 512$ unless otherwise specified. We use gradient clipping with a maximum gradient norm of 3 and a dropout", "BIBREF39). concurrent's encoder, which is architecturally identical to BERT, had similar performance to a stand-alone BERT system. The linguistic and category-related features in the modular detector gave it slight leverage over the plain BERT-based models. Algorithmic Analysis ::: Join Embedding. We continue by analyzing the abilities of the proposed join embedding mechanism. Algorithmic Analysis ::: Join Embedding ::: Join Embedding Ablation. The join embedding combines two separately pretrained models through a gated embedding instead of the more traditional practice of stripping off any final classification layers and concatenating the exposed hidden states BIBREF40. We accordingly ablated the join embedding mechanism by training a new model where the pre-trained detector is frozen and its pre-output hidden states $\\mathbf {b}_i$ are concatenated to the encoder's hidden states before decoding. Doing so reduced performance to 90.78 BLEU and 37.57 Accuracy (from the 93.52/46.8 with", "have knowledge of). This loss function is related to previous work on grammar correction BIBREF24, and cost-sensitive learning BIBREF25. Methods for Neutralizing Text ::: CONCURRENT. Our second algorithm takes the problematic source $\\textbf {s}$ and directly generates a neutralized $\\mathbf {\\hat{t}}$. While this renders the system easier to train and operate, it limits interpretability and controllability. Model description. The concurrent system is an encoder-decoder neural network. The encoder is BERT. The decoder is the same as that of Section SECREF28: an attentional LSTM with copy and coverage mechanisms. The decoder's inputs are set to: Hidden states $\\mathbf {H} = \\mathbf {W}^H\\ \\mathbf {B}$, where $\\mathbf {B} = (\\mathbf {b}_1, ..., \\mathbf {b}_{n}) \\in \\mathcal {R}^{b \\times n}$ is the BERT-embedded source and $\\mathbf {W}^H \\in \\mathcal {R}^{h \\times b}$ is a matrix of learned parameters. Initial states $\\mathbf {c}_0 = \\mathbf {W}^{c0}\\ \\sum \\mathbf {b}_i / n$ and", "The proportion of correctly selected words is the system's \u201caccuracy\u201d. Results are given in Table TABREF51. Note that concurrent lacks an interpretive window into its detection behavior, so we estimate an upper bound on the model's detection abilities by (1) feeding the encoder's hidden states into a fully connected + softmax layer that predicts the probability of a token being subjectively biased, and (2) training this layer as a sequence tagger according to the procedure of Section SECREF19. The low human performance can be attributed to the difficulty of identifying bias. Issues of bias are typically reserved for senior Wikipedia editors (Section SECREF14) and untrained workers performed worse (37.39%) on the same task in BIBREF2 (and can struggle on other tasks requiring linguistic knowledge BIBREF39). concurrent's encoder, which is architecturally identical to BERT, had similar performance to a stand-alone BERT system. The linguistic and category-related features in the modular", "out dialog-specific artifacts (interjections, phatics, etc) by removing all sentences with less than 4 tokens before sampling a test set. Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text. We conclude that the proposed methods, while imperfect, are capable of providing useful suggestions for how subjective bias in real-world news or political text can be reduced. Error Analysis. To better understand the limits of our models and the proposed task of bias neutralization, we randomly sample 50 errors produced by our models on the Wikipedia test set and bin them into the following categories: No change. The model failed to remove or change the source sentence. Bad change. The model modified the source but introduced an edit which failed to match the ground-truth target (i.e. the Wikipedia editor's change). Disfluency. Errors in language modeling and text generation. Noise. The datapoint is noisy", "its pre-output hidden states $\\mathbf {b}_i$ are concatenated to the encoder's hidden states before decoding. Doing so reduced performance to 90.78 BLEU and 37.57 Accuracy (from the 93.52/46.8 with the join embedding). This suggests learned embeddings can be a high-performance and end-to-end conduit between sub-modules of machine learning systems. Algorithmic Analysis ::: Join Embedding ::: Join Embedding Control. We proceed to demonstrate how the join embedding creates controllability in the neutralization process. Recall that modular relies on a probability distribution $\\mathbf {p}$ to determine which words require editing (Equation DISPLAY_FORM31). Typically, this distribution comes from the detection module (Section SECREF19), but we can also feed in user-specified distributions that force the model to target particular words. This can let human advisors correct errors or push the model's behavior towards some desired outcome. We find that the model is indeed capable of being", "neutral examples when possible. We pretrained the tagging module for 4 epochs. We pretrained the editing module on the neutral portion of our WNC for 4 epochs. The joint system was trained on the same data as the tagger for 25,000 steps (about 7 epochs). We perform interference using beam search and a beam width of 4. All computations were performed on a single NVIDIA TITAN X GPU; training the full system took approximately 10 hours. We report statistical significance with bootstrap resampling and a 95% confidence level BIBREF32, BIBREF33. Evaluation. We evaluate our models according to five metrics. BLEU BIBREF13 and accuracy (the proportion of decodings that exactly matched the editors changes) are quantitative. We also hired fluent English-speaking crowdworkers on Amazon Mechanical Turk. Workers were shown the BIBREF2 and Wikipedia definition of a \u201cbiased statement\u201d and six example sentences, then subjected to a five-question qualification test where they had to identify"]}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "predicted_answer": "Unanswerable.", "predicted_evidence": []}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "predicted_answer": "State-of-the-art models achieved state-of-the-art results without gazetteers features, unlike earlier approaches that heavily relied on them.", "predicted_evidence": ["For the gazetteer embeddings, we use 128 dim for the embedding matrices $\\mathbf {G}$ and $\\mathbf {S}$, 128 dim output for $\\mathbf {W}$, which yields a gazetteer embedding $\\mathbf {g}_i$ with 128 dim. The parameters are randomly initialized and trained. We apply gazetteer dropout of 0.1 which we tuned on the development set; we tried values form 0.05 to 0.6. All parameters except the ELMo embeddings are trained. We train using the Adam BIBREF26 optimizer with learning rate of 0.001 for 100 epochs. We use early stopping with patience 25 on the development set. Batch size of 64, dropout rate of 0.5 and L2 regularization of 0.1. Experiments ::: Results. The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven", "work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of", "that use self-attention combined with match span encoding. We enhance gazetteer matching with multi-token and single-token matches in the same representation. We demonstrate how to use Wikidata with entity popularity filtering as a resource for building gazetteers. GazSelfAttn evaluations on CoNLL-03 and Ontonotes 5 datasets show F$_1$ score improvement over baseline model from 92.34 to 92.86 and from 89.11 to 89.32 respectively. Moreover, we perform ablation experiments to study the contribution of the different model components. Related Work. Recently, researchers added gazetteers to neural sequence models. BIBREF12 demonstrated small improvements on large datasets and bigger improvements on small datasets. BIBREF13 proposed to train a gazetteer attentive network to learn name regularities and spans of NER entities. BIBREF14 demonstrated that trained gazetteers scoring models combined with hybrid semi-Markov conditional random field (HSCRF) layer improve overall performance. The", "and spans of NER entities. BIBREF14 demonstrated that trained gazetteers scoring models combined with hybrid semi-Markov conditional random field (HSCRF) layer improve overall performance. The HSCRF layer predicts a set of candidate spans that are rescored using a gazetteer classifier model. The HSCRF approach differs from the common approach of including gazetteers as an embedding in the model. Unlike the work of BIBREF14, our GazSelfAttn does not require training a separate gazetteer classifier and the HSCRF layer, thus our approach works with any standard output layer such as conditional random field (CRF) BIBREF15. BIBREF16 proposed an auto-encoding loss with hand-crafted features, including gazetteers, to improve accuracy. However, they did not find that gazetteer features significantly improve accuracy. Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building", "model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate. Experiments ::: Ablation study. Table TABREF22 shows ablation experiments. We remove components of the gazetteer embedding model from the Neural CRF model. In each experiment, we removed only the specified component. Ablations show decreased F$_1$ score on the development and test set if any of the components is removed. The highest degradation is when single matches are removed which underscores the importance of the combining the gazetteer matching techniques for NER. We observe that match span encoding is more important for the CoNLL-02 compared to Ononotes 5 because the former has more entities with multiple tokens. Removing the self-attention shows that self-attention is", "improve accuracy. Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building gazetteers with classical machine learning models. Compared to Wikidata, the data extracted from Wikipedia is smaller and noisier. Similar to this paper, BIBREF19 used Wikidata as a gazetteer resource. However, they did not use entity popularity to filter ambiguous entities and their gazetteer model features use simple one-hot encoding. Approach ::: Model Architecture. We add GazSelfAttn embeddings to the popular Neural CRF model architecture with ELMo LM embeddings from BIBREF2. Figure FIGREF5 depicts the model, which consists of Glove word embeddings BIBREF20, Char-CNN BIBREF21, BIBREF1, ELMo embeddings, Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) labels encoding BIBREF22. Note that, we concatenate the gazetteer embeddings to the Bi-LSTM input. Approach :::", "Introduction. Named-entity recognition (NER) is the task of tagging relevant entities such as person, location and organization in unstructured text. Modern NER has been dominated by neural models BIBREF0, BIBREF1 combined with contextual embeddings from language models (LMs) BIBREF2, BIBREF3, BIBREF4. The LMs are pre-trained on large amounts of unlabeled text which allows the NER model to use the syntactic and semantic information captured by the LM embeddings. On the popular benchmark datasets CoNLL-03 BIBREF5 and Ontonotes 5 BIBREF6, neural models with LMs achieved state-of-the-art results without gazetteers features, unlike earlier approaches that heavily relied on them BIBREF7. Gazetteers are lists that contain entities such as cities, countries, and person names. The gazetteers are matched against unstructured text to provide additional features to the model. Data for building gazetteers is available for multiple language from structured data resources such as Wikipedia, DBpedia", "that are input to the LSTM become zero. We tune the gazetteer dropout hyperparameter on the validation set. Experiments ::: Setup. Datasets. We evaluate on the English language versions of CoNLL-03 dataset BIBREF5 and the human annotated portion of the Ontonotes 5 BIBREF6 dataset. CoNLL-03 labels cover 4 entity types: person, location, organization, and miscellaneous. The Onotonotes 5 dataset is larger and its labels cover 18 types: person, NORP, facility, organization, GPE, location, product, event, work of art, law, language, date, time, percent, money, quantity, ordinal, cardinal. px Gazetteers. We use the Wikidata gazetteers with types merged to the granularity of the CoNLL-03 and Ononotes 5 datasets. We filter non-relevant types (e.g., genome names, disease) and get a total of one million records. For CoNLL-03 and Ontonotes 5, the percentage of entities covered by gazetteers are 96% and 78% respectively, and percentage of gazetteers wrongly assigned to non-entity tokens are 41%"]}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "predicted_answer": "Neural CRF model with and without ELMo embeddings is used as a baseline model.", "predicted_evidence": ["work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of", "model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate. Experiments ::: Ablation study. Table TABREF22 shows ablation experiments. We remove components of the gazetteer embedding model from the Neural CRF model. In each experiment, we removed only the specified component. Ablations show decreased F$_1$ score on the development and test set if any of the components is removed. The highest degradation is when single matches are removed which underscores the importance of the combining the gazetteer matching techniques for NER. We observe that match span encoding is more important for the CoNLL-02 compared to Ononotes 5 because the former has more entities with multiple tokens. Removing the self-attention shows that self-attention is", "For the gazetteer embeddings, we use 128 dim for the embedding matrices $\\mathbf {G}$ and $\\mathbf {S}$, 128 dim output for $\\mathbf {W}$, which yields a gazetteer embedding $\\mathbf {g}_i$ with 128 dim. The parameters are randomly initialized and trained. We apply gazetteer dropout of 0.1 which we tuned on the development set; we tried values form 0.05 to 0.6. All parameters except the ELMo embeddings are trained. We train using the Adam BIBREF26 optimizer with learning rate of 0.001 for 100 epochs. We use early stopping with patience 25 on the development set. Batch size of 64, dropout rate of 0.5 and L2 regularization of 0.1. Experiments ::: Results. The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven", "and spans of NER entities. BIBREF14 demonstrated that trained gazetteers scoring models combined with hybrid semi-Markov conditional random field (HSCRF) layer improve overall performance. The HSCRF layer predicts a set of candidate spans that are rescored using a gazetteer classifier model. The HSCRF approach differs from the common approach of including gazetteers as an embedding in the model. Unlike the work of BIBREF14, our GazSelfAttn does not require training a separate gazetteer classifier and the HSCRF layer, thus our approach works with any standard output layer such as conditional random field (CRF) BIBREF15. BIBREF16 proposed an auto-encoding loss with hand-crafted features, including gazetteers, to improve accuracy. However, they did not find that gazetteer features significantly improve accuracy. Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building", "improve accuracy. Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building gazetteers with classical machine learning models. Compared to Wikidata, the data extracted from Wikipedia is smaller and noisier. Similar to this paper, BIBREF19 used Wikidata as a gazetteer resource. However, they did not use entity popularity to filter ambiguous entities and their gazetteer model features use simple one-hot encoding. Approach ::: Model Architecture. We add GazSelfAttn embeddings to the popular Neural CRF model architecture with ELMo LM embeddings from BIBREF2. Figure FIGREF5 depicts the model, which consists of Glove word embeddings BIBREF20, Char-CNN BIBREF21, BIBREF1, ELMo embeddings, Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) labels encoding BIBREF22. Note that, we concatenate the gazetteer embeddings to the Bi-LSTM input. Approach :::", "records. For CoNLL-03 and Ontonotes 5, the percentage of entities covered by gazetteers are 96% and 78% respectively, and percentage of gazetteers wrongly assigned to non-entity tokens are 41% and 41.5% respectively. Evaluation. We use the standard CoNLL evaluation script which reports entity F1 scores. The F1 scores are averages over 5 runs. Configuration. We use the Bi-LSTM-CNN-CRF model architecture with ELMo language model embeddings from BIBREF2, which consist of 50 dim pre-trained Glove word embeddings BIBREF20, 128 dim Char-CNN BIBREF21, BIBREF1 embeddings with filter size of 3 and randomly initialized 16 dim char embeddings, 1024 pre-trained ELMo pre-trained embeddings, two layer 200 dim Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) spans BIBREF22. For the gazetteer embeddings, we use 128 dim for the embedding matrices $\\mathbf {G}$ and $\\mathbf {S}$, 128 dim output for $\\mathbf {W}$, which yields a gazetteer embedding $\\mathbf {g}_i$ with 128", "types and spans embeddings to get a matrix $\\mathbf {E}_i$. Equation . We compute $\\mathbf {A}_i$ using scaled dot-product self-attention BIBREF23, where $d$ is the dimensionality of the gazetteer embeddings. The attention contextualizes the embeddings with multiple gazetteer matches per token $t_i$. Equation . To add model flexibility, we compute $\\mathbf {H}_i$ with a position-wise feed-forward layer and GELU activation BIBREF24. Equation . Finally, we perform max pooling across the embeddings $\\mathbf {H}_i$ to obtain the final gazetteer embedding $\\mathbf {g}_i$. Approach ::: Gazetteer Dropout. To prevent the neural NER model from overfitting on the gazetteers, we use gazetteers dropout BIBREF25. We randomly set to zero gazetteer embeddings $\\mathbf {g}_i$, so the gazetteer vectors that are input to the LSTM become zero. We tune the gazetteer dropout hyperparameter on the validation set. Experiments ::: Setup. Datasets. We evaluate on the English language versions of CoNLL-03", "that are input to the LSTM become zero. We tune the gazetteer dropout hyperparameter on the validation set. Experiments ::: Setup. Datasets. We evaluate on the English language versions of CoNLL-03 dataset BIBREF5 and the human annotated portion of the Ontonotes 5 BIBREF6 dataset. CoNLL-03 labels cover 4 entity types: person, location, organization, and miscellaneous. The Onotonotes 5 dataset is larger and its labels cover 18 types: person, NORP, facility, organization, GPE, location, product, event, work of art, law, language, date, time, percent, money, quantity, ordinal, cardinal. px Gazetteers. We use the Wikidata gazetteers with types merged to the granularity of the CoNLL-03 and Ononotes 5 datasets. We filter non-relevant types (e.g., genome names, disease) and get a total of one million records. For CoNLL-03 and Ontonotes 5, the percentage of entities covered by gazetteers are 96% and 78% respectively, and percentage of gazetteers wrongly assigned to non-entity tokens are 41%"]}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "predicted_answer": "They do not use Wikipedia, they use Wikidata.", "predicted_evidence": ["Properties describe an entity relations. For example, Boston instance_of City and Boston part_of Massachusetts; both instance_of and part_of are properties. Also, each entity is associated with sitelink count which tacks mentions of the entity on Wikimedia website and can be used as proxy for popularity. To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long. Example aliases for Boston are \u201cBeantown\u201d and \u201cThe Cradle of Liberty\u201d. If there are multiple types per alias, we use the sitelink count to keep the six most popular types. The sitelink filtering is important to reduce the infrequent meanings of an entity in the gazetteer data. The Wikidata types that we obtain after", "Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) labels encoding BIBREF22. Note that, we concatenate the gazetteer embeddings to the Bi-LSTM input. Approach ::: Gazetteers. In this section, we address the issue of building a high-quality gazetteer dictionary $M$ that maps entities to types, e.g., $M$[Andy Murray] $\\rightarrow $ Person. In this work, we use Wikidata, an open source structured knowledge-base, as the source of gazetteers. Although, Wikidata and DBpedia are similar knowledge bases, we choose Wikidata because, as of 2019, it provides data on around 45 million entities compared to around 5 million in DBpedia. Wikidata is organized as entities and properties. Entities can be concrete (Boston, NATO, Michael Jordan) and abstract (City, Organization, Person). Properties describe an entity relations. For example, Boston instance_of City and Boston part_of Massachusetts; both instance_of and part_of are properties. Also, each entity is associated with", "improve accuracy. Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building gazetteers with classical machine learning models. Compared to Wikidata, the data extracted from Wikipedia is smaller and noisier. Similar to this paper, BIBREF19 used Wikidata as a gazetteer resource. However, they did not use entity popularity to filter ambiguous entities and their gazetteer model features use simple one-hot encoding. Approach ::: Model Architecture. We add GazSelfAttn embeddings to the popular Neural CRF model architecture with ELMo LM embeddings from BIBREF2. Figure FIGREF5 depicts the model, which consists of Glove word embeddings BIBREF20, Char-CNN BIBREF21, BIBREF1, ELMo embeddings, Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) labels encoding BIBREF22. Note that, we concatenate the gazetteer embeddings to the Bi-LSTM input. Approach :::", "sitelink count to keep the six most popular types. The sitelink filtering is important to reduce the infrequent meanings of an entity in the gazetteer data. The Wikidata types that we obtain after processing the Wikidata dumps are fine-grained. However, certain NER tasks require coarse-grained types. For instance, CoNLL-03 task has a single Location label that consists of cities, states, countries, and other geographic location. To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure induced by the subclass_of property. Examples of subclass_of hierarchies in Wikidata are: City $\\rightarrow $ Human Settlement $\\rightarrow $ Geographic Location, and Artist $\\rightarrow $ Creator $\\rightarrow $ Person. We change the types granularity depending on the NER task by traversing up, from fine-grained types to the target coarse-grained types. For instance, we merge the Artist and Painter types to Person, and the River and Mountain types to Location.", "against unstructured text to provide additional features to the model. Data for building gazetteers is available for multiple language from structured data resources such as Wikipedia, DBpedia BIBREF8 and Wikidata BIBREF9. In this paper, we propose GazSelfAttn, a novel gazetteer embedding approach that uses self-attention and match span encoding to build enhanced gazetteer representation. GazSelfAttn embeddings are concatenated with the input to a LSTM BIBREF10 or CNN BIBREF11 sequence layer and are trained end-to-end with the model. In addition, we show how to extract general gazetteers from the Wikidata, a structured knowledge-base which is part of the Wikipedia project. Our contributions are the following: [topsep=1pt, leftmargin=15pt, itemsep=-1pt] We propose novel gazetteer embeddings that use self-attention combined with match span encoding. We enhance gazetteer matching with multi-token and single-token matches in the same representation. We demonstrate how to use Wikidata with", "that use self-attention combined with match span encoding. We enhance gazetteer matching with multi-token and single-token matches in the same representation. We demonstrate how to use Wikidata with entity popularity filtering as a resource for building gazetteers. GazSelfAttn evaluations on CoNLL-03 and Ontonotes 5 datasets show F$_1$ score improvement over baseline model from 92.34 to 92.86 and from 89.11 to 89.32 respectively. Moreover, we perform ablation experiments to study the contribution of the different model components. Related Work. Recently, researchers added gazetteers to neural sequence models. BIBREF12 demonstrated small improvements on large datasets and bigger improvements on small datasets. BIBREF13 proposed to train a gazetteer attentive network to learn name regularities and spans of NER entities. BIBREF14 demonstrated that trained gazetteers scoring models combined with hybrid semi-Markov conditional random field (HSCRF) layer improve overall performance. The", "that are input to the LSTM become zero. We tune the gazetteer dropout hyperparameter on the validation set. Experiments ::: Setup. Datasets. We evaluate on the English language versions of CoNLL-03 dataset BIBREF5 and the human annotated portion of the Ontonotes 5 BIBREF6 dataset. CoNLL-03 labels cover 4 entity types: person, location, organization, and miscellaneous. The Onotonotes 5 dataset is larger and its labels cover 18 types: person, NORP, facility, organization, GPE, location, product, event, work of art, law, language, date, time, percent, money, quantity, ordinal, cardinal. px Gazetteers. We use the Wikidata gazetteers with types merged to the granularity of the CoNLL-03 and Ononotes 5 datasets. We filter non-relevant types (e.g., genome names, disease) and get a total of one million records. For CoNLL-03 and Ontonotes 5, the percentage of entities covered by gazetteers are 96% and 78% respectively, and percentage of gazetteers wrongly assigned to non-entity tokens are 41%", "Introduction. Named-entity recognition (NER) is the task of tagging relevant entities such as person, location and organization in unstructured text. Modern NER has been dominated by neural models BIBREF0, BIBREF1 combined with contextual embeddings from language models (LMs) BIBREF2, BIBREF3, BIBREF4. The LMs are pre-trained on large amounts of unlabeled text which allows the NER model to use the syntactic and semantic information captured by the LM embeddings. On the popular benchmark datasets CoNLL-03 BIBREF5 and Ontonotes 5 BIBREF6, neural models with LMs achieved state-of-the-art results without gazetteers features, unlike earlier approaches that heavily relied on them BIBREF7. Gazetteers are lists that contain entities such as cities, countries, and person names. The gazetteers are matched against unstructured text to provide additional features to the model. Data for building gazetteers is available for multiple language from structured data resources such as Wikipedia, DBpedia"]}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "predicted_answer": "LibriSpeech", "predicted_evidence": ["would be similar after PCA BIBREF44 . Then a pair of transformation matrices, INLINEFORM0 and INLINEFORM1 , is learned, where INLINEFORM2 transforms a vector INLINEFORM3 in INLINEFORM4 to the space of INLINEFORM5 , that is, INLINEFORM6 , while INLINEFORM7 maps a vector INLINEFORM8 in INLINEFORM9 to the space of INLINEFORM10 . INLINEFORM11 and INLINEFORM12 are learned iteratively by the algorithm proposed previously BIBREF44 . In our evaluation as mentioned below, labeled pairs of the audio and text embeddings of each word is available, that is, we know INLINEFORM0 and INLINEFORM1 for each word INLINEFORM2 . So we can train the transformation matrices INLINEFORM3 and INLINEFORM4 using the gradient descent method to minimize the following objective function: DISPLAYFORM0  where the last two terms in ( EQREF15 ) are cycle-constraints to ensure that both INLINEFORM0 and INLINEFORM1 are almost unchanged after transformed to the other space and back. In this way we say the two sets of", "two terms in ( EQREF15 ) are cycle-constraints to ensure that both INLINEFORM0 and INLINEFORM1 are almost unchanged after transformed to the other space and back. In this way we say the two sets of embeddings are parallelized. Dataset. We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features. Model Implementation. In Stage 1, The phonetic encoder INLINEFORM0 , speaker encoder INLINEFORM1 and decoder INLINEFORM2 were all 2-layer GRUs with hidden layer size 128, 128 and 256, respectively. The speaker discriminator INLINEFORM3 is a fully-connected feedforward network with 2 hidden layers with size 128. The value of INLINEFORM4 we used in INLINEFORM5 in ( EQREF7 ) was set to 0.01. In Stage 2, the two encoders", "were embedded in (ii)(iii) (AUD-(ph-+se), AUD-(ph+se)), the phonetic structures were inevitably disturbed (0.519, 0.598 vs 0.637). On the other hand, column (b) (TXT-(se,1h)) considered only semantics but not phonetic structure at all, the relatively lower accuracies implied the three versions of audio embedding did bring some good extent of semantics, except (i) AUD-ph, but obviously weaker than the phonetic information in column (a). Also, the Stage 2 training in rows (ii)(iii) (AUD-(ph-+se), AUD-(ph+se)) gave higher accuracies than row (i) (AUD-ph) (0.339, 0.332 vs 0.124 in column (b)), which implied the Stage 2 training was successful. However, column (c) (TXT-(se,ph)) is for the text embedding considering both the semantic and phonetic information, so the two versions of phonetic-and-semantic audio embedding for rows (ii)(iii) had very close distributions (0.750, 0.800 in column (c)), or carried good extent of both semantics and phonetic structure. The above are made clearer by", "in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces. It will be clear below such a mapping relation can be used to evaluate the phonetic and semantic information carried by the audio embeddings. Mini-Batch Cycle Iterative Closest Point (MBC-ICP) BIBREF44 previously proposed as described below is used here. Given two sets of embeddings as mentioned above, INLINEFORM0 and INLINEFORM1 , they are first projected to their respective top INLINEFORM2 principal components by PCA. Let the projected sets of vectors of INLINEFORM3 and INLINEFORM4 be INLINEFORM5 and INLINEFORM6 respectively. If INLINEFORM7 can be mapped to the space of INLINEFORM8 by an affine transformation, the distributions of INLINEFORM9 and INLINEFORM10 would be similar after PCA BIBREF44 . Then a pair of transformation matrices, INLINEFORM0 and INLINEFORM1 , is learned, where INLINEFORM2 transforms a vector INLINEFORM3 in INLINEFORM4 to the space", "is encoded in the speaker vector INLINEFORM6 . The speaker discriminator INLINEFORM7 learns to maximize INLINEFORM8 in ( EQREF9 ), while the phonetic encoder INLINEFORM9 learns to minimize INLINEFORM10 , DISPLAYFORM0  where INLINEFORM0 is a real number. The optimization procedure of Stage 1 consists of four parts: (1) training INLINEFORM0 , INLINEFORM1 and INLINEFORM2 by minimizing INLINEFORM3 , (2) training INLINEFORM4 by minimizing INLINEFORM5 , (3) training INLINEFORM6 by minimizing INLINEFORM7 , and (4) training INLINEFORM8 by maximizing INLINEFORM9 . Parts (1)(2)(3) are jointly trained together, while iteratively trained with part (4) BIBREF45 . Stage 2 - Semantic Embedding over Phonetic Embeddings Obtained in Stage 1. As shown in Figure FIGREF12 , similar to the Word2Vec skip-gram model BIBREF0 , we use two encoders: semantic encoder INLINEFORM0 and context encoder INLINEFORM1 to embed the semantics over phonetic embeddings INLINEFORM2 obtained in Stage 1. On the one hand, given", "INLINEFORM3 is a fully-connected feedforward network with 2 hidden layers with size 128. The value of INLINEFORM4 we used in INLINEFORM5 in ( EQREF7 ) was set to 0.01. In Stage 2, the two encoders INLINEFORM0 and INLINEFORM1 were both 2-hidden-layer fully-connected feedforward networks with size 256. The size of embedding vectors was set to be 128. The context window size was 5, and the negative sampling number was 5. For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5. Evaluation by Parallelizing Audio and Text Embeddings. Each text word corresponds to many audio realizations in spoken form. So we first took the average of the audio embeddings for all those realizations to be the audio embedding for the spoken word considered. In this way, each word has a unique", "BIBREF0 , we use two encoders: semantic encoder INLINEFORM0 and context encoder INLINEFORM1 to embed the semantics over phonetic embeddings INLINEFORM2 obtained in Stage 1. On the one hand, given a spoken word INLINEFORM3 , we feed its phonetic vector INLINEFORM4 obtained from Stage 1 into INLINEFORM5 as in the middle of Figure FIGREF12 , producing the semantic embedding (in yellow) of the spoken word INLINEFORM6 . On the other hand, given the context window size INLINEFORM7 , which is a hyperparameter, if a spoken word INLINEFORM8 is in the context window of INLINEFORM9 , then its phonetic vector INLINEFORM10 is a context vector of INLINEFORM11 . For each context vector INLINEFORM12 of INLINEFORM13 , we feed it into the context encoder INLINEFORM14 in the upper part of Figure FIGREF12 , and the output is the context embedding INLINEFORM15 . Given a pair of phonetic vectors INLINEFORM0 , the training criteria for INLINEFORM1 and INLINEFORM2 is to maximize the similarity between", "Introduction. Word embedding or Word2Vec BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 has been widely used in the area of natural language processing BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , in which text words are transformed into vector representations of fixed dimensionality BIBREF11 , BIBREF12 , BIBREF13 . This is because these vector representations carry plenty of semantic information learned from the context of the considered words in the text training corpus. Similarly, audio Word2Vec has also been proposed in the area of speech signal processing, in which spoken words (signal segments for words without knowing the underlying word it represents) are transformed into vector representations of fixed dimensionality BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . These vector representations carry the phonetic structures of the spoken words learned from the signals within the spoken"]}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "predicted_answer": "disentangled", "predicted_evidence": ["INLINEFORM1 are uttered by the same speaker ( INLINEFORM2 ), we want their speaker embeddings INLINEFORM3 and INLINEFORM4 to be as close as possible. But if INLINEFORM5 , we want the distance between INLINEFORM6 and INLINEFORM7 larger than a threshold INLINEFORM8 . As shown in the upper right corner of Figure FIGREF3 , a speaker discriminator INLINEFORM0 takes two phonetic vectors INLINEFORM1 and INLINEFORM2 as input and tries to tell if the two vectors come from the same speaker. The learning target of the phonetic encoder INLINEFORM3 is to \"fool\" this speaker discriminator INLINEFORM4 , keeping it from discriminating the speaker identity correctly. In this way, only the phonetic structure information is learned in the phonetic vector INLINEFORM5 , while only the speaker characteristics is encoded in the speaker vector INLINEFORM6 . The speaker discriminator INLINEFORM7 learns to maximize INLINEFORM8 in ( EQREF9 ), while the phonetic encoder INLINEFORM9 learns to minimize", "The proposed framework of phonetic-and-semantic embedding of spoken words consists of two stages: Stage 1 - Phonetic embedding with speaker characteristics disentangled. Stage 2 - Semantic embedding over phonetic embeddings obtained in Stage 1. In addition, we propose an approach for parallelizing the audio and text embeddings to be used for evaluating the phonetic and semantic information carried by the audio embeddings. These are described in Subsections SECREF2 , SECREF11 and SECREF14 respectively. Stage 1 - Phonetic Embedding with Speaker Characteristics Disentangled. A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding. So Stage 1 is to obtain phonetic", "will be used in the next stage as the phonetic embedding. The two encoders INLINEFORM10 , INLINEFORM11 and the decoder INLINEFORM12 are jointly learned by minimizing the reconstruction loss below: DISPLAYFORM0  It will be clear below how to make INLINEFORM0 and INLINEFORM1 separately encode the phonetic structure and speaker characteristics. The speaker encoder training requires speaker information for the spoken words. Assume the spoken word INLINEFORM0 is uttered by speaker INLINEFORM1 . When the speaker information is not available, we can simply assume that the spoken words in the same utterance are produced by the same speaker. As shown in the lower part of Figure FIGREF3 , INLINEFORM2 is learned to minimize the following loss: DISPLAYFORM0  In other words, if INLINEFORM0 and INLINEFORM1 are uttered by the same speaker ( INLINEFORM2 ), we want their speaker embeddings INLINEFORM3 and INLINEFORM4 to be as close as possible. But if INLINEFORM5 , we want the distance between", "spoken words, each represented as INLINEFORM2 , where INLINEFORM3 is the acoustic feature vector for the tth frame and INLINEFORM4 is the total number of frames in the spoken word. The goal of Stage 1 is to disentangle the phonetic structure and speaker characteristics in acoustic features, and extract a vector representation for the phonetic structure only. As shown in the middle of Figure FIGREF3 , a sequence of acoustic features INLINEFORM0 is entered to a phonetic encoder INLINEFORM1 and a speaker encoder INLINEFORM2 to obtain a phonetic vector INLINEFORM3 in orange and a speaker vector INLINEFORM4 in green. Then the phonetic and speaker vectors INLINEFORM5 , INLINEFORM6 are used by the decoder INLINEFORM7 to reconstruct the acoustic features INLINEFORM8 . This phonetic vector INLINEFORM9 will be used in the next stage as the phonetic embedding. The two encoders INLINEFORM10 , INLINEFORM11 and the decoder INLINEFORM12 are jointly learned by minimizing the reconstruction loss", "two terms in ( EQREF15 ) are cycle-constraints to ensure that both INLINEFORM0 and INLINEFORM1 are almost unchanged after transformed to the other space and back. In this way we say the two sets of embeddings are parallelized. Dataset. We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features. Model Implementation. In Stage 1, The phonetic encoder INLINEFORM0 , speaker encoder INLINEFORM1 and decoder INLINEFORM2 were all 2-layer GRUs with hidden layer size 128, 128 and 256, respectively. The speaker discriminator INLINEFORM3 is a fully-connected feedforward network with 2 hidden layers with size 128. The value of INLINEFORM4 we used in INLINEFORM5 in ( EQREF7 ) was set to 0.01. In Stage 2, the two encoders", "latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding. So Stage 1 is to obtain phonetic embeddings only with speaker characteristics disentangled. Also, because the training of phonetic-and-semantic embedding is challenging, in the initial effort we slightly simplify the task by assuming all training utterances have been properly segmented into spoken words. Because there exist many approaches for segmenting utterances automatically BIBREF25 , and automatic segmentation plus phonetic embedding of spoken words has been successfully trained and reported before BIBREF25 , such an assumption is reasonable here. We denote the audio corpus as INLINEFORM0 , which consists of INLINEFORM1 spoken words, each represented as INLINEFORM2 , where INLINEFORM3 is the acoustic feature vector for the tth frame and INLINEFORM4 is the total number of frames in the spoken word. The goal of Stage", "Donald Trump\", should also be retrieved. This is usually referred to as \u201csemantic retrieval\", which can be achieved by the phonetic-and-semantic embedding discussed here. This paper proposes a two-stage framework of phonetic-and-semantic embedding for spoken words. Stage 1 performs phonetic embedding but with speaker characteristics disentangled using separate phonetic and speaker encoders and a speaker discriminator. Stage 2 then performs semantic embedding in addition. We further propose to evaluate the phonetic-and-semantic nature of the audio embeddings obtained in Stage 2 by parallelizing with text embeddings BIBREF43 , BIBREF44 . Very encouraging results including those for an application task of spoken document retrieval were obtained in the initial experiments. Proposed Approach. The proposed framework of phonetic-and-semantic embedding of spoken words consists of two stages: Stage 1 - Phonetic embedding with speaker characteristics disentangled. Stage 2 - Semantic embedding", "audio embedding for rows (ii)(iii) had very close distributions (0.750, 0.800 in column (c)), or carried good extent of both semantics and phonetic structure. The above are made clearer by the numbers in bold which are the highest for each row, and the numbers in red which are the highest for each column. It is also clear that the speaker characteristics disentanglement is helpful, since row (iii) for AUD-(ph+se) was always better than row (ii) for AUD-(ph-+se). Similar trends can be observed in the other parts of Table TABREF18 for 3000 and 5000 pairs, except the accuracies were lower, probably because for more pairs the parallelizing transformation became more difficult and less accurate. The only difference is that in these parts column (a) for TXT-ph had the highest accuracies, probably because the goal of semantic embedding for rows (ii)(iii) (AUD-(ph-+se), AUD-(ph+se)) was really difficult, and disturbed or even dominated by phonetic structures. Similar trends can be observed in"]}
{"question_id": "fef5b65263c81299acc350a101dabaf5a8cb9c6e", "predicted_answer": "Unanswerable", "predicted_evidence": ["phonetic structures and semantics for spoken words is naturally very challenging. Text words can be trained and embedded as vectors carrying plenty of semantics because the phonetic structures are not considered at all. On the other hand, because spoken words are just a different version of representations for text words, it is also natural to believe they do carry some semantic information, except disturbed by phonetic structures plus some other acoustic factors such as speaker characteristics and background noise BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 . So the goal of embedding spoken words to carry both phonetic structures and semantics is possible, although definitely hard. But a nice feature of such embeddings is that they may include both phonetic structures and semantics BIBREF36 , BIBREF37 . A direct application for such phonetic-and-semantic embedding of spoken words is spoken document retrieval BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 .", "of spoken content retrieval. Conclusions and Future Work. In this paper we propose a framework to embed spoken words into vector representations carrying both the phonetic structure and semantics of the word. This is intrinsically challenging because the phonetic structure and the semantics of spoken words inevitably disturbs each other. But this phonetic-and-semantic embedding nature is desired and attractive, for example in the application task of spoken document retrieval. A parallelizing transformation between the audio and text embeddings is also proposed to evaluate whether such a goal is achieved.", "BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . These vector representations carry the phonetic structures of the spoken words learned from the signals within the spoken words, and have been shown to be useful in spoken term detection, in which the spoken terms are detected simply based on the phonetic structures. Such Audio Word2Vec representations do not carry semantics, because they are learned from individual spoken words only without considering the context. Audio Word2Vec was recently extended to Segmental Audio Word2Vec BIBREF25 , in which an utterance can be automatically segmented into a sequence of spoken words BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 and then transformed into a sequence of vectors of fixed dimensionality by Audio Word2Vec, and the spoken word segmentation and Audio Word2Vec can be jointly trained from an audio corpus. In this way the Audio Word2Vec was upgraded from word-level to utterance-level. This offers the opportunity for", "word segmentation and Audio Word2Vec can be jointly trained from an audio corpus. In this way the Audio Word2Vec was upgraded from word-level to utterance-level. This offers the opportunity for Audio Word2Vec to include semantic information in addition to phonetic structures, since the context among spoken words in utterances bring semantic information. This is the goal of this work, and this paper reports the first set of results towards such a goal. In principle, the semantics and phonetic structures in words inevitably disturb each other. For example, the words \u201cbrother\" and \u201csister\" are close in semantics but very different in phonetic structure, while the words \u201cbrother\" and \u201cbother\" are close in phonetic structure but very different in semantics. This implies the goal of embedding both phonetic structures and semantics for spoken words is naturally very challenging. Text words can be trained and embedded as vectors carrying plenty of semantics because the phonetic structures are", "and semantics BIBREF36 , BIBREF37 . A direct application for such phonetic-and-semantic embedding of spoken words is spoken document retrieval BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 . This task is slightly different from spoken term detection, in the latter case spoken terms are simply detected based on the phonetic structures. Here the goal of the task is to retrieve all spoken documents (sets of consecutive utterances) relevant to the spoken query, which may or may not include the query. For example, for the spoken query of \u201cPresident Donald Trump\", not only those documents including the spoken query should be retrieved based on the phonetic structures, but those documents including semantically related words such as \u201cWhite House\" and \u201ctrade policy\", but not necessarily \u201cPresident Donald Trump\", should also be retrieved. This is usually referred to as \u201csemantic retrieval\", which can be achieved by the phonetic-and-semantic embedding discussed here. This paper proposes", "Introduction. Word embedding or Word2Vec BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 has been widely used in the area of natural language processing BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , in which text words are transformed into vector representations of fixed dimensionality BIBREF11 , BIBREF12 , BIBREF13 . This is because these vector representations carry plenty of semantic information learned from the context of the considered words in the text training corpus. Similarly, audio Word2Vec has also been proposed in the area of speech signal processing, in which spoken words (signal segments for words without knowing the underlying word it represents) are transformed into vector representations of fixed dimensionality BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . These vector representations carry the phonetic structures of the spoken words learned from the signals within the spoken", "word \u201cowned\" also has three common phonetically similar words \u201carmed\", \u201cown\" and \u201conly\" in the top-10 nearest neighbors of AUD-(ph+se) and AUD-ph. This is even clearer for the function word \u201cdidn't\". These clearly illustrate the phonetic-and-semantic nature of AUD-(ph+se). Results of Spoken Document Retrieval. The goal here is to retrieve not only those spoken documents including the spoken query (e.g. \u201cPresident Donald Trump\") based on the phonetic structures, but those including words semantically related to the query word (e.g. \u201cWhite House\"). Below we show the effectiveness of the phonetic-and-semantc embedding proposed here in this application. We used the 960 hours of \u201cclean\" and \u201cother\" parts of LibriSpeech dataset as the target archive for retrieval, which consisted of 1478 audio books with 5466 chapters. Each chapter included 1 to 204 utterances or 5 to 6529 spoken words. In our experiments, the queries were the keywords in the book titles, and the spoken documents were the", "The proposed framework of phonetic-and-semantic embedding of spoken words consists of two stages: Stage 1 - Phonetic embedding with speaker characteristics disentangled. Stage 2 - Semantic embedding over phonetic embeddings obtained in Stage 1. In addition, we propose an approach for parallelizing the audio and text embeddings to be used for evaluating the phonetic and semantic information carried by the audio embeddings. These are described in Subsections SECREF2 , SECREF11 and SECREF14 respectively. Stage 1 - Phonetic Embedding with Speaker Characteristics Disentangled. A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding. So Stage 1 is to obtain phonetic"]}
{"question_id": "f40e23adc8245562c8677f0f86fa5175179b5422", "predicted_answer": "Yes", "predicted_evidence": ["INLINEFORM3 is a fully-connected feedforward network with 2 hidden layers with size 128. The value of INLINEFORM4 we used in INLINEFORM5 in ( EQREF7 ) was set to 0.01. In Stage 2, the two encoders INLINEFORM0 and INLINEFORM1 were both 2-hidden-layer fully-connected feedforward networks with size 256. The size of embedding vectors was set to be 128. The context window size was 5, and the negative sampling number was 5. For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5. Evaluation by Parallelizing Audio and Text Embeddings. Each text word corresponds to many audio realizations in spoken form. So we first took the average of the audio embeddings for all those realizations to be the audio embedding for the spoken word considered. In this way, each word has a unique", "of dot product of INLINEFORM0 and INLINEFORM1 is used to evaluate the similarity. With ( EQREF13 ), if INLINEFORM2 and INLINEFORM3 are in the same context window, we want INLINEFORM4 and INLINEFORM5 to be as similar as possible. We also use the negative sampling technique, in which only some pairs INLINEFORM6 are randomly sampled as negative examples instead of enumerating all possible negative pairs. Parallelizing Audio and Text Embeddings for Evaluation Purposes. In this paper we further propose an approach of parallelizing a set of audio embeddings (for spoken words) with a set of text embeddings (for text words) which will be useful in evaluating the phonetic and semantic information carried by these embeddings. Assume we have the audio embeddings for a set of spoken words INLINEFORM0 INLINEFORM1 , where INLINEFORM2 is the embedding obtained for a spoken word INLINEFORM3 and INLINEFORM4 is the total number of distinct spoken words in the audio corpus. On the other hand, assume we", "is encoded in the speaker vector INLINEFORM6 . The speaker discriminator INLINEFORM7 learns to maximize INLINEFORM8 in ( EQREF9 ), while the phonetic encoder INLINEFORM9 learns to minimize INLINEFORM10 , DISPLAYFORM0  where INLINEFORM0 is a real number. The optimization procedure of Stage 1 consists of four parts: (1) training INLINEFORM0 , INLINEFORM1 and INLINEFORM2 by minimizing INLINEFORM3 , (2) training INLINEFORM4 by minimizing INLINEFORM5 , (3) training INLINEFORM6 by minimizing INLINEFORM7 , and (4) training INLINEFORM8 by maximizing INLINEFORM9 . Parts (1)(2)(3) are jointly trained together, while iteratively trained with part (4) BIBREF45 . Stage 2 - Semantic Embedding over Phonetic Embeddings Obtained in Stage 1. As shown in Figure FIGREF12 , similar to the Word2Vec skip-gram model BIBREF0 , we use two encoders: semantic encoder INLINEFORM0 and context encoder INLINEFORM1 to embed the semantics over phonetic embeddings INLINEFORM2 obtained in Stage 1. On the one hand, given", "would be similar after PCA BIBREF44 . Then a pair of transformation matrices, INLINEFORM0 and INLINEFORM1 , is learned, where INLINEFORM2 transforms a vector INLINEFORM3 in INLINEFORM4 to the space of INLINEFORM5 , that is, INLINEFORM6 , while INLINEFORM7 maps a vector INLINEFORM8 in INLINEFORM9 to the space of INLINEFORM10 . INLINEFORM11 and INLINEFORM12 are learned iteratively by the algorithm proposed previously BIBREF44 . In our evaluation as mentioned below, labeled pairs of the audio and text embeddings of each word is available, that is, we know INLINEFORM0 and INLINEFORM1 for each word INLINEFORM2 . So we can train the transformation matrices INLINEFORM3 and INLINEFORM4 using the gradient descent method to minimize the following objective function: DISPLAYFORM0  where the last two terms in ( EQREF15 ) are cycle-constraints to ensure that both INLINEFORM0 and INLINEFORM1 are almost unchanged after transformed to the other space and back. In this way we say the two sets of", "Donald Trump\", should also be retrieved. This is usually referred to as \u201csemantic retrieval\", which can be achieved by the phonetic-and-semantic embedding discussed here. This paper proposes a two-stage framework of phonetic-and-semantic embedding for spoken words. Stage 1 performs phonetic embedding but with speaker characteristics disentangled using separate phonetic and speaker encoders and a speaker discriminator. Stage 2 then performs semantic embedding in addition. We further propose to evaluate the phonetic-and-semantic nature of the audio embeddings obtained in Stage 2 by parallelizing with text embeddings BIBREF43 , BIBREF44 . Very encouraging results including those for an application task of spoken document retrieval were obtained in the initial experiments. Proposed Approach. The proposed framework of phonetic-and-semantic embedding of spoken words consists of two stages: Stage 1 - Phonetic embedding with speaker characteristics disentangled. Stage 2 - Semantic embedding", "in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces. It will be clear below such a mapping relation can be used to evaluate the phonetic and semantic information carried by the audio embeddings. Mini-Batch Cycle Iterative Closest Point (MBC-ICP) BIBREF44 previously proposed as described below is used here. Given two sets of embeddings as mentioned above, INLINEFORM0 and INLINEFORM1 , they are first projected to their respective top INLINEFORM2 principal components by PCA. Let the projected sets of vectors of INLINEFORM3 and INLINEFORM4 be INLINEFORM5 and INLINEFORM6 respectively. If INLINEFORM7 can be mapped to the space of INLINEFORM8 by an affine transformation, the distributions of INLINEFORM9 and INLINEFORM10 would be similar after PCA BIBREF44 . Then a pair of transformation matrices, INLINEFORM0 and INLINEFORM1 , is learned, where INLINEFORM2 transforms a vector INLINEFORM3 in INLINEFORM4 to the space", "two terms in ( EQREF15 ) are cycle-constraints to ensure that both INLINEFORM0 and INLINEFORM1 are almost unchanged after transformed to the other space and back. In this way we say the two sets of embeddings are parallelized. Dataset. We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features. Model Implementation. In Stage 1, The phonetic encoder INLINEFORM0 , speaker encoder INLINEFORM1 and decoder INLINEFORM2 were all 2-layer GRUs with hidden layer size 128, 128 and 256, respectively. The speaker discriminator INLINEFORM3 is a fully-connected feedforward network with 2 hidden layers with size 128. The value of INLINEFORM4 we used in INLINEFORM5 in ( EQREF7 ) was set to 0.01. In Stage 2, the two encoders", "of spoken content retrieval. Conclusions and Future Work. In this paper we propose a framework to embed spoken words into vector representations carrying both the phonetic structure and semantics of the word. This is intrinsically challenging because the phonetic structure and the semantics of spoken words inevitably disturbs each other. But this phonetic-and-semantic embedding nature is desired and attractive, for example in the application task of spoken document retrieval. A parallelizing transformation between the audio and text embeddings is also proposed to evaluate whether such a goal is achieved."]}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "predicted_answer": "Unanswerable", "predicted_evidence": ["( 26 ):  $$\\frac{S_i}{S_0} \\ge 0.85$$   (Eq. 26)  where: $S_0$ is the highest relevant score. In other words, the score ratio of a relevant article and the most relevant article should not be lower than 85% (choosing the value 0.85 for this threshold is simply heuristic based). This is to prevent a relevant article to have a very low score as opposed to the most relevant article. We ran SVM-Rank with different combinations of features listed in Table 2 , but due to limited space, we only report the result of those combinations which achieved highest F1-score. We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles. Results from Table 3 indicate that (LSI, Manhattan, Jaccard) is the triple of features which achieves the best result and the most stability. The contribution of each feature was investigated by using leave-one-out test. Table 4 shows that when all six features are utilized, the F1-score is", "\\quad \\text{if} \\quad y_{u,v}^{(i)}=1$$   (Eq. 14)  where: $f(x)=w^Tx$ is a linear scoring function, $(x_u,x_v)$ is a pairwise and $\\xi _{u,v}^{(i)}$ is the loss. The document pairwise in our model is a pair of a query and an article. Based on the corpus constructed from all of the single-paragraph articles (see Section \"Data Observation\" ), three basic models were built: TF-IDF, LSI and Latent Dirichlet Allocation (LDA) BIBREF12 . Note that, LSI and LDA model transform articles and queries from their TF-IDF-weighted space into a latent space of a lower dimension. For COLIEE 2016 corpora, the dimension of both LSI and LDA is 300 instead of over 2,100 of TF-IDF model. Those features were extracted by using gensim library BIBREF13 . Additionally, to capture the similarity between a query and an article, we investigated other potential features described in Table 2 . Normally, the Jaccard coefficient measures similarity between two finite sets based on the ratio between the size of the", "are sensitive to the initial value of parameters. Different values lead to large difference in results ( $\\pm $ 5%). Therefore, each model was run $n$ times (n=10) and we chose the best-optimized parameters against the validation set. Table 7 shows that CNN with additional features performs better. Also, CNN with LSI produces a better result as opposed to CNN with TF-IDF. We suspect that this is because TF-IDF vector is large but quite sparse (most values are zero), therefore it increases the number of parameters in CNN and consequently makes the model to be overfitted easily. To achieve the best configuration of CNN architecture, the original CNN model was run with different settings of number filter and hidden layer dimension. According to Table 8 , the change of hyperparameter does not significantly affect to the performance of CNN. We, therefore, chose the configuration with the best performance and least number of parameters: 10 filters and 200 hidden layer size. Information", "that a simple expansion of articles via their references does not always positively contribute to the performance of the model. Since linear kernel was used to train the SVM-Rank model, the role of trade-off training parameter was analyzed by tuning $C$ value from 100 to 2000 with step size 100. Empirically, F1-score peaks at 0.6087 with $C$ = 600 when it comes to COLIEE 2016 training dataset. We, therefore, use this value for training the L2R model. Formal run phase 1 - COLIEE 2016. In COLIEE 2016 competition, Table 6 shows the top three systems and the baseline for the formal run in phase 1 BIBREF21 . Among 7 submissions, iLis7 BIBREF22 was ranked first with outstanding performance (0.6261) by exploiting ensemble methods for legal IR. Several features such as syntactic similarity, lexical similarity, semantic similarity, were used as features for two ensemble methods Least Square Method (LSM) and Linear Discriminant Analysis (LDA). HUKB-2 BIBREF23 used a fundamental feature BM25 and", "of COLIEE. Next, we describe our method for legal IR and legal QA tasks. After building a legal QA system, we show experimental results along with discussion and analysis. We finish by drawing some important conclusions. Basic Idea. In the context of COLIEE 2016, our approach is to build a pipeline framework which addresses two important tasks: IR and QA. In Figure 1 , in training phase, a legal text corpus was built based on all articles. Each training query-article pair for LIR task and LQA task was represented as a feature vector. Those feature vectors were utilized to train a learning-to-rank (L2R) model (Ranking SVM) for IR and a classifier (CNN) for QA. The red arrows mean that those steps were prepared in advance. In the testing phase, given a query $q$ , the system extracts its features and computes the relevance score corresponding to each article by using the L2R model. Higher score yielded by SVM-Rank means the article is more relevant. As shown in Figure 1 , the article", "function. In our model, 10 convolution filters (length = 2) were applied to two adjacent input nodes because these nodes are the same feature type. An average pooling layer (length = 100) is then utilized to synthesize important features. To enhance the performance of CNN, two additional statistic features: TF-IDF and LSI were concatenated with the result of the pooling layer, then fed them into a 2-layer Perceptron model to predict the answer. In Legal QA task, the proposed model was compared to the original CNN model and separate TF-IDF, LSI features. For evaluation, we took out 10% samples from training set for validation, and carried out experiments on dataset with balanced label distribution for training set, validation set and testing set. In CNN models, we found that these models are sensitive to the initial value of parameters. Different values lead to large difference in results ( $\\pm $ 5%). Therefore, each model was run $n$ times (n=10) and we chose the best-optimized", "to build a legal IR, traditional models such as TF-IDF, BM25 or PL2F can be used to generate basic features for matching documents with a query. Nevertheless, to improve not only the accuracy but also the robustness of ranking function, it is essential to take into account a combination of fundamental features and other potential features. Hence, the idea is to build a L2R model, which incorporates various features to generate an optimal ranking function. Among different L2R methods, Ranking SVM (SVM-Rank) BIBREF9 , a state-of-the-art pairwise ranking method and also a strong method for IR BIBREF10 , BIBREF11 , was used. Our model is an extended version of Kim's model BIBREF2 with two new aspects. Firstly, there is a big distinction between our features and Kim's features. While Kim used three types of features: lexical words, dependency pairs, and TF-IDF score; we conducted a series of experiments to discover a set of best features among six features as shown in Table 2 . Secondly,", "vector of the most relevant article sentence A sentence represented by a set of words was converted to a word embedding vector $v_1^{200}$ by using bag-of-words model (BOW) BIBREF18 . BOW model generates a vector representation for a sentence by taking a summation over embedding of words in the sentence. The vector is then normalized by the length of the sentence:  $$s= \\frac{1}{n}\\sum _{i= 1}^{n}s_{i}$$   (Eq. 22)  where: $s$ is a $d$ -dimensional vector of a sentence, $s_{i}$ is a $d$ -dimensional vector of $i^{th}$ word in the sentence, $n$ is the length of sentence. A word embedding model ( $d=200$ ) was trained by using Word2Vec BIBREF19 on the data of Japanese law corpus BIBREF1 . The corpus contains all Civil law articles of Japan's constitution with 13.5 million words from 642 cleaned and tokenized articles. A filter was denoted as a weight vector $w$ with length $h$ ; $w$ will have $h$ parameters to be estimated. For each input vector $S \\in \\mathbb {R}^{d} $ , the feature"]}
{"question_id": "fac273ecb3e72f2dc94cdbc797582d7225a8e070", "predicted_answer": "additional features to the CNN model", "predicted_evidence": ["Introduction. Legal text, along with other natural language text data, e.g. scientific literature, news articles or social media, has seen an exponential growth on the Internet and in specialized systems. Unlike other textual data, legal texts contain strict logical connections of law-specific words, phrases, issues, concepts and factors between sentences or various articles. Those are for helping people to make a correct argumentation and avoid ambiguity when using them in a particular case. Unfortunately, this also makes information retrieval and question answering on legal domain become more complicated than others. There are two primary approaches to information retrieval (IR) in the legal domain BIBREF0 : manual knowledge engineering (KE) and natural language processing (NLP). In the KE approach, an effort is put into translating the way legal experts remember and classify cases into data structures and algorithms, which will be used for information retrieval. Although this", "question answering system for Japan Civil Code. Experimental results show that feature selection affects significantly to the performance of SVM-Rank, in which a set of features consisting of (LSI, Manhattan, Jaccard) gives promising results for information retrieval task. For question answering task, the CNN model is sensitive to initial values of parameters and exerts higher accuracy when adding auxiliary features. In our current work, we have not yet fully explored the characteristics of legal texts in order to utilize these features for building legal QA system. Properties such as references between articles or structured relations in legal sentences should be investigated more deeply. In addition, there should be more evaluation of SVM-Rank and other L2R methods to observe how they perform on this legal data using the same feature set. These are left as our future work. Acknowledgement. This work was supported by JSPS KAKENHI Grant number 15K16048, JSPS KAKENHI Grant Number", "articles found in advance BIBREF2 . Given a legal question, retrieving relevant legal articles and deciding whether the content of a relevant article can be used to answer the question are two vital steps in building a legal question answering system. Kim et al. BIBREF2 exploited Ranking SVM with a set of features for legal IR and Convolutional Neural Network (CNN) BIBREF3 combining with linguistic features for question answering (QA) task. However, generating linguistic features is a non-trivial task in the legal domain. Carvalho et al. BIBREF1 utilized n-gram features to rank articles by using an extension of TF-IDF. For QA task, the authors adopted AdaBoost BIBREF4 with a set of similarity features between a query and an article pair BIBREF5 to classify a query-article pair into \u201cYES\" or \u201cNO\". However, overfitting in training may be a limitation of this method. Sushimita et al. BIBREF6 used the voting of Hiemstra, BM25 and PL2F for IR task. Meanwhile, Tran et al. BIBREF7 used", "or \u201cNO\". However, overfitting in training may be a limitation of this method. Sushimita et al. BIBREF6 used the voting of Hiemstra, BM25 and PL2F for IR task. Meanwhile, Tran et al. BIBREF7 used Hidden Markov model (HMM) as a generative query model for legal IR task. Kano BIBREF8 addressed legal IR task by using a keyword-based method in which the score of each keyword was computed from a query and its relevant articles using inverse frequency. After calculating, relevant articles were retrieved based on three ranked scores. These methods, however, lack the analysis of feature contribution, which can reveal the relation between legal and NLP domain. This paper makes the following contributions: In the following sections, we first show our idea along with data analysis in the context of COLIEE. Next, we describe our method for legal IR and legal QA tasks. After building a legal QA system, we show experimental results along with discussion and analysis. We finish by drawing some", "In other words, an article could refer to the whole other articles or to their paragraphs. In BIBREF1 , if an article has a reference to other articles, the authors expanded it with words of referential ones. In our experiment, however, we found that this approach makes the system confused to rank articles and leads to worse performance. Because of that, we ignored the reference and only took into account individual articles themselves. The results of splitting and non-splitting are shown in Table 5 . Legal Question Answering. Legal Question Answering is a form of textual entailment problem BIBREF14 , which can be viewed as a binary classification task. To capture the relation between a question and an article, a set of features can be used. In the COLLIE 2015, Kim BIBREF3 efficiently applied Convolution Neural Network (CNN) for the legal QA task. However, the small dataset is a limit of deep learning models. Therefores, we provided additional features to the CNN model. The idea", "of COLIEE. Next, we describe our method for legal IR and legal QA tasks. After building a legal QA system, we show experimental results along with discussion and analysis. We finish by drawing some important conclusions. Basic Idea. In the context of COLIEE 2016, our approach is to build a pipeline framework which addresses two important tasks: IR and QA. In Figure 1 , in training phase, a legal text corpus was built based on all articles. Each training query-article pair for LIR task and LQA task was represented as a feature vector. Those feature vectors were utilized to train a learning-to-rank (L2R) model (Ranking SVM) for IR and a classifier (CNN) for QA. The red arrows mean that those steps were prepared in advance. In the testing phase, given a query $q$ , the system extracts its features and computes the relevance score corresponding to each article by using the L2R model. Higher score yielded by SVM-Rank means the article is more relevant. As shown in Figure 1 , the article", "similarity, semantic similarity, were used as features for two ensemble methods Least Square Method (LSM) and Linear Discriminant Analysis (LDA). HUKB-2 BIBREF23 used a fundamental feature BM25 and applied mutatis mutandis for articles. If both an article and a query have conditional parts, they are divided into two parts like conditional parts and the rest part before measuring their similarity. This investigation in conditional parts is valuable since it is a common structure in laws. Their F1-score in formal rune is the second highest (0.5532), which is slightly higher than our system (0.5478) using SVM-Rank and a set of features LSI, Manhattan, Jaccard. This shows that for phase 1, our model with a set of defined features is relatively competitive. Legal Question Answering System. In this stage, we illustrate our framework on COLIEE 2016 data. The framework was trained on XML files, from H18 to H23 and tested on XML file H24. Given a legal question, the framework first retrieves", "applied Convolution Neural Network (CNN) for the legal QA task. However, the small dataset is a limit of deep learning models. Therefores, we provided additional features to the CNN model. The idea behind the QA is that we use CNN BIBREF2 with additional features. This is because: (i) CNN is capable to capture local relationship between neighboring words, which helps CNN to achieve excellent performance in NLP problems BIBREF15 , BIBREF2 , BIBREF16 , BIBREF17 and (ii) we can integrate our knowledge in legal domain in the form of statistical features, e.g. TF-IDF and LSI. In Figure 2 , the input features $v_1,v_2,...,v_{400}$ are constructed and fed to the network as follows :  $v_1,v_3,v_5,...,v_{399}$ : a word embedding vector of the question sentence  $v_2,v_4,...,v_{400}$ : a word embedding vector of the most relevant article sentence A sentence represented by a set of words was converted to a word embedding vector $v_1^{200}$ by using bag-of-words model (BOW) BIBREF18 . BOW model"]}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "predicted_answer": "ASR datasets vary in number of hours, from 150 hours to 300 hours, and AST models were trained on a 20-hour subset of the Spanish-English parallel data. GlobalPhone corpora from seven languages were used, each with around 20 hours of speech. The Fisher corpus contained 160 hours of Spanish telephone speech translated into English text. The AISHELL dataset contained 150 hours of read speech, and a 20-hour subset of the Romanized AISHELL (zh-ai-small) was created by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets). Finally, a model was pre-trained using 300 hours of Switchboard English.", "predicted_evidence": ["BLEU points. This is nearly as much as the 6 point improvement reported by BIBREF4 when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words. This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining. However, there are big differences even amongst the languages with similar amounts of pretraining data. Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure FIGREF11. Therefore, although pretraining data size clearly influences AST performance, this appears to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness. Results", "as future work. Results and Discussion ::: Augmenting the parallel data. Table TABREF16 (top) shows how data augmentation affects the results of the baseline 20h AST system, as well as three of the best-performing pretrained models from Table TABREF7. For these experiments only, we changed the learning rates of the augmented-data systems so that all models took about the same amount of time to train (see Figure FIGREF17). Despite a more aggressive learning schedule, the performance of the augmented-data systems surpasses that of the baseline and pretrained models, even those trained on the largest ASR sets (150-hr Chinese and 300-hr English). For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for", "BLEU score BIBREF13 on four reference translations. Experimental Setup ::: Parallel data. For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech. Experimental Setup ::: Pretraining data. Since we focus on investigating factors that might affect the AST improvements over the baseline when pretraining, we have chosen ASR datasets for pretraining that contrast in the number of hours and/or in the language similarity with Spanish. Statistics for each dataset are in the left half of Table TABREF7, with further details below. To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr),", "weight of 0.6. Results and Discussion ::: Baseline and ASR results. Our baseline 20-hour AST system obtains a BLEU score of 10.3 (Table TABREF7, first row), 0.5 BLEU point lower than that reported by BIBREF4. This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs. WERs for our pre-trained models (Table TABREF7) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone. These are considerably worse than state-of-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST). Results and Discussion ::: Pretraining the", "contains 150 hours of read speech. Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets). Finally, to reproduce one of the experiments from BIBREF4, we pre-trained one model using 300 hours of Switchboard English BIBREF17. This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by BIBREF4, the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages. Experimental Setup ::: Preprocessing. We compute 13-dim MFCCs and cepstral mean and variance", "(code-switching), so pretraining on English may provide an unfair advantage relative to other languages. Experimental Setup ::: Preprocessing. We compute 13-dim MFCCs and cepstral mean and variance normalization along speakers using Kaldi BIBREF12 on our ASR and AST audio. To shorten the training time, we trimmed utterances from the AST data to 16 seconds (or 12 seconds for the 160h augmented dataset). To account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) BIBREF18. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone", "the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model\u2014i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours", "To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important? Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results? To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is"]}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "predicted_answer": "Unanswerable", "predicted_evidence": ["Introduction. Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions BIBREF7. However, building high-quality end-to-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help. A number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario BIBREF8, BIBREF3, BIBREF5 or to pre-train parts of the model before fine-tuning on the end-to-end AST task BIBREF3. Others assume, as we do here, that no additional source language resources are available, in", "BIBREF3, BIBREF5 or to pre-train parts of the model before fine-tuning on the end-to-end AST task BIBREF3. Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option. In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. BIBREF4 showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian BIBREF9 got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining. Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of", "explored what factors help pretraining for low-resource AST. We performed careful comparisons to tease apart the effects of language relatedness and data size, ultimately finding that rather than either of these, the WER of the pre-trained ASR model is likely the best direct predictor of AST performance. Given equivalent amounts of data, we did not find multilingual pretraining to help more than monolingual pretraining, but we did find an added benefit from using speed perturbation to augment the AST data. Finally, analysis of the pretrained models suggests that those models with better WER are transparently encoding more language-universal phonetic information in the later RNN layers, and this appears to help with AST.", "(code-switching), so pretraining on English may provide an unfair advantage relative to other languages. Experimental Setup ::: Preprocessing. We compute 13-dim MFCCs and cepstral mean and variance normalization along speakers using Kaldi BIBREF12 on our ASR and AST audio. To shorten the training time, we trimmed utterances from the AST data to 16 seconds (or 12 seconds for the 160h augmented dataset). To account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) BIBREF18. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone", "for models with better ASR/AST performance (i.e., zh $>$ fr $>$ pt). That is, the later RNN layers more transparently encode language-universal phonetic information. Phone classification accuracy in the RNN layers drops for both English and Spanish after fine-tuning on the AST data. This is slightly surprising for Spanish, since the fine-tuning data (unlike the pretraining data) is actually Spanish speech. However, we hypothesize that for AST, higher layers of the encoder may be recruited more to encode semantic information needed for the translation task, and therefore lose some of the linear separability in the phonetic information. Nevertheless, we still see the same pattern where better end-to-end models have higher classification accuracy in the later layers. Conclusions. This paper explored what factors help pretraining for low-resource AST. We performed careful comparisons to tease apart the effects of language relatedness and data size, ultimately finding that rather than", "To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important? Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results? To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is", "an ASR model, we transfer only its encoder parameters to the AST task. Previous experiments BIBREF4 showed that the encoder accounts for most of the benefits of transferring the parameters. Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments. In addition to pretraining, we experimented with data augmentation. Specifically, we augmented the AST data using Kaldi's BIBREF12 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively. To evaluate ASR performance we compute the word error rate (WER). To evaluate AST performance we calculate the 4-gram BLEU score BIBREF13 on four reference translations. Experimental Setup ::: Parallel data. For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of", "the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation. Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers. Methodology. For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9. After pretraining an ASR model, we transfer only its encoder parameters to the AST task. Previous experiments BIBREF4 showed that the encoder accounts for most of the benefits of transferring the parameters."]}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "predicted_answer": "encoder-decoder model from BIBREF4", "predicted_evidence": ["as future work. Results and Discussion ::: Augmenting the parallel data. Table TABREF16 (top) shows how data augmentation affects the results of the baseline 20h AST system, as well as three of the best-performing pretrained models from Table TABREF7. For these experiments only, we changed the learning rates of the augmented-data systems so that all models took about the same amount of time to train (see Figure FIGREF17). Despite a more aggressive learning schedule, the performance of the augmented-data systems surpasses that of the baseline and pretrained models, even those trained on the largest ASR sets (150-hr Chinese and 300-hr English). For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for", "there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone diacritics. Experimental Setup ::: Model architecture and training. Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) BIBREF21, with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time BIBREF22 as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism BIBREF23 to predict the word at the", "the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation. Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers. Methodology. For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9. After pretraining an ASR model, we transfer only its encoder parameters to the AST task. Previous experiments BIBREF4 showed that the encoder accounts for most of the benefits of transferring the parameters.", "weight of 0.6. Results and Discussion ::: Baseline and ASR results. Our baseline 20-hour AST system obtains a BLEU score of 10.3 (Table TABREF7, first row), 0.5 BLEU point lower than that reported by BIBREF4. This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs. WERs for our pre-trained models (Table TABREF7) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone. These are considerably worse than state-of-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST). Results and Discussion ::: Pretraining the", "with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set. Analyzing the models' representations. Finally, we hope to gain some understanding into why pretraining on ASR helps with AST, and specifically how the neural network representations change during pretraining and fine-tuning. We follow BIBREF34 and BIBREF9, who built diagnostic classifiers BIBREF35 to examine the representation of phonetic information in end-to-end ASR and AST systems, respectively. Unlike BIBREF34, BIBREF9, who used non-linear classifiers, we use a linear classifier to predict phone labels from the internal representations of the trained ASR or AST model. Using a linear", "Unlike BIBREF34, BIBREF9, who used non-linear classifiers, we use a linear classifier to predict phone labels from the internal representations of the trained ASR or AST model. Using a linear classifier allows us to make more precise claims: if the classifier performs better using the representation from a particular layer, we can say that layer represents the phonetic information in a more linearly separable way. Using a nonlinear classifier raises questions about how to choose the complexity of the classifier itself, and therefore makes any results difficult to interpret. We hypothesized that pretraining allows the models to abstract away from nonlinguistic acoustic differences, and to better represent phonetic information: crucially, both in the trained language and in other languages. To test this hypothesis, we used two phone-labelled datasets distinct from all our ASR and AST datasets: the English TIMIT corpus (a language different to all of our trained models, with hand-labeled", "BLEU points. This is nearly as much as the 6 point improvement reported by BIBREF4 when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words. This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining. However, there are big differences even amongst the languages with similar amounts of pretraining data. Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure FIGREF11. Therefore, although pretraining data size clearly influences AST performance, this appears to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness. Results", "explored what factors help pretraining for low-resource AST. We performed careful comparisons to tease apart the effects of language relatedness and data size, ultimately finding that rather than either of these, the WER of the pre-trained ASR model is likely the best direct predictor of AST performance. Given equivalent amounts of data, we did not find multilingual pretraining to help more than monolingual pretraining, but we did find an added benefit from using speed perturbation to augment the AST data. Finally, analysis of the pretrained models suggests that those models with better WER are transparently encoding more language-universal phonetic information in the later RNN layers, and this appears to help with AST."]}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "predicted_answer": "Mandarin Chinese, Croatian, Czech, French, Polish, Portuguese, and Swedish for the GlobalPhone corpora. Spanish and English for the Fisher corpus. Mandarin Chinese for the AISHELL-1 corpus.", "predicted_evidence": ["To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related\u2014especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries. To explore the effects of using a large amount of pretraining data from an unrelated language, we used the AISHELL-1 corpus of Mandarin Chinese BIBREF16, which contains 150 hours of read speech. Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare", "BLEU score BIBREF13 on four reference translations. Experimental Setup ::: Parallel data. For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech. Experimental Setup ::: Pretraining data. Since we focus on investigating factors that might affect the AST improvements over the baseline when pretraining, we have chosen ASR datasets for pretraining that contrast in the number of hours and/or in the language similarity with Spanish. Statistics for each dataset are in the left half of Table TABREF7, with further details below. To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr),", "BIBREF3, BIBREF5 or to pre-train parts of the model before fine-tuning on the end-to-end AST task BIBREF3. Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option. In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. BIBREF4 showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian BIBREF9 got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining. Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of", "to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness. Results and Discussion ::: Multilingual pretraining. Although our main focus is monolingual pretraining, we also looked briefly at multilingual pretraining, inspired by recent work on multilingual ASR BIBREF28, BIBREF29 and evidence that multilingual pretraining followed by fine-tuning on a distinct target language can improve ASR on the target language BIBREF10, BIBREF30, BIBREF31. These experiments did not directly compare pretraining using a similar amount of monolingual data, but such a comparison was done by BIBREF32, BIBREF33 in their work on learning feature representations for a target language with no transcribed data. They found a benefit for multilingual vs monolingual pretraining given the same amount of data. Following up on this work, we tried pretraining using 124 hours of multilingual", "(code-switching), so pretraining on English may provide an unfair advantage relative to other languages. Experimental Setup ::: Preprocessing. We compute 13-dim MFCCs and cepstral mean and variance normalization along speakers using Kaldi BIBREF12 on our ASR and AST audio. To shorten the training time, we trimmed utterances from the AST data to 16 seconds (or 12 seconds for the 160h augmented dataset). To account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) BIBREF18. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone", "Introduction. Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions BIBREF7. However, building high-quality end-to-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help. A number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario BIBREF8, BIBREF3, BIBREF5 or to pre-train parts of the model before fine-tuning on the end-to-end AST task BIBREF3. Others assume, as we do here, that no additional source language resources are available, in", "contains 150 hours of read speech. Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets). Finally, to reproduce one of the experiments from BIBREF4, we pre-trained one model using 300 hours of Switchboard English BIBREF17. This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by BIBREF4, the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages. Experimental Setup ::: Preprocessing. We compute 13-dim MFCCs and cepstral mean and variance", "these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result. In related work on multilingual pretraining for low-resource ASR, Adams et al. BIBREF10 showed that pre-training on more languages helps, but it is not clear whether the improvement is due to including more languages, or just more data. To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a"]}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "predicted_answer": "Unanswerable", "predicted_evidence": ["\\right\\rbrace .$$   (Eq. 5)  We treat the embedding of the clean input $x$ as real data and the embedding of $\\widetilde{x}$ , which can either be augmented from $x$ or drawn from a different modality, as being fake. And so, as GAN training progresses, the encoder $g_\\theta $ should learn to remove extraneous information to ASR to be able to fool the discriminator. In practice, we found that including a random Gaussian noise $\\varepsilon $ to the input prior of the generator helps improve training. Also, weights in the parameter set $\\mathcal {W}$ should be clipped to ensure the duality of ( 5 ) holds up to a constant multiple BIBREF16 . The adapted WGAN training procedure is detailed in Algorithm \"EXPERIMENTAL SETUP\" . Corpora and Tasks. We evaluated the enhancer framework on the Wall Street Journal (WSJ) corpus with simulated far-field effects. The dev93 and eval92 sets were used for hyperparameter selection and evaluation respectively. The reverberant speech is generated with room", "using the Adam optimizer. We evaluate all models on both clean and far-field test sets. To study the effects of data augmentation, we train a new seq-to-seq model with the same architecture and training procedure as the baseline. However this time, in each epoch, we randomly select 40% of the training utterances and apply the train RIRs to them (in our previous experiments we had observed that 40% augmentation results in the best validation performance). For the enhancer models, $\\lambda $ in Equation 2 was tuned over the dev set by doing a logarithmic sweep in [0.01, 10]. $\\lambda = 1$ gave the best performance. We use Algorithm \"EXPERIMENTAL SETUP\" to train the WGAN enhancer. The clipping parameter was 0.05 and $\\varepsilon $ was random normal with 0.001 standard deviation. We found that having a schedule for $n_\\text{critic}$ was crucial. Namely, we do not update the encoder parameters with WGAN gradients for the first 3000 steps. Then, we use the normal $n_\\text{critic}=5$ . We", "is documented in Section \"RELATED WORK\" . Section \"ROBUST ASR\" defines our notations and details the robust ASR GAN. Section \"EXPERIMENTAL SETUP\" explains the experimental setup. Section \"RESULTS\" shows results on the Wall Street Journal (WSJ) dataset with simulated far-field effects. Finishing thoughts are found in Section \"CONCLUSION\" . RELATED WORK. A vast majority of work in robust ASR deals with reverberations and ambient noise; BIBREF2 provides an extensive survey in this effort. One of the most effective approaches in this variability is to devise a strong front-end such as the weighted prediction error (WPE) speech dereverberation BIBREF6 , BIBREF7 and train the resulting neural network with realistic augmented data BIBREF8 , BIBREF9 . A shift from more traditional signal processing techniques to more modern, data-driven methods was seen when the denoising autoencoder BIBREF10 was employed to induce invariance to reverberations BIBREF11 . This is novel in that the autoencoder", "for an ASR model to generalize from homogeneous near-field audio to far-field audio. To overcome this, we train a stronger baseline with simulated far-field audio examples. This model had the same architecture but 40% of the examples that the model was trained on were convolved with a randomly chosen room impulse response during training. We can see from Table 3 that simple data augmentation can significantly improve performance on far-field audio without compromising the performance on near-field audio, implying that seq-to-seq models have a strong ability to learn from far-field examples. Even with data augmentation, however, there is still a large gap between the WERs on near-field and far-field test sets. The bottom two rows of Table 3 show the performance of the methods introduced in this paper on the same test sets. An $L^1$ -distance penalty can lower the test set WER by 1.32% absolute. Using a GAN enhancer can reduce the WER by an additional 1.07%. Overall, the gap between", "Journal (WSJ) corpus with simulated far-field effects. The dev93 and eval92 sets were used for hyperparameter selection and evaluation respectively. The reverberant speech is generated with room impulse response (RIR) augmentation as in BIBREF18 , where each audio is convolved with a randomly chosen RIR signal. The clean and far-field audio durations are kept the same with valid convolution so that the encoder distance enhancer can be applied. We collected 1088 impulse responses, using a linear array of 8 microphones, 120 and 192 of which were held out for development and evaluation. The speaker was placed in a variety of configurations, ranging from 1 to 3 meters distance and 60 to 120 degrees inclination with respect to the array, for 20 different rooms. Mel spectrograms of 20 ms samples with 10 ms stride and 40 bins were used as input features to all of our baseline and enhancer models. Network Architecture. For the acoustic model, we used the sequence-to-sequence framework with", "with 10 ms stride and 40 bins were used as input features to all of our baseline and enhancer models. Network Architecture. For the acoustic model, we used the sequence-to-sequence framework with soft attention based on BIBREF5 . The architecture of the encoder is described in Table 1 . The decoder consisted of a single 256 dimensional GRU layer with a hybrid attention mechanism similar to the models described in BIBREF19 . The discriminator network of the WGAN enhancer is described in Table 2 . All convolutional layers use leaky ReLU activation BIBREF20 with 0.2 slope for the leak, and batch normalization BIBREF21 . Training. To establish a baseline, in the first experiment, we trained a simple attention based seq-to-seq model. All the seq-to-seq networks in our experiments were trained using the Adam optimizer. We evaluate all models on both clean and far-field test sets. To study the effects of data augmentation, we train a new seq-to-seq model with the same architecture and", "in this paper on the same test sets. An $L^1$ -distance penalty can lower the test set WER by 1.32% absolute. Using a GAN enhancer can reduce the WER by an additional 1.07%. Overall, the gap between near-field and far-field performance decreases by almost 27% compared to the model that only uses data augmentation. An additional benefit of our methods is that the $L^1$ -distance penalty and GAN loss function act as regularizers which reduce generalization error on near field data. The enhancer models have lower WERs even on near-field data compared to the baseline models. CONCLUSION. We introduced a GAN-based framework to train robust ASR models in a scalable, data-driven way, and showed that inducing invariance at the encoder embedding level considerably improves the recognition of simulated far-field speech by vanilla seq-to-seq models. This method has effectively imbued the seq-to-seq encoder with a far-field front-end. We anticipate that coupling the new framework with specialized", "Introduction. Automatic speech recognition (ASR) is becoming increasingly more integral in our day-to-day lives enabling virtual assistants and smart speakers like Siri, Google Now, Cortana, Amazon Echo, Google Home, Apple HomePod, Microsoft Invoke, Baidu Duer and many more. While recent breakthroughs have tremendously improved ASR performance BIBREF0 , BIBREF1 these models still suffer considerable degradation from reasonable variations in reverberations, ambient noise, accents and Lombard reflexes that humans have little or no issue recognizing. Most of these problems can be mitigated by training the models on a large volume of data that exemplify these effects. However, in the case of non-stationary processes, such as accents, accurate data augmentation is most likely infeasible, and in general, collecting high quality datasets can be expensive and time-consuming. Past robust ASR literature has considered hand-engineered front-ends and data-driven approaches in an attempt to"]}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "predicted_answer": "Na\u00efve Bayes, J48, and random forest classifiers", "predicted_evidence": ["on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets. Conclusions and Future Work. We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of", "problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use. Obviously, this is very preliminary work. Our work established baselines for automated extraction of condition-action rules from medical guidelines, but its performance is still inferior to a collection of manually created extraction rules. To close this gap we are currently augmenting our model with semantic information along the lines of BIBREF7 and BIBREF4 . In addition, we are beginning to experiment with some discourse relations \u2013 these are important, for example, in understanding of lists and tables. We also plan to make our annotated datasets more convenient to use by re-annotating them with", "(NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))))) (. ?)))\" is the constituent parsed tree of \"In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?\". \"(PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension)))))\" and \"(PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))\" are two candidate condition parts in this example. We created features for our model based on POS tags and their combinations. The sets of features and the combinations are learned automatically from annotated examples. We used these novel features to make our model more domain-independent. For each sentence, we extracted POS tags, sequences of 3 POS tags, and combination of all POS tags of candidate conditions as features. For example, \"PP IN NP NP NNS PP IN NP NN PPINNP INNPNP NPNPNNS NPNNSPP NNSPPIN PPINNP INNPNN PPINNPNPNNSPPINNPNN PP IN NP", "They include document-centric models, decision trees and probabilistic models, and \"Task-Network Models\"(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing networks of clinical actions and decisions that unfold over time. Serban et. al BIBREF6 developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Kaiser et. al BIBREF7 developed a method to identify activities to be performed during a treatment which are described in a guideline document. They used relations of the UMLS Semantic Network BIBREF8 to identify these activities in a guideline document. Wenzina and Kaiser BIBREF4 developed a rule-based method to automatically identifying conditional activities in guideline documents.They achieved a recall of 75% and a precision of 88% on chapter 4 of asthma guidelines which was mentioned before. Condition-Action", "{then} action\u201d. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d, we have a condition-action sentence without an \"{if}\" term. We propose a supervised machine learning model classifying sentences as to whether they express a condition or not. After we determine a sentence contain a condition, we use natural language processing and information extraction tools to extract conditions and resulting activities. With the help of a domain expert, we annotated three sets of guidelines to create gold standards to measure the performance of our condition-action extracting models. The sets of guidelines are: hypertension BIBREF1 , chapter4 of asthma BIBREF2 , and rhinosinusitis BIBREF3 . Chapter 4 of asthma guidelines was selected for comparison with prior work of Wenzina and Kaiser BIBREF4 . We have annotated the guidelines for the conditions,", "asthma, and rhinosinusitis guidelines and gold standard datasets were applied to evaluate our model. Since two of these annotated corpora are new, our model is establishing a baseline. The asthma corpus was investigated previously by BIBREF4 . We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than", "or subordinating conjunction. \"To\" is tagged as \"TO\" and \"when\" and \"which\" are tagged as \"WHADV\". We used regular expressions to find those parses which are promising candidates for extraction of condition-action pairs; for example, we selected sentences which include these tags: IN, TO and WHADVP. We extracted part of speech (POS) tags as our features for our model. Each candidate sentence has at least one candidate condition part. We extract these parts by regular expressions. Each part of sentence which starts with below patterns is a candidate condition part: \"\\((SBAR|PP) \\(IN\" \"\\(SBAR \\(WHADVP\" \"\\(PP \\(TO\" For example, \"(ROOT (S (PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension))))) (, ,) (VP (VBZ does) (S (VP (VBG initiating) (S (NP (NP (JJ antihypertensive) (JJ pharmacologic) (NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))))) (. ?)))\" is the constituent parsed tree of \"In adults with", "information (this work is ongoing). Classification. Most of the condition-action sentences have a modifier in the sentences. For example, in \"In the population aged 18 years or older with CKD and hypertension, initial (or add-on) antihypertensive treatment should include an ACEI or ARB to improve kidney outcomes\", we have \"the population aged 18 years or older with CKD and hypertension\" as a condition and \"{in}\" is the modifier. \"If\", \"in\", \"for\", \"to\", \"which\", and \"when\" are the most frequent modifiers in our guidelines. We used CoreNLP BIBREF9 Shift-Reduce Constituency Parser to parse sentences in guidelines. As we mentioned, \"if\", \"in\", \"for\", \"to\", \"which\", and \"when\" are the most frequent modifiers in our guidelines. \"If\", \"in\", and \"for\" are tagged as \"IN\" which represents preposition or subordinating conjunction. \"To\" is tagged as \"TO\" and \"when\" and \"which\" are tagged as \"WHADV\". We used regular expressions to find those parses which are promising candidates for extraction of"]}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "predicted_answer": "The performance is still inferior to a collection of manually created extraction rules.", "predicted_evidence": ["on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets. Conclusions and Future Work. We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of", "(NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))))) (. ?)))\" is the constituent parsed tree of \"In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?\". \"(PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension)))))\" and \"(PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))\" are two candidate condition parts in this example. We created features for our model based on POS tags and their combinations. The sets of features and the combinations are learned automatically from annotated examples. We used these novel features to make our model more domain-independent. For each sentence, we extracted POS tags, sequences of 3 POS tags, and combination of all POS tags of candidate conditions as features. For example, \"PP IN NP NP NNS PP IN NP NN PPINNP INNPNP NPNPNNS NPNNSPP NNSPPIN PPINNP INNPNN PPINNPNPNNSPPINNPNN PP IN NP", "problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use. Obviously, this is very preliminary work. Our work established baselines for automated extraction of condition-action rules from medical guidelines, but its performance is still inferior to a collection of manually created extraction rules. To close this gap we are currently augmenting our model with semantic information along the lines of BIBREF7 and BIBREF4 . In addition, we are beginning to experiment with some discourse relations \u2013 these are important, for example, in understanding of lists and tables. We also plan to make our annotated datasets more convenient to use by re-annotating them with", "{then} action\u201d. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d, we have a condition-action sentence without an \"{if}\" term. We propose a supervised machine learning model classifying sentences as to whether they express a condition or not. After we determine a sentence contain a condition, we use natural language processing and information extraction tools to extract conditions and resulting activities. With the help of a domain expert, we annotated three sets of guidelines to create gold standards to measure the performance of our condition-action extracting models. The sets of guidelines are: hypertension BIBREF1 , chapter4 of asthma BIBREF2 , and rhinosinusitis BIBREF3 . Chapter 4 of asthma guidelines was selected for comparison with prior work of Wenzina and Kaiser BIBREF4 . We have annotated the guidelines for the conditions,", "They include document-centric models, decision trees and probabilistic models, and \"Task-Network Models\"(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing networks of clinical actions and decisions that unfold over time. Serban et. al BIBREF6 developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Kaiser et. al BIBREF7 developed a method to identify activities to be performed during a treatment which are described in a guideline document. They used relations of the UMLS Semantic Network BIBREF8 to identify these activities in a guideline document. Wenzina and Kaiser BIBREF4 developed a rule-based method to automatically identifying conditional activities in guideline documents.They achieved a recall of 75% and a precision of 88% on chapter 4 of asthma guidelines which was mentioned before. Condition-Action", "asthma, and rhinosinusitis guidelines and gold standard datasets were applied to evaluate our model. Since two of these annotated corpora are new, our model is establishing a baseline. The asthma corpus was investigated previously by BIBREF4 . We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than", "or subordinating conjunction. \"To\" is tagged as \"TO\" and \"when\" and \"which\" are tagged as \"WHADV\". We used regular expressions to find those parses which are promising candidates for extraction of condition-action pairs; for example, we selected sentences which include these tags: IN, TO and WHADVP. We extracted part of speech (POS) tags as our features for our model. Each candidate sentence has at least one candidate condition part. We extract these parts by regular expressions. Each part of sentence which starts with below patterns is a candidate condition part: \"\\((SBAR|PP) \\(IN\" \"\\(SBAR \\(WHADVP\" \"\\(PP \\(TO\" For example, \"(ROOT (S (PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension))))) (, ,) (VP (VBZ does) (S (VP (VBG initiating) (S (NP (NP (JJ antihypertensive) (JJ pharmacologic) (NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))))) (. ?)))\" is the constituent parsed tree of \"In adults with", "tags, and combination of all POS tags of candidate conditions as features. For example, \"PP IN NP NP NNS PP IN NP NN PPINNP INNPNP NPNPNNS NPNNSPP NNSPPIN PPINNP INNPNN PPINNPNPNNSPPINNPNN PP IN NP NN PPINNP INNPNN PPINNPNN PP IN NP JJ NN NNS PPINNP INNPJJ NPJJNN JJNNNNS PPINNPJJNNNNS\" represents \"In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?\" in our model. Note that the glued together part of speech tags are not a formatting error but features automatically derived by our model (from consecutive part of speech tags). Gold Standard Datasets. We use three medical guidelines documents to create gold standard datasets. They provide statements, tables, and figures about hypertension, rhinosinusitis, and asthma. The creation of the gold standard datasets is described below in detail. Our data preparation process proceeded as follows: We started by converting the guidelines from PDF or html to text"]}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "predicted_answer": "Unanswerable", "predicted_evidence": ["tags, and combination of all POS tags of candidate conditions as features. For example, \"PP IN NP NP NNS PP IN NP NN PPINNP INNPNP NPNPNNS NPNNSPP NNSPPIN PPINNP INNPNN PPINNPNPNNSPPINNPNN PP IN NP NN PPINNP INNPNN PPINNPNN PP IN NP JJ NN NNS PPINNP INNPJJ NPJJNN JJNNNNS PPINNPJJNNNNS\" represents \"In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?\" in our model. Note that the glued together part of speech tags are not a formatting error but features automatically derived by our model (from consecutive part of speech tags). Gold Standard Datasets. We use three medical guidelines documents to create gold standard datasets. They provide statements, tables, and figures about hypertension, rhinosinusitis, and asthma. The creation of the gold standard datasets is described below in detail. Our data preparation process proceeded as follows: We started by converting the guidelines from PDF or html to text", "asthma. The creation of the gold standard datasets is described below in detail. Our data preparation process proceeded as follows: We started by converting the guidelines from PDF or html to text format, editing sentences only to manage conversion errors, the majority of which were bullet points. Tables and some figures pose a problem, and we are simply treating them as unstructured text. We are not dealing at this time with the ambiguities introduced by this approach; we do have plans to address it in future work. Using regular expressions, as described above, we selected candidate sentences from text files. Note that candidate sentences do not always include a modifier such as \"if\" or \"in\". For example, in \"Patients on long-term steroid tablets (e.g. longer than three months) or requiring frequent courses of steroid tablets (e.g. three to four per year) will be at risk of systemic side-effects\", there is no modifier in the sentence. The annotation of the guidelines text (the next", "asthma, and rhinosinusitis guidelines and gold standard datasets were applied to evaluate our model. Since two of these annotated corpora are new, our model is establishing a baseline. The asthma corpus was investigated previously by BIBREF4 . We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than", "Introduction. Clinical decision-support system (CDSS) is any computer system intended to provide decision support for healthcare professionals, and using clinical data or knowledge BIBREF0 . The classic problem of diagnosis is only one of the clinical decision problems. Deciding which questions to ask, tests to order, procedures to perform, treatment to indicate, or which alternative medical care to try, are other examples of clinical decisions. CDSSs generally fall into two categories BIBREF0  Most of the questions physicians need to consult about with CDSSs are from the latter category. Medical guidelines (also known as clinical guidelines, clinical protocols or clinical practice guidelines) are most useful at the point of care and answering to \"what to do\" questions. Medical guidelines are systematically developed statements to assist with practitioners' and patients' decisions. They establish criteria regarding diagnosis, management, and treatment in specific areas of healthcare.", "CA if the statement is a condition-action sentence; and CC (condition-consequence) if the statement has a condition which has a non-action consequence. For a CDSS, to determine both \"what is true\" about a patient and \"what to do\" with a patient, CC and CA statements can be merged to one category. There are limitations in this specification of classification categories. For example, guidelines may contain statements with a condition referring to a consequence in another statement. Or, we can see condition and effect in two different sentences: \"However, there are some cases for which the results for black persons were different from the results for the general population (question 3, evidence statements 2, 10, and 17). In those cases, separate evidence statements were developed.\" In this work we focus only on statements that follow the above sentence categorization rules. This allows us to make clear comparison to prior work e.g. by Wenzina and Kaiser BIBREF4 . They annotated chapter 4", "with some discourse relations \u2013 these are important, for example, in understanding of lists and tables. We also plan to make our annotated datasets more convenient to use by re-annotating them with standard annotation tools e.g. BRAT BIBREF11 .", "problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use. Obviously, this is very preliminary work. Our work established baselines for automated extraction of condition-action rules from medical guidelines, but its performance is still inferior to a collection of manually created extraction rules. To close this gap we are currently augmenting our model with semantic information along the lines of BIBREF7 and BIBREF4 . In addition, we are beginning to experiment with some discourse relations \u2013 these are important, for example, in understanding of lists and tables. We also plan to make our annotated datasets more convenient to use by re-annotating them with", "on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets. Conclusions and Future Work. We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of"]}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "predicted_answer": "Condition-action sentence", "predicted_evidence": ["access to condition-action statements describing under what circumstances an action can be performed. Medical guidelines contain many condition-action statements. Condition-action statements provide information about expected process flow. If a guideline-based CDSS could extract and formalize these statements, it could help practitioners in the decision-making process. For example, it could help automatically asses the relationship between therapies, guidelines and outcomes, and in particular could help the impact of changing guidelines. However, completely automated extraction of condition-action statements does not seem possible. This is due among other things to the variety of linguistic expressions used in condition-action sentences. For example, they are not always in form of \"{if} condition {then} action\u201d. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's", "identifying conditional activities in guideline documents.They achieved a recall of 75% and a precision of 88% on chapter 4 of asthma guidelines which was mentioned before. Condition-Action Extraction. Medical guidelines\u2019 condition-action statements provide information to determine \"what to do\" with a patient. Other types of consequences of a condition in a sentence may help practitioner to find what is true about a patient. In this paper, we propose an automated process to find and extract condition-action statements from medical guidelines. We employed NLP tools and concepts in the process to achieve more general models. We define the task as classification task. Given an input statement, classify it to one of the three categories: NC (no condition) if the statement doesn\u2019t have a condition; CA if the statement is a condition-action sentence; and CC (condition-consequence) if the statement has a condition which has a non-action consequence. For a CDSS, to determine both \"what is", "{then} action\u201d. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d, we have a condition-action sentence without an \"{if}\" term. We propose a supervised machine learning model classifying sentences as to whether they express a condition or not. After we determine a sentence contain a condition, we use natural language processing and information extraction tools to extract conditions and resulting activities. With the help of a domain expert, we annotated three sets of guidelines to create gold standards to measure the performance of our condition-action extracting models. The sets of guidelines are: hypertension BIBREF1 , chapter4 of asthma BIBREF2 , and rhinosinusitis BIBREF3 . Chapter 4 of asthma guidelines was selected for comparison with prior work of Wenzina and Kaiser BIBREF4 . We have annotated the guidelines for the conditions,", "we focus only on statements that follow the above sentence categorization rules. This allows us to make clear comparison to prior work e.g. by Wenzina and Kaiser BIBREF4 . They annotated chapter 4 of asthma and other guidelines. They used information extraction rules and semantic pattern rules to extract conditional activities, condition-action statements. We use POS tags as features in the classification models. In our opinion, using POS tags instead of semantic pattern rules makes our model more domain-independent, and therefore more suitable for establishing baselines, not only for text mining of medical guidelines but also in other domains, such as text mining of business rules. But we also expect to improve the performance of our extraction programs by adding semantic and discourse information (this work is ongoing). Classification. Most of the condition-action sentences have a modifier in the sentences. For example, in \"In the population aged 18 years or older with CKD and", "information (this work is ongoing). Classification. Most of the condition-action sentences have a modifier in the sentences. For example, in \"In the population aged 18 years or older with CKD and hypertension, initial (or add-on) antihypertensive treatment should include an ACEI or ARB to improve kidney outcomes\", we have \"the population aged 18 years or older with CKD and hypertension\" as a condition and \"{in}\" is the modifier. \"If\", \"in\", \"for\", \"to\", \"which\", and \"when\" are the most frequent modifiers in our guidelines. We used CoreNLP BIBREF9 Shift-Reduce Constituency Parser to parse sentences in guidelines. As we mentioned, \"if\", \"in\", \"for\", \"to\", \"which\", and \"when\" are the most frequent modifiers in our guidelines. \"If\", \"in\", and \"for\" are tagged as \"IN\" which represents preposition or subordinating conjunction. \"To\" is tagged as \"TO\" and \"when\" and \"which\" are tagged as \"WHADV\". We used regular expressions to find those parses which are promising candidates for extraction of", "problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use. Obviously, this is very preliminary work. Our work established baselines for automated extraction of condition-action rules from medical guidelines, but its performance is still inferior to a collection of manually created extraction rules. To close this gap we are currently augmenting our model with semantic information along the lines of BIBREF7 and BIBREF4 . In addition, we are beginning to experiment with some discourse relations \u2013 these are important, for example, in understanding of lists and tables. We also plan to make our annotated datasets more convenient to use by re-annotating them with", "BIBREF2 , and rhinosinusitis BIBREF3 . Chapter 4 of asthma guidelines was selected for comparison with prior work of Wenzina and Kaiser BIBREF4 . We have annotated the guidelines for the conditions, consequences, modifiers of conditions, and type of consequences. These annotate sets of guidelines are available for experiments https://www.dropbox.com/. Related Work. We will briefly discuss the modeling and annotation of condition-action for medical usage in this section. Our corpus and method of identifying conditions in clinical guidelines is explained in section 3. Research on CIGs started about 20 years ago and became more popular in the late-1990s and early 2000s. Different approaches have been developed to represent and execute clinical guidelines over patient-specific clinical data. They include document-centric models, decision trees and probabilistic models, and \"Task-Network Models\"(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing", "on respective datasets. The results show that generally random forest classifier seems to work best in extracting Condition-Action statements. Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets. Conclusions and Future Work. We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of"]}
{"question_id": "97708d93bccc832ea671dc31a76dad6a121fcd60", "predicted_answer": "Word-overlap Metrics (WOMs), Semantic Similarity (sim), a large number of 21 automated metrics including novel grammar-based ones", "predicted_evidence": ["the above previous work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics. We consider this paper to be the most complete study to date, across metrics, systems, datasets and domains, focusing on recent advances in data-driven NLG. In contrast to previous work, we are the first to:  $\\bullet $ Target end-to-end data-driven NLG, where we compare 3 different approaches. In contrast to NLG methods evaluated in previous work, our systems can produce ungrammatical output by (a) generating word-by-word, and (b) learning from noisy data.  $\\bullet $ Compare a large number of 21 automated metrics, including novel grammar-based ones.  $\\bullet $ Report results on two different domains and three different datasets, which allows us to draw more general conclusions.  $\\bullet $ Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the", "and 8 attributes. Word-based Metrics (WBMs). NLG evaluation has borrowed a number of automatic metrics from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to ground-truth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is. The following order reflects the degree these metrics move from simple $n$ -gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words.  $\\bullet $ Word-overlap Metrics (WOMs): We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .  $\\bullet $ Semantic Similarity (sim): We calculate the Semantic Text Similarity measure designed by BIBREF24 . This measure is based on distributional similarity and Latent Semantic Analysis (LSA)", "Introduction. Automatic evaluation measures, such as bleu BIBREF0 , are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012\u20132015 relies on automatic metrics BIBREF1 . Automatic evaluation is popular because it is cheaper and faster to run than human evaluation, and it is needed for automatic benchmarking and tuning of algorithms. The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences. This is rarely the case, as shown by various studies in NLG ( BIBREF2 ; BIBREF3 , BIBREF4 ), as well as in related fields, such as dialogue systems BIBREF5 , machine translation (MT) BIBREF6 , and image captioning BIBREF7 , BIBREF8 . This paper follows on from the above previous work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics. We consider this paper to be the most complete study to", "make two strong assumptions: They treat human-generated references as a gold standard, which is correct and complete. We argue that these assumptions are invalid for corpus-based NLG, especially when using crowdsourced datasets. Grammar-based metrics, on the other hand, do not rely on human-generated references and are not influenced by their quality. However, these metrics can be easily manipulated with grammatically correct and easily readable output that is unrelated to the input. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. BIBREF40 ; extrinsic evaluation metrics, such as NLG's contribution to task success, e.g. BIBREF41 , BIBREF42 , BIBREF43 ; building", "`accuracy', `adequacy', or `correctness'). In general, correlations reported by previous work range from weak to strong. The results confirm that metrics can be reliable indicators at system-level BIBREF4 , while they perform less reliably at sentence-level BIBREF2 . Also, the results show that the metrics capture realization better than sentence planning. There is a general trend showing that best-performing metrics tend to be the more complex ones, combining word-overlap, semantic similarity and term frequency weighting. Note, however, that the majority of previous works do not report whether any of the metric correlations are significantly different from each other. Conclusions. This paper shows that state-of-the-art automatic evaluation metrics for NLG systems do not sufficiently reflect human ratings, which stresses the need for human evaluations. This result is opposed to the current trend of relying on automatic evaluation identified in BIBREF1 . A detailed error analysis", "assessing output quality within the dialogue context, e.g. BIBREF40 ; extrinsic evaluation metrics, such as NLG's contribution to task success, e.g. BIBREF41 , BIBREF42 , BIBREF43 ; building discriminative models, e.g. BIBREF34 , BIBREF36 ; or reference-less quality prediction as used in MT, e.g. BIBREF33 . We see our paper as a first step towards reference-less evaluation for NLG by introducing grammar-based metrics. In current work BIBREF44 , we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Finally, note that the datasets considered in this study are fairly small (between 404 and 2.3k human references per domain). To remedy this, systems train on de-lexicalised versions BIBREF10 , which bears the danger of ungrammatical lexicalisation BIBREF13 and a possible overlap between testing and training set BIBREF15 . There are", "us to draw more general conclusions.  $\\bullet $ Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level.  $\\bullet $ Make all associated code and data publicly available, including detailed analysis results. End-to-End NLG Systems. In this paper, we focus on recent end-to-end, data-driven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data ( BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 ; BIBREF13 , BIBREF13 ; BIBREF14 , BIBREF15 ). These approaches do not require costly semantic alignment between Meaning Representations (MR) and human references (also referred to as \u201cground truth\" or \u201ctargets\"), but are based on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. BIBREF16 , and as such, enable rapid development of NLG components in new domains. In particular, we", "reflect human ratings, which stresses the need for human evaluations. This result is opposed to the current trend of relying on automatic evaluation identified in BIBREF1 . A detailed error analysis suggests that automatic metrics are particularly weak in distinguishing outputs of medium and good quality, which can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics can be useful for error analysis by helping to find cases where the system is performing poorly. In addition, we find reliable results on system-level, which suggests that metrics can be useful for system development. Future Directions. Word-based metrics make two strong assumptions: They treat human-generated references as a gold standard, which is correct and complete. We argue that these assumptions are invalid for corpus-based NLG, especially"]}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "predicted_answer": "End-to-end, data-driven NLG methods", "predicted_evidence": ["us to draw more general conclusions.  $\\bullet $ Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level.  $\\bullet $ Make all associated code and data publicly available, including detailed analysis results. End-to-End NLG Systems. In this paper, we focus on recent end-to-end, data-driven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data ( BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 ; BIBREF13 , BIBREF13 ; BIBREF14 , BIBREF15 ). These approaches do not require costly semantic alignment between Meaning Representations (MR) and human references (also referred to as \u201cground truth\" or \u201ctargets\"), but are based on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. BIBREF16 , and as such, enable rapid development of NLG components in new domains. In particular, we", "can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. BIBREF16 , and as such, enable rapid development of NLG components in new domains. In particular, we compare the performance of the following systems:  $\\bullet $ rnnlg: The system by BIBREF10 uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface realisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far.  $\\bullet $ TGen: The system by BIBREF9 learns to incrementally generate deep-syntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module.  $\\bullet $ lols: The system by BIBREF15 learns sentence planning and surface realisation using Locally Optimal Learning to Search (lols), an imitation learning framework", "the above previous work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics. We consider this paper to be the most complete study to date, across metrics, systems, datasets and domains, focusing on recent advances in data-driven NLG. In contrast to previous work, we are the first to:  $\\bullet $ Target end-to-end data-driven NLG, where we compare 3 different approaches. In contrast to NLG methods evaluated in previous work, our systems can produce ungrammatical output by (a) generating word-by-word, and (b) learning from noisy data.  $\\bullet $ Compare a large number of 21 automated metrics, including novel grammar-based ones.  $\\bullet $ Report results on two different domains and three different datasets, which allows us to draw more general conclusions.  $\\bullet $ Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the", "and 8 attributes. Word-based Metrics (WBMs). NLG evaluation has borrowed a number of automatic metrics from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to ground-truth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is. The following order reflects the degree these metrics move from simple $n$ -gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words.  $\\bullet $ Word-overlap Metrics (WOMs): We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .  $\\bullet $ Semantic Similarity (sim): We calculate the Semantic Text Similarity measure designed by BIBREF24 . This measure is based on distributional similarity and Latent Semantic Analysis (LSA)", "make two strong assumptions: They treat human-generated references as a gold standard, which is correct and complete. We argue that these assumptions are invalid for corpus-based NLG, especially when using crowdsourced datasets. Grammar-based metrics, on the other hand, do not rely on human-generated references and are not influenced by their quality. However, these metrics can be easily manipulated with grammatically correct and easily readable output that is unrelated to the input. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. BIBREF40 ; extrinsic evaluation metrics, such as NLG's contribution to task success, e.g. BIBREF41 , BIBREF42 , BIBREF43 ; building", "assessing output quality within the dialogue context, e.g. BIBREF40 ; extrinsic evaluation metrics, such as NLG's contribution to task success, e.g. BIBREF41 , BIBREF42 , BIBREF43 ; building discriminative models, e.g. BIBREF34 , BIBREF36 ; or reference-less quality prediction as used in MT, e.g. BIBREF33 . We see our paper as a first step towards reference-less evaluation for NLG by introducing grammar-based metrics. In current work BIBREF44 , we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Finally, note that the datasets considered in this study are fairly small (between 404 and 2.3k human references per domain). To remedy this, systems train on de-lexicalised versions BIBREF10 , which bears the danger of ungrammatical lexicalisation BIBREF13 and a possible overlap between testing and training set BIBREF15 . There are", "Introduction. Automatic evaluation measures, such as bleu BIBREF0 , are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012\u20132015 relies on automatic metrics BIBREF1 . Automatic evaluation is popular because it is cheaper and faster to run than human evaluation, and it is needed for automatic benchmarking and tuning of algorithms. The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences. This is rarely the case, as shown by various studies in NLG ( BIBREF2 ; BIBREF3 , BIBREF4 ), as well as in related fields, such as dialogue systems BIBREF5 , machine translation (MT) BIBREF6 , and image captioning BIBREF7 , BIBREF8 . This paper follows on from the above previous work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics. We consider this paper to be the most complete study to", ".  $\\bullet $ Semantic Similarity (sim): We calculate the Semantic Text Similarity measure designed by BIBREF24 . This measure is based on distributional similarity and Latent Semantic Analysis (LSA) and is further complemented with semantic relations extracted from WordNet. Grammar-based metrics (GBMs). Grammar-based measures have been explored in related fields, such as MT BIBREF25 or grammatical error correction BIBREF26 , and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important properties of texts here \u2013 readability and grammaticality:  $\\bullet $ Readability quantifies the difficulty with which a reader understands a text, as used for e.g. evaluating summarisation BIBREF27 or text simplification BIBREF28 . We measure readability by the Flesch Reading Ease score (re) BIBREF29 , which calculates a ratio between the number of characters per sentence, the number"]}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "predicted_answer": "None provided, answer is \"Unanswerable\".", "predicted_evidence": ["This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection. A simple greedy algorithm is used to find the best summary under a length constraint. In order to keep the method efficient, we outline different methods to select a small number of candidate sentences from each document in the input collection before constructing the summary. We test these modifications on the DUC2004 dataset for multi-document summarization. The results show an improvement of Rouge scores over the original centroid method. The performance is on par with state-of-the-art methods which shows that the similarity between a summary centroid and the input centroid is a well-suited function for global summary optimization. The summarization approach presented in this paper is fast, unsupervised and simple to implement. Nevertheless, it performs as well as more complex state-of-the-art", "for global summary optimization. The summarization approach presented in this paper is fast, unsupervised and simple to implement. Nevertheless, it performs as well as more complex state-of-the-art approaches in terms of Rouge scores on the DUC2004 dataset. It can be used as a strong baseline for future research or as a fast and easy-to-deploy summarization tool. Original Centroid-based Method. The original centroid-based model is described by BIBREF5 . It represents sentences as BOW vectors with TF-IDF weighting. The centroid vector is the sum of all sentence vectors and each sentence is scored by the cosine similarity between its vector representation and the centroid vector. Cosine similarity measures how close two vectors INLINEFORM0 and INLINEFORM1 are based on their angle and is defined as follows: DISPLAYFORM0  A summary is selected by de-queuing the ranked list of sentences in decreasing order until the desired summary length is reached.  BIBREF7 implement this original model", "sentence lists. Other approaches work at summary-level rather than sentence-level and aim to optimize functions of sets of sentences to find good summaries, such as KL-divergence between probability distributions BIBREF3 or submodular functions that represent coverage, diversity, etc. BIBREF4  The centroid-based model belongs to the former group: it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection BIBREF5 . The sentences are ranked by their cosine similarity to the centroid vector. This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 . This baseline can easily be adapted to work at the summary-level instead the sentence level. This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection. A simple greedy", "that summarize the input well together instead of finding sentences that summarize the input well independently. This strategy should also be less dependent on anti-redundancy filtering since a combination of redundant sentences is probably less similar to the centroid than a more diverse selection that covers different prevalent topics. In the experiments, we will therefore call this modification the \"global\" variant of the centroid model. The same principle is used by the KLSum model BIBREF3 in which the optimal summary minimizes the KL-divergence of the probability distribution of words in the input from the distribution in the summary. KLSum uses a greedy algorithm to find the best summary. Starting with an empty summary, the algorithm includes at each iteration the sentence that maximizes the similarity to the centroid when added to the already selected sentences. We also use this algorithm for sentence selection. The procedure is depicted in Algorithm SECREF5 below. [H] [1]", "documents. In the new-TF-IDF example, the second and third sentences were preselected because high ranking features such as \"robot\" and \"arm\" appeared for the first time in the respective documents. Related Work. In addition to various works on sophisticated models for multi-document summarization, other experiments have been done showing that simple modifications to the standard baseline methods can perform quite well.  BIBREF7 improved the centroid-based method by representing sentences as sums of word embeddings instead of TF-IDF vectors so that semantic relationships between sentences that have no words in common can be captured. BIBREF10 also evaluated summaries from SumRepo and did experiments on improving baseline systems such as the centroid-based and the KL-divergence method with different anti-redundancy filters. Their best optimized baseline obtained a performance similar to the ICSI method in SumRepo. Conclusion. In this paper we show that simple modifications to the", "and modified models slightly differently to BIBREF7 : all words in the vocabulary are ranked by their value in the centroid vector. On a development dataset, a parameter is tuned that defines the proportion of the ranked vocabulary that is represented in the centroid vector and the rest is set to zero. This variant resulted in more stable behavior for different amounts of input documents. Modified Summary Selection. The similarity to the centroid vector can also be used to score a summary instead of a sentence. By representing a summary as the sum of its sentence vectors, it can be compared to the centroid, which is different from adding centroid-similarity scores of individual sentences. With this modification, the summarization task is explicitly modelled as finding a combination of sentences that summarize the input well together instead of finding sentences that summarize the input well independently. This strategy should also be less dependent on anti-redundancy filtering since a", "for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction. Preprocessing. In the summarization methods proposed in this paper, the preprocessing includes sentence segmentation, lowercasing and stopword removal. Parameter Tuning. The similarity threshold for avoiding redundancy ( INLINEFORM0 ) and the vocabulary-included-in-centroid ratio ( INLINEFORM1 ) are tuned with the original centroid model on our development set. Values from 0 to 1 with step size INLINEFORM2 were tested using a grid search. The optimal values for INLINEFORM3 and INLINEFORM4 were INLINEFORM5 and INLINEFORM6 , respectively. These values were used for all tested variants of the centroid model. For the different methods of choosing INLINEFORM7 sentences of each document before summarization, we tuned INLINEFORM8 separately for each, with values from 1 to 10, using the global model. The best INLINEFORM9 found for INLINEFORM10", "that are mentioned in that sentence for the first time in the document. The intuition is that sentences are preferred if they introduce new important information to a document. Note that in each of these candidate selection methods, the centroid vector is always computed as the sum of all sentence vectors, including the ones of the ignored sentences. Datasets. For testing, we use the DUC2004 Task 2 dataset from the Document Understanding Conference (DUC). The dataset consists of 50 document clusters containing 10 documents each. For tuning hyperparameters, we use the CNN/Daily Mail dataset BIBREF8 which provides summary bulletpoints for individual news articles. In order to adapt the dataset for MDS, 50 CNN articles were randomly selected as documents to initialize 50 clusters. For each of these seed articles, 9 articles with the highest word-overlap in the first 3 sentences were added to that cluster. This resulted in 50 documents clusters, each containing 10 topically related"]}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "predicted_answer": "Rouge scores", "predicted_evidence": ["with different anti-redundancy filters. Their best optimized baseline obtained a performance similar to the ICSI method in SumRepo. Conclusion. In this paper we show that simple modifications to the centroid-based method can bring its performance to the same level as state-of-the-art methods on the DUC2004 dataset. The resulting summarization methods are unsupervised, efficient and do not require complicated feature engineering or training. Changing from a ranking-based method to a global optimization method increases performance and makes the summarizer less dependent on explicitly checking for redundancy. This can be useful for input document collections with differing levels of content diversity. The presented methods for restricting the input to a maximum of INLINEFORM0 sentences per document lead to additional improvements while reducing computation effort, if global optimization is being used. These methods could be useful for other summarization models that rely on pairwise", "INLINEFORM7 sentences of each document before summarization, we tuned INLINEFORM8 separately for each, with values from 1 to 10, using the global model. The best INLINEFORM9 found for INLINEFORM10 -first, INLINEFORM11 -best, new-tfidf were 7, 2 and 3 respectively. Results. Table TABREF9 shows the Rouge scores measured in our experiments. The first two sections show results for baseline and SOTA summaries from SumRepo. The third section shows the summarization variants presented in this paper. \"G\" indicates that the global greedy algorithm was used instead of sentence-level ranking. In the last section, \"- R\" indicates that the method was tested without the anti-redundancy filter. Both the global optimization and the sentence preselection have a positive impact on the performance. The global + new-TF-IDF variant outperforms all but the DPP model in Rouge-1 recall. The global + N-first variant outperforms all other models in Rouge-2 recall. However, the Rouge scores of the SOTA methods", "This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection. A simple greedy algorithm is used to find the best summary under a length constraint. In order to keep the method efficient, we outline different methods to select a small number of candidate sentences from each document in the input collection before constructing the summary. We test these modifications on the DUC2004 dataset for multi-document summarization. The results show an improvement of Rouge scores over the original centroid method. The performance is on par with state-of-the-art methods which shows that the similarity between a summary centroid and the input centroid is a well-suited function for global summary optimization. The summarization approach presented in this paper is fast, unsupervised and simple to implement. Nevertheless, it performs as well as more complex state-of-the-art", "+ new-TF-IDF variant outperforms all but the DPP model in Rouge-1 recall. The global + N-first variant outperforms all other models in Rouge-2 recall. However, the Rouge scores of the SOTA methods and the introduced centroid variants are in a very similar range. Interestingly, the original centroid-based model, without any of the new modifications introduced in this paper, already shows quite high Rouge scores in comparison to the other baseline methods. This is due to the anti-redundancy filter and the selection of top-ranking features. In order to see whether the global sentence selection alleviates the need for an anti-redundancy filter, the original method and the global method (without INLINEFORM0 sentences per document selection) were tested without it (section 4 in Table TABREF9 ). In terms of Rouge-1 recall, the original model is clearly very dependent on checking for redundancy when including sentences, while the global variant does not change its performance much without the", "for global summary optimization. The summarization approach presented in this paper is fast, unsupervised and simple to implement. Nevertheless, it performs as well as more complex state-of-the-art approaches in terms of Rouge scores on the DUC2004 dataset. It can be used as a strong baseline for future research or as a fast and easy-to-deploy summarization tool. Original Centroid-based Method. The original centroid-based model is described by BIBREF5 . It represents sentences as BOW vectors with TF-IDF weighting. The centroid vector is the sum of all sentence vectors and each sentence is scored by the cosine similarity between its vector representation and the centroid vector. Cosine similarity measures how close two vectors INLINEFORM0 and INLINEFORM1 are based on their angle and is defined as follows: DISPLAYFORM0  A summary is selected by de-queuing the ranked list of sentences in decreasing order until the desired summary length is reached.  BIBREF7 implement this original model", "documents. In the new-TF-IDF example, the second and third sentences were preselected because high ranking features such as \"robot\" and \"arm\" appeared for the first time in the respective documents. Related Work. In addition to various works on sophisticated models for multi-document summarization, other experiments have been done showing that simple modifications to the standard baseline methods can perform quite well.  BIBREF7 improved the centroid-based method by representing sentences as sums of word embeddings instead of TF-IDF vectors so that semantic relationships between sentences that have no words in common can be captured. BIBREF10 also evaluated summaries from SumRepo and did experiments on improving baseline systems such as the centroid-based and the KL-divergence method with different anti-redundancy filters. Their best optimized baseline obtained a performance similar to the ICSI method in SumRepo. Conclusion. In this paper we show that simple modifications to the", "each of these seed articles, 9 articles with the highest word-overlap in the first 3 sentences were added to that cluster. This resulted in 50 documents clusters, each containing 10 topically related articles. The reference summaries for each cluster were created by interleaving the sentences of the article summaries until a length contraint (100 words) was reached. Baselines & Evaluation.  BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing. We calculate Rouge scores with the Rouge toolkit BIBREF9 . In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction. Preprocessing. In the summarization methods", "sentence lists. Other approaches work at summary-level rather than sentence-level and aim to optimize functions of sets of sentences to find good summaries, such as KL-divergence between probability distributions BIBREF3 or submodular functions that represent coverage, diversity, etc. BIBREF4  The centroid-based model belongs to the former group: it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection BIBREF5 . The sentences are ranked by their cosine similarity to the centroid vector. This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 . This baseline can easily be adapted to work at the summary-level instead the sentence level. This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection. A simple greedy"]}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "predicted_answer": "Original centroid-based model described by BIBREF5.", "predicted_evidence": ["+ new-TF-IDF variant outperforms all but the DPP model in Rouge-1 recall. The global + N-first variant outperforms all other models in Rouge-2 recall. However, the Rouge scores of the SOTA methods and the introduced centroid variants are in a very similar range. Interestingly, the original centroid-based model, without any of the new modifications introduced in this paper, already shows quite high Rouge scores in comparison to the other baseline methods. This is due to the anti-redundancy filter and the selection of top-ranking features. In order to see whether the global sentence selection alleviates the need for an anti-redundancy filter, the original method and the global method (without INLINEFORM0 sentences per document selection) were tested without it (section 4 in Table TABREF9 ). In terms of Rouge-1 recall, the original model is clearly very dependent on checking for redundancy when including sentences, while the global variant does not change its performance much without the", "as follows: DISPLAYFORM0  A summary is selected by de-queuing the ranked list of sentences in decreasing order until the desired summary length is reached.  BIBREF7 implement this original model with the following modifications: In order to avoid redundant sentences in the summary, a new sentence is only included if it does not exceed a certain maximum similarity to any of the already included sentences. To focus on only the most important terms of the input documents, the values in the centroid vector which fall below a tuned threshold are set to zero. This model, which includes the anti-redundancy filter and the selection of top-ranking features, is treated as the \"original\" centroid-based model in this paper. We implement the selection of top-ranking features for both the original and modified models slightly differently to BIBREF7 : all words in the vocabulary are ranked by their value in the centroid vector. On a development dataset, a parameter is tuned that defines the", "for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction. Preprocessing. In the summarization methods proposed in this paper, the preprocessing includes sentence segmentation, lowercasing and stopword removal. Parameter Tuning. The similarity threshold for avoiding redundancy ( INLINEFORM0 ) and the vocabulary-included-in-centroid ratio ( INLINEFORM1 ) are tuned with the original centroid model on our development set. Values from 0 to 1 with step size INLINEFORM2 were tested using a grid search. The optimal values for INLINEFORM3 and INLINEFORM4 were INLINEFORM5 and INLINEFORM6 , respectively. These values were used for all tested variants of the centroid model. For the different methods of choosing INLINEFORM7 sentences of each document before summarization, we tuned INLINEFORM8 separately for each, with values from 1 to 10, using the global model. The best INLINEFORM9 found for INLINEFORM10", "that summarize the input well together instead of finding sentences that summarize the input well independently. This strategy should also be less dependent on anti-redundancy filtering since a combination of redundant sentences is probably less similar to the centroid than a more diverse selection that covers different prevalent topics. In the experiments, we will therefore call this modification the \"global\" variant of the centroid model. The same principle is used by the KLSum model BIBREF3 in which the optimal summary minimizes the KL-divergence of the probability distribution of words in the input from the distribution in the summary. KLSum uses a greedy algorithm to find the best summary. Starting with an empty summary, the algorithm includes at each iteration the sentence that maximizes the similarity to the centroid when added to the already selected sentences. We also use this algorithm for sentence selection. The procedure is depicted in Algorithm SECREF5 below. [H] [1]", "and modified models slightly differently to BIBREF7 : all words in the vocabulary are ranked by their value in the centroid vector. On a development dataset, a parameter is tuned that defines the proportion of the ranked vocabulary that is represented in the centroid vector and the rest is set to zero. This variant resulted in more stable behavior for different amounts of input documents. Modified Summary Selection. The similarity to the centroid vector can also be used to score a summary instead of a sentence. By representing a summary as the sum of its sentence vectors, it can be compared to the centroid, which is different from adding centroid-similarity scores of individual sentences. With this modification, the summarization task is explicitly modelled as finding a combination of sentences that summarize the input well together instead of finding sentences that summarize the input well independently. This strategy should also be less dependent on anti-redundancy filtering since a", "each of these seed articles, 9 articles with the highest word-overlap in the first 3 sentences were added to that cluster. This resulted in 50 documents clusters, each containing 10 topically related articles. The reference summaries for each cluster were created by interleaving the sentences of the article summaries until a length contraint (100 words) was reached. Baselines & Evaluation.  BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods . We evaluate summaries generated by a selection of these methods on the same data that we use for testing. We calculate Rouge scores with the Rouge toolkit BIBREF9 . In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall. The baselines include a basic centroid-based model without an anti-redundancy filter and feature reduction. Preprocessing. In the summarization methods", "for global summary optimization. The summarization approach presented in this paper is fast, unsupervised and simple to implement. Nevertheless, it performs as well as more complex state-of-the-art approaches in terms of Rouge scores on the DUC2004 dataset. It can be used as a strong baseline for future research or as a fast and easy-to-deploy summarization tool. Original Centroid-based Method. The original centroid-based model is described by BIBREF5 . It represents sentences as BOW vectors with TF-IDF weighting. The centroid vector is the sum of all sentence vectors and each sentence is scored by the cosine similarity between its vector representation and the centroid vector. Cosine similarity measures how close two vectors INLINEFORM0 and INLINEFORM1 are based on their angle and is defined as follows: DISPLAYFORM0  A summary is selected by de-queuing the ranked list of sentences in decreasing order until the desired summary length is reached.  BIBREF7 implement this original model", "In terms of Rouge-1 recall, the original model is clearly very dependent on checking for redundancy when including sentences, while the global variant does not change its performance much without the anti-redundancy filter. This matches the expectation that the globally motivated method handles redundancy implicitly. Example Summaries. Table TABREF10 shows generated example summaries using the global centroid method with the three sentence preselection methods. For readability, truncated sentences (due to the 100-word limit) at the end of the summaries are excluded. The original positions of the summary sentences, i.e. the indices of the document and the sentence inside the document are given. As can be seen in the examples, the N-first method is restricted to sentences appearing early in documents. In the new-TF-IDF example, the second and third sentences were preselected because high ranking features such as \"robot\" and \"arm\" appeared for the first time in the respective documents."]}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "predicted_answer": "Different methods are explored for reducing the number of input sentences before applying the greedy sentence selection algorithm, such as selecting the first sentences, selecting the best sentences based on cosine similarity to a centroid vector, or ranking sentences by their TF-IDF scores.", "predicted_evidence": ["maximizes the similarity to the centroid when added to the already selected sentences. We also use this algorithm for sentence selection. The procedure is depicted in Algorithm SECREF5 below. [H] [1] Input: INLINEFORM0 Output: INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 INLINEFORM5 INLINEFORM6 INLINEFORM7 INLINEFORM8 Greedy Sentence Selection Preselection of Sentences. The modified sentence selection method is less efficient than the orginal method since at each iteration the score of a possible summary has to be computed for all remaining candidate sentences. It may not be noticeable for a small number of input sentences. However, it would have an impact if the amount of input documents was larger, e.g. for the summarization of top-100 search results in document retrieval. Therefore, we explore different methods for reducing the number of input sentences before applying the greedy sentence selection algorithm to make the model more suited for larger inputs. It is also important", "we explore different methods for reducing the number of input sentences before applying the greedy sentence selection algorithm to make the model more suited for larger inputs. It is also important to examine how this affects Rouge scores. We test the following methods of selecting INLINEFORM0 sentences from each document as candidates for the greedy sentence selection algorithm: The first INLINEFORM0 sentences of the document are selected. This results in a mixture of a lead- INLINEFORM1 baseline and the centroid-based method. The sentences are ranked separately in each document by their cosine similarity to the centroid vector, in decreasing order. The INLINEFORM0 best sentences of each document are selected as candidates. Each sentence is scored by the sum of the TF-IDF scores of the terms that are mentioned in that sentence for the first time in the document. The intuition is that sentences are preferred if they introduce new important information to a document. Note that in each", "In terms of Rouge-1 recall, the original model is clearly very dependent on checking for redundancy when including sentences, while the global variant does not change its performance much without the anti-redundancy filter. This matches the expectation that the globally motivated method handles redundancy implicitly. Example Summaries. Table TABREF10 shows generated example summaries using the global centroid method with the three sentence preselection methods. For readability, truncated sentences (due to the 100-word limit) at the end of the summaries are excluded. The original positions of the summary sentences, i.e. the indices of the document and the sentence inside the document are given. As can be seen in the examples, the N-first method is restricted to sentences appearing early in documents. In the new-TF-IDF example, the second and third sentences were preselected because high ranking features such as \"robot\" and \"arm\" appeared for the first time in the respective documents.", "that are mentioned in that sentence for the first time in the document. The intuition is that sentences are preferred if they introduce new important information to a document. Note that in each of these candidate selection methods, the centroid vector is always computed as the sum of all sentence vectors, including the ones of the ignored sentences. Datasets. For testing, we use the DUC2004 Task 2 dataset from the Document Understanding Conference (DUC). The dataset consists of 50 document clusters containing 10 documents each. For tuning hyperparameters, we use the CNN/Daily Mail dataset BIBREF8 which provides summary bulletpoints for individual news articles. In order to adapt the dataset for MDS, 50 CNN articles were randomly selected as documents to initialize 50 clusters. For each of these seed articles, 9 articles with the highest word-overlap in the first 3 sentences were added to that cluster. This resulted in 50 documents clusters, each containing 10 topically related", "sentence lists. Other approaches work at summary-level rather than sentence-level and aim to optimize functions of sets of sentences to find good summaries, such as KL-divergence between probability distributions BIBREF3 or submodular functions that represent coverage, diversity, etc. BIBREF4  The centroid-based model belongs to the former group: it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection BIBREF5 . The sentences are ranked by their cosine similarity to the centroid vector. This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 . This baseline can easily be adapted to work at the summary-level instead the sentence level. This is done by representing a summary as the centroid of its sentence vectors and maximizing the similarity between the summary centroid and the centroid of the document collection. A simple greedy", "Introduction. Extractive multi-document summarization (MDS) aims to summarize a collection of documents by selecting a small number of sentences that represent the original content appropriately. Typical objectives for assembling a summary include information coverage and non-redundancy. A wide variety of methods have been introduced to approach MDS. Many approaches are based on sentence ranking, i.e. assigning each sentence a score that indicates how well the sentence summarizes the input BIBREF0 , BIBREF1 , BIBREF2 . A summary is created by selecting the top entries of the ranked list of sentences. Since the sentences are often treated separately, these models might allow redundancy in the summary. Therefore, they are often extended by an anti-redundancy filter while de-queuing ranked sentence lists. Other approaches work at summary-level rather than sentence-level and aim to optimize functions of sets of sentences to find good summaries, such as KL-divergence between probability", "documents. In the new-TF-IDF example, the second and third sentences were preselected because high ranking features such as \"robot\" and \"arm\" appeared for the first time in the respective documents. Related Work. In addition to various works on sophisticated models for multi-document summarization, other experiments have been done showing that simple modifications to the standard baseline methods can perform quite well.  BIBREF7 improved the centroid-based method by representing sentences as sums of word embeddings instead of TF-IDF vectors so that semantic relationships between sentences that have no words in common can be captured. BIBREF10 also evaluated summaries from SumRepo and did experiments on improving baseline systems such as the centroid-based and the KL-divergence method with different anti-redundancy filters. Their best optimized baseline obtained a performance similar to the ICSI method in SumRepo. Conclusion. In this paper we show that simple modifications to the", "and modified models slightly differently to BIBREF7 : all words in the vocabulary are ranked by their value in the centroid vector. On a development dataset, a parameter is tuned that defines the proportion of the ranked vocabulary that is represented in the centroid vector and the rest is set to zero. This variant resulted in more stable behavior for different amounts of input documents. Modified Summary Selection. The similarity to the centroid vector can also be used to score a summary instead of a sentence. By representing a summary as the sum of its sentence vectors, it can be compared to the centroid, which is different from adding centroid-similarity scores of individual sentences. With this modification, the summarization task is explicitly modelled as finding a combination of sentences that summarize the input well together instead of finding sentences that summarize the input well independently. This strategy should also be less dependent on anti-redundancy filtering since a"]}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "predicted_answer": "No", "predicted_evidence": []}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "predicted_answer": "Unanswerable", "predicted_evidence": []}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "predicted_answer": "Unanswerable.", "predicted_evidence": []}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "predicted_answer": "Unanswerable", "predicted_evidence": []}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "predicted_answer": "multilingual word vectors, training data from multiple languages in the CoNLL 2009 dataset", "predicted_evidence": ["labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset. To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages. The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of", "parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels. Data. We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences", "Introduction. The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks. ammar2016malopa found that using training data from multiple languages annotated with Universal Dependencies BIBREF1 , and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages\u2014which we call polyglot training\u2014to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English", "trains a polyglot model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Conclusion. In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data. Acknowledgments. We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback. This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN. Views", "other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other. We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while", "space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 . Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined. Hence, we use separate label representations for each language's labels. Similarly, while (for example) eng:look and spa:mira may be semantically connected, the senses look.01 and mira.01 may not correspond. Hence, predicate sense representations are also language-specific. Language Identification. In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and", "representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He2017-deepsrl, who did not handle predicate senses, and of marcheggiani2017lstm, who used a separate model. These two predictions are made independently, with separate softmaxes over different last-layer parameters; we then combine the losses for each task when training. For predicate sense disambiguation, since the predicate has been identified, we choose from a small set of valid predicate senses as the tag for that token. This set of possible senses is selected based on the training data: we map from lemmatized tokens to predicates and from predicates to the set of all senses of that predicate. Most predicates are only observed to have one or two corresponding senses, making the set of available senses at test time quite small (less than five senses/predicate on average across all languages). If a particular lemma", "1. Additionally, BIO consistency constraints are removed from the original model\u2014 each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by marcheggiani2017lstm. The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-of-speech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these. Each training instance consists of the annotations for a single predicate. These representations are then passed through a deep, multi-layer bidirectional LSTM BIBREF4 , BIBREF5 with highway connections BIBREF6 . We use the hidden representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He2017-deepsrl, who did not"]}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "predicted_answer": "CoNLL 2009 dataset", "predicted_evidence": ["labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset. To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages. The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of", "1. Additionally, BIO consistency constraints are removed from the original model\u2014 each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by marcheggiani2017lstm. The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-of-speech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these. Each training instance consists of the annotations for a single predicate. These representations are then passed through a deep, multi-layer bidirectional LSTM BIBREF4 , BIBREF5 with highway connections BIBREF6 . We use the hidden representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He2017-deepsrl, who did not", "representations produced by the deep biLSTM for both argument labeling and predicate sense disambiguation in a multitask setup; this is a modification to the models of He2017-deepsrl, who did not handle predicate senses, and of marcheggiani2017lstm, who used a separate model. These two predictions are made independently, with separate softmaxes over different last-layer parameters; we then combine the losses for each task when training. For predicate sense disambiguation, since the predicate has been identified, we choose from a small set of valid predicate senses as the tag for that token. This set of possible senses is selected based on the training data: we map from lemmatized tokens to predicates and from predicates to the set of all senses of that predicate. Most predicates are only observed to have one or two corresponding senses, making the set of available senses at test time quite small (less than five senses/predicate on average across all languages). If a particular lemma", "We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table TABREF3 . Model. Given a sentence with a marked predicate, the CoNLL 2009 shared task requires disambiguation of the sense of the predicate, and labeling all its dependent arguments. The shared task assumed predicates have already been identified, hence we do not handle the predicate identification task. Our basic model adapts the span-based dependency SRL model of He2017-deepsrl. This adaptation treats the dependent arguments as argument spans of length 1. Additionally, BIO consistency constraints are removed from the original model\u2014 each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by", "trains a polyglot model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Conclusion. In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data. Acknowledgments. We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback. This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN. Views", "Introduction. The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks. ammar2016malopa found that using training data from multiple languages annotated with Universal Dependencies BIBREF1 , and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages\u2014which we call polyglot training\u2014to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English", "Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency. Simple Polyglot Sharing. In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings. This polyglot model is trained on the union of annotations in the two languages. We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch. The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a", "parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels. Data. We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences"]}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "predicted_answer": "A traditional monolingual version parser.", "predicted_evidence": ["labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset. To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages. The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of", "Introduction. The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks. ammar2016malopa found that using training data from multiple languages annotated with Universal Dependencies BIBREF1 , and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages\u2014which we call polyglot training\u2014to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English", "space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 . Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined. Hence, we use separate label representations for each language's labels. Similarly, while (for example) eng:look and spa:mira may be semantically connected, the senses look.01 and mira.01 may not correspond. Hence, predicate sense representations are also language-specific. Language Identification. In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and", "This adds a greater degree of language-specific processing while still sharing representations across languages. It also uses the language identification vector and multilingual word vectors in the input. Experiments. We present our results in Table TABREF11 . We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure FIGREF10 ). The language ID model performs inconsistently; it is better than the simple polyglot model in some cases, including Czech, but not in all. The language-specific LSTMs model performs best on a few languages, such as Catalan and Chinese, but worst on others. While these results may reflect differences between languages in the optimal amount of crosslingual sharing, we focus on the", "parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels. Data. We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences", "Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency. Simple Polyglot Sharing. In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings. This polyglot model is trained on the union of annotations in the two languages. We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch. The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a", "trains a polyglot model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Conclusion. In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data. Acknowledgments. We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback. This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN. Views", "other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other. We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while"]}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "predicted_answer": "Catalan, Chinese, Czech, English, German, Japanese and Spanish", "predicted_evidence": ["parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels. Data. We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences", "labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset. To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages. The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of", "trains a polyglot model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Conclusion. In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data. Acknowledgments. We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback. This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN. Views", "Introduction. The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks. ammar2016malopa found that using training data from multiple languages annotated with Universal Dependencies BIBREF1 , and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages\u2014which we call polyglot training\u2014to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English", "and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. marcheggiani2017gcn apply the recently-developed graph convolutional networks to SRL, obtaining state of the art results on English and Chinese. All of these approaches are orthogonal to ours, and might benefit from polyglot training. Other polyglot models have been proposed for semantics. Richardson2018-ov-naacl train on multiple (natural language)-(programming language) pairs to improve a model that translates API text into code signature representations. Duong2017-qy treat English and German semantic parsing as a multi-task learning problem and saw improvement over monolingual baselines, especially for small datasets. Most relevant to our work is Johannsen2015-nb, which trains a polyglot model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional", "best on a few languages, such as Catalan and Chinese, but worst on others. While these results may reflect differences between languages in the optimal amount of crosslingual sharing, we focus on the simple polyglot results in our analysis, which sufficiently demonstrate that polyglot training can improve performance over monolingual training. We also report performance of state-of-the-art systems in each of these languages, all of which make explicit use of syntactic features, marcheggiani2017lstm excepted. While this results in better performance on many languages, our model has the advantage of not relying on a syntactic parser, and is hence more applicable to languages with lower resources. However, the results suggest that syntactic information is critical for strong performance on German, which has the fewest predicates and thus the least semantic annotation for a semantics-only model to learn from. Nevertheless, our baseline is on par with the best published scores for Chinese,", "Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency. Simple Polyglot Sharing. In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings. This polyglot model is trained on the union of annotations in the two languages. We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch. The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a", "In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters. Language-Specific LSTMs. This third variant takes inspiration from the \u201cfrustratingly easy\u201d architecture of daumeiii2007easy for domain adaptation. In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language. Each of these language-specific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to the third layer. This adds a greater degree of language-specific processing while still sharing representations across languages. It also uses the language identification vector and multilingual word vectors in"]}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "predicted_answer": "Unanswerable", "predicted_evidence": ["to train our word embeddings with \u201cvirtual sentences.\u201d The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms). Segmentation Performance. We compared our full method (TextTiling with heuristic-max based on embeddings trained by virtual sentences) with several baselines: Random. We randomly segmented conversation sessions. In this baseline, we were equipped with the prior probability of segmentation. MMD. We applied the MinMax-Dotplotting (MMD) approach proposed by Ye et al. BIBREF24 . We ran the executable program provided by the authors. TextTiling w/ tf INLINEFORM0 idf features. We implemented TextTiling ourselves according to BIBREF12 . We tuned the hyperparameter INLINEFORM0 in Equation ()on the validation set to make the number of segmentation close to that of manual annotation, and reported precision, recall, and the F-score on the test set in Table TABREF18 . As seen, our approach significantly", "set to make the number of segmentation close to that of manual annotation, and reported precision, recall, and the F-score on the test set in Table TABREF18 . As seen, our approach significantly outperforms baselines by a large margin in terms of both precision and recall. Besides, we can see that MMD obtains low performance, which is mainly because the approach cannot be easily adapted to other datasets like short sentences of conversation utterances. In summary, we achieve an INLINEFORM1 -score higher than baseline methods by more than 20%, showing the effectiveness of enhancing TextTiling with modern word embeddings. We further conducted in-depth analysis of different strategies of training word-embeddings and matching heuristics in Table TABREF21 . For word embeddings, we trained them on the 3M-sentence dataset with three strategies: (1) virtual-sentence context proposed in our paper; (2) within-sentence context, where all words (except the current one) within a sentence (either a", "session. In this paper, we addressed the problem of session segmentation for open-domain dialogue systems. We proposed an embedding-enhanced TextTiling approach, where we trained embeddings with the novel notion of virtual sentences; we also proposed several heuristics for similarity measure. Experimental results show that both our embedding learning and similarity measuring are effective in session segmentation, and that with our approach, we can improve the performance of a retrieval-based dialogue system. We thank anonymous reviewers for useful comments and Jingbo Zhu for sharing the MMD executable program. This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61272343 and 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032), and the National Basic Research Program (973 Program No. 2014CB340405).", "the probability of a word by DISPLAYFORM0  The goal of word embedding learning is to maximize the average probability of all words (suppose we have INLINEFORM0 running words): DISPLAYFORM0  We used hierarchical softmax to approximate the probability. To model the context, we further adopt the continuous bag-of-words (CBOW) method. The context is defined by the sum of neighboring words' (input) vectors in a fixed-size window ( INLINEFORM0 to INLINEFORM1 ) within a sentence: DISPLAYFORM0  Notice that the context vector INLINEFORM0 in Equation ( EQREF12 ) and the output vector INLINEFORM1 in Equation ( EQREF9 ) are different as suggested in BIBREF25 , BIBREF26 , but the details are beyond the scope of our paper. Virtual Sentences In a conversation corpus, successive sentences have a stronger interaction than general texts. For example, in Figure FIGREF2 , the words thank and welcome are strongly correlated, but they hardly appear in the a sentence and thus a same window. Therefore,", "score of words in INLINEFORM4 . This method is denoted as heuristic-max. Alternatively, we may substitute the INLINEFORM0 operator in Equation ( EQREF16 ) with INLINEFORM1 , resulting in the heuristic-avg variant, which is equivalent to the average of word-by-word cosine similarity. However, as shown in Subsection SECREF22 , intensive similarity averaging has a \u201cblurring\u201d effect and will lead to significant performance degradation. This also shows that our proposed heuristic-max does capture useful interaction between two successive utterances in a dialogue. Experiments. In this section, we evaluate our embedding-enhanced TextTiling method as well as the effect of session segmentation. In Subsection SECREF17 , we describe the datasets used in our experiments. Subsection SECREF22 presents the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system.", "from a large conversation corpus or generate a new sentence as the reply. In open-domain conversations, context information (one or a few previous utterances) is particularly important to language understanding BIBREF1 , BIBREF9 , BIBREF10 , BIBREF11 . As dialogue sentences are usually casual and short, a single utterance (e.g., \u201cThank you.\u201d in Figure FIGREF2 ) does not convey much meaning, but its previous utterance (\u201c...writing an essay\u201d) provides useful background information of the conversation. Using such context will certainly benefit the conversation system. However, tracking all previous utterances as the context is unwise. First, commercial chat-bots usually place high demands on efficiency. In a retrieval-based system, for example, performing a standard process of candidate retrieval and re-ranking for each previous utterance may well exceed the time limit (which is very short, e.g., 500ms). Second, we observe that not all sentences in the current conversation session are", "interaction than general texts. For example, in Figure FIGREF2 , the words thank and welcome are strongly correlated, but they hardly appear in the a sentence and thus a same window. Therefore, traditional within-sentence CBOW may not capture the interaction between a query and its corresponding reply. In this paper, we propose the concept of virtual sentences to learn word embeddings for conversation data. We concatenate a query INLINEFORM0 and its reply INLINEFORM1 as a virtual sentence INLINEFORM2 . We also use all words (other than the current one) in the virtual sentence as context (Figure 2). Formally, the context INLINEFORM3 of the word INLINEFORM4 is given by DISPLAYFORM0  In this way, related words across two successive utterances from different agents can have interaction during word embedding learning. As will be shown in Subsection SECREF22 , virtual sentences yield a higher performance for dialogue segmentation. Measuring Similarity. In this part, we introduce several", "the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system. Dataset. To evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese. We sampled 200 sessions as our experimental corpus. Session segmentation was manually annotated before experiments, serving as the ground truth. The 200 sessions were randomly split by 1:1 for validation and testing. Notice that, our method does not require labeled training samples; massive data with labels of high quality are quite expensive to obtain. We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with \u201cvirtual sentences.\u201d The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms)."]}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "predicted_answer": "Session segmentation.", "predicted_evidence": ["the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system. Dataset. To evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese. We sampled 200 sessions as our experimental corpus. Session segmentation was manually annotated before experiments, serving as the ground truth. The 200 sessions were randomly split by 1:1 for validation and testing. Notice that, our method does not require labeled training samples; massive data with labels of high quality are quite expensive to obtain. We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with \u201cvirtual sentences.\u201d The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).", "typically a classification problem with a few pre-defined conversation states/topics, and hence it can hardly be generalized to general-purpose session segmentation. Text Segmentation. An early and classic work on text segmentation is TextTiling, proposed in BIBREF12 . The idea is to measure the similarity between two successive sentences with smoothing techniques; then segmentation is accomplished by thresholding of the depth of a \u201cvalley.\u201d In the original form of TextTiling, the cosine of term frequency features is used as the similarity measure. Joty et al. BIBREF22 apply divisive clustering instead of thresholding for segmentation. Malioutov et al. BIBREF23 formalize segmentation as a graph-partitioning problem and propose a minimum cut model based on tf INLINEFORM0 idf features to segment lectures. Ye et al. BIBREF24 minimize between-segment similarity while maximizing within-segment similarity. However, the above complicated approaches are known as global methods: when we", "TextTiling approach largely outperforms baselines. We conducted an external experiment to show the effect of session segmentation in dialogue systems. We integrated the segmentation mechanism into a state-of-the-practice retrieval-based system and evaluated the results by manual annotation, similar to our previous work BIBREF27 , BIBREF31 , BIBREF32 . Concretely, we compared our session segmentation with fixed-length context, used in BIBREF11 . That is to say, the competing method always regards two previous utterances as context. We hired three workers to annotate the results with three integer scores (0\u20132 points, indicating bad, borderline, and good replies, respectively.) We sampled 30 queries from the test set of 100 sessions. For each query, we retrieved 10 candidates and computed p@1 and nDCG scores BIBREF33 (averaged over three annotators). Provided with previous utterances as context, each worker had up to 1000 sentences to read during annotation. Table TABREF26 presents the", "session. In this paper, we addressed the problem of session segmentation for open-domain dialogue systems. We proposed an embedding-enhanced TextTiling approach, where we trained embeddings with the novel notion of virtual sentences; we also proposed several heuristics for similarity measure. Experimental results show that both our embedding learning and similarity measuring are effective in session segmentation, and that with our approach, we can improve the performance of a retrieval-based dialogue system. We thank anonymous reviewers for useful comments and Jingbo Zhu for sharing the MMD executable program. This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61272343 and 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032), and the National Basic Research Program (973 Program No. 2014CB340405).", "and nDCG scores BIBREF33 (averaged over three annotators). Provided with previous utterances as context, each worker had up to 1000 sentences to read during annotation. Table TABREF26 presents the results of the dialogue system with session segmentation. As demonstrated, our method outperforms the simple fixed-context approach in terms of both metrics. We computed the inner-annotator agreement: std INLINEFORM0 0.309; 3-discrete-class Fleiss' kappa score INLINEFORM1 0.411, indicating moderate agreement BIBREF34 . Case Study. We present a case study on our website: https://sites.google.com/site/sessionsegmentation/. From the case study, we see that the proposed approach is able to segment the dialogue session appropriately, so as to better utilize background information from a conversation session. In this paper, we addressed the problem of session segmentation for open-domain dialogue systems. We proposed an embedding-enhanced TextTiling approach, where we trained embeddings with the", "and re-ranking for each previous utterance may well exceed the time limit (which is very short, e.g., 500ms). Second, we observe that not all sentences in the current conversation session are equally important. The sentence \u201cWant to take a walk?\u201d is irrelevant to the current context, and should not be considered when the computer synthesizes the reply. Therefore, it raises the question of session segmentation in conversation systems. Document segmentation for general-purpose corpora has been widely studied in NLP. For example, Hearst BIBREF12 proposes the TextTiling approach; she measures the similarity of neighboring sentences based on bag-of-words features, and performs segmentation by thresholding. However, such approaches are not tailored to the dialogue genre and may not be suitable for conversation session segmentation. In this paper, we address the problem of session segmentation for open-domain conversations. We leverage the classic TextTiling approach, but enhance it with", "research shows that previous utterances in a conversation session are important because they capture rich background information. Sordoni et al. BIBREF11 summarize a single previous sentence as bag-of-words features, which are fed to a recurrent neural network for reply generation. Serban et al. BIBREF17 design an attention-based neural network over all previous conversation turns/rounds, but this could be inefficient if a session lasts long in real commercial applications. By contrast, our paper addresses the problem of session segmentation so as to retain near, relevant context utterances and to eliminate far, irrelevant ones. A similar (but different) research problem is topic tracking in conversations, e.g., BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . In these approaches, the goal is typically a classification problem with a few pre-defined conversation states/topics, and hence it can hardly be generalized to general-purpose session segmentation. Text Segmentation. An early and", "to segment lectures. Ye et al. BIBREF24 minimize between-segment similarity while maximizing within-segment similarity. However, the above complicated approaches are known as global methods: when we perform segmentation between two successive sentences, future context information is needed. Therefore, they are inapplicable to real-time chat-bots, where conversation utterances can be viewed as streaming data. In our study, we prefer the simple yet effective TextTiling approach for open-domain dialogue session segmentation, but enhance it with modern advances of word embeddings, which are robust in capturing semantics of words. We propose a tailored algorithm for word embedding learning by combining a query and context as a \u201cvirtual document\u201d; we also propose several heuristics for similarity measuring. TextTiling. We apply a TextTiling-like algorithm for session segmentation. The original TextTiling is proposed by Hearst BIBREF12 . The main idea is to measure the similarity of each"]}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_answer": "DuMi chat corpus from Chinese.", "predicted_evidence": ["the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system. Dataset. To evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese. We sampled 200 sessions as our experimental corpus. Session segmentation was manually annotated before experiments, serving as the ground truth. The 200 sessions were randomly split by 1:1 for validation and testing. Notice that, our method does not require labeled training samples; massive data with labels of high quality are quite expensive to obtain. We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with \u201cvirtual sentences.\u201d The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).", "to train our word embeddings with \u201cvirtual sentences.\u201d The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms). Segmentation Performance. We compared our full method (TextTiling with heuristic-max based on embeddings trained by virtual sentences) with several baselines: Random. We randomly segmented conversation sessions. In this baseline, we were equipped with the prior probability of segmentation. MMD. We applied the MinMax-Dotplotting (MMD) approach proposed by Ye et al. BIBREF24 . We ran the executable program provided by the authors. TextTiling w/ tf INLINEFORM0 idf features. We implemented TextTiling ourselves according to BIBREF12 . We tuned the hyperparameter INLINEFORM0 in Equation ()on the validation set to make the number of segmentation close to that of manual annotation, and reported precision, recall, and the F-score on the test set in Table TABREF18 . As seen, our approach significantly", "TextTiling approach largely outperforms baselines. We conducted an external experiment to show the effect of session segmentation in dialogue systems. We integrated the segmentation mechanism into a state-of-the-practice retrieval-based system and evaluated the results by manual annotation, similar to our previous work BIBREF27 , BIBREF31 , BIBREF32 . Concretely, we compared our session segmentation with fixed-length context, used in BIBREF11 . That is to say, the competing method always regards two previous utterances as context. We hired three workers to annotate the results with three integer scores (0\u20132 points, indicating bad, borderline, and good replies, respectively.) We sampled 30 queries from the test set of 100 sessions. For each query, we retrieved 10 candidates and computed p@1 and nDCG scores BIBREF33 (averaged over three annotators). Provided with previous utterances as context, each worker had up to 1000 sentences to read during annotation. Table TABREF26 presents the", "between two agents. Dialogue Systems and Context Modeling. Human-computer dialogue systems can be roughly divided into several categories. Template- and rule-based systems are mainly designed for certain domains BIBREF4 , BIBREF5 , BIBREF13 . Although manually engineered templates can also be applied in the open domain like BIBREF14 , but their generated sentences are subject to 7 predefined forms, and hence are highly restricted. Retrieval methods search for a candidate reply from a large conversation corpus given a user-issued utterance as a query BIBREF7 . Generative methods can synthesize new replies by statistical machine translation BIBREF15 , BIBREF16 or neural networks BIBREF8 . The above studies do not consider context information in reply retrieval or generation. However, recent research shows that previous utterances in a conversation session are important because they capture rich background information. Sordoni et al. BIBREF11 summarize a single previous sentence as", "session. In this paper, we addressed the problem of session segmentation for open-domain dialogue systems. We proposed an embedding-enhanced TextTiling approach, where we trained embeddings with the novel notion of virtual sentences; we also proposed several heuristics for similarity measure. Experimental results show that both our embedding learning and similarity measuring are effective in session segmentation, and that with our approach, we can improve the performance of a retrieval-based dialogue system. We thank anonymous reviewers for useful comments and Jingbo Zhu for sharing the MMD executable program. This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61272343 and 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032), and the National Basic Research Program (973 Program No. 2014CB340405).", "measuring. TextTiling. We apply a TextTiling-like algorithm for session segmentation. The original TextTiling is proposed by Hearst BIBREF12 . The main idea is to measure the similarity of each adjacent sentence pair; then \u201cvalleys\u201d of similarities are detected for segmentation. Concretely, the \u201cdepth of the valley\u201d is defined by the similarity differences between the peak point in each side and the current position. We may obtain some statistics of depth scores like the mean INLINEFORM0 and standard deviation INLINEFORM1 , and perform segmentation by a cutoff threshold. where INLINEFORM0 is a hyperparameter adjusting the number of segmentation boundaries; INLINEFORM1 and INLINEFORM2 are the average and standard deviation of depth scores, respectively. In the scenario of human-computer conversations, we compute the depth solely by the similarity difference between its left peak (previous context) and the current position. This is because we cannot obtain future utterances during", "Introduction. Human-computer dialog/conversation is one of the most challenging problems in artificial intelligence. Given a user-issued utterance (called a query in this paper), the computer needs to provide a reply to the query. In early years, researchers have developed various domain-oriented dialogue systems, which are typically based on rules or templates BIBREF4 , BIBREF5 , BIBREF6 . Recently, open-domain conversation systems have attracted more and more attention in both academia and industry (e.g., XiaoBing from Microsoft and DuMi from Baidu). Due to high diversity, we can hardly design rules or templates in the open domain. Researchers have proposed information retrieval methods BIBREF7 and modern generative neural networks BIBREF8 , BIBREF9 to either search for a reply from a large conversation corpus or generate a new sentence as the reply. In open-domain conversations, context information (one or a few previous utterances) is particularly important to language", "for conversation session segmentation. In this paper, we address the problem of session segmentation for open-domain conversations. We leverage the classic TextTiling approach, but enhance it with modern embedding-based similarity measures. Compared with traditional bag-of-words features, embeddings map discrete words to real-valued vectors, capturing underlying meanings in a continuous vector space; hence, it is more robust for noisy conversation corpora. Further, we propose a tailored method for word embedding learning. In traditional word embedding learning, the interaction between two words in a query and a reply is weaker than that within an utterance. We propose to combine a query and its corresponding reply as a \u201cvirtual sentence,\u201d so that it provides a better way of modeling utterances between two agents. Dialogue Systems and Context Modeling. Human-computer dialogue systems can be roughly divided into several categories. Template- and rule-based systems are mainly designed"]}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "predicted_answer": "Yes", "predicted_evidence": ["(Eq. 7)  where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:  $$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$   (Eq. 8)  where $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds. Experiments. We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a", "1). In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to $E$ if $E$ does not contain them (Lines 6-10). As there may be a large number of questions generated in the loop, we limit the maximum number of iterations with $I_{max}$ (Line 4). The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:  $$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$   (Eq. 7)  where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:  $$\\textsc", "in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from serban-EtAl:2016:P16-1 are either ungrammatical (such as \u201cwho was someone who was involved in the leukemia ?\u201d and \u201cwhats the title of a book of the subject of the bible ?\u201d), unnatural (\u201cwhat 's one of the mountain where can you found in argentina in netflix ?\u201d) or confusing (\u201cwho was someone who was involved in the leukemia ?\u201d). Domain Relevance. We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset. Shown in Table 3 , we compare our domain-relevance evaluation method (section", "KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs. Evaluation on Freebase. We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare", "embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document. Simple as it is, our method outperforms all previous methods proving its effectiveness. The reason can be that word embeddings captures the similarity between distinct words (such as \u201cfinance\u201d and \u201ceconomy\u201d), while it is hard for traditional methods. On the order hand, LDA only learns probabilities of words belonging to topics. Evaluation on the Domain-specific KB. The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table", "and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d. Conclusion. We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from serban-EtAl:2016:P16-1", "questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with serban-EtAl:2016:P16-1 where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from", "web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g. performsActivity $\\Rightarrow $ how to) and placeholders for the subject (#X#) and the object (#Y#). A seed question set is then generated by applying the templates on the KB. The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate. The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple"]}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "predicted_answer": "By calculating cosine similarity between document embedding and in-domain data.", "predicted_evidence": ["embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document. Simple as it is, our method outperforms all previous methods proving its effectiveness. The reason can be that word embeddings captures the similarity between distinct words (such as \u201cfinance\u201d and \u201ceconomy\u201d), while it is hard for traditional methods. On the order hand, LDA only learns probabilities of words belonging to topics. Evaluation on the Domain-specific KB. The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table", "in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from serban-EtAl:2016:P16-1 are either ungrammatical (such as \u201cwho was someone who was involved in the leukemia ?\u201d and \u201cwhats the title of a book of the subject of the bible ?\u201d), unnatural (\u201cwhat 's one of the mountain where can you found in argentina in netflix ?\u201d) or confusing (\u201cwho was someone who was involved in the leukemia ?\u201d). Domain Relevance. We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset. Shown in Table 3 , we compare our domain-relevance evaluation method (section", "snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset. Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods: phan2008learning first derives latent topics with LDA BIBREF15 from Wikipedia, then uses the topics as appended features to expand the short text. chen2011short further expanded phan2008learning by using multi-granularity topics. ma-EtAl:2015:VSM-NLP adopts a Bayesian model that the probability a document $D$ belongs to a topic $t$ equals to the prior of $t$ times the probability each word $w$ in $D$ comes from $t$ . Our method first concatenates training documents of the same domain into one \u201cdomain document\u201d, then calculates each document embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document. Simple as it is, our method outperforms", "1). In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to $E$ if $E$ does not contain them (Lines 6-10). As there may be a large number of questions generated in the loop, we limit the maximum number of iterations with $I_{max}$ (Line 4). The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:  $$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$   (Eq. 7)  where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:  $$\\textsc", "KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs. Evaluation on Freebase. We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare", "(Eq. 7)  where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:  $$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$   (Eq. 8)  where $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds. Experiments. We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a", "and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d. Conclusion. We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from serban-EtAl:2016:P16-1", "questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from serban-EtAl:2016:P16-1 on 500 random-selected triples from Freebase. We also demonstrated generated questions from our in-house KB of power tool domain, which are fluent and domain-relevant in general. Our current system only generates questions without answers, leaving automatic answer mining as our future work."]}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "predicted_answer": "106", "predicted_evidence": ["web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g. performsActivity $\\Rightarrow $ how to) and placeholders for the subject (#X#) and the object (#Y#). A seed question set is then generated by applying the templates on the KB. The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate. The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple", "as \u201cperformsActivity\u201d). A KB can also be viewed as a list of triples in the format of $\\langle $ subject, predicate, object $\\rangle $ , where subjects and objects are entities, and predicates are relations. System. Shown in Figure 1 , our system contains the sub-modules of question template construction, seed question generation, question expansion and selection. Given an input KB, a small set of question templates is first constructed such that each template is associated with a predicate, then a seed question set is generated by applying the template set on the input KB, before finally more questions are generated from related questions that are iteratively retrieved from a search engine with already-obtained questions as search queries (section \"Experiments\" ). Taking our in-house KB of power tool domain as an example, template \u201chow to use #X#\u201d is first constructed for predicate \u201cperformsActivity\u201d. In addition, seed question \u201chow to use jigsaw\u201d is generated by applying the", "of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with serban-EtAl:2016:P16-1 on 500 random selected triples from Freebase BIBREF7 . Evaluated by 3 human graders, questions generated by our system are significantly better then serban-EtAl:2016:P16-1 on grammaticality and naturalness. Knowledge Base. A knowledge base (KB) can be viewed as a directed graph, in which nodes are entities (such as \u201cjigsaw\u201d and \u201cCurveCut\u201d) and edges are relations of entities (such as \u201cperformsActivity\u201d). A KB can also be viewed as a list of triples in the format of $\\langle $ subject, predicate, object $\\rangle $ , where subjects and objects are entities, and predicates are", "KB of power tool domain as an example, template \u201chow to use #X#\u201d is first constructed for predicate \u201cperformsActivity\u201d. In addition, seed question \u201chow to use jigsaw\u201d is generated by applying the template on triple \u201c $\\langle $ jigsaw, performsActivity, CurveCut $\\rangle $ \u201d, before finally questions (Figure 2 ) are retrieved from Google with the seed question. Question expansion and selection. [t] seed question set $S$ candidate questions $E$ $E \\leftarrow S$ $Q \\leftarrow S$ $I \\leftarrow 0$ len $(Q) > 0$ and $I < I_{max}$ $I = I + 1$ $q_{cur}$ $\\leftarrow $ $E$0 .Pop() $E$1 in WebExp $E$2 not $E$3 .contains $E$4 $E$5 .Append( $E$6 ) $E$7 .Push( $E$8 ) Question expansion method Shown in Algorithm \"Experiments\" , the expanded question set $E$ is initialized as the seed question set (Line 1). In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to $E$ if $E$ does not contain them (Lines 6-10). As there may be a large number of", "and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d. Conclusion. We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from serban-EtAl:2016:P16-1", "forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work BIBREF9 , BIBREF10 relies on massive human-labeled data. Treating question generation as a machine translation problem, serban-EtAl:2016:P16-1 train a neural machine translation (NMT) system with 10,000 $\\langle $ triple, question $\\rangle $ pairs. At test time, input triples are \u201ctranslated\u201d into questions with the NMT system. On the other hand, the question part of the 10,000 pairs are human generated, which requires a large amount of human effort. In addition, the grammaticality and naturalness of generated questions can not be guaranteed (as seen in Table 1 ). We propose a system for generating questions from KB that significantly reduces the human effort by leveraging the massive web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g.", "KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs. Evaluation on Freebase. We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare", "embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document. Simple as it is, our method outperforms all previous methods proving its effectiveness. The reason can be that word embeddings captures the similarity between distinct words (such as \u201cfinance\u201d and \u201ceconomy\u201d), while it is hard for traditional methods. On the order hand, LDA only learns probabilities of words belonging to topics. Evaluation on the Domain-specific KB. The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table"]}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "predicted_answer": "By using a 4-gram language model score.", "predicted_evidence": ["(Eq. 7)  where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:  $$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$   (Eq. 8)  where $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds. Experiments. We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a", "questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best. We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical. Shown in Table 1 , we compare our questions with serban-EtAl:2016:P16-1 where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from", "1). In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to $E$ if $E$ does not contain them (Lines 6-10). As there may be a large number of questions generated in the loop, we limit the maximum number of iterations with $I_{max}$ (Line 4). The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:  $$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$   (Eq. 7)  where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:  $$\\textsc", "KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs. Evaluation on Freebase. We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare", "snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset. Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods: phan2008learning first derives latent topics with LDA BIBREF15 from Wikipedia, then uses the topics as appended features to expand the short text. chen2011short further expanded phan2008learning by using multi-granularity topics. ma-EtAl:2015:VSM-NLP adopts a Bayesian model that the probability a document $D$ belongs to a topic $t$ equals to the prior of $t$ times the probability each word $w$ in $D$ comes from $t$ . Our method first concatenates training documents of the same domain into one \u201cdomain document\u201d, then calculates each document embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document. Simple as it is, our method outperforms", "and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d. Conclusion. We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from serban-EtAl:2016:P16-1", "embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document. Simple as it is, our method outperforms all previous methods proving its effectiveness. The reason can be that word embeddings captures the similarity between distinct words (such as \u201cfinance\u201d and \u201ceconomy\u201d), while it is hard for traditional methods. On the order hand, LDA only learns probabilities of words belonging to topics. Evaluation on the Domain-specific KB. The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance. We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table", "forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work BIBREF9 , BIBREF10 relies on massive human-labeled data. Treating question generation as a machine translation problem, serban-EtAl:2016:P16-1 train a neural machine translation (NMT) system with 10,000 $\\langle $ triple, question $\\rangle $ pairs. At test time, input triples are \u201ctranslated\u201d into questions with the NMT system. On the other hand, the question part of the 10,000 pairs are human generated, which requires a large amount of human effort. In addition, the grammaticality and naturalness of generated questions can not be guaranteed (as seen in Table 1 ). We propose a system for generating questions from KB that significantly reduces the human effort by leveraging the massive web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g."]}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "predicted_answer": "tennis press-conference transcripts, game statistics, live text play-by-play commentaries", "predicted_evidence": ["post-match interviews. We therefore use a data-driven approach to distinguish between typical and atypical questions. For any given question, we consider how frequently its words appear in post-match press conferences in general. Specifically, we take the set of all questions as the set of documents, $D$ . We compute the inverse document frequency for each word (after stemming) that has appeared in our dataset, excluding the set $S$ consisting of stop words and a special token for entity names. For a question $q$ that contains the set of unique words $\\lbrace w_1, w_2, ... , w_N\\rbrace \\notin S$ , we compute its atypicality score $Sc(q)$ as: $", "an automatic way to quantify gender bias in sport journalism. Dataset Description. We collect tennis press-conference transcripts from ASAP Sport's website (http://www.asapsports.com/), whose tennis collection dates back to 1992 and is still updated for current tournaments. For our study, we take post- game interviews for tennis singles matches played between Jan, 2000 to Oct 18, 2015. We also obtain easily-extractable match information from a dataset provided by Tennis-Data, which covers the majority of the matches played on the men's side from 2000-2015 and on the women's side from 2007-2015. We match interview transcripts with game statistics by date and player name, keeping only the question and answer pairs from games where the statistics are successfully merged. This gives us a dataset consisting of 6467 interview transcripts and a total of 81906 question snippets posed to 167 female players and 191 male players. To model tennis-game-specific language, we use live text", "$  We use the overall mean atypicality score of the entire question dataset as the cutoff point: questions with scores above the overall mean are considered atypical and the rest are considered typical. Below are some examples: Figure 1 shows that a gender bias with respect to whether game-related language is used exists for both typical and atypical questions. However, additional analysis reveals that the difference in mean perplexity values between genders is highly statistically significantly larger for atypical questions, suggesting that gender bias is more salient among the more unusual queries. Higher ranked players generally attract more media attention, and therefore may be targeted differently by journalists. To understand the effect of player ranking, we divide players into two groups: top 10 players and the rest. For our analysis, we use the ranking of the player at the time the interview was conducted. (It is therefore possible that questions posed to the same player but", "questions posed to male players are significantly closer to game language ( $p$ -value $<$ 0.05), indicating that the observed gender difference is not simply explained by a few highly interviewed players. Relation to Other Factors. We further investigate how the level of gender bias is tied to different factors: how typical the question is (section UID20 ), the ranking of the player (section UID24 ), and whether the player won or lost the match (section UID26 ). For all the following experiments, we use per-question perplexity for comparisons: per-player perplexity is not used due to limited sample size. One might wonder whether the perplexity disparities we see in questions asked of female vs. male players are due to \u201coff-the-wall\u201d queries, rather than to those that are more typical in post-match interviews. We therefore use a data-driven approach to distinguish between typical and atypical questions. For any given question, we consider how frequently its words appear in post-match", "us a dataset consisting of 6467 interview transcripts and a total of 81906 question snippets posed to 167 female players and 191 male players. To model tennis-game-specific language, we use live text play-by-play commentaries collected from the website Sports Mole (http://www.sportsmole.co.uk/). These tend to be short, averaging around 40 words. Here is a sample, taken from the Federer-Murray match at the 2015 Wimbledon semi-final: \u201cThe serve-and-volley is being used frequently by Federer and it's enabling him to take control behind his own serve. Three game points are earned before an ace down the middle seal [sic] the love hold.\u201d For our analysis, we create a gender-balanced set of commentaries consisting of descriptions for 1981 games played for each gender. Method. As a preliminary step, we apply a word-level analysis to understand if there appear to be differences in word usage when journalists interview male players compared to female players. We then introduce our method for", "that mere variations in the lingo of different sports do not introduce extra noise in our language models. Tennis is also useful for our investigation because, as BIBREF1 [ BIBREF1 ] noted, it \u201cmarks the only professional sports where male and female athletes generally receive similar amounts of overall broadcast media coverage during the major tournaments.\" Using our methodology, we are able to quantify gender bias with respect to how game-related interview questions are. We also provide a more fine-grained analysis of how gender differences in journalistic questioning are displayed under various scenarios. To help with further analysis of interview questions and answers, we introduce a dataset of tennis post-match interview transcripts along with corresponding match information. Related Work. In contrast with our work, prior investigations of bias in sport journalism rely on manual coding or are based on simple lists of manually defined keywords. These focus on bias with respect to", "practiced, tiebreaker, maybe, see, impression, serve, history, volley, chance, height, support, shots, server(s), greatest, way, tiebreaks, tiebreakers, era, lucky, luck; yet, new, nervous, improve, seed, friends, nerves, mom, every, matter, become, meet, winning, type, won, draw, found, champion, stop, fight, wind, though, father, thing, love. Game Language Model. To quantify how game-related a question is in a data-driven fashion, we train a bigram language model using KenLM BIBREF18 on the gender-balanced set of live-text play-by-play commentaries introduced in Section \"Dataset Description\" . For an individual question $q$ , we measure its perplexity $PP(q)$ with respect to this game language model $P_{\\textnormal {\\tiny \\tiny commentary}}$ as an indication of how game-related the question is: the higher the perplexity value, the less game-related the question. Perplexity, a standard measure of language-model fit BIBREF19 , is defined as follows for an $N$ -word sequence $w_1 w_2", "Work. In contrast with our work, prior investigations of bias in sport journalism rely on manual coding or are based on simple lists of manually defined keywords. These focus on bias with respect to race, nationality, and gender BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF1 , BIBREF7 ; see BIBREF8 [ BIBREF8 ] for a review. Much of the work on gender bias in sports reporting has focused on \u201cair-time\u201d BIBREF9 , BIBREF10 . Other studies looked at stereotypical descriptions and framing BIBREF11 , BIBREF12 , BIBREF13 , BIBREF0 . For surveys, see BIBREF14 [ BIBREF14 ] or BIBREF15 [ BIBREF15 ], inter alia. Several studies have focused on the particular case of gender-correlated differences in tennis coverage BIBREF16 , BIBREF17 , BIBREF1 . We extend this line of work by proposing an automatic way to quantify gender bias in sport journalism. Dataset Description. We collect tennis press-conference transcripts from ASAP Sport's website (http://www.asapsports.com/), whose tennis"]}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "predicted_answer": "The documents mention multiple datasets, please specify which one.", "predicted_evidence": ["with it, predict its behavior, and diagnose its errors. Acknowledgments. JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship. We are grateful to Lisa Anne Hendricks for assistance with the Caltech\u2013UCSD Birds dataset, and to Liang Huang and Sebastian Schuster for useful feedback. Agents. Learned agents have the following form: where $h$ is a hidden state, $z$ is a message from the other agent, $u$ is a distribution over actions, and $x$ is an observation of the world. A single hidden layer with 256 units and a $\\tanh $ nonlinearity is used for the MLP. The GRU hidden state is also of size 256, and the message vector is of size 64. Agents are trained via interaction with the world as in Hausknecht15DRQN using the adam optimizer BIBREF28 and a discount factor of 0.9. The step size was chosen as $0.003$ for reference games and $0.0003$ for the driving game. An $\\epsilon $ -greedy exploration strategy is employed, with the exploration parameter for", "to each other, must each navigate between randomly assigned start and goal positions without colliding. This task takes a number of steps to complete, and potentially involves a much broader range of communication strategies. To obtain human annotations for this task, we recorded both actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other. We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set. We use the version of the XKCD dataset prepared by McMahan15Colors. Here the input feature vector is simply the LAB representation of each color, and the message inventory taken to be all unigrams that appear at least five times. We use the dataset of Welinder10Birds with natural language annotations from Reed16Birds. The model's input feature representations are a final 256-dimensional hidden feature vector from a compact bilinear pooling model", "Welinder10Birds with natural language annotations from Reed16Birds. The model's input feature representations are a final 256-dimensional hidden feature vector from a compact bilinear pooling model BIBREF24 pre-trained for classification. The message inventory consists of the 50 most frequent bigrams to appear in natural language descriptions; example human traces are generated by for every frequent (bigram, image) pair in the dataset. Driving data is collected from pairs of human workers on Mechanical Turk. Workers received the following description of the task: Your goal is to drive the red car onto the red square. Be careful! You're driving in a thick fog, and there is another car on the road that you cannot see. However, you can talk to the other driver to make sure you both reach your destinations safely. Players were restricted to messages of 1\u20133 words, and required to send at least one message per game. Each player was paid $0.25 per game. 382 games were collected with 5", "your destinations safely. Players were restricted to messages of 1\u20133 words, and required to send at least one message per game. Each player was paid $0.25 per game. 382 games were collected with 5 different road layouts, each represented as an 8x8 grid presented to players as in fig:drive-examples. The action space is discrete: players can move forward, back, turn left, turn right, or wait. These were divided into a 282-game training set and 100-game test set. The message inventory consists of all messages sent more than 3 times. Input features consists of indicators on the agent's current position and orientation, goal position, and map identity. Data is available for download at http://github.com/jacobandreas/neuralese. Metrics. A mechanism for understanding the behavior of a learned model should allow a human user both to correctly infer its beliefs and to successfully interoperate with it; we accordingly report results of both \u201cbelief\u201d and \u201cbehavior\u201d evaluations. To support easy", "players observe a pair of candidate referents. A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset BIBREF17 , BIBREF18 and the Caltech\u2013UCSD Birds dataset BIBREF19 with accompanying natural language descriptions BIBREF20 . We use standard train / validation / test splits for both of these datasets. The final task we consider is the driving task (fig:tasksc) first discussed in the introduction. In this task, two cars, invisible to each other, must each navigate between randomly assigned start and goal positions without colliding. This task takes a number of steps to complete, and potentially involves a much broader range of", "(p(z^{\\prime }) / p(z))$ as constant, those these could be more accurately approximated with a learned density estimator. This model is trained alongside the learned agent to imitate its decisions, but does not get to observe the recurrent state, like so: Here the multilayer perceptron has a single hidden layer with $\\tanh $ nonlinearities and size 128. It is also trained with adam and a step size of 0.0003. We use exactly the same model and parameters to implement representations of $p(z|x)$ for human speakers, but in this case the vector $z$ is taken to be a distribution over messages in the natural language inventory, and the model is trained to maximize the likelihood of labeled human traces.", "should allow a human user both to correctly infer its beliefs and to successfully interoperate with it; we accordingly report results of both \u201cbelief\u201d and \u201cbehavior\u201d evaluations. To support easy reproduction and comparison (and in keeping with standard practice in machine translation), we focus on developing automatic measures of system performance. We use the available training data to develop simulated models of human decisions; by first showing that these models track well with human judgments, we can be confident that their use in evaluations will correlate with human understanding. We employ the following two metrics: This evaluation focuses on the denotational perspective in semantics that motivated the initial development of our model. We have successfully understood the semantics of a message $z_r$ if, after translating $z_r \\mapsto z_h$ , a human listener can form a correct belief about the state in which $z_r$ was produced. We construct a simple state-guessing game where the", "Introduction. Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network. DCPs have been shown to solve a variety of coordination problems, including reference games BIBREF0 , logic puzzles BIBREF1 , and simple control BIBREF2 . Appealingly, the agents' communication protocol can be learned via direct backpropagation through the communication channel, avoiding many of the challenging inference problems associated with learning in classical decentralized decision processes BIBREF3 . But analysis of the strategies induced by DCPs has remained a challenge. As an example, fig:teaser depicts a driving game in which two cars, which are unable to see each other, must both cross an intersection without colliding. In order to ensure success, it is clear that the cars must communicate with each"]}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "predicted_answer": "The data collection method is not provided in the documents. \n\nUnanswerable", "predicted_evidence": ["sets, to test the model's ability to recognize wordings not heard during training. The dataset has a .csv file for each split that lists the speaker ID, file path, transcription, and slots for all the .wav files in that split. Related datasets. Here we review some related public datasets and show the gap that Fluent Speech Commands fills. The Google Speech Commands dataset BIBREF26 (to which the name \u201cFluent Speech Commands\u201d is an homage) is a free dataset of 30 single-word spoken commands (\u201cyes\u201d, \u201cno\u201d, \u201cstop\u201d, \u201cgo\u201d, etc.). This dataset is suitable for keyword spotting experiments, but not for SLU. ATIS is an SLU dataset consisting of utterances related to travel planning. This dataset can only be obtained expensively from the Linguistic Data Consortium. The Snips NLU Benchmark BIBREF2 has a rich set of virtual assistant commands, but contains only text, with no audio, and hence is not suitable for end-to-end SLU experiments. The Grabo, Domotica, and Patcor datasets are three related", "order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset. The data was validated by a separate set of crowdsourcers. All audios deemed by the crowdsourcers to be unintelligible or contain the wrong phrase were removed. The total number of speakers, utterances, and hours of audio remaining is shown in Table TABREF12 . Dataset splits. The utterances are randomly divided into train, valid, and test splits in such a way that no speaker appears in more than one split. Each split contains all possible wordings for each intent, though our code has the option to include data for only certain wordings for different sets, to test the model's ability to recognize wordings not heard during training. The dataset has a .csv file for each split that lists the speaker ID, file path, transcription, and slots for", "during pre-training. For the test set, the frozen model and partially unfrozen model perform roughly equally well (Table TABREF28 , \u201cfull\u201d column), possibly because the test set is \u201ceasier\u201d than the validation set. In all cases, the pre-trained models outperform the randomly initialized model. Partial dataset. To simulate a smaller dataset, we randomly selected 10% of the training set, and used this instead of the entire training set. Fig. shows the validation accuracy (on the entire validation set, not a subset) over time. A similar trend is observed as for the entire dataset: unfreezing the word layers works best. The gap in final test accuracy between the randomly initialized model and the pre-trained models increases (Table TABREF28 , \u201c10%\u201d column); the final test accuracy for the pre-trained models drops only slightly, further highlighting the advantage of our proposed method. Generalizing to new wordings. What happens if new wordings appear in the test data that never appear in", "a rich set of virtual assistant commands, but contains only text, with no audio, and hence is not suitable for end-to-end SLU experiments. The Grabo, Domotica, and Patcor datasets are three related datasets of spoken commands for robot control and card games developed by KU Leuven and used in BIBREF8 . These datasets are free, but have only a small number of speakers and phrases. In contrast to these datasets, Fluent Speech Commands is simultaneously audio-based, reasonably large, and free, and contains several multiple-word commands corresponding to each of the intents. Model and Pre-training Strategy. The model proposed in this paper, shown in Fig. FIGREF17 , is a deep neural network consisting of a stack of modules, where the first modules are pre-trained to predict phonemes and words. The word and phoneme classifiers are discarded, and the entire model is then trained end-to-end on the supervised SLU task. In what follows, we justify these design decisions and give more details", "well. The lack of a good open-source dataset for end-to-end SLU experiments makes it difficult for most people to perform high-quality, reproducible research on this topic. We therefore created a new SLU dataset, the \u201cFluent Speech Commands\u201d dataset, which Fluent.ai releases along with this paper. Dataset. This section describes the structure and creation of Fluent Speech Commands. Audio and labels. The dataset is composed of 16 kHz single-channel .wav audio files. Each audio file contains a recording of a single command that one might use for a smart home or virtual assistant, like \u201cput on the music\u201d or \u201cturn up the heat in the kitchen\u201d. Each audio is labeled with three slots: action, object, and location. A slot takes on one of multiple values: for instance, the \u201clocation\u201d slot can take on the values \u201cnone\u201d, \u201ckitchen\u201d, \u201cbedroom\u201d, or \u201cwashroom\u201d. We refer to the combination of slot values as the intent of the utterance. The dataset has 31 unique intents in total. We do not distinguish", "The model proposed here is somewhat similar to their multi-task model, although we do not use or require the ASR targets during SLU training. The work listed above deals with very high resource SLU\u2014in BIBREF7 , for instance, the Google Home BIBREF23 dataset consists of 24 million labeled utterances. In contrast, Renkens et al. in BIBREF8 consider the problem of end-to-end SLU with limited training data, and find that capsule networks BIBREF24 , compared to conventional neural network models, are more easily capable of learning end-to-end SLU from scratch. However, they do not consider the effect of pre-training on other speech data. This previous work has all been conducted on datasets that are closed-source or too small to test hypotheses about the amount of data required to generalize well. The lack of a good open-source dataset for end-to-end SLU experiments makes it difficult for most people to perform high-quality, reproducible research on this topic. We therefore created a new", "on the values \u201cnone\u201d, \u201ckitchen\u201d, \u201cbedroom\u201d, or \u201cwashroom\u201d. We refer to the combination of slot values as the intent of the utterance. The dataset has 31 unique intents in total. We do not distinguish between domain, intent, and slot prediction, as is sometimes done in SLU BIBREF25 . The dataset can be used as a multi-label classification task, where the goal is to predict the action, object, and location labels. Since the slots are not actually independent of each other, a more careful approach would model the relationship between slots, e.g. using an autoregressive model, as in BIBREF7 . We use the simpler multi-label classification approach in this paper, so as to avoid the issues sometimes encountered training autoregressive models and instead focus on questions related to generalization using a simpler model. Alternately, the 31 distinct intents can be \u201cflattened\u201d and used as 31 distinct labels for a single-label classification task. For each intent, there are multiple possible", "models and learning these hierarchical representations without a large amount of training data is difficult. The computer vision BIBREF14 , BIBREF15 , natural language processing BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , and ASR BIBREF21 , BIBREF22 communities have attacked the problem of limited supervised training data with great success by pre-training deep models on related tasks for which there is more training data. Following their lead, we propose an efficient ASR-based pre-training methodology in this paper and show that it may be used to improve the performance of end-to-end SLU models, especially when the amount of training data is very small. Our contributions are as follows: Related work. Three key papers describing end-to-end SLU were written by Qian et al. BIBREF4 , Serdyuk et al. BIBREF5 , and Chen et al. BIBREF6 . Serdyuk et al. in BIBREF5 use no pre-training whatsoever. Qian et al. in BIBREF4 use an auto-encoder to initialize the SLU model. Chen et al."]}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "predicted_answer": "MAE and Accuracy$\\pm k$", "predicted_evidence": ["by the listing price. The predictions were unnormalized before final evaluations. Further, we only considered the negotiations where an agreement was reached. These were the instances for which ground truth was available ($\\sim 75\\%$ of the data). We use a two-layer GRU with a dropout of $0.1$ and 50 hidden units. The models were trained for a maximum of 5000 iterations, with AdamW optimizer BIBREF13, a learning rate of 2x$10^{^-5}$ and a batch size of 4. We used a linear warmup schedule for the first $0.1$ fraction of the steps. All the hyper-parameters were optimized on the provided development set. Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\in \\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\pm k$: the percentage of cases where the predicted price", "our model empirically with a number of baseline methods. This section presents the methods we compare to, the training setup and the evaluation metrics. Methods: The first baseline is the Listing Price (LP) where the model ignores the negotiation and returns the listing price of the product. Similarly, we use Target Price (TP), where the model just returns the target price for the buyer. We also consider the mean of Listing and Target price (TP+LP/2) as another baseline. Although trivial, these baselines help in benchmarking our results and also show good performance in some cases. Next, we build another baseline which completely ignores the natural language incorporation. In this case, the model only sees a sequence of prices shared across the messages in the negotiation. We keep the input format the same as our model and all the parameters are randomly initialized to remove learning from natural language. We refer to this model as Prices-only. We compare two variants for BERT-based", "SECREF3). Without this usage, the fine-tuning pipeline proves to be inadequate. Overall, BERT-GRU achieves $67.08\\%$ Accuracy$\\pm 10$ with just the product scenario, reaching to $71.16\\%$ with $60\\%$ of the messages and crosses $90\\%$ as more information about the final price is revealed. Paired Bootstrap Resampling BIBREF14 with $10,000$ bootstraps shows that for a given $f$, BERT-GRU is better than its Prices-only counterpart with $95\\%$ statistical significance. The prices discussed during the negotiation still play a crucial role in making the predictions. In fact, in only $65\\%$ of the negotiations, the first price is quoted within the first $0.4$ fraction of the events. This is visible in higher performance as more events are seen after this point. This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Price-only model for these categories over others. The models also show evidence of capturing buyer interest. By constructing", "capture the sequential nature of negotiation events, we pass these [CLS] representations through Gated-Recurrent Units (GRU). Recurrent Networks have been shown to be useful along with Transformer architectures BIBREF12. Finally, a feed-forward network is applied to predict the agreed price for the negotiation. The model is end-to-end trained and fine-tuned using the Mean Squared Error (MSE) loss between the predicted price and the ground-truth. Experimental Details. We perform experiments on the CB dataset to primarily answer two questions: 1) Is it feasible to predict negotiation outcomes without observing the complete conversation between the buyer and seller? 2) To what extent does the natural language incorporation help in the prediction? In order to answer these questions, we compare our model empirically with a number of baseline methods. This section presents the methods we compare to, the training setup and the evaluation metrics. Methods: The first baseline is the Listing", "history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on. Incorporating natural language in both the scenario and event messages paves the way to leverage such cues and make better predictions early on in the conversation, as depicted in the plots. Both BERT and BERT-GRU consistently perform well on the complete test set. There is no clear winner, although using a recurrent network proves to be more helpful in the early stages of the negotiation. Note that BERT method still employs multiple [SEP] tokens along with alternating segment embeddings (Section SECREF3). Without this usage, the fine-tuning pipeline proves to be inadequate. Overall, BERT-GRU achieves $67.08\\%$ Accuracy$\\pm 10$ with just the product scenario, reaching to $71.16\\%$ with $60\\%$", "format the same as our model and all the parameters are randomly initialized to remove learning from natural language. We refer to this model as Prices-only. We compare two variants for BERT-based models. First, for the BERT method, we keep only the first [CLS] token in the input and then train the model with fine-tuning using a single feed-forward network on top of the [CLS] representation. Secondly, we call our complete approach as BERT+GRU, where we use a recurrent network with BERT fine-tuning, as depicted in Figure FIGREF3. Training Details: Given the multiple segments in our model input and small data size, we use BERT-base BIBREF8, having output dimension of 768. To tackle the variance in product prices across different categories, all prices in the inputs and outputs were normalized by the listing price. The predictions were unnormalized before final evaluations. Further, we only considered the negotiations where an agreement was reached. These were the instances for which", "for Housing, Bike and Car, resulting in relative better performance of Price-only model for these categories over others. The models also show evidence of capturing buyer interest. By constructing artificial negotiations, we observe that the model predictions at $f$=$0.2$ increase when the buyer shows more interest in the product, indicating more willingness to pay. With the capability to incorporate cues from natural language, such a framework can be used in the future to get negotiation feedback, in order to guide the planning of a negotiating agent. This can be a viable middle-ground between following the average human behavior through supervised learning or exploring the wild by optimizing on rewards using reinforcement learning BIBREF6, BIBREF4. Conclusion. We presented a framework to attempt early predictions of the agreed product prices in buyer-seller negotiations. We construct sentence templates to encode the product scenario, exchanged messages and associated auxiliary", "BIBREF8 in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner (Figure FIGREF3). Early prediction of outcomes is essential for effective planning of an automatically negotiating agent. Although there have been attempts to gain insights into negotiations BIBREF9, BIBREF10, to the best of our knowledge, we are the first to study early natural language cues through a data-driven neural system (Section SECREF3). Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well. We provide a sample negotiation from the test set BIBREF4 along with our model predictions in Table TABREF1. Problem Setup. We study human-human negotiations in the buyer-seller bargaining scenario, which has been a"]}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "predicted_answer": "Craigslist Bargaining dataset (CB) and IWSLT German-English spoken-domain translation dataset.", "predicted_evidence": ["history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on. Incorporating natural language in both the scenario and event messages paves the way to leverage such cues and make better predictions early on in the conversation, as depicted in the plots. Both BERT and BERT-GRU consistently perform well on the complete test set. There is no clear winner, although using a recurrent network proves to be more helpful in the early stages of the negotiation. Note that BERT method still employs multiple [SEP] tokens along with alternating segment embeddings (Section SECREF3). Without this usage, the fine-tuning pipeline proves to be inadequate. Overall, BERT-GRU achieves $67.08\\%$ Accuracy$\\pm 10$ with just the product scenario, reaching to $71.16\\%$ with $60\\%$", "negotiation from the test set BIBREF4 along with our model predictions in Table TABREF1. Problem Setup. We study human-human negotiations in the buyer-seller bargaining scenario, which has been a key research area in the literature BIBREF0. In this section, we first describe our problem setup and key terminologies by discussing the dataset used. Later, we formalize our problem definition. Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics.", "the category, target price and the product title in natural language sentences. These sentences are concatenated to form our Scenario $S$. Moving ahead in a similar manner, we define templates to capture the negotiator identity (buyer/seller) and any message which is conveyed. As shown in Figure FIGREF3, the scenario $S$ and the events are separated with the usage of [SEP] tokens. Following BIBREF11, who use BERT for extractive text summarization, we add a [CLS] token at the beginning of each segment. We also alternate between a sequence of 0s and 1s for segment embeddings to differentiate between the scenario and the events. Architecture and Learning: BERT representation for each [CLS] token is a contextualized encoding for the associated word sequence after it. In order to further capture the sequential nature of negotiation events, we pass these [CLS] representations through Gated-Recurrent Units (GRU). Recurrent Networks have been shown to be useful along with Transformer", "to attempt early predictions of the agreed product prices in buyer-seller negotiations. We construct sentence templates to encode the product scenario, exchanged messages and associated auxiliary information into the same hidden space. By combining a recurrent network and the pre-trained BERT encoder, our model leverages natural language cues in the exchanged messages to predict the negotiation outcomes early on in the conversation. With this capability, such a framework can be used in a feedback mechanism to guide the planning of a negotiating agent. Category-wise performance. We show the category-wise performance in Figure FIGREF11.", "BIBREF8 in pre-trained language encoders, we attempt to predict negotiation outcomes early on in the conversation, in a completely data-driven manner (Figure FIGREF3). Early prediction of outcomes is essential for effective planning of an automatically negotiating agent. Although there have been attempts to gain insights into negotiations BIBREF9, BIBREF10, to the best of our knowledge, we are the first to study early natural language cues through a data-driven neural system (Section SECREF3). Our evaluations show that natural language allows the models to make better predictions by looking at only a fraction of the negotiation. Rather than just realizing the strategy in natural language, our empirical results suggest that language can be crucial in the planning as well. We provide a sample negotiation from the test set BIBREF4 along with our model predictions in Table TABREF1. Problem Setup. We study human-human negotiations in the buyer-seller bargaining scenario, which has been a", "SECREF3). Without this usage, the fine-tuning pipeline proves to be inadequate. Overall, BERT-GRU achieves $67.08\\%$ Accuracy$\\pm 10$ with just the product scenario, reaching to $71.16\\%$ with $60\\%$ of the messages and crosses $90\\%$ as more information about the final price is revealed. Paired Bootstrap Resampling BIBREF14 with $10,000$ bootstraps shows that for a given $f$, BERT-GRU is better than its Prices-only counterpart with $95\\%$ statistical significance. The prices discussed during the negotiation still play a crucial role in making the predictions. In fact, in only $65\\%$ of the negotiations, the first price is quoted within the first $0.4$ fraction of the events. This is visible in higher performance as more events are seen after this point. This number is lower than average for Housing, Bike and Car, resulting in relative better performance of Price-only model for these categories over others. The models also show evidence of capturing buyer interest. By constructing", "is either the Buyer or Seller, Type can be one of (message, offer, accept, reject or quit) and Data consists of either the corresponding natural language dialogue, offer price or can be empty. Nearly $80\\%$ of events in CB dataset are of type `message', each consisting a textual message as Data. An offer is usually made and accepted at the end of each negotiation. Since the offers directly contain the agreed price (which we want to predict), we only consider `message' events in our models. Given the scenario $S$ and first $n$ events $E_n$, our problem is then to learn the function $f_{n}$: $A = f_{n}(S, E_n)$ where $A$ refers to the final agreed price between the two negotiating parties.  Approach. Pre-trained language models, such as BERT BIBREF7, BIBREF8 have recently gained huge success on a wide range of NLP tasks. However, since our framework deals with various auxiliary pieces (category, price, etc.), we cannot directly leverage these language models, which have only been", "capture the sequential nature of negotiation events, we pass these [CLS] representations through Gated-Recurrent Units (GRU). Recurrent Networks have been shown to be useful along with Transformer architectures BIBREF12. Finally, a feed-forward network is applied to predict the agreed price for the negotiation. The model is end-to-end trained and fine-tuned using the Mean Squared Error (MSE) loss between the predicted price and the ground-truth. Experimental Details. We perform experiments on the CB dataset to primarily answer two questions: 1) Is it feasible to predict negotiation outcomes without observing the complete conversation between the buyer and seller? 2) To what extent does the natural language incorporation help in the prediction? In order to answer these questions, we compare our model empirically with a number of baseline methods. This section presents the methods we compare to, the training setup and the evaluation metrics. Methods: The first baseline is the Listing"]}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "predicted_answer": "They identified the POS tags of each sentence of the speech and checked all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct, but it points in a certain direction. They used an automatic evaluation to evaluate the grammatical correctness and identified for each sentence of the speech its POS tags and then checked all sentences of the entire corpus whether one has the same sequence of POS tags.", "predicted_evidence": ["We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated speeches can be found in the appendix of this report. Manual Evaluation. For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores. Automatic Evaluation. The automatic evaluation aims to evaluate both the grammatical correctness and the consistency of the speech in terms of its content. For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the", "across the speech-termination token or the generated speech reaches a certain length. The crucial part of this method is the measure of similarity between two sentences. Our similarity is composed of structural and textual similarity. Both are normalized to a range between 0 and 1 and weighted through a factor INLINEFORM0 . We compute the similarity between two sentences INLINEFORM1 and INLINEFORM2 as follows: INLINEFORM3  For the structural similarity we compare the POS tags of both sentences and determine the longest sequence of congruent POS tags. The length of this sequence, normalized by the length of the shorter sentence, gives us the structural similarity. The structural similarity measure aims to support smooth sentence transitions. That is, if we find sentences which have a very similar sentence structure, it is very likely that they connect well to either of their following sentences. The textual similarity is defined by the number of trigrams that occur in both sentences,", "its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually. In order to evaluate the content of the generated speech we determine the mixture of topics covered by the speech and order them by their topic coverage. That gives us information about the primary topic and secondary topics. Then we do the same for each speech in our dataset which is of the same class and compare the topic order with the one of the generated speech. We sum up the topic coverage values of each topic that occurs in both speeches at the same position. The highest achieved value is used as", "indicates that the topic model may need some improvement. Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual evaluation. Most of the speeches which achieved a high score in the manual evaluation scored also high in the automatic evaluation. Furthermore, it also confirms that the overall the grammatical correctness of the speeches is very good while the content is a bit behind. Conclusion. In this report we have presented a novel approach of training a system on speech transcripts in order to generate new speeches. We have shown that n-grams and J&K POS tag filter are very effective as language and topic model for this task. We have shown how to combine these models to a system that produces good results. Furthermore, we have presented different methods to evaluate the quality of generated texts. In an experimental evaluation our system performed very well. In particular, the grammatical correctness", "similar sentence structure, it is very likely that they connect well to either of their following sentences. The textual similarity is defined by the number of trigrams that occur in both sentences, normalized by the length of the longer sentence. This similarity aims to find sentences which use the same words. The obvious advantage of the sentence-based approach is that every sentence is grammatically correct since they originate directly from the training data. However, connecting sentences reasonable is a very challenging task. A further step to improve this approach would be to extend the similarity measure by a topical similarity and a semantic similarity. The topical similarity should measure the topical correspondence of the originating speeches, while the semantic similarity should help to find sentences which express the same meaning although using different words. However, the results from the word-based approach were more promising and therefore we have decided to discard", "assessing the error we used cross-entropy loss function. Furthermore we used Stochastic Gradient Descent (SGD) to minimize the loss and Backpropagation Through Time (BPTT) to calculate the gradients. After training the network for 100 time epochs ( INLINEFORM0 14 h) the results were still pretty bad. Most of the generated sentences were grammatically incorrect. There are many options to improve the performance of RNNs but due to the good performance shown by n-grams, the time-consuming training, and the limited time for this project we have decided to not further purse this approach. Latent Dirichlet Allocation. As alternative to the J&K POS tag filter we used LDA as topic model. In particular we used the approach from Lau et al. UID18 . That is, we removed all occurrences of stop words, stemmed the remaining words, replaced the 1000 most-frequent bigrams with single tokens, and deleted the 200 most frequent terms from the vocabulary before applying ordinary LDA. Since our dataset", "references in the speech segments, determined the targets of those references, and decided whether a reference represents an instance of agreement or disagreement. However, we focus only on the individual speech segments and disregard references. For our work we have removed single-sentence speeches, HTML-tags and corrected punctuation marks. In order to enable simple sentence splitting we replaced all sentence delimiters by a stop-token. Furthermore, we inserted special tokens which indicate the start and the end of a speech. Then we divided all the speeches into the four classes given by the combination of possible political parties and speech opinions. Table TABREF1 shows the four speech classes and table TABREF2 gives a quantitative overview of the corpus\u2019 content. It can be seen that the classes RY and DN contain the majority of the speeches. Language Model. We use a simple statistical language model based on n-grams. In particular, we use 6-grams. That is, for each sequence of", "the topic order with the one of the generated speech. We sum up the topic coverage values of each topic that occurs in both speeches at the same position. The highest achieved value is used as evaluation score. That is, finding a speech which covers the same topics with the same order of significance give us a score of 1. Results. In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement. Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual"]}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "predicted_answer": "similarity between two sentences, composed of structural and textual similarity", "predicted_evidence": ["across the speech-termination token or the generated speech reaches a certain length. The crucial part of this method is the measure of similarity between two sentences. Our similarity is composed of structural and textual similarity. Both are normalized to a range between 0 and 1 and weighted through a factor INLINEFORM0 . We compute the similarity between two sentences INLINEFORM1 and INLINEFORM2 as follows: INLINEFORM3  For the structural similarity we compare the POS tags of both sentences and determine the longest sequence of congruent POS tags. The length of this sequence, normalized by the length of the shorter sentence, gives us the structural similarity. The structural similarity measure aims to support smooth sentence transitions. That is, if we find sentences which have a very similar sentence structure, it is very likely that they connect well to either of their following sentences. The textual similarity is defined by the number of trigrams that occur in both sentences,", "similar sentence structure, it is very likely that they connect well to either of their following sentences. The textual similarity is defined by the number of trigrams that occur in both sentences, normalized by the length of the longer sentence. This similarity aims to find sentences which use the same words. The obvious advantage of the sentence-based approach is that every sentence is grammatically correct since they originate directly from the training data. However, connecting sentences reasonable is a very challenging task. A further step to improve this approach would be to extend the similarity measure by a topical similarity and a semantic similarity. The topical similarity should measure the topical correspondence of the originating speeches, while the semantic similarity should help to find sentences which express the same meaning although using different words. However, the results from the word-based approach were more promising and therefore we have decided to discard", "whole sentences from the training data and concatenate them in a meaningful way. We start by picking a speech of the desired class at random and take the first sentence of it. This will be the start sentence of our speech. Then we pick 20 speeches at random from the same class. We compare our first sentence with each sentence in those 20 speeches by calculating a similarity measure. The next sentence is than determined by the successor of the sentence with the highest similarity. In case no sentence shows sufficient similarity (similarity score below threshold) we just take the successor of our last sentence. In the next step we pick again 20 speeches at random and compare each sentence with the last one in order to find the most similar sentence. This will be repeated until we come across the speech-termination token or the generated speech reaches a certain length. The crucial part of this method is the measure of similarity between two sentences. Our similarity is composed of", "help to find sentences which express the same meaning although using different words. However, the results from the word-based approach were more promising and therefore we have decided to discard the sentence-based approach. Experiments. This section describes the experimental setup we used to evaluate our system. Furthermore, we present here two different approach of evaluating the quality of generated speeches. Setup. In order to test our implemented methods we performed an experimental evaluation. In this experiment we generated ten speeches, five for class DN and five for class RY. We set the weighting factor INLINEFORM0 to 0.5 which means the topic and the language model have both equal impact on predicting the next word. The quality of the generated speeches was then evaluated. We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated", "indicates that the topic model may need some improvement. Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual evaluation. Most of the speeches which achieved a high score in the manual evaluation scored also high in the automatic evaluation. Furthermore, it also confirms that the overall the grammatical correctness of the speeches is very good while the content is a bit behind. Conclusion. In this report we have presented a novel approach of training a system on speech transcripts in order to generate new speeches. We have shown that n-grams and J&K POS tag filter are very effective as language and topic model for this task. We have shown how to combine these models to a system that produces good results. Furthermore, we have presented different methods to evaluate the quality of generated texts. In an experimental evaluation our system performed very well. In particular, the grammatical correctness", "We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated speeches can be found in the appendix of this report. Manual Evaluation. For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores. Automatic Evaluation. The automatic evaluation aims to evaluate both the grammatical correctness and the consistency of the speech in terms of its content. For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the", "its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually. In order to evaluate the content of the generated speech we determine the mixture of topics covered by the speech and order them by their topic coverage. That gives us information about the primary topic and secondary topics. Then we do the same for each speech in our dataset which is of the same class and compare the topic order with the one of the generated speech. We sum up the topic coverage values of each topic that occurs in both speeches at the same position. The highest achieved value is used as", "assessing the error we used cross-entropy loss function. Furthermore we used Stochastic Gradient Descent (SGD) to minimize the loss and Backpropagation Through Time (BPTT) to calculate the gradients. After training the network for 100 time epochs ( INLINEFORM0 14 h) the results were still pretty bad. Most of the generated sentences were grammatically incorrect. There are many options to improve the performance of RNNs but due to the good performance shown by n-grams, the time-consuming training, and the limited time for this project we have decided to not further purse this approach. Latent Dirichlet Allocation. As alternative to the J&K POS tag filter we used LDA as topic model. In particular we used the approach from Lau et al. UID18 . That is, we removed all occurrences of stop words, stemmed the remaining words, replaced the 1000 most-frequent bigrams with single tokens, and deleted the 200 most frequent terms from the vocabulary before applying ordinary LDA. Since our dataset"]}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "predicted_answer": "3857", "predicted_evidence": ["CTM aims to capture correlations between them. They argue that a document about genetics is more likely also about disease than X-ray astronomy. The CTM builds on the LDA model but they use a hierarchical topic model of documents that replaces the Dirichlet distribution of per-document topic proportions with a logistic normal. According to their results the model gives better predictive performance and uncovers interesting descriptive statistics. Ivyer et al. UID35 apply Recursive Neural Networks (RNN) to political ideology detection. The RNNs were initialized with word2vec embeddings. The word vector dimensions were set to 300 to allow direct comparison with other experiments. However, they claim that smaller vector sizes (50, 100) do not significantly change accuracy. They performed experiments on two different dataset: the Convote dataset UID41 and the Ideological Books Corpus (IBC) UID37 . They claim that their model outperforms existing models on these two datasets. There has", "by the training data and requires no further specification. Data set. The main data source for this project is the Convote data set UID41 . It contains a total of 3857 speech segments from 53 US Congressional floor debates from the year 2005. Each speech segment can be referred to its debate, its speaker, the speaker\u2019s party and the speaker\u2019s vote which serves as the ground-truth label for the speech. The dataset was originally created in the course of the project Get out the vote UID34 . The authors used the dataset to train a classifier in order to determine whether a speech represents support of or opposition to proposed legislation. They did not only analyze the speeches individually but also investigated agreements and disagreements with the opinions of other speakers. That is, they identified references in the speech segments, determined the targets of those references, and decided whether a reference represents an instance of agreement or disagreement. However, we focus only on", "different probabilities of appearance. Words with the highest probabilities represent the topics. However, LDA is a bag-of-words model which means that the word orders are not preserved. That means LDA does not capture collocations or multiword named entities. Lau et al. UID18 claim that collocations empirically enhance topic models. In an experiment they replaced the top-ranked bigrams with single tokens, deleted the 200 most frequent terms from the vocabulary and performed ordinary LDA. The results from experiments on four distinct datasets have shown that this bigram-variant is very beneficial for LDA topic models. F\u00fcrnkranz UID19 has studied the usage of n-grams in the text-categorization domain. He has shown that using bi- and trigrams in addition to the set-of-word representation improves the classification performance significantly. Furthermore, he has shown that sequences longer than three words reduce the classification performance. That also indicates that collocations play", "stemmed the remaining words, replaced the 1000 most-frequent bigrams with single tokens, and deleted the 200 most frequent terms from the vocabulary before applying ordinary LDA. Since our dataset contains speech segments from 53 different debates we set the number of underlying topics to 53. Some of the results represented quite meaningful topics. However, the majority did not reveal any useful information. Table TABREF9 shows some examples of good and bad results from LDA. It can be seen that the extracted terms of the bad examples are very generic and do not necessarily indicate a meaningful topic. Sentence-based approach. For the speech generation task we have also pursued a sentence-based approach in the beginning of this project. The idea of the sentence-based approach is to take whole sentences from the training data and concatenate them in a meaningful way. We start by picking a speech of the desired class at random and take the first sentence of it. This will be the start", "methods that were not used in the final implementation. Then we describe a performed experiment and how we evaluated the results. Finally, we conclude our work and give an outlook. The appendix of this report contains the generated speeches from the experiment. Related work. Creating models for a corpus that allow retrieving certain information is a major part of this project as well as in the entire NLP domain. Blei et al. UID17 present in their paper a model which is known as latent Dirichlet allocation (LDA). LDA has become one of the most popular topic models in the NLP domain. LDA is generative probabilistic model that discovers automatically the underlying topics. Each document is modeled as a mixture of various topics. These topics can be understood as a collection of words that have different probabilities of appearance. Words with the highest probabilities represent the topics. However, LDA is a bag-of-words model which means that the word orders are not preserved. That means", "the classification performance significantly. Furthermore, he has shown that sequences longer than three words reduce the classification performance. That also indicates that collocations play a crucial role when it comes to inferring the latent structure of documents. Cavnar and Trenkle UID20 have also used an n-gram-based approach for text categorization. Their system is based on calculating and comparing profiles of N-gram frequencies. They compute for every category a representing profile from the training data. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document\u2019s profile and each of the category profiles and selects the category whose profile has the smallest distance. Smadja UID21 presents a tool, Xtract, which implements methods to extracts variable-length collocations. The extraction process is done in several stages. In the first stage the system determines the top-ranked", "experiments on two different dataset: the Convote dataset UID41 and the Ideological Books Corpus (IBC) UID37 . They claim that their model outperforms existing models on these two datasets. There has been a lot of research in the field of Natural Language Generation (NLG). The paper Building Applied Natural Language Generation Systems UID29 discusses the main requirements and tasks of NLG systems. Among others, they investigate a so-called Corpus-based approach. That is, a collection of example inputs is mapped to output texts of the corpus. This is basically what we plan to do because we have already all the speech segments labeled with the political party and the opinion. However, our generator will have a simpler architecture but we will use the described list of tasks as a guideline. Most NLG systems are designed to create a textual representation of some input data. That is, the input data determines the content. For example SumTime-Mousam UID30 generates a textual weather", "assessing the error we used cross-entropy loss function. Furthermore we used Stochastic Gradient Descent (SGD) to minimize the loss and Backpropagation Through Time (BPTT) to calculate the gradients. After training the network for 100 time epochs ( INLINEFORM0 14 h) the results were still pretty bad. Most of the generated sentences were grammatically incorrect. There are many options to improve the performance of RNNs but due to the good performance shown by n-grams, the time-consuming training, and the limited time for this project we have decided to not further purse this approach. Latent Dirichlet Allocation. As alternative to the J&K POS tag filter we used LDA as topic model. In particular we used the approach from Lau et al. UID18 . That is, we removed all occurrences of stop words, stemmed the remaining words, replaced the 1000 most-frequent bigrams with single tokens, and deleted the 200 most frequent terms from the vocabulary before applying ordinary LDA. Since our dataset"]}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "predicted_answer": "Both manual and automatic evaluation methods are presented.", "predicted_evidence": ["We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated speeches can be found in the appendix of this report. Manual Evaluation. For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores. Automatic Evaluation. The automatic evaluation aims to evaluate both the grammatical correctness and the consistency of the speech in terms of its content. For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the", "indicates that the topic model may need some improvement. Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual evaluation. Most of the speeches which achieved a high score in the manual evaluation scored also high in the automatic evaluation. Furthermore, it also confirms that the overall the grammatical correctness of the speeches is very good while the content is a bit behind. Conclusion. In this report we have presented a novel approach of training a system on speech transcripts in order to generate new speeches. We have shown that n-grams and J&K POS tag filter are very effective as language and topic model for this task. We have shown how to combine these models to a system that produces good results. Furthermore, we have presented different methods to evaluate the quality of generated texts. In an experimental evaluation our system performed very well. In particular, the grammatical correctness", "the topic order with the one of the generated speech. We sum up the topic coverage values of each topic that occurs in both speeches at the same position. The highest achieved value is used as evaluation score. That is, finding a speech which covers the same topics with the same order of significance give us a score of 1. Results. In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement. Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual", "help to find sentences which express the same meaning although using different words. However, the results from the word-based approach were more promising and therefore we have decided to discard the sentence-based approach. Experiments. This section describes the experimental setup we used to evaluate our system. Furthermore, we present here two different approach of evaluating the quality of generated speeches. Setup. In order to test our implemented methods we performed an experimental evaluation. In this experiment we generated ten speeches, five for class DN and five for class RY. We set the weighting factor INLINEFORM0 to 0.5 which means the topic and the language model have both equal impact on predicting the next word. The quality of the generated speeches was then evaluated. We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated", "its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually. In order to evaluate the content of the generated speech we determine the mixture of topics covered by the speech and order them by their topic coverage. That gives us information about the primary topic and secondary topics. Then we do the same for each speech in our dataset which is of the same class and compare the topic order with the one of the generated speech. We sum up the topic coverage values of each topic that occurs in both speeches at the same position. The highest achieved value is used as", "list of accepted POS tag patterns. In their experiment this method identifies 99% of the technical multiword terms in the test data. Wacholder UID27 presents an approach for identifying significant topics within a document. The proposed method bases on the identification of Noun Phrases (NPs) and consists of three steps. First, a list of candidate significant topics consisting of all simplex NPs is extracted from the document. Next, these NPs are clustered by head. Finally, a significance measure is obtained by ranking frequency of heads. Those NPs with heads that occur with greater frequency in the document are more significant than NPs whose head occurs less frequently. Blei and Lafferty UID28 propose their Correlated Topic model (CTM). While LDA assumes all latent topics are independent CTM aims to capture correlations between them. They argue that a document about genetics is more likely also about disease than X-ray astronomy. The CTM builds on the LDA model but they use a", "It combines the bigram topic model UID24 and LDA collocation model UID25 . One of the key features of this model is to decide whether two consecutive words should be treated as a single token or not depending on their nearby context. Compared to LDA the extracted topics are semantically more meaningful. This model shows also really good results in information retrieval (IR) tasks. Justeson and Katz UID26 present a method to extract technical terms from documents. Their approach is not restricted to technical terms but applies to all multiword named entities of length two or three. The foundations of their method are bi- and trigrams which have a certain POS tag structure. That is, they extract all bi- and trigrams from the corpus, identify their POS tags and check them against a predefined list of accepted POS tag patterns. In their experiment this method identifies 99% of the technical multiword terms in the test data. Wacholder UID27 presents an approach for identifying significant", "the classification performance significantly. Furthermore, he has shown that sequences longer than three words reduce the classification performance. That also indicates that collocations play a crucial role when it comes to inferring the latent structure of documents. Cavnar and Trenkle UID20 have also used an n-gram-based approach for text categorization. Their system is based on calculating and comparing profiles of N-gram frequencies. They compute for every category a representing profile from the training data. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document\u2019s profile and each of the category profiles and selects the category whose profile has the smallest distance. Smadja UID21 presents a tool, Xtract, which implements methods to extracts variable-length collocations. The extraction process is done in several stages. In the first stage the system determines the top-ranked"]}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "predicted_answer": "Classification task and question answering task", "predicted_evidence": ["over the ELMo baseline (up to 20 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. Experiments ::: Downstream task benchmark. Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14). As shown in Table TABREF8, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.5 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during", "the growing computational and memory requirements of these models may hamper wide adoption. In this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models. We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (BIBREF5) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are", "Introduction. The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks BIBREF0, BIBREF1, BIBREF2. While these models lead to significant improvement, they often have several hundred million parameters and current research on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the environmental cost of exponentially scaling these models' computational requirements as mentioned in BIBREF3, BIBREF4. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption. In this paper, we show that it is possible to reach similar performances on many downstream-tasks", "Modeling loss has little impact while the two distillation losses account for a large portion of the performance. Related work. Task-specific distillation Most of the prior works focus on building task-specific distillation setups. BIBREF15 transfer fine-tune classification model BERT to an LSTM-based classifier. BIBREF16 distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. BIBREF17 use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher's knowledge to pre-train with additional distillation signal. Multi-distillation BIBREF18 combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge", "distillation signal. Multi-distillation BIBREF18 combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: BIBREF19 adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher's knowledge with initialization and additional losses leads to substantial gains. Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance BIBREF20. Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models", "model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers library from HuggingFace BIBREF6. Knowledge distillation. Knowledge distillation BIBREF7, BIBREF8 is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model's predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with", "we used a softmax-temperature: $p_i = \\frac{\\exp (z_i / T)}{\\sum _j \\exp (z_j / T)}$ where $T$ controls the smoothness of the output distribution and $z_i$ is the model score for the class $i$. The same temperature $T$ is applied to the student and the teacher at training time, while at inference, $T$ is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss $L_{ce}$ with the supervised training loss, in our case the masked language modeling loss $L_{mlm}$ BIBREF0. We found it beneficial to add a cosine embedding loss ($L_{cos}$) which will tend to align the directions of the student and teacher hidden states vectors. DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer", "self-attention at test time without significantly degrading the performance BIBREF20. Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models (BIBREF21). Pruning and quantization are orthogonal to the present work. Conclusion and future work. We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. We showed that a general-purpose language model can be successfully trained with distillation and analyzed the various components with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge applications."]}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "predicted_answer": "Around 90 hours.", "predicted_evidence": ["we used a softmax-temperature: $p_i = \\frac{\\exp (z_i / T)}{\\sum _j \\exp (z_j / T)}$ where $T$ controls the smoothness of the output distribution and $z_i$ is the model score for the class $i$. The same temperature $T$ is applied to the student and the teacher at training time, while at inference, $T$ is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss $L_{ce}$ with the supervised training loss, in our case the masked language modeling loss $L_{mlm}$ BIBREF0. We found it beneficial to add a cosine embedding loss ($L_{cos}$) which will tend to align the directions of the student and teacher hidden states vectors. DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer", "sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two. Distillation We applied best practices for training BERT model recently proposed in BIBREF2. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus BIBREF9. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model BIBREF2 required 1 day of training on 1024 32GB V100. Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark", "Modeling loss has little impact while the two distillation losses account for a large portion of the performance. Related work. Task-specific distillation Most of the prior works focus on building task-specific distillation setups. BIBREF15 transfer fine-tune classification model BERT to an LSTM-based classifier. BIBREF16 distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. BIBREF17 use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher's knowledge to pre-train with additional distillation signal. Multi-distillation BIBREF18 combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge", "model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers library from HuggingFace BIBREF6. Knowledge distillation. Knowledge distillation BIBREF7, BIBREF8 is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model's predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with", "self-attention at test time without significantly degrading the performance BIBREF20. Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models (BIBREF21). Pruning and quantization are orthogonal to the present work. Conclusion and future work. We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. We showed that a general-purpose language model can be successfully trained with distillation and analyzed the various components with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge applications.", "a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 86.9 F1 and 79.1 EM, i.e. within 2 points of the full model. Size and inference speed To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table TABREF8) the number of parameters of each model along with the inference time needed to do a full pass on the STS-B development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a", "between the model's predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: $L_{ce} = \\sum _i t_i * \\log (s_i)$ where $t_i$ (resp. $s_i$) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following BIBREF8 we used a softmax-temperature: $p_i = \\frac{\\exp (z_i / T)}{\\sum _j \\exp (z_j / T)}$ where $T$ controls the smoothness of the output distribution and $z_i$ is the model score for the class $i$.", "On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available. Experiments ::: Ablation study. In this section, we investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table TABREF11 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance. Related work. Task-specific distillation Most of the prior works focus on building"]}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "predicted_answer": "LibriSpeech corpus BIBREF46, GlobalPhone corpus BIBREF47", "predicted_evidence": ["were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand segments from 250 thousand segments to form a small database to investigate the influence of database size. English served as the high-resource source language for model pre-training. The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning. MFCCs of 39-dim were used as the acoustic features. The length of the input sequence was limited to 50 frames. All datasets", "and the other 1 thousand used for query and 4 thousand for fine-tuning. MFCCs of 39-dim were used as the acoustic features. The length of the input sequence was limited to 50 frames. All datasets were segmented according to the word boundaries obtained by forced alignment with respect to the reference transcriptions. Although the oracle word boundaries were used here for the query-by-example STD in the preliminary tests, the comparison in the following experiment was fair since all approaches used the same segmentation. Mean average precision (MAP) was used as the evaluation measure for query-by-example STD. Proposed Model: Sequence Autoencoder (SASA). Both the proposed model ( INLINEFORM0 ) and baseline model ( INLINEFORM1 , described in the next subsection) were implemented with Tensorflow. The network structure and the hyper parameters were set as below: Both RNN Encoder and Decoder consisted one hidden layer of GRU cells BIBREF20 , BIBREF21 . The number of units in the layer would", "as 100, 200, 400, 600, 800 and 1000. To match up the dimensionality with INLINEFORM0 , we tested INLINEFORM1 with dimensionality 117, 234, 390, 585, 819, 1014 ( INLINEFORM2 ) and denoted them by INLINEFORM3 where INLINEFORM4 is the dimensionality. INLINEFORM5 get higher MAP values than INLINEFORM6 no matter the vector dimension and the size of database. The highest MAP score INLINEFORM7 can achieve is 0.881 ( INLINEFORM8 on small database), while the highest score of the INLINEFORM9 model is 0.490 ( INLINEFORM10 on small database). The size of database has large influence on the results. The MAP scores of the two models both drop in the large database. For example, INLINEFORM11 drops from 0.490 to 0.158, decaying by 68%, and the performance of INLINEFORM12 drops from 0.881 to 0.317, decaying by 64%. As shown in Fig. FIGREF12 , larger dimensionality does not imply better performance in query-by-example STD. The MAP scores gradually improve until reaching the dimensionality of 400 in", "done off-line. In the lower left corner of Figure FIGREF5 , when a spoken query is entered, the input spoken query is similarly encoded by the same RNN encoder into a vector. The system then returns a list of audio segments in the archive ranked according to the cosine similarities evaluated between the vector representation of the query and those of all segments in the archive. Note that the computation requirements for the online process here are extremely low. Experimental Setup. Here we provide detail of our experiment including the dataset, model setup, and the baseline model. Dataset. Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English). In this 960-hour English dataset, 2.2 million audio word segments were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand", "the reconstruction error, measured by the general mean squared error INLINEFORM15 . Because the input sequence is taken as the learning target, the training process does not need any labeled data. The fixed-length vector representation INLINEFORM16 will be a meaningful representation for the input audio segment INLINEFORM17 because the whole input sequence INLINEFORM18 can be reconstructed from INLINEFORM19 by the RNN Decoder. Using historyless decoder is critical here. We found out that the performance in the STD experiment was undermined despite the low reconstruction error. This shows that the vector representations learned from INLINEFORM0 do not include useful information. This might be caused by a strong decoder as the model focuses less on including more information into the vector representation. We eventually solved the problem by using a historyless decoder. Historyless decoder is a weakened decoder. The input of the decoder is removed, and this forces the model to rely more", "by 64%. As shown in Fig. FIGREF12 , larger dimensionality does not imply better performance in query-by-example STD. The MAP scores gradually improve until reaching the dimensionality of 400 in INLINEFORM13 and 234 in INLINEFORM14 , and start to decrease as the dimension increases. In the rest of the experiments, we would use 400 GRU units in the INLINEFORM15 hidden layer, and set INLINEFORM16 ( INLINEFORM17 ). Analysis of Language Transfer. To evaluate the quality of language transfer, we trained the Audio Word2Vec model by INLINEFORM0 from the source language, English, and applied it on different target languages, French (FRE), German (GER), Czech (CZE), and Spanish (ESP). We computed the average cosine similarity of the vector representations for each pair of the audio segments in the retrieval database of the target languages (20K segments for each language), and compare it with the phoneme sequence edit distance (PSED). The average and variance (the length of the black line on", "retrieval database of the target languages (20K segments for each language), and compare it with the phoneme sequence edit distance (PSED). The average and variance (the length of the black line on each bar) of the cosine similarity for groups of pairs clustered by the phoneme sequence edit distances (PSED) between the two words are shown in Fig. FIGREF14 . For comparison, we also provide the results obtained from the English retrieval database (250K segments), where the segments were not seen by the model in training procedure. In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data", "through concatenating the INLINEFORM6 average vectors sequentially into a vector representation of dimensionality INLINEFORM7 . Although INLINEFORM8 is simple, similar approaches have been used in STD and achieved successful results BIBREF2 , BIBREF3 , BIBREF4 . Experiments. In this section, we first examine how changing the hidden layer size of the RNN Encoder/Decoder, the dimension of Audio Word2Vec, affects the MAP performance of query-by-example STD (Section 6.1). After obtaining the best hidden layer size, we analyze the transferability of the Audio Word2Vec by comparing the cosine similarity of the learned representations to phoneme sequence edit distance (Section 6.2) . Visualization of multiple word pairs in different target languages is also provided (Section 6.3). Last but not least, we performed the query-by-example STD on target languages (Section 6.4). These experiments together verify that INLINEFORM0 is capable of extracting common phonetic structure in human language"]}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "predicted_answer": "They do not explicitly compare the performance of the representations obtained from the naive encoder versus ones learned from a large amount of source language data.", "predicted_evidence": ["by 64%. As shown in Fig. FIGREF12 , larger dimensionality does not imply better performance in query-by-example STD. The MAP scores gradually improve until reaching the dimensionality of 400 in INLINEFORM13 and 234 in INLINEFORM14 , and start to decrease as the dimension increases. In the rest of the experiments, we would use 400 GRU units in the INLINEFORM15 hidden layer, and set INLINEFORM16 ( INLINEFORM17 ). Analysis of Language Transfer. To evaluate the quality of language transfer, we trained the Audio Word2Vec model by INLINEFORM0 from the source language, English, and applied it on different target languages, French (FRE), German (GER), Czech (CZE), and Spanish (ESP). We computed the average cosine similarity of the vector representations for each pair of the audio segments in the retrieval database of the target languages (20K segments for each language), and compare it with the phoneme sequence edit distance (PSED). The average and variance (the length of the black line on", "(wenig) in Fig. FIGREF18 . Language Transferring on STD. Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare the retrieval performance in MAP of INLINEFORM0 with different levels of accessibility to the low-resource target language along with two baseline models, INLINEFORM1 and INLINEFORM2 trained purely by the target languages. For the four target languages, the total available amount of audio word segments in the training set were 4 thousands for each language. In Table TABREF20 , we took different partitions of the target language training sets to fine tune the INLINEFORM3 pretrained by the source languages. The amount of audio word segments in these partitions are: 1K, 2K, 3K, 4K, and 0, which means no fine-tuning. From Table TABREF20 , INLINEFORM0 trained by source language generally outperforms the INLINEFORM1 trained by the limited amount of target language (\" INLINEFORM2 No Transfer\"),", "sequence-to-sequence autoencoder demonstrates a promising result BIBREF14 . The model is trained to minimize the reconstruction error of the input audio sequence and then provides the embedding, namely Audio Word2Vec, from its bottleneck layer. This is done without any annotation effort. Although deep learning approaches have produced satisfactory result, the data-hungry nature of the deep model makes it hard to produce the same performance with low-resource data. Both supervised and unsupervised approaches assume that a large amount of audio data of the target language is available. A question arises whether it is possible to transfer the Audio Word2Vec model learned from a high-resource language into a model targeted at a low-resource language. While this problem is not yet to be fully examined in Audio Word2Vec, works in neural machine translation (NMT) successfully transfer the model learned on high-resource languages to low-resource languages. In BIBREF15 , BIBREF16 , the authors", "through concatenating the INLINEFORM6 average vectors sequentially into a vector representation of dimensionality INLINEFORM7 . Although INLINEFORM8 is simple, similar approaches have been used in STD and achieved successful results BIBREF2 , BIBREF3 , BIBREF4 . Experiments. In this section, we first examine how changing the hidden layer size of the RNN Encoder/Decoder, the dimension of Audio Word2Vec, affects the MAP performance of query-by-example STD (Section 6.1). After obtaining the best hidden layer size, we analyze the transferability of the Audio Word2Vec by comparing the cosine similarity of the learned representations to phoneme sequence edit distance (Section 6.2) . Visualization of multiple word pairs in different target languages is also provided (Section 6.3). Last but not least, we performed the query-by-example STD on target languages (Section 6.4). These experiments together verify that INLINEFORM0 is capable of extracting common phonetic structure in human language", "were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand segments from 250 thousand segments to form a small database to investigate the influence of database size. English served as the high-resource source language for model pre-training. The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning. MFCCs of 39-dim were used as the acoustic features. The length of the input sequence was limited to 50 frames. All datasets", "The network structure and the hyper parameters were set as below: Both RNN Encoder and Decoder consisted one hidden layer of GRU cells BIBREF20 , BIBREF21 . The number of units in the layer would be discussed in the experiment. The networks were trained by SGD without momentum. The initial learning rate was 1 and decayed with a factor of 0.95 every 500 batches. Baseline: Naive Encoder (NENE). We used naive encoder ( INLINEFORM0 ) as the baseline approach. In this encoder, the input acoustic feature sequence INLINEFORM1 = ( INLINEFORM2 ), where INLINEFORM3 was the 39-dimension MFCC feature vector at time t, were divided into INLINEFORM4 partitions with roughly equal length INLINEFORM5 . Then, we averaged each partition into a single 39-dimension vector, and finally got the vector representation through concatenating the INLINEFORM6 average vectors sequentially into a vector representation of dimensionality INLINEFORM7 . Although INLINEFORM8 is simple, similar approaches have been used", "least, we performed the query-by-example STD on target languages (Section 6.4). These experiments together verify that INLINEFORM0 is capable of extracting common phonetic structure in human language and thus is transferable to various languages. Analysis on Dimension of Audio Word2Vector. Before evaluating the language transfer result, we first experimented on the primary INLINEFORM0 model in the source language (English). The results are shown in Fig. FIGREF12 . Here we compare the representations of INLINEFORM1 and INLINEFORM2 . Furthermore, we examined the influence of the dimension of Audio Word2Vector in terms of MAP. We also compared the MAP results on large testing database (250K segments) and small database (20K). In Fig. FIGREF12 , we varied the dimension of Audio Word2Vector as 100, 200, 400, 600, 800 and 1000. To match up the dimensionality with INLINEFORM0 , we tested INLINEFORM1 with dimensionality 117, 234, 390, 585, 819, 1014 ( INLINEFORM2 ) and denoted them by", "has shown to be helpful in STD BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , BIBREF45 . In this paper, we focus on studying the capability of transfer learning of Audio Word2Vec. In the proposed approach, we first train an INLINEFORM0 using the high-resource source language, as shown in the upper part of Fig. FIGREF4 , and then the encoder is used to transform the audio segment of a low-resource target language. It is also possible to fine-tune the parameters of INLINEFORM1 with the target language. In the following experiments, we found that in some cases the STD performance of the encoder without fine-tuning with the low-resource target language can be as good as the one with fine-tuning. An Example Application: Query-by-example STD. The audio segment representation INLINEFORM0 learned in the last section can be applied in many possible scenarios. Here in the preliminary tests we consider the unsupervised query-by-example STD, whose target is to locate"]}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "predicted_answer": "French and German.", "predicted_evidence": ["the variance in Spanish is also bigger. The smaller gap and bigger variance together indicate that the model is weaker on Spanish at identifying audio segments of different words and thus affects the MAP performance in Spanish. Conclusion and Future Work. In this paper, we verify the capability of language transfer of Audio Word2Vec using Sequence-to-sequence Autoencoer ( INLINEFORM0 ). We demonstrate that INLINEFORM1 can learn the sequential phonetic structure commonly appearing in human language and thus make it possible to apply an Audio Word2Vec model learned from high-resource language to low-resource languages. The capability of language transfer in Audio Word2Vec is beneficial to many real world applications, for example, the query-by-example STD shown in this work. For the future work, we are examining the performance of the transferred system in other application scenarios, and exploring the performance of Audio Word2Vec under automatic segmentation.", "has shown to be helpful in STD BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , BIBREF45 . In this paper, we focus on studying the capability of transfer learning of Audio Word2Vec. In the proposed approach, we first train an INLINEFORM0 using the high-resource source language, as shown in the upper part of Fig. FIGREF4 , and then the encoder is used to transform the audio segment of a low-resource target language. It is also possible to fine-tune the parameters of INLINEFORM1 with the target language. In the following experiments, we found that in some cases the STD performance of the encoder without fine-tuning with the low-resource target language can be as good as the one with fine-tuning. An Example Application: Query-by-example STD. The audio segment representation INLINEFORM0 learned in the last section can be applied in many possible scenarios. Here in the preliminary tests we consider the unsupervised query-by-example STD, whose target is to locate", "BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . These vector representations carry the phonetic structures of the spoken words learned from the signals within the spoken words, and have been shown to be useful in spoken term detection, in which the spoken terms are detected simply based on the phonetic structures. Such Audio Word2Vec representations do not carry semantics, because they are learned from individual spoken words only without considering the context. Audio Word2Vec was recently extended to Segmental Audio Word2Vec BIBREF25 , in which an utterance can be automatically segmented into a sequence of spoken words BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 and then transformed into a sequence of vectors of fixed dimensionality by Audio Word2Vec, and the spoken word segmentation and Audio Word2Vec can be jointly trained from an audio corpus. In this way the Audio Word2Vec was upgraded from word-level to utterance-level. This offers the opportunity for", "in Audio Word2Vec, works in neural machine translation (NMT) successfully transfer the model learned on high-resource languages to low-resource languages. In BIBREF15 , BIBREF16 , the authors first train a source model with high-resource language pair. The source model is used to initialize the target model which is then trained by low-resource language pairs. For audio, all languages are uttered by human beings with a similar vocal tract structure, and therefore share some common acoustic patterns. This fact implies that knowledge obtained from one spoken language can be transferred onto other languages. This paper verifies that sequence-to-sequence autoencoder is not only able to transform audio word segments into fixed-length vectors, the model is also transferable to the languages it has never heard before. We also demonstrate its promising applications with a query-by-example spoken term detection (STD) experiment. In the query-by-example STD experiment, even without tunning with", "mean value of representations for the audio segments of a specific word, denoted by INLINEFORM0 (word). Then the average representation INLINEFORM1 was projected from 400-dimensional to 2-dimensional using PCA BIBREF49 . The result of the difference vector from each word pair, e.g. INLINEFORM2 (parlons) - INLINEFORM3 (parler), is shown. Although the representations for French and German word audio segments were extracted from the model trained by English audio word segments and never heard any French and German, the direction and magnitude of the different vectors are coherent. In Fig. FIGREF18 , INLINEFORM4 (parlons) - INLINEFORM5 (parler) is close to INLINEFORM6 (utilison) - INLINEFORM7 (utiliser); and INLINEFORM8 (tage) - INLINEFORM9 (tag) is close to INLINEFORM10 (wenige) - INLINEFORM11 (wenig) in Fig. FIGREF18 . Language Transferring on STD. Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare", "word segmentation and Audio Word2Vec can be jointly trained from an audio corpus. In this way the Audio Word2Vec was upgraded from word-level to utterance-level. This offers the opportunity for Audio Word2Vec to include semantic information in addition to phonetic structures, since the context among spoken words in utterances bring semantic information. This is the goal of this work, and this paper reports the first set of results towards such a goal. In principle, the semantics and phonetic structures in words inevitably disturb each other. For example, the words \u201cbrother\" and \u201csister\" are close in semantics but very different in phonetic structure, while the words \u201cbrother\" and \u201cbother\" are close in phonetic structure but very different in semantics. This implies the goal of embedding both phonetic structures and semantics for spoken words is naturally very challenging. Text words can be trained and embedded as vectors carrying plenty of semantics because the phonetic structures are", "least, we performed the query-by-example STD on target languages (Section 6.4). These experiments together verify that INLINEFORM0 is capable of extracting common phonetic structure in human language and thus is transferable to various languages. Analysis on Dimension of Audio Word2Vector. Before evaluating the language transfer result, we first experimented on the primary INLINEFORM0 model in the source language (English). The results are shown in Fig. FIGREF12 . Here we compare the representations of INLINEFORM1 and INLINEFORM2 . Furthermore, we examined the influence of the dimension of Audio Word2Vector in terms of MAP. We also compared the MAP results on large testing database (250K segments) and small database (20K). In Fig. FIGREF12 , we varied the dimension of Audio Word2Vector as 100, 200, 400, 600, 800 and 1000. To match up the dimensionality with INLINEFORM0 , we tested INLINEFORM1 with dimensionality 117, 234, 390, 585, 819, 1014 ( INLINEFORM2 ) and denoted them by", "representation. We eventually solved the problem by using a historyless decoder. Historyless decoder is a weakened decoder. The input of the decoder is removed, and this forces the model to rely more on the vector representation. The historyless decoder is also used in recent NLP works BIBREF35 , BIBREF36 , BIBREF37 . Language Transfer. In the study of linguistic, scholars define a set of universal phonetic rules which describe how sounds are commonly organized across different languages. Actually, in real life, we often find languages sharing similar phonemes especially the ones spoken in nearby regions. These facts implies that when switching target languages, we do not need to learn the new audio pattern from scratch due to the transferability in spoken languages. Language transfer has shown to be helpful in STD BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , BIBREF45 . In this paper, we focus on studying the capability of transfer learning of Audio"]}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "predicted_answer": "Unanswerable (there is no information about Roberta in the documents)", "predicted_evidence": ["tweets. Then the slur matching component obtains an precision of around 56.5% and can identify roughly 3 times of hateful tweets compared with the supervised LSTM classifier. The last column of this section shows the performance of our model on a collection of human annotated tweets as introduced in the previous work BIBREF0 . The recall is rather low because the data we used to train our model is quite different from this dataset which contains tweets related to a TV show BIBREF0 . The precision is only slightly lower than previous supervised models that were trained using the same dataset. Table 3 shows the number of hateful tweets our bootstrapping system identified in each iteration during training. Specifically, the columns Slur Match and LSTMs show the number of hateful tweets identified by the slur learning component and the weakly supervised LSTM classifier respectively. We can see that both learning components steadily label new hateful tweets in each iteration and the LSTM", "were obtained using only 20 seed slur terms and a large set of unlabeled tweets. The two-path weakly supervised bootstrapping system ran for four iterations. The second section of Table 2 shows the results for the two-path weakly supervised system. The first two rows show the evaluation results for each of the two learning components in the two-path system, the LSTM classifier and the slur learner, respectively. The third row shows the results for the full system. We can see that the full system Union is significantly better than the supervised LSTM model in terms of recall and F-score. Furthermore, we can see that a significant portion of hateful tweets were identified by both components and the weakly supervised LSTM classifier is especially capable to identify a large number of hateful tweets. Then the slur matching component obtains an precision of around 56.5% and can identify roughly 3 times of hateful tweets compared with the supervised LSTM classifier. The last column of this", "generated in certain scenarios. For instance, BIBREF5 conducted their experiments on 1525 annotated sentences from a company's log file and a certain newsgroup. BIBREF6 labeled around 9000 human labeled paragraphs from Yahoo!'s news group post and American Jewish Congress's website, and the labeling is restricted to anti-Semitic hate speech. BIBREF7 studied use of profanity on a dataset of 6,500 labeled comments from Yahoo! Buzz. BIBREF2 built a balanced corpus of 24582 tweets consisting of anti-black and non-anti black tweets. The tweets were manually selected from Twitter accounts that were believed to be racist based upon their reactions to anti-Obama articles. BIBREF8 collected hateful tweets related to the murder of Drummer Lee Rigby in 2013. BIBREF0 collected tweets using hateful slurs, specific hashtags as well as suspicious user IDs. Consequently, all of the 1,972 racist tweets are by 9 users, and the majority of sexist tweets are related to an Australian TV show.  BIBREF9 is", "unavailable later in the project, the other annotator annotated the remaining sampled tweets. Experimental Results. Supervised Baselines The first section of Table 2 shows the performance of the two supervised models when applied to 62 million tweets collected around election time. We can see that the logistic regression model suffers from an extremely low precision, which is less than 10%. While this classifier aggressively labeled a large number of tweets as hateful, only 121,512 tweets are estimated to be truly hateful. In contrast, the supervised LSTM classifier has a high precision of around 79%, however, this classifier is too conservative and only labeled a small set of tweets as hateful. The Two-path Bootstrapping System Next, we evaluate our weakly supervised classifiers which were obtained using only 20 seed slur terms and a large set of unlabeled tweets. The two-path weakly supervised bootstrapping system ran for four iterations. The second section of Table 2 shows the", "be used to learn more slur terms and retrain the classifier in the next iteration. The whole process then iterates. After each iteration, we have to determine if a stopping criterion is met and we should terminate the bootstrapping process. In general, a tuned threshold score is applied or a small annotated dataset is used to evaluate the learned classifiers. We adopt the latter method. Specifically, the bootstrapping system stops when the precision of the LSTM classifier is lower than $0.6$ when evaluated using an existing small annotated tweet set BIBREF0 . Automatic Data Labeling of Initial Data. Seeing a hate slur term in a tweet strongly indicates that the tweet is hateful. Therefore, we use 20 manually selected slur terms to match with a large unlabeled tweet collection in order to quickly construct the initial small set of hateful tweets. Table 1 shows the 20 seed slurs we used. We obtained our initial list of slurs from Hatebase, the Racial Slurs Database , and a page of LGBT", "were posted before and after the US election day. We evaluate both precision and recall for both types of systems. Ideally, we can easily measure precision as well as recall for each system if we have ground truth labels for each tweet. However, it is impossible to obtain annotations for such a large set of tweets. The actual distribution of hateful tweets in the 62 million tweets is unknown. Instead, to evaluate each system, we randomly sampled 1,000 tweets from the whole set of hateful tweets that had been tagged as hateful by the corresponding system. Then we annotate the sampled tweets and use them to estimate precision and recall of the system. In this case, $ precision = \\frac{n}{ 1000 } $ $ recall \\propto precision \\cdot N $  Here, $n$ refers to the number of hateful tweets that human annotators identified in the 1,000 sampled tweets, and $N$ refers to the total number of hateful tweets the system tagged in the 62 million tweets. We further calculated system recall by", "quickly construct the initial small set of hateful tweets. Table 1 shows the 20 seed slurs we used. We obtained our initial list of slurs from Hatebase, the Racial Slurs Database , and a page of LGBT slang terms. We ranked the slur terms by their frequencies in tweets, eliminating ambiguous and outdated terms. The slur \"gypsy\", for example, refers to derogatorily to people of Roma descent, but currently in popular usage is an idealization of a trendy bohemian lifestyle. The word \"bitch\" is ambiguous, sometimes a sexist slur but other times innocuously self-referential or even friendly. For these reasons, we only selected the top 20 terms we considered reliable (shown in Table 1 ). We use both the singular and the plural form for each of these seed slur terms. Slur Term Learner. The slur term learning component extracts individual words from a set of hateful tweets as new slurs. Intuitively, if a word occurs significantly more frequently in hateful tweets than in randomly selected", "slogan) among the most frequent. The specific televised events also engender proportionally large amounts of hateful language as they can be commonly experienced by all television-owning Americans and therefore a widely available target for hateful messages. Conclusions. Our work focuses on the need to capture both explicit and implicit hate speech from an unbiased corpus. To address these issues, we proposed a weakly supervised two-path bootstrapping model to identify hateful language in randomly sampled tweets. Starting from 20 seed rules, we found 210 thousand hateful tweets from 62 million tweets collected during the election. Our analysis shows a strong correlation between temporal distributions of hateful tweets and the election time, as well as the partisan impetus behind large amounts of inflammatory language. In the future, we will look into linguistic phenomena that often occur in hate speech, such as sarcasm and humor, to further improve hate speech detection performance."]}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "predicted_answer": "Not provided.", "predicted_evidence": ["Introduction. Text classification is a problem in library, information and computer science and one of the most classical and prominent tasks in Natural Language Processing (NLP). In particular, document classification is a procedure of assigning one or more labels to a document from a predetermined set of labels. Automatic document classification tasks can be divided into three categories: supervised, unsupervised and semi-supervised. This study focuses on supervised document classification. Research so far has focused on short text BIBREF0, BIBREF1, BIBREF2, BIBREF3, whereas the main objective of this paper is to address the classification of lengthy legal documents. In fact, pre-existing models could not be applied on our corpus, which consists of excessively lengthy legal documents. In the legal field, companies manage millions of documents per year, depending on the size of the company. Therefore, automatic categorisation of documents into different groups significantly enhances", "(also known as audio classification) is an essential pre-processing step in audio analysis that separates different types of sound (e.g. speech, music, silence etc.) and splits audio signals into chunks in order to further improve the comprehension of these signals BIBREF7. Analogously, the present paper shows that splitting overly lengthy legal documents into smaller parts before processing them, boosts the final results. Related Work. In several industries that produce or handle colossal amounts of text data such as the legal industry, document categorisation is still often performed manually by human experts. Automatic categorisation of documents is highly beneficial for reducing the human effort spent on time-consuming operations. In particular, deep neural networks have achieved state-of-the-art results in document classification over the last few years, outperforming the human classifiers in numerous cases. Related Work ::: Document Classification Datasets. The majority of", "the legal field, companies manage millions of documents per year, depending on the size of the company. Therefore, automatic categorisation of documents into different groups significantly enhances the efficiency of document management and decreases the time spent by legal experts analysing documents. Recently, several quite sophisticated frameworks have been proposed to address the document classification task. However, as proven by BIBREF3 regarding the document classification task, complex neural networks such as Bidirectional Encoder Representations from Transformers (BERT; BIBREF4) can be distilled and yet achieve similar performance scores. In addition, BIBREF5 shows that complex architectures are more sensitive to hyperparameter fluctuations and are susceptible to domains that consist of data with dissimilar characteristics. In this study, rather than employing an overly complex neural architecture, we focus on a relatively simpler neural structure that, in short, creates text", "of data with dissimilar characteristics. In this study, rather than employing an overly complex neural architecture, we focus on a relatively simpler neural structure that, in short, creates text embeddings using Doc2Vec BIBREF6 and then passes them through a Bi-directional LSTM (BiLSTM) with attention before making the final prediction. Furthermore, an important contribution of this paper to automatic document classification is the concept of dividing documents into chunks before processing. It is demonstrated that the segmentation of lengthy documents into smaller chunks of text allows the context of each document to be encapsulated in an improved way, leading to enhanced results. The intuition behind this idea was formed by investigating automatic audio segmentation research. Audio segmentation (also known as audio classification) is an essential pre-processing step in audio analysis that separates different types of sound (e.g. speech, music, silence etc.) and splits audio signals", "can achieve relatively high accuracy when classifying different documents. Nevertheless, even the slightest improvement of 1% or less will result in the correct classification of thousands of additional documents, which is crucial in the legal industry when handling large numbers of documents. This research allows these simple classifiers to achieve even greater results, by combining them with different architectures. As Table TABREF13 and Table TABREF15 indicate, dividing the document in chunks - up to certain thresholds - results in improved models compared to those where the whole document is input into the classifier. Note that the model with one chunk denotes the model which takes as input the whole document to produce the document embedding and thereby is used as a benchmark in order to be able to identify the effectiveness of the document segmentation method. More specifically, splitting the document into chunks yields higher test accuracy than having the whole document as", "more documents introduced undesirable noise that results from company names, numbers such as transaction amounts and dates, job titles and addresses. Consequently, Doc2Vec is proven to generate more accurate document embeddings when trained on just 150 randomly chosen documents (30 for each filing type). Results and Discussion. Recently, reproducibility is becoming a growing concern for the NLP community BIBREF14. In fact, the majority of the papers we consider in this study fail to report the validation set results. To address these issues, apart from the F1 scores on the test sets we also report the F1 scores for the validation sets. Legal documents contain domain-specific vocabulary and each type of document is normally defined in a very unambiguous way. Hence, even simple classifiers can achieve relatively high accuracy when classifying different documents. Nevertheless, even the slightest improvement of 1% or less will result in the correct classification of thousands of", "state-of-the-art results in document classification over the last few years, outperforming the human classifiers in numerous cases. Related Work ::: Document Classification Datasets. The majority of researchers evaluate their document classifying models on the following four datasets: Reuters-21578 BIBREF8, ArXiv Academic Paper Dataset - AAPD BIBREF9, IMDB reviews BIBREF10, and Yelp 2014 reviews BIBREF11. However, these commonly used datasets do not contain large documents, which conflicts with one of the main objectives of this study. Note that our definition of `document' in this specific context is a document that has at least 5000 words. For that purpose, we use a dataset provided by the U.S Securities and Exchange Commission (SEC), namely EDGAR BIBREF12. As anticipated, most models that have achieved inspiring results have very poor performance or even fail when they are tested on large documents from the EDGAR corpus. As shown in Table TABREF1 and Table TABREF3, the differences", "that both the aforementioned models were trained on a rather different corpora. The main difference between the datasets used by those researchers and the EDGAR dataset is the size of the documents, which explains why these models could not be utilised in the present study. In particular, BERT was incompatible with our dataset due to the maximum input sequence length that imposes, namely the 512 terms threshold. Methods. The novelty of this work is the application of audio segmentation used for speech recognition BIBREF13 in document classification. The ultimate purpose of audio segmentation is to divide the signal into segments, each of which contains distinct audio information. In our case, the same occurs during the document segmentation, where the split chunks become the inputs of our neural network. From a human perspective, when reading a rather long document or book, we are constantly storing and updating our memory with the essential parts or information of that record. Once"]}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "predicted_answer": "Into chunks", "predicted_evidence": ["of data with dissimilar characteristics. In this study, rather than employing an overly complex neural architecture, we focus on a relatively simpler neural structure that, in short, creates text embeddings using Doc2Vec BIBREF6 and then passes them through a Bi-directional LSTM (BiLSTM) with attention before making the final prediction. Furthermore, an important contribution of this paper to automatic document classification is the concept of dividing documents into chunks before processing. It is demonstrated that the segmentation of lengthy documents into smaller chunks of text allows the context of each document to be encapsulated in an improved way, leading to enhanced results. The intuition behind this idea was formed by investigating automatic audio segmentation research. Audio segmentation (also known as audio classification) is an essential pre-processing step in audio analysis that separates different types of sound (e.g. speech, music, silence etc.) and splits audio signals", "neural network. From a human perspective, when reading a rather long document or book, we are constantly storing and updating our memory with the essential parts or information of that record. Once enough information is stored in our memory we can form connections so as to gain a deeper understanding of the context and potentially extract valuable insight. In the same way, instead of passing the whole document to Doc2Vec, we split the document into multiple chunks (Figure FIGREF5). Hence, the machine can imitate human behaviour by identifying and determining the relevance of each chunk. We create different models with respect to the number of chunks that we divide the initial text into, in order to observe how the different number of chunks affect the efficiency of the final model. These chunks are then used to train Doc2Vec. In short, the intuition behind Doc2Vec is analogous to the intuition behind Word2Vec, where the words are used to make predictions about the target word (central", "(also known as audio classification) is an essential pre-processing step in audio analysis that separates different types of sound (e.g. speech, music, silence etc.) and splits audio signals into chunks in order to further improve the comprehension of these signals BIBREF7. Analogously, the present paper shows that splitting overly lengthy legal documents into smaller parts before processing them, boosts the final results. Related Work. In several industries that produce or handle colossal amounts of text data such as the legal industry, document categorisation is still often performed manually by human experts. Automatic categorisation of documents is highly beneficial for reducing the human effort spent on time-consuming operations. In particular, deep neural networks have achieved state-of-the-art results in document classification over the last few years, outperforming the human classifiers in numerous cases. Related Work ::: Document Classification Datasets. The majority of", "that both the aforementioned models were trained on a rather different corpora. The main difference between the datasets used by those researchers and the EDGAR dataset is the size of the documents, which explains why these models could not be utilised in the present study. In particular, BERT was incompatible with our dataset due to the maximum input sequence length that imposes, namely the 512 terms threshold. Methods. The novelty of this work is the application of audio segmentation used for speech recognition BIBREF13 in document classification. The ultimate purpose of audio segmentation is to divide the signal into segments, each of which contains distinct audio information. In our case, the same occurs during the document segmentation, where the split chunks become the inputs of our neural network. From a human perspective, when reading a rather long document or book, we are constantly storing and updating our memory with the essential parts or information of that record. Once", "of type \u201c10-K\u201d should be categorised to that filing type, we remove the first six lines where the identical text is located. The model is then able to focus on finding common features that exist in documents of the same filing type, rather than focusing on just capturing the few sentences that are the same in almost all of the documents of type \u201c10-K\u201d. A similar procedure is followed with the documents of type \u201c10-Q\u201d. Experimental Setup ::: Model Configuration. As Table TABREF13 shows, we create seven different models that correspond to the number of chunks that the text is divided into before passing through Doc2Vec. Each model is optimised separately to ensure fair comparison. For the optimisation of the BiLSTM with attention model, we use Adam optimiser with a learning rate of 0.001, batch size of 1,000 and distinct values for each one of the other hyperparameters. Analogously, the SVM classifier consists of the Radial Basis Function (RBF) as the kernel function and a different", "are then used to train Doc2Vec. In short, the intuition behind Doc2Vec is analogous to the intuition behind Word2Vec, where the words are used to make predictions about the target word (central word). The additional part of Doc2Vec is that it also considers the document ID when predicting a word. Ultimately, after the training each chunk has the form of an embedding. In the next phase, we aggregate the different chunk embeddings of a document into one vector through the use of a BiLSTM (see Figure FIGREF10). First, the different chunk embeddings $E_{i}^1, E_{i}^2,..., E_{i}^n$ of a document are sequentially fed to the BiLSTM model. Then, the outputs of the forward and the backward layer are concatenated; $h_{it}=[\\overrightarrow{h_{it}}\\overleftarrow{h_{it}}]$. $h_{it}$ denotes the resulting vectors. The final classification is subjected to the various features that each chunk contains. Thus, the attention mechanisms are introduced so as to enable the assignment of different weights", "Introduction. Text classification is a problem in library, information and computer science and one of the most classical and prominent tasks in Natural Language Processing (NLP). In particular, document classification is a procedure of assigning one or more labels to a document from a predetermined set of labels. Automatic document classification tasks can be divided into three categories: supervised, unsupervised and semi-supervised. This study focuses on supervised document classification. Research so far has focused on short text BIBREF0, BIBREF1, BIBREF2, BIBREF3, whereas the main objective of this paper is to address the classification of lengthy legal documents. In fact, pre-existing models could not be applied on our corpus, which consists of excessively lengthy legal documents. In the legal field, companies manage millions of documents per year, depending on the size of the company. Therefore, automatic categorisation of documents into different groups significantly enhances", "can achieve relatively high accuracy when classifying different documents. Nevertheless, even the slightest improvement of 1% or less will result in the correct classification of thousands of additional documents, which is crucial in the legal industry when handling large numbers of documents. This research allows these simple classifiers to achieve even greater results, by combining them with different architectures. As Table TABREF13 and Table TABREF15 indicate, dividing the document in chunks - up to certain thresholds - results in improved models compared to those where the whole document is input into the classifier. Note that the model with one chunk denotes the model which takes as input the whole document to produce the document embedding and thereby is used as a benchmark in order to be able to identify the effectiveness of the document segmentation method. More specifically, splitting the document into chunks yields higher test accuracy than having the whole document as"]}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "predicted_answer": "Bi-directional RNN with word embeddings, Hierarchical attention networks, FastText, character-aware neural language model", "predicted_evidence": ["labels instead of words. Bi-directional RNN BIBREF6 with word embeddings: It is a classical bi-directional RNN classifier, basic but effective. We also employed LSTM for it, and input the word embeddings. Hierarchical attention networks BIBREF10 : It is the state-of-the-art RNN-based document classifier. Following their method, the documents were segmented into shorter sentences of 100 words, and hierarchically encoded with bi-directional RNNs. FastText BIBREF11 : It is the state-of-the-art baseline for text classification, which simply takes n-gram features and classifies sentences by hierarchical softmax. We used the word embedding version but did not use the bigram version because the other models for comparison do not use bigram inputs. Hyperparameters. The setup of the hyperparameters in our experiments is shown in Table TABREF22 . They were tuned on the development set of 4,000 reviews, 2,000 from another subset in the public Ctrip data pack, and the other 2,000 randomly chosen", "follows: Methodology. The architecture of our proposed model is as shown in Fig. FIGREF3 . It looks similar to the character-aware neural language model proposed by BIBREF1 , but we represent a word by the sequence of radical embeddings instead of character embeddings. Besides, unlike the former model, there are no highway layers in the proposed model, because we find that highway layers do not bring significant improvements to our proposed model (see Section SECREF31 ). Representation of Characters: Sequences of Radical-level Embeddings. For every character, we use a sequence of INLINEFORM0 radical-level embeddings to represent it. They are not treated as in a bag because the position of each radical is related to how it is informative BIBREF3 . For a Chinese character, it is the sequence of the radical embeddings. When it comes to the other characters, including kanas of Japanese, alphabets, digits, punctuation marks, and special characters, it is the sequence comprised of the", "millions of parameters, which costs quite a few gigabytes to store. BIBREF19 proposed a convolutional neural network (CNN) that takes characters as the input for text classification and outperforms the previous models for large datasets. They showed the character-level CNNs are effective for text classification without the need for words. BIBREF1 introduced a recurrent neural network (RNN) language model that takes character embeddings encoded by convolutional layers as the input. Their model has much fewer parameters than the models using word embeddings, and reached the performance of the state-of-the-art on English, and outperformed baselines on morphologically rich languages. However, for Chinese and Japanese, the character vocabulary is also large, and the character embeddings are blind to the semantic information of the radicals. Conclusion and Outlook. We have proposed a model that takes radicals of characters as the inputs for sentiment classification on Chinese and Japanese,", "than the word embedding-based models for Ctrip dataset and Rakuten dataset, respectively. The accuracy is statistically the same as the character embedding-based model, approximately 98% of the word embedding-based model. The losses of the models are also close. The hierarchical attention networks and fastText achieved approximately 11% and 19% lower loss on Ctrip dataset. But on Rakuten dataset whose percentage of Chinese characters is higher, the differences between them and the proposed model drops to 0% and 9% respectively. The Proposed Model Is the Most Cost-effective. The performance of the proposed model is not significantly different from the character embedding-based baseline, and very close to the word embedding-based baselines, with a smaller vocabulary and fewer parameters. It indicates that radical-embeddings are at least as effective as the character-embeddings for Chinese and Japanese, but require less space. It suggests that for Chinese and Japanese, the radical", "the sequence of the word features. An LSTM unit contains a forget gate INLINEFORM0 to decide whether to keep the memory, an input gate INLINEFORM1 to decide whether to update the memory and an output gate INLINEFORM2 to control the output. Let INLINEFORM3 be the output of a LSTM unit at time INLINEFORM4 , INLINEFORM5 be the candidate cell state at time INLINEFORM6 , INLINEFORM7 be the cell state at time INLINEFORM8 . They are given by: DISPLAYFORM0  where, INLINEFORM0 and INLINEFORM1 are the element-wise sigmoid function and multiplication operator. Our proposed model contains two RNN layers that read document data from different directions. Let INLINEFORM0 be a document composed of INLINEFORM1 words. One of the RNN layers reads the document from the first word to the INLINEFORM2 th word, the other reads the document from the INLINEFORM3 th word to the first word. Let INLINEFORM4 be the final output of the former RNN layer and INLINEFORM5 be the final output of the latter. We", "and radical sequences of the characters as 500, 4 and 3, respectively. We split the Chinese characters in CJK Unified Ideographs of ISO/IEC 10646-1:2000 character set, until there is no component can be split further, according to CHISE Character Structure Information Database . Then the Chinese character is represented by the sequence of the radicals from the left to the right, the top to the bottom as shown in Fig. FIGREF26 . The sequences are zero-padded to the same length. For an unknown Chinese character not in the set, we treat it as a special character. Results. The number of parameters, test accuracy, and cross entropy loss of each model are as shown in Fig. FIGREF28 . The proposed model has 13% fewer parameters than the character embedding-based model, 91% and 82% fewer parameters than the word embedding-based models for Ctrip dataset and Rakuten dataset, respectively. The accuracy is statistically the same as the character embedding-based model, approximately 98% of the word", "(e.g., see the datasets introduced in Section SECREF11 ). Hence the conventional character embedding-based method is not able to give us a slim vocabulary on Chinese and Japanese. For convenience, let us collectively call hanzi and kanji as Chinese characters. Chinese characters are ideographs composed with semantic and phonetic radicals, both of which are available for character recognition, and the semantic information may be embedded in Chinese characters by the semantic radicals BIBREF2 . Besides, though the character vocabulary is huge, the number of the radicals is much fewer. Accordingly, we explored a model that represents the Chinese characters by the sequence of the radicals. We applied our proposed model to sentiment classification tasks on Chinese and Japanese and achieved the follows: Methodology. The architecture of our proposed model is as shown in Fig. FIGREF3 . It looks similar to the character-aware neural language model proposed by BIBREF1 , but we represent a word", "to the semantic information of the radicals. Conclusion and Outlook. We have proposed a model that takes radicals of characters as the inputs for sentiment classification on Chinese and Japanese, whose character vocabulary can be as scalable as word vocabulary. Our proposed model is as powerful as the character embedding-based model, and close to the word embedding-based model for the sentiment classification task, with much smaller vocabulary and fewer parameters. The results show that the radical embeddings are cost-effective for Chinese and Japanese. They are useful for the circumstances where the storage is limited. There are still a lot to do on radical embeddings. For example, a radical may be related to the meaning sometimes, but express the pronunciation at other times. We will work on dealing with such phenomena for machine learning in the future. The authors would like to thank Rakuten, Inc. and the Advanced Language Information Forum (ALAGIN) for generously providing us the"]}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "predicted_answer": "Unanswerable", "predicted_evidence": ["Introduction. Sequence to sequence models directly estimate the posterior probability of a target sequence $y$ given a source sequence $x$ BIBREF0, BIBREF1, BIBREF2, BIBREF3 and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data BIBREF4, BIBREF5. The noisy channel approach is an alternative which is used in statistical machine translation BIBREF6, BIBREF7. It entails a channel model probability $p(x|y)$ that operates in the reverse direction as well as a language model probability $p(y)$. The language model can be estimated on unpaired data and can take a separate form to the channel model. Noisy channel modeling mitigates explaining away effects that result in the source being ignored for highly likely output prefixes BIBREF8. Previous work on neural noisy channel modeling relied on a complex latent variable model that", "sentences and apply it to score the source given an incomplete target sentence. This bases decoding decisions on scoring the entire source sequence and it is very simple and effective (\u00a7SECREF2). We analyze this approach for various target prefix sizes and find that it is most accurate for large target context sizes. Our simple noisy channel approach consistently outperforms strong baselines such as online ensembles and left-to-right re-ranking setups (\u00a7SECREF3). Approach. The noisy channel approach applies Bayes' rule to model $p(y|x) = p(x|y) p(y)/ p(x)$, that is, the channel model $p(x|y)$ operating from the target to the source and a language model $p(y)$. We do not model $p(x)$ since it is constant for all $y$. We compute the channel model probabilities as follows: We refer to $p(y|x)$ as the direct model. A critical choice in our approach is to model $p(x|y)$ with a standard Transformer architecture BIBREF3 as opposed to a model which factors over target prefixes BIBREF9. This", "target in an n-best list reranking setup. The n-best list is generated by the direct model and we re-rank the list in setups where we only have a fraction of the candidate hypothesis and the source sentence. We report BLEU of the selected full candidate hypothesis. Figure FIGREF15 shows that for any given fraction of the target, scoring the entire source (src 1) has better or comparable performance than all other source prefix lengths. It is therefore beneficial to have a channel model that scores the entire source sentence. Experiments ::: Online Decoding. Next, we evaluate online decoding with a noisy channel setup compared to just a direct model () as well as an ensemble of two direct models (). Table TABREF16 shows that adding a language model to () gives a good improvement BIBREF18 over a single direct model but ensembling two direct models is slightly more effective (). The noisy channel approach () improves by 1.9 BLEU over on news2017 and by 0.9 BLEU over the ensemble. Without", "a single direct model but ensembling two direct models is slightly more effective (). The noisy channel approach () improves by 1.9 BLEU over on news2017 and by 0.9 BLEU over the ensemble. Without per word scores, accuracy drops because the direct model and the channel model are not balanced and their weight shifts throughout decoding. Our simple approach outperforms strong online ensembles which illustrates the advantage over incremental architectures BIBREF9 that do not match vanilla seq2seq models by themselves. Experiments ::: Analysis. Using the channel model in online decoding enables searching a much larger space compared to n-best list re-ranking. However, online decoding is also challenging because the channel model needs to score the entire source sequence given a partial target which can be hard. To measure this, we simulate different target prefix lengths in an n-best list re-ranking setup. The n-best list is generated by the direct model and we re-rank it for different", "Experiments ::: Re-ranking. Next, we switch to n-best re-ranking where we have the full target sentence available compared to online decoding. Noisy channel model re-ranking has been used by the top ranked entries of the WMT 2019 news translation shared task for English-German, German-English, Englsh-Russian and Russian-English BIBREF19. We compare to various baselines including right-to-left sequence to sequence models which are a popular choice for re-ranking and regularly feature in successful WMT submissions BIBREF20, BIBREF21, BIBREF22. Table TABREF20 shows that the noisy channel model outperforms the baseline () by up to 4.0 BLEU for very large beams, the ensemble by up to 2.9 BLEU () and the best right-to-left configuration by 1.4 BLEU (). The channel approach improves more than other methods with larger n-best lists by adding 2.4 BLEU from $k_1=5$ to $k_1=100$. Other methods improve a lot less with larger beams, e.g., has the next largest improvement of 1.4 BLEU when", "model without suffering from explaining away effects BIBREF23, BIBREF24. Test results on all language directions confirm that performs best (Table TABREF21). Conclusion. Previous work relied on incremental channel models which do not make use of the entire source even though it is available and, as we demonstrate, beneficial. Standard sequence to sequence models are a simple parameterization for the channel probability that naturally exploits the entire source. This parameterization outperforms strong baselines such as ensembles of direct models and right-to-left models. Channel models are particularly effective with large context sizes and an interesting future direction is to iteratively refine the output while conditioning on previous contexts.", "away effects that result in the source being ignored for highly likely output prefixes BIBREF8. Previous work on neural noisy channel modeling relied on a complex latent variable model that incrementally processes source and target prefixes BIBREF9. This trades efficiency for accuracy because their model performs significantly less well than a vanilla sequence to sequence model. For languages with similar word order, it can be sufficient to predict the first target token based on a short source prefix, but for languages where word order differs significantly, we may need to take the entire source sentence into account to make a decision. In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an incomplete target sentence. This bases decoding decisions on scoring the entire source sequence and it is very simple and effective (\u00a7SECREF2). We", "than other methods with larger n-best lists by adding 2.4 BLEU from $k_1=5$ to $k_1=100$. Other methods improve a lot less with larger beams, e.g., has the next largest improvement of 1.4 BLEU when increasing the beam size but this is still significantly lower than for the noisy channel approach. Adding a language model benefits all settings (, , ) but the channel approach benefits most ( vs ). The direct model with a language model () performs better than for online decoding, likely because the constrained re-ranking setup mitigates explaining away effects (cf. Table TABREF16). Interestingly, both or give only modest improvements compared to . Although previous work demonstrated that reranking with can improve over , we show that the channel model is important to properly leverage the language model without suffering from explaining away effects BIBREF23, BIBREF24. Test results on all language directions confirm that performs best (Table TABREF21). Conclusion. Previous work relied on"]}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "predicted_answer": "English-German, German-English, English-Russian, Russian-English, Chinese-English, and IWSLT'14 De-En", "predicted_evidence": ["use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (Zh-En), we pre-process WMT'17 data following BIBREF12, we develop on dev2017 and test on news2017. For IWSLT'14 De-En we follow the setup of BIBREF13 and measure case-sensitive tokenized BLEU. For WMT De-En, En-De and Zh-En we measure detokenized BLEU BIBREF14. Experiments ::: Language Models.. We train two big Transformer language models with 12 blocks BIBREF15: one on the German newscrawl data distributed by WMT'18 comprising 260M sentences and another one on the English newscrawl data comprising 193M sentences. Both use a BPE vocabulary of 32K types. We train on 32 Nvidia V100 GPUs with 16-bit floating point operations BIBREF16 and training took four days. Experiments ::: Sequence to Sequence Model training.. For En-De, De-En, Zh-En we use", "where $t$ is the length of the target prefix $y$, $s$ is the source sentence length and $\\lambda $ is a tunable weight. Initially, we used separate weights for $p(x|y)$ and $p(y)$ but we found that a single weight resulted in the same accuracy and was easier to tune. Scaling by $t$ and $s$ makes the scores of the direct and channel model comparable to each other throughout decoding. In n-best re-ranking, we have complete target sentences which are of roughly equal length and therefore do not use per word scores. Experiments ::: Datasets.. For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017. For reranking, we train models with a 40K joint byte pair encoding vocabulary (BPE; BIBREF11). To be able to use the language model during online decoding, we use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies", "than other methods with larger n-best lists by adding 2.4 BLEU from $k_1=5$ to $k_1=100$. Other methods improve a lot less with larger beams, e.g., has the next largest improvement of 1.4 BLEU when increasing the beam size but this is still significantly lower than for the noisy channel approach. Adding a language model benefits all settings (, , ) but the channel approach benefits most ( vs ). The direct model with a language model () performs better than for online decoding, likely because the constrained re-ranking setup mitigates explaining away effects (cf. Table TABREF16). Interestingly, both or give only modest improvements compared to . Although previous work demonstrated that reranking with can improve over , we show that the channel model is important to properly leverage the language model without suffering from explaining away effects BIBREF23, BIBREF24. Test results on all language directions confirm that performs best (Table TABREF21). Conclusion. Previous work relied on", "source tokens can be scored in parallel. Inference is mostly slowed down by the autoregressive nature of decoding. Scoring the entire source enables capturing more dependencies between the source and target, since the beginning of the target must explain the entire source, not just the beginning. This is especially critical when the word order between the source and target language varies considerably, and likely accounts for the lower performance of the direct model of BIBREF9 in comparison to a standard seq2seq model. Approach ::: Model combinaton.. Since the direct model needs to be evaluated for pre-pruning, we also include these probabilities in making decoding decisions. We use the following linear combination of the channel model, the language model and the direct model for decoding: where $t$ is the length of the target prefix $y$, $s$ is the source sentence length and $\\lambda $ is a tunable weight. Initially, we used separate weights for $p(x|y)$ and $p(y)$ but we found that", "Introduction. Sequence to sequence models directly estimate the posterior probability of a target sequence $y$ given a source sequence $x$ BIBREF0, BIBREF1, BIBREF2, BIBREF3 and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data BIBREF4, BIBREF5. The noisy channel approach is an alternative which is used in statistical machine translation BIBREF6, BIBREF7. It entails a channel model probability $p(x|y)$ that operates in the reverse direction as well as a language model probability $p(y)$. The language model can be estimated on unpaired data and can take a separate form to the channel model. Noisy channel modeling mitigates explaining away effects that result in the source being ignored for highly likely output prefixes BIBREF8. Previous work on neural noisy channel modeling relied on a complex latent variable model that", "target in an n-best list reranking setup. The n-best list is generated by the direct model and we re-rank the list in setups where we only have a fraction of the candidate hypothesis and the source sentence. We report BLEU of the selected full candidate hypothesis. Figure FIGREF15 shows that for any given fraction of the target, scoring the entire source (src 1) has better or comparable performance than all other source prefix lengths. It is therefore beneficial to have a channel model that scores the entire source sentence. Experiments ::: Online Decoding. Next, we evaluate online decoding with a noisy channel setup compared to just a direct model () as well as an ensemble of two direct models (). Table TABREF16 shows that adding a language model to () gives a good improvement BIBREF18 over a single direct model but ensembling two direct models is slightly more effective (). The noisy channel approach () improves by 1.9 BLEU over on news2017 and by 0.9 BLEU over the ensemble. Without", "Experiments ::: Re-ranking. Next, we switch to n-best re-ranking where we have the full target sentence available compared to online decoding. Noisy channel model re-ranking has been used by the top ranked entries of the WMT 2019 news translation shared task for English-German, German-English, Englsh-Russian and Russian-English BIBREF19. We compare to various baselines including right-to-left sequence to sequence models which are a popular choice for re-ranking and regularly feature in successful WMT submissions BIBREF20, BIBREF21, BIBREF22. Table TABREF20 shows that the noisy channel model outperforms the baseline () by up to 4.0 BLEU for very large beams, the ensemble by up to 2.9 BLEU () and the best right-to-left configuration by 1.4 BLEU (). The channel approach improves more than other methods with larger n-best lists by adding 2.4 BLEU from $k_1=5$ to $k_1=100$. Other methods improve a lot less with larger beams, e.g., has the next largest improvement of 1.4 BLEU when", "types. We train on 32 Nvidia V100 GPUs with 16-bit floating point operations BIBREF16 and training took four days. Experiments ::: Sequence to Sequence Model training.. For En-De, De-En, Zh-En we use big Transformers and for IWSLT De-En a base Transformer BIBREF3 as implemented in fairseq BIBREF17. For online decoding experiments, we do not share encoder and decoder embeddings since the source and target vocabularies were learned separately. We report average accuracy of three random initializations of a each configuration. We generally use $k_1=5$ and $k_2=10$. We tune $\\lambda _1$, and a length penalty on the validation set. Experiments ::: Simple Channel Model. We first motivate a standard sequence to sequence model to parameterize $p(x|y)$ as opposed to a model that is trained to operate over prefixes. We train Transformer models to translate from the target to the source (En-De) and compare two variants: i) a standard sequence to sequence model trained to predict full source"]}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "predicted_answer": "7.24% in absolute F1-score when training with 2000 instances per domain, and achieves an even higher improvement of 14.57% when only 500 training instances are used.", "predicted_evidence": ["again surpass the BoE models. ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of $timex$ and $location$ slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than $timex$ and $location$ . Though not as popular as $5.37$0 and $5.37$1 , slots such as $5.37$2 , $5.37$3 , $5.37$4 , and $5.37$5 appear", "This is because ZAT can benefit from other reusable slots than $timex$ and $location$ . Though not as popular as $5.37$0 and $5.37$1 , slots such as $5.37$2 , $5.37$3 , $5.37$4 , and $5.37$5 appear across many domains. We plot the averaged performances on varying amounts of training data for each target domain in Figure 3 . Note that the improvements are even higher for the experiments with smaller training data. In particular, ZAT shows an improvement of $14.67$ in absolute F1-score over CRF when training with 500 instances. ZAT achieves an F1-score of 76.04% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 75%. Thus the ZAT model achieves better F1-score with only one-fourth the training data. Table 3 shows the performances of CT and ZAT when no target domain data is available. Both models are able to achieve reasonable zero-shot performance for most domains, and ZAT shows an average improvement of $5.07$ over CT.", "of CT and ZAT when no target domain data is available. Both models are able to achieve reasonable zero-shot performance for most domains, and ZAT shows an average improvement of $5.07$ over CT. Model Variants. In Table 4 , we ablate our full model by removing the CRF layer ( $-CRF$ ) and character-level word embeddings ( $-CHAR$ ). Without CRF, the model suffers a loss of 1%-1.8% points. The character-level word embeddings are also important: without this, the performance is down by 0.5%-2.7%. We study the impact of fine-tuning the pre-trained word embeddings ( $+WEFT$ ). When there is no target domain data available, fine-tuning hurts performance. But, with a moderate amount of target domain data, fine-tuning improves performance. Analysis. To better understand our model, in Figure 7 , we visualize the attention weights for the input sentence \"Can I wear jeans to a casual dinner?\" with different slots: (a) category, (b) item, and (c) time. From (a) and (b), it is clear that the", "(which do not have the slot) with a ratio of 1 to 3. After the base model is trained, domain adaptation is simply done by further training the base model on varying amounts of the training data of the target domain. Note that the size of the joint dataset for each target domain is 18,000, which is dramatically smaller than millions of examples used for training expert models in the BoE approach. Furthermore, there are a lot of utterances in the joint dataset where no slots from the target domain is present. Comparative Results. Table 2 shows the F1-scores obtained by the different methods for each of the 10 domains. LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also", "understanding models for multiple languages and show good results. BIBREF28 presented a zero-shot model for question generation from knowledge graphs, and BIBREF29 presented a model for zero-shot transfer learning for event extraction. Conclusion. In this paper, we introduce a novel Zero-Shot Adaptive Transfer method for slot tagging that utilizes the slot description for transferring reusable concepts across domains to avoid some drawbacks of prior approaches such as increased training time and suboptimal concept alignments. Experiment results show that our model performs significantly better than state-of-the-art systems by a large margin of 7.24% in absolute F1-score when training with 2000 instances per domain, and achieves an even higher improvement of 14.57% when only 500 training instances are used. We provide extensive analysis of the results to shed light on future work. We plan to extend our model to consider more context and utilize exogenous resources like parsing", ". Our zero-shot model architecture differs from this by adding: 1) an attention layer to produce the slot-aware representations of input words, 2) a CRF layer to better satisfy global consistency constraints, 3) character-level embeddings to incorporate morphological information. Despite its simplicity, we show that our model outperforms all existing methods including the previous zero-shot learning approach in domain adaptation settings. We first describe our approach called Zero-Shot Adaptive Transfer model (ZAT) in detail. We then describe the dataset we used for our experiments. Using this data, we conduct experiments comparing our ZAT model with a set of state-of-the-art models: Bag-of-Expert (BoE) models and their non-expert counterparts BIBREF4 , and the Concept Tagger model BIBREF5 , showing that our model can lead to significant F1-score improvements. This is followed by an in-depth analysis of the results. We then provide a survey of related work and concluding remarks.", "labeled datasets using domain adaptation approaches such as feature augmentation BIBREF1 . A disadvantage of this approach is the increase in training time as the amount of reusable data grows. The reusable data might contain hundreds of thousands of samples, making iterative refinement prohibitive. In contrast, the model-driven approach utilizes \u201cexpert\" models for summarizing the data for reusable slots BIBREF3 , BIBREF4 . The outputs of the expert models are directly used when training new domains, allowing for faster training. A drawback of this approach is that it requires explicit concept alignments which itself is not a trivial task, potentially missing lots of reusable concepts. Additionally, it's not easy to generalize these models to new, unseen slots. In this paper, we present a new domain adaptation technique for slot tagging inspired by recent advances in zero-shot learning. Traditionally, slot tagging is formulated as a sequence labeling task using the BIO representation", "This is followed by a feed forward layer where the contextual word representations are combined with a slot encoding to produce vectors of 128 dimensions. The slot encoding is the average vector of the word embeddings for the slot description. This feeds into another 128 dimensional bi-directional LSTM layer followed by a softmax layer that outputs the prediction for that slot. Domain Adaptation using Zero-Shot Model. For domain adaptation with zero-shot models, we first construct a joint training dataset by combining the training datasets of size 2000 from all domains except for a target domain. We then train a base model on the joint dataset. We sample input examples during training and evaluation for each slot to include both positive examples (which have the slot) and negative examples (which do not have the slot) with a ratio of 1 to 3. After the base model is trained, domain adaptation is simply done by further training the base model on varying amounts of the training data of"]}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "predicted_answer": "Indigenous languages and code-mixed languages.", "predicted_evidence": ["world's largest population and are rich in diverse languages, analysing these indigenous language will be useful to companies because they have large share of users in India and China. In the current study, the types of languages i.e. indigenous languages and code mix languages are discussed prior to the approaches, methodologies used by the researchers and challenges faced by them. Introduction ::: Indigenous Languages. Indigenous languages are the languages that are native to a region or spoken by a group of people in a particular state. It is not necessarily a national language. For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages. Introduction ::: Code Mix Languages. Code-mixing is mixing two or more languages while communicating in person or over the web. Code-mixing is basically observed in the multilingual speakers. Code-mixed languages are a challenge to the sentiment analysis problem. A classic example of the code-mix language", "internet users in India and most people know more than one language. They express their feelings or emotions using more than one languages, thus generating a new code-mixed/code-switched language. The problem of code-mixing and code-switching are well studied in the field of NLP BIBREF0 , BIBREF1 . Information extraction from Indian internet user-generated texts become more difficult due to this multilingual nature. Much research has been conducted in this field such as language identification BIBREF2 , BIBREF3 , part-of-speech tagging BIBREF4 . Joshi et al. JoshiPSV16 have performed sentiment analysis in Hindi-English (HI-EN) code-mixed data and almost no work exists on sentiment analysis of Bengali-English (BN-EN) code-mixed texts. The Sentiment Analysis of Indian Language (Code-Mixed) (SAIL _Code-Mixed) is a shared task at ICON-2017. Two most popular code-mixed languages namely Hindi and Bengali mixed with English were considered for the sentiment identification task. A total of 40", "has been done in SA for indigenous languages. 23 papers are being studied to find the trends in the field of SA. 67% of the papers reviewed have used Machine learning, deep learning and advanced deep learning algorithms. Only 29% of researchers have used lexicon-based approach. SVM (Support Vector Machine) and LR (Logical Regression) performed the best among the machine learning approach. CNN performed the best in the deep learning techniques and BERT was the choice by the researchers in the advanced deep learning techniques. The code-mix languages are the new non official language which we can see on the web. There isn\u2019t much work done on code-mix language data. Also, a lot of work is done in SA of Hindi language as compared to the other Indian languages like Gujarati, Marathi, Telugu. There is a lot of work carried out in the sentence level of sentiment analysis. There is a need for more SA work to be carried out at document level or aspect. Also, there are very few papers which", "Introduction. The past decade witnessed rapid growth and widespread usage of social media platforms by generating a significant amount of user-generated text. The user-generated texts contain high information content in the form of news, expression, or knowledge. Automatically mining information from user-generated data is unraveling a new field of research in Natural Language Processing (NLP) and has been a difficult task due to unstructured and noisy nature. In spite of the existing challenges, much research has been conducted on user-generated data in the field of information extraction, sentiment analysis, event extraction, user profiling and many more. According to Census of India, there are 22 scheduled languages and more than 100 non scheduled languages in India. There are 462 million internet users in India and most people know more than one language. They express their feelings or emotions using more than one languages, thus generating a new code-mixed/code-switched language.", "tasks available on sentiment analysis of Indian language tweets BIBREF13 , BIBREF14 . The shared task on sentiment analysis in Indian languages (SAIL) tweets focused on sentiment analysis of three Indian languages: Bengali, Hindi, and Tamil BIBREF13 . Dataset and Evaluation. This section describes statistics of the dataset and the evaluation procedure. Preparing a gold standard dataset is the first step towards achieving good accuracy. Several tasks in the field of NLP suffer from lack of gold standard dataset. In the case of Indian languages, there is no such code-mixed dataset available for research purpose. Thus, we developed the dataset and the details are provided below. Dataset. Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi", "languages. Data for the indigenous languages is available across the web but is mainly collected from social media platforms like Twitter, Facebook and YouTube. Some researchers have extracted their data from Twitter BIBREF9, BIBREF16, BIBREF17, BIBREF20, BIBREF23, BIBREF24, BIBREF25, while some have opted for extracting the data manually or by performing web scrapping on different websites like Facebook, microblogs, e-commerce websites, YouTube etc. BIBREF7, BIBREF8, BIBREF11, BIBREF12, BIBREF14, BIBREF22. Authors in BIBREF13 have accessed the FIRE 2015 dataset. The dataset has 792 utterances and has 8 different languages other than English. Researchers in BIBREF19 collected 3000 positive and 3000 negative Odia movie reviews. Authors in BIBREF10 collected 1400 Telugu sentences from e-Newspapers from data 1st December 2016 to 31st December 2016. The study in BIBREF5 contained the speeches of different leaders who spoke about different domain topics like festivals, environment, society", "_Code-Mixed) is a shared task at ICON-2017. Two most popular code-mixed languages namely Hindi and Bengali mixed with English were considered for the sentiment identification task. A total of 40 participants registered for the shared task and only nine teams have submitted their predicted outputs. Out of nine unique submitted systems for evaluation, eight teams submitted fourteen runs for HI-EN dataset whereas seven teams submitted nine runs for BN-EN dataset. The training and test dataset were provided after annotating the languages and sentiment (positive, negative, and neutral) tags. The language tags were automatically annotated with the help of different dictionaries whereas the sentiment tags were manually annotated. The submitted systems are ranked using the macro average f-score. The paper is organized as following manner. Section SECREF2 describes the NLP in Indian languages mainly related to code-mixing and sentiment analysis. The detailed statistics of the dataset and", "The paper is organized as following manner. Section SECREF2 describes the NLP in Indian languages mainly related to code-mixing and sentiment analysis. The detailed statistics of the dataset and evaluation are described in Section SECREF3 . The baseline systems and participant's system description are described in Section SECREF4 . Finally, conclusion and future research are drawn in Section SECREF5 . Related Work. With the rise of social media and user-generated data, information extraction from user-generated text became an important research area. Social media has become the voice of many people over decades and it has special relations with real time events. The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been"]}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "predicted_answer": "BIBREF19 and BIBREF17", "predicted_evidence": ["amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods. Both BIBREF19 and BIBREF17 use time-consuming instruction text preprocessing over the skip-thought technique BIBREF18. This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's. By using online-instruction encoding with the self-attention encoder, we were able train the model for its main task in under 30 hours. Furthermore, the proposed approach offers more flexibility for dataset alterations. Qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores. Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure FIGREF16 we present a few typical results on the intended recipe", "Materials and Methods ::: Loss function. To align text and image embeddings of matching recipe-image pairs alongside each other, we maximize the cosine distance between positive pairs and minimize it between negative pairs. We have trained our model using cosine similarity loss with margin as in BIBREF19 and with the triplet loss proposed by BIBREF17. Both objective functions and the semantic regularization by BIBREF19 aim at maximizing intra-class correlation and minimizing inter-class correlation. Let us define the text query embedding as $\\phi ^q$ and the embedding of the image query as $\\phi ^d$, then the cosine embedding loss can be defined as follows: where $cos(x,y)$ is the normalized cosine similarity and $\\alpha $ is a margin ($-1\\leqslant \\alpha \\leqslant 1)$, that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum $\\alpha $ similarity, where a maximum margin of zero or negative margins allow no correlation", "the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$. Materials and Methods ::: Training configuration. We used Adam BIBREF25 optimizer with an initial learning rate of $10^{-4}$. At the beginning of the training session, we freeze the pretrained ResNet-50 weights and optimize only the text-processing branch until we do no longer make progress. Then, we alternate train image and text branch until we switched modality for 10 times. Lastly, we fine-tune the overall model by releasing all trainable parameters in the model. Our optimization strategy differs from BIBREF19 in that we use an aggressive learning rate decay, namely exponential decay, so that the learning rate is halved all 20 epochs. Since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective BIBREF19 and for the triplet loss", "and 54,565 and 54,885 for the validation and testing sets, respectively. Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods. Both BIBREF19 and BIBREF17 use time-consuming instruction text preprocessing over the", "the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective BIBREF19 and for the triplet loss BIBREF17. Experimental Setup and Results. Recipe1M is already distributed in three parts, the training, validation and testing sets. We did not make any changes to these partitions. Except with our more sensitive preprocessing algorithm, we accept more recipes from the raw corpus. BIBREF19 used 238,399 samples for their effective training set and for the validation and testing set 51,119 and 51,303 samples, respectively. By filtering out noisy instructions sentences (e.g. instructions containing only punctuation) we increased the effective dataset size to 254,238 samples for the training set and 54,565 and 54,885 for the validation and testing sets, respectively. Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is", "The large corpus is supplemented with semantic information (1048 meal classes) for injecting an additional source of information in potential models. In the table in Figure FIGREF1, the structure of recipes belonging to different semantic classes is displayed. Using a slightly adjusted pre-processing than that in BIBREF15 (elimination of noisy instruction sentences), the training set, validation set and test set contain 254,238 and 54,565 and 54,885 matching pairs, respectively. In BIBREF15, the authors chose the overall amount of instructions per recipe as one criterion for a valid matching pair. But we simply removed instruction sentences that contain only punctuation and gained some extra data for training and validation. Materials and Methods ::: Model Architecture. The proposed model architecture is based on a multi-path approach for each of the involved input data types namely, instructions, ingredients and images, similarly to BIBREF19. In Figure FIGREF4, the overall structure", "architecture is based on a multi-path approach for each of the involved input data types namely, instructions, ingredients and images, similarly to BIBREF19. In Figure FIGREF4, the overall structure is presented. For the instruction encoder, we utilized a self-attention mechanism BIBREF20, which learns which words of the instructions are relevant with a certain ingredient. In order to encode the ingredients, a bidirectional RNN is used, since ingredients are an unordered list of words. All RNNs in the ingredients path were implemented with Long Short-Term Memory (LSTM) cells BIBREF21. We fixed the ingredient representation to have a length of 600, independent of the amount of ingredients. Lastly, the outputs of the self-attention-instruction encoder with ingredient attention and the output of the bidirectional LSTM ingredient-encoder are concatenated and mapped to the joint embedding space. The image analysis path is composed of a ResNet-50 model BIBREF22, pretrained on the ImageNet", "applied. Carvalho et al. BIBREF17 proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. BIBREF15, they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup. To address the challenge of encoding long sequences (like recipe instructions), BIBREF15 chose to represent single instructions as sentence embedding using the skip-thought technique BIBREF18. These encoded instruction sentences are referred to as skip-instructions and their embedding is not fine tuned when learning the image-text joint embedding. In this study, we present a method for the joint learning of meal image and recipe embedding, using a multi-path structure that incorporates natural language processing paths, as well as image analysis paths. The main contribution of the proposed method is"]}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "predicted_answer": "The results are presented in Table TABREF11 in comparison to baseline methods.", "predicted_evidence": ["and 54,565 and 54,885 for the validation and testing sets, respectively. Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods. Both BIBREF19 and BIBREF17 use time-consuming instruction text preprocessing over the", "Materials and Methods ::: Loss function. To align text and image embeddings of matching recipe-image pairs alongside each other, we maximize the cosine distance between positive pairs and minimize it between negative pairs. We have trained our model using cosine similarity loss with margin as in BIBREF19 and with the triplet loss proposed by BIBREF17. Both objective functions and the semantic regularization by BIBREF19 aim at maximizing intra-class correlation and minimizing inter-class correlation. Let us define the text query embedding as $\\phi ^q$ and the embedding of the image query as $\\phi ^d$, then the cosine embedding loss can be defined as follows: where $cos(x,y)$ is the normalized cosine similarity and $\\alpha $ is a margin ($-1\\leqslant \\alpha \\leqslant 1)$, that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum $\\alpha $ similarity, where a maximum margin of zero or negative margins allow no correlation", "the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective BIBREF19 and for the triplet loss BIBREF17. Experimental Setup and Results. Recipe1M is already distributed in three parts, the training, validation and testing sets. We did not make any changes to these partitions. Except with our more sensitive preprocessing algorithm, we accept more recipes from the raw corpus. BIBREF19 used 238,399 samples for their effective training set and for the validation and testing set 51,119 and 51,303 samples, respectively. By filtering out noisy instructions sentences (e.g. instructions containing only punctuation) we increased the effective dataset size to 254,238 samples for the training set and 54,565 and 54,885 for the validation and testing sets, respectively. Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is", "how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum $\\alpha $ similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively. $\\phi ^d$ is the corresponding image counterpart to $\\phi ^q$ if $y=1$ or a randomly chosen sample $\\phi ^d \\in S \\wedge \\phi ^d \\ne \\phi ^{d(q)}$ if $y=-1$, where $\\phi ^{d(q)}$ is the true match for $\\phi ^q$ and $S$ is the dataset we sample from it. Furthermore, we complement the cosine similarity with cross-entropy classification loss ($L_{reg}$), leading to the applied objective function. with $c_r$ and $c_v$ as semantic recipe-class and semantic image-class, respectively, while $c_r=c_v$ if the food image and recipe text are a positive pair. For the triplet loss, we define $\\phi ^q$ as query embedding, $\\phi ^{d+}$ as matching image counterpart and $\\phi ^{d-}$ as another", "$c_r=c_v$ if the food image and recipe text are a positive pair. For the triplet loss, we define $\\phi ^q$ as query embedding, $\\phi ^{d+}$ as matching image counterpart and $\\phi ^{d-}$ as another random sample taken from $S$. Further $\\phi ^{d_{sem}+} \\in S \\wedge \\phi ^{d_{sem}+} \\ne \\phi ^{d(q)}$ is a sample from $S$ sharing the same semantic class as $\\phi ^q$ and $\\phi ^{d_{sem}-}$ is a sample from any other class. The triplet loss is formulated as follows: where $\\beta \\in [0,1]$ weights between quadratic and linear loss, $\\alpha \\in [0,2]$ is the margin and $\\gamma \\in [0,1]$ weights between semantic- and sample-loss. The triplet loss encourages the embedding vectors of a matching pair to be larger by a margin above its non-matching counterpart. Further, the semantic loss encourages the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$. Materials and Methods ::: Training configuration. We", "amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods. Both BIBREF19 and BIBREF17 use time-consuming instruction text preprocessing over the skip-thought technique BIBREF18. This process doubles the overall training time from three days to six days using two Nvidia Titan X GPU's. By using online-instruction encoding with the self-attention encoder, we were able train the model for its main task in under 30 hours. Furthermore, the proposed approach offers more flexibility for dataset alterations. Qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores. Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure FIGREF16 we present a few typical results on the intended recipe", "the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$. Materials and Methods ::: Training configuration. We used Adam BIBREF25 optimizer with an initial learning rate of $10^{-4}$. At the beginning of the training session, we freeze the pretrained ResNet-50 weights and optimize only the text-processing branch until we do no longer make progress. Then, we alternate train image and text branch until we switched modality for 10 times. Lastly, we fine-tune the overall model by releasing all trainable parameters in the model. Our optimization strategy differs from BIBREF19 in that we use an aggressive learning rate decay, namely exponential decay, so that the learning rate is halved all 20 epochs. Since the timing of freezing layers proved not to be of importance unless the recipe path is trained first, we used the same strategy under the cosine distance objective BIBREF19 and for the triplet loss", "The large corpus is supplemented with semantic information (1048 meal classes) for injecting an additional source of information in potential models. In the table in Figure FIGREF1, the structure of recipes belonging to different semantic classes is displayed. Using a slightly adjusted pre-processing than that in BIBREF15 (elimination of noisy instruction sentences), the training set, validation set and test set contain 254,238 and 54,565 and 54,885 matching pairs, respectively. In BIBREF15, the authors chose the overall amount of instructions per recipe as one criterion for a valid matching pair. But we simply removed instruction sentences that contain only punctuation and gained some extra data for training and validation. Materials and Methods ::: Model Architecture. The proposed model architecture is based on a multi-path approach for each of the involved input data types namely, instructions, ingredients and images, similarly to BIBREF19. In Figure FIGREF4, the overall structure"]}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "predicted_answer": "Yes, in the Encoding with LSTM and Gated CNN method, each paragraph is encoded separately.", "predicted_evidence": ["Encoding with LSTM and Gated CNN. In this method, paragraphs are encoded separately, and the concatenation of the resulted encoding is going through the classifier. First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN BIBREF1 for extraction of single encoding for each paragraph. The accuracy is barely above $50\\%$ , which depicts that this method is not very promising. Fine-tuning BERT. We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. The classification is completely based on the BERT paper, i.e., we represent the first and second paragraph as a single packed sequence, with the first paragraph using the A embedding and the second paragraph using the B embedding. In the case of feature extraction, the network weights freeze and CLS token are fed to the classifier. In the", "Paragraph Ordering Dataset. We have prepared a dataset, ParagraphOrdreing, which consists of around 300,000 paragraph pairs. We collected our data from Project Gutenberg. We have written an API for gathering and pre-processing in order to have the appropriate format for the defined task. Each example contains two paragraphs and a label which determines whether the second paragraph comes really after the first paragraph (true order with label 1) or the order has been reversed (Table 1 ). The detailed statistics of the data can be found in Table 2 . Approach. Different approaches have been used to solve this task. The best result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around $84\\%$ accuracy on test set which outperforms other models significantly. Encoding with LSTM and Gated CNN. In this method, paragraphs are encoded separately, and the concatenation of the resulted encoding is going through the classifier. First, each paragraph is encoded", "the first paragraph using the A embedding and the second paragraph using the B embedding. In the case of feature extraction, the network weights freeze and CLS token are fed to the classifier. In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task. First, just the last sentence of the first paragraph and the beginning sentence of the second paragraph has been used for classification. We wanted to know whether two sentences are enough for ordering classification or not. After that, we increased the number of tokens and accuracy respectively increases. We found this method very promising and the accuracy significantly increases with respect to previous methods (Table 3 ). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories.", "Arrow of Time\" paper BIBREF6 for defining our task. They sought to understand the arrow of time in the videos; Given ordered frames from the video, whether the video is playing backward or forward. They hypothesized that the deep learning algorithm should have the good grasp of the physics principle (e.g. water flows downward) to be able to predict the frame orders in time. Getting inspiration from this work, we have defined a similar task in the domain of NLP. Given two paragraphs, whether the second paragraph comes really after the first one or the order has been reversed. It is the way of learning the arrow of times in the stories and can be very beneficial in neural story generation tasks. Moreover, this is a self-supervised task, which means the labels come from the text itself. Paragraph Ordering Dataset. We have prepared a dataset, ParagraphOrdreing, which consists of around 300,000 paragraph pairs. We collected our data from Project Gutenberg. We have written an API for", "Introduction. Recurrent neural networks (RNN) and architectures based on RNNs like LSTM BIBREF0 has been used to process sequential data more than a decade. Recently, alternative architectures such as convolutional networks BIBREF1 , BIBREF2 and transformer model BIBREF3 have been used extensively and achieved the state of the art result in diverse natural language processing (NLP) tasks. Specifically, pre-trained models such as the OpenAI transformer BIBREF4 and BERT BIBREF5 which are based on transformer architecture, have significantly improved accuracy on different benchmarks. In this paper, we are introducing a new dataset which we call ParagraphOrdering, and test the ability of the mentioned models on this newly introduced dataset. We have got inspiration from \"Learning and Using the Arrow of Time\" paper BIBREF6 for defining our task. They sought to understand the arrow of time in the videos; Given ordered frames from the video, whether the video is playing backward or forward."]}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "predicted_answer": "Error detection system by Rei2016", "predicted_evidence": ["systems BIBREF12, BIBREF23 have generated coherent and readable content using word-level models. A comparison between Character-based and Word-based LSTMs in BIBREF12 proved that for a sample of generated text sequence, word level models have lower perplexity than character level deep learners. This is because the character-based text generators suffer from spelling errors and incoherent text fragments. Experimental Methodology ::: Experimental Setup ::: RNN architecture. Traditional language models like N-grams are limited by the history or the sequence of the textual content that these models are able to look back upon while training. However, RNNs are able to retain the long term information provided by some text sequence, making it work as a \u201cmemory\u201d-based model. However while building a model, RNNs are not the best performers when it comes to preserving long term dependencies. For this reason we use Long Short Term Memory architectures (LSTM) networks which are able to learn a", "feed $\\widehat{w_{1}}$ to the RNN model and get the next word by evaluating $P(\\widehat{w_{2}} ~|~\\widehat{w_{0}},\\widehat{w_{1}})$. This is done repeatedly to generate a text sequence with n words: $\\widehat{w_{0}}$, $\\widehat{w_{1}}$, $\\widehat{w_{2}}$, ..., $\\widehat{w_{n}}$. Sampling parameters. We vary our sampling parameters to generate the email body samples. For our implementation, we choose temperature as the best parameter. Given a sequence of words for training, $w_{0}$, $w_{1}$, $w_{2}$, ..., $w_{n}$, the goal of the trained LSTM network is to predict the best set of words that follow the training sequence as the output ($\\widehat{w_{0}}$, $\\widehat{w_{1}}$, $\\widehat{w_{2}}$, ..., $\\widehat{w_{n}}$). Based on the input set of words, the model builds a probability distribution $P(w_{t+1} ~|~ w_{t^{\\prime }\\le t}) = softmax(\\widehat{w_{t}})$, here $softmax$ normalization with temperature control (Temp) is defined as: $P(softmax(\\widehat{w_{t}^{j}})) =", "We include examples from further evaluation of steps. (D) Training on Legitimates + 50% Malicious content: In this training step, we consider a total of 50% of the malicious data (1140 phishing emails) and 603 legitimate emails. This is done to observe whether training on an unbalanced data, with twice the ratio of malign instances than legitimate ones, can successfully incorporate obvious malicious flags like poisonous links, attachments, etc. We show two examples of emails generated using deep learners at varying sampling temperatures. Example I at Temperature = 0.7: If you are still online. genuine information in the message, notice your account has been frozen to your account in order to restore your account as click on CONTINUE Payment Contact $<$LINK$>$ UK. Example IT at Temperature = 0.5: Hi will have temporarily information your account will be restricted during that the Internet accounts and upgrading password An data Thank you for your our security of your Account Please", "in Section SECREF19 Experimental Methodology ::: Data description. To best emulate a benign email, a text generator must learn the text representation in actual legitimate emails. Therefore, it is necessary to incorporate benign emails in training the model. However, as a successful attacker, our main aim is to create the perfect deceptive email - one which despite having malign components like poisoned links or attachments, looks legitimate enough to bypass statistical detectors and human supervision. Primarily, for the reasons stated above, we have used multiple email datasets, belonging to both legitimate and malicious classes, for training the system model and also in the quantitative evaluation and comparison steps. For our training model, we use a larger ratio of malicious emails compared to legitimate data (approximate ratio of benign to malicious is 1:4). Legitimate dataset. We use three sets of legitimate emails for modeling our legitimate content. The legitimate emails were", "from issues, deterministic samples can suffer from repetitive text while the samples generated using the stochastic mechanism are prone to spelling mistakes, grammatical errors, nonsensical words. We generate our samples by varying the temperature values to 0.2, 0.5, 0.7 and 1.0. For our evaluation and detection experiments, we randomly select 25 system generated samples, 2 samples generated at a temperature of 0.2, 10 samples at temperature 0.5, 5 samples at a temperature of 0.7 and 8 samples at temperature 1.0. Experimental Methodology ::: Experimental Setup ::: Customization of Malicious Intent. One important aspect of malicious emails is their harmful intent. The perfect attack vector will have malicious elements like a poisonous link or malware attachment wrapped in legitimate context, something which is sly enough to fool both a state-of-the-art email classifier as well as the victim. One novelty of this system training is the procedure of injecting malicious intent during", "We perform a simple quantitative evaluation by using three text-based classification algorithms on our generated emails. Using the Python SciKit-Learn library, we test three popular text-based filtering algorithms - Support Vector Machines BIBREF26, Naive Bayes BIBREF27 and Logistic Regression BIBREF28. The training set was modeled as a document-term matrix and the word count vector is used as the feature for building the models. For our evaluation, we train models using Support Vector Machines (SVM), Naive Bayes (NB) and Logistic Regression (LR) models on a training data of 300 legitimate emails from WikiLeaks archives and 150 phishing emails from Cornell PhishBowl BIBREF29. We test the data on 100 legitimate emails from WikiLeaks archives that were not included in the training set and 25 `fake' emails that were generated by our natural language generation model. Analysis and Results. We discuss the results of the generative RNN model in this section. We give examples of the email", "distribution $P(w_{t+1} ~|~ w_{t^{\\prime }\\le t}) = softmax(\\widehat{w_{t}})$, here $softmax$ normalization with temperature control (Temp) is defined as: $P(softmax(\\widehat{w_{t}^{j}})) = \\frac{K(\\widehat{w_t^j}, Temp)}{\\sum _{j=1}^{n} K(\\widehat{w_t^j}, Temp)} $, where $K(\\widehat{w_t^j}, Temp) = e^{\\frac{\\widehat{w_{t}^{j}}}{Temp}}$ The novelty or eccentricity of the RNN text generative model can be evaluated by varying the Temperature parameter between $0 < Temp. \\le 1.0$ to generate samples of text (the maximum value is 1.0). We vary the nature of the model's predictions using two main mechanisms - deterministic and stochastic. Lower values of $Temp.$ generates relatively deterministic samples while higher values can make the process more stochastic. Both the mechanisms suffer from issues, deterministic samples can suffer from repetitive text while the samples generated using the stochastic mechanism are prone to spelling mistakes, grammatical errors, nonsensical words. We", "BIBREF24 to compute the training loss, $Adam$ optimization technique BIBREF25 is used for updating weights. The system was trained on an Amazon Web Services EC2 Deep Learning instance using an Nvidia Tesla K80 GPU. The training takes about 4 hours. Experimental Methodology ::: Experimental Setup ::: Text Generation and Sampling. The trained model is used to generate the email body based on the nature of the input. We varied the sampling technique of generating the new characters for the text generation. Generation phase. Feeding a word ($\\widehat{w_{0}}$) into the trained LSTM network model, will output the word most likely to occur after $\\widehat{w_{0}}$ as $\\widehat{w_{1}}$ depending on $P(\\widehat{w_{1}}~|~\\widehat{w_{0}})$. If we want to generate a text body of $n$ words, we feed $\\widehat{w_{1}}$ to the RNN model and get the next word by evaluating $P(\\widehat{w_{2}} ~|~\\widehat{w_{0}},\\widehat{w_{1}})$. This is done repeatedly to generate a text sequence with n words:"]}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "predicted_answer": "No", "predicted_evidence": ["text. Moreover, Example (A) has lesser magnitude of indication of malicious intent. And the amount of malicious intent in Example (B), although notable to the human eye, is enough to fool a simple text-based email classification algorithm. Example (C) has multiple link tags implying possible malicious intent or presence of poisonous links. However, the position of these links play an important role in deceiving the classifier. A majority of phishing emails have links at the end of the text body or after some action words like click, look, here, confirm etc. In this case, the links have been placed at arbitrary locations inside the text sequence - thereby making it harder to detect. These misclassification or errors on part of the classifier can be eliminated by human intervention or by designing a more sensitive and sophisticated detection algorithm. Conclusions and Future Work. While the RNN model generated text which had `some' malicious intent in them - the examples shown above are", "from issues, deterministic samples can suffer from repetitive text while the samples generated using the stochastic mechanism are prone to spelling mistakes, grammatical errors, nonsensical words. We generate our samples by varying the temperature values to 0.2, 0.5, 0.7 and 1.0. For our evaluation and detection experiments, we randomly select 25 system generated samples, 2 samples generated at a temperature of 0.2, 10 samples at temperature 0.5, 5 samples at a temperature of 0.7 and 8 samples at temperature 1.0. Experimental Methodology ::: Experimental Setup ::: Customization of Malicious Intent. One important aspect of malicious emails is their harmful intent. The perfect attack vector will have malicious elements like a poisonous link or malware attachment wrapped in legitimate context, something which is sly enough to fool both a state-of-the-art email classifier as well as the victim. One novelty of this system training is the procedure of injecting malicious intent during", "= 0.5: Hi will have temporarily information your account will be restricted during that the Internet accounts and upgrading password An data Thank you for your our security of your Account Please click on it using the $<$NET$>$ server This is an new offer miles with us as a qualified and move in The generated text reflects malicious features like URL links and tone of urgency. We can assume that the model picks up important cues of malign behavior. The model then learns to incorporate such cues into the sampled data during training phase. Analysis and Results ::: Evaluation using Detection Algorithm. We train text classification models using Support Vector Machines (SVM), Naive Bayes (NB) and Logistic Regression (LR) models on a training data of 300 legitimate emails from WikiLeaks archives and 150 phishing emails from Cornell PhishBowl BIBREF29. We test the data on 100 legitimate emails from WikiLeaks archives that were not included in the training set and 25 `fake' emails that were", "in Section SECREF19 Experimental Methodology ::: Data description. To best emulate a benign email, a text generator must learn the text representation in actual legitimate emails. Therefore, it is necessary to incorporate benign emails in training the model. However, as a successful attacker, our main aim is to create the perfect deceptive email - one which despite having malign components like poisoned links or attachments, looks legitimate enough to bypass statistical detectors and human supervision. Primarily, for the reasons stated above, we have used multiple email datasets, belonging to both legitimate and malicious classes, for training the system model and also in the quantitative evaluation and comparison steps. For our training model, we use a larger ratio of malicious emails compared to legitimate data (approximate ratio of benign to malicious is 1:4). Legitimate dataset. We use three sets of legitimate emails for modeling our legitimate content. The legitimate emails were", "something which is sly enough to fool both a state-of-the-art email classifier as well as the victim. One novelty of this system training is the procedure of injecting malicious intent during training and generating malicious content in the synthetic emails. We followed a percentage based influx of malicious content into the training model along with the legitimate emails. The training models were built by varying the percentage (5%, 10%, 30% and 50%) of phishing emails selected from the entire phishing dataset along with the entire legitimate emails dataset. We trained separate RNN models on all these configurations. For studying the varying content in emails, we generate samples using temperature values at 0.2, 0.5, 0.7 and 1.0. Experimental Methodology ::: Detection using Existing Algorithms. We perform a simple quantitative evaluation by using three text-based classification algorithms on our generated emails. Using the Python SciKit-Learn library, we test three popular text-based", "which are given to a supervised SVM for analysis of email nature. Newer techniques for phishing email detection based on textual content analysis have been proposed in BIBREF17, BIBREF0, BIBREF18, BIBREF19. Masquerade attacks are generated by the system proposed in BIBREF6, which tunes the generated emails based on legitimate content and style of a famous personality. Moreover, this technique can be exploited by phishers for launching email masquerade attacks, therefore making such a system extremely dangerous. Experimental Methodology. The section has been divided into four subsections. We describe the nature and source of the training and evaluation data in Section SECREF6 The pre-processing steps are demonstrated in Section SECREF14 The system setup and experimental settings have been described in Section SECREF19 Experimental Methodology ::: Data description. To best emulate a benign email, a text generator must learn the text representation in actual legitimate emails. Therefore,", "an email. A few observations from the datasets above: the malicious content is relatively more verbose than than the legitimate counterparts. Moreover, the size of the malicious data is comparatively higher compared to the legitimate content. Experimental Methodology ::: Data Filtering and Preprocessing. We considered some important steps for preprocessing the important textual content in the data. Below are the common preprocessing steps applied to the data: Removal of special characters like @, #, $, % as well as common punctuations from the email body. emails usually have other URLs or email IDs. These can pollute and confuse the learning model as to what are the more important words in the text. Therefore, we replaced the URLs and the email addresses with the $<$LINK$>$ and $<$EID$>$ tags respectively. Replacement of named entities with the $<$NET$>$ tag. We use Python NLTK NER for identification of the named entities. On close inspection of the training data, we found that the", "or outbox can study the nature/content of the emails sent or received by the target. He can then synthesize targeted malicious emails masqueraded as a benign email by incorporating features observed in the target's emails. The chances of such an attack being detected by an automated pre-trained classifier is reduced. The malicious email remain undetected, thereby increasing the chances of a successful attack. Current Natural Language Generation (NLG) techniques have allowed researchers to generate natural language text based on a given context. Highly sophisticated and trained NLG systems can involve text generation based on predefined grammar like the Dada Engine BIBREF6 or leverage deep learning neural networks like RNN BIBREF7 for generating text. Such an approach essentially facilitates the machine to learn a model that emulates the input to the system. The system can then be made to generate text that closely resembles the input structure and form. Such NLG systems can therefore"]}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "predicted_answer": "Yes", "predicted_evidence": ["$<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to bring the afternoon $<$NET$>$ I spoke to $<$NET$>$ Let me know if This kind of repetitive text generation was observed a number of times. However, we have not yet investigated the reasons for these repetitions. This could be an inherent problem of the LSTM model, or it could be because of the relatively small training dataset we have used. A third issue could be the temperature setting. More experiments are needed to determine the actual causes. The second aspect of error analysis is to look at the misclassification by the statistical detection algorithms.", "test dataset using SVM, Naive Bayes and Logistic Regression classifiers. Despite the incoherent nature of the generated emails, the text-based classifiers do not achieve a 100% accuracy as well as F1-scores. Analysis and Results ::: Comparison of emails with another NLG model. The authors in BIBREF6 discuss using a Recursive Transition Network for generating fake emails similar in nature to legitimate emails. The paper discusses a user study testing the efficacy of these fake emails and their effectiveness in being used for deceiving people. The authors use only legitimate emails to train their model and generate emails similar to their training data - termed as `fake' emails. In this section, we compare a couple of examples selected randomly from the emails generated by the Dada Engine used in BIBREF6 and the outputs of our Deep Learning system generated emails. Generated by the RNN (Example I): Hi will have temporarily information your account will be restricted during that the", "below: 197 Phishing emails collected by the second author - called Verma phish below. 3392 Phishing emails from Jose Nazario's Phishing corpus (Source 2) Evaluation dataset. We compared our system's output against a small set of automatically generated emails provided by the authors of BIBREF6. The provided set consists of 12 emails automatically generated using the Dada Engine and manually generated grammar rules. The set consists of 6 emails masquerading as Hillary Clinton emails and 6 emails masquerading as emails from Sarah Palin. Tables TABREF12 and TABREF13 describe some statistical details about the legitimate and malicious datasets used in this system. We define length ($L$) as the number of words in the body of an email. We define Vocabulary ($V$) as the number of unique words in an email. A few observations from the datasets above: the malicious content is relatively more verbose than than the legitimate counterparts. Moreover, the size of the malicious data is comparatively", "in Section SECREF19 Experimental Methodology ::: Data description. To best emulate a benign email, a text generator must learn the text representation in actual legitimate emails. Therefore, it is necessary to incorporate benign emails in training the model. However, as a successful attacker, our main aim is to create the perfect deceptive email - one which despite having malign components like poisoned links or attachments, looks legitimate enough to bypass statistical detectors and human supervision. Primarily, for the reasons stated above, we have used multiple email datasets, belonging to both legitimate and malicious classes, for training the system model and also in the quantitative evaluation and comparison steps. For our training model, we use a larger ratio of malicious emails compared to legitimate data (approximate ratio of benign to malicious is 1:4). Legitimate dataset. We use three sets of legitimate emails for modeling our legitimate content. The legitimate emails were", "and 150 phishing emails from Cornell PhishBowl BIBREF29. We test the data on 100 legitimate emails from WikiLeaks archives that were not included in the training set and 25 `fake' emails that were generated by our natural language generation model trained on a mix of legitimate and 50% malicious emails. We randomly select the emails (the distribution is: 2 samples generated at a temperature of 0.2, 10 samples at temperature 0.5, 5 samples at a temperature of 0.7 and 8 samples at temperature 1.0) for our evaluation. We use the Scikit-Learn Python library to generate the document-term matrix and the word count vector from a given sample of email text body used as a feature for training the classification models. The Table TABREF28 reports the accuracy, precision, recall, and F1-scores on the test dataset using SVM, Naive Bayes and Logistic Regression classifiers. Despite the incoherent nature of the generated emails, the text-based classifiers do not achieve a 100% accuracy as well as", "systems BIBREF12, BIBREF23 have generated coherent and readable content using word-level models. A comparison between Character-based and Word-based LSTMs in BIBREF12 proved that for a sample of generated text sequence, word level models have lower perplexity than character level deep learners. This is because the character-based text generators suffer from spelling errors and incoherent text fragments. Experimental Methodology ::: Experimental Setup ::: RNN architecture. Traditional language models like N-grams are limited by the history or the sequence of the textual content that these models are able to look back upon while training. However, RNNs are able to retain the long term information provided by some text sequence, making it work as a \u201cmemory\u201d-based model. However while building a model, RNNs are not the best performers when it comes to preserving long term dependencies. For this reason we use Long Short Term Memory architectures (LSTM) networks which are able to learn a", "We perform a simple quantitative evaluation by using three text-based classification algorithms on our generated emails. Using the Python SciKit-Learn library, we test three popular text-based filtering algorithms - Support Vector Machines BIBREF26, Naive Bayes BIBREF27 and Logistic Regression BIBREF28. The training set was modeled as a document-term matrix and the word count vector is used as the feature for building the models. For our evaluation, we train models using Support Vector Machines (SVM), Naive Bayes (NB) and Logistic Regression (LR) models on a training data of 300 legitimate emails from WikiLeaks archives and 150 phishing emails from Cornell PhishBowl BIBREF29. We test the data on 100 legitimate emails from WikiLeaks archives that were not included in the training set and 25 `fake' emails that were generated by our natural language generation model. Analysis and Results. We discuss the results of the generative RNN model in this section. We give examples of the email", "an email. A few observations from the datasets above: the malicious content is relatively more verbose than than the legitimate counterparts. Moreover, the size of the malicious data is comparatively higher compared to the legitimate content. Experimental Methodology ::: Data Filtering and Preprocessing. We considered some important steps for preprocessing the important textual content in the data. Below are the common preprocessing steps applied to the data: Removal of special characters like @, #, $, % as well as common punctuations from the email body. emails usually have other URLs or email IDs. These can pollute and confuse the learning model as to what are the more important words in the text. Therefore, we replaced the URLs and the email addresses with the $<$LINK$>$ and $<$EID$>$ tags respectively. Replacement of named entities with the $<$NET$>$ tag. We use Python NLTK NER for identification of the named entities. On close inspection of the training data, we found that the"]}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "predicted_answer": "The CNN model produced the highest macro average F1-score of 0.65, while the BiLSTM (BOW) model produced comparable results and even better regarding the classes Personal information and Biographical sketch, making it a valid baseline for the task.", "predicted_evidence": ["of the BiLSTM (W2V) which uses a conditional random field layer for the output. Experimental Setup. We split our 1008 obituaries into training set (70 %) and test set (30 %). From the training set, 10 % are used for validation. The batch size is set to 8 and the optimizer to rmsprop for all experiments. We do not perform hyperparameter tuning. Experimental Setup ::: Results. The CNN model has the highest macro average $\\textrm {F}_1$ score with a value of 0.65. This results from the high values for the classes Family and Funeral information. The $\\textrm {F}_1$ score for the class Other is 0.52 in contrast with the $\\textrm {F}_1$ of the other three models, which is lower than 0.22. The macro average $\\textrm {F}_1$ for the BiLSTM (BOW) model is 0.58. It also has highest F1-scores for the classes Personal Information and Biographical Sketch among all models. For the classes Family, and Funeral information has comparable scores to the CNN model. Interestingly this model performs the", "classes Personal Information and Biographical Sketch among all models. For the classes Family, and Funeral information has comparable scores to the CNN model. Interestingly this model performs the best among the BiLSTM variants. The BiLSTM (W2V) model performs overall worse than the one which makes use only of a BOW. It also has the worst macro average $\\textrm {F}_1$ together with the BiLSTM-CRF with a value of 0.50. The BiLSTM-CRF performs better than the other BiLSTM variants on the rare classes Gratitude and Other. Since we have few samples labelled as Tribute none of our models predict a sentence as such, resulting in precision, recall, and $\\textrm {F}_1$ value of 0 for each model. From the results we conclude that the CNN model works best. Apart from the high $\\textrm {F}_1$ it is also the only model that predicts the class Gratitude as well as the class Other better than the other models. Experimental Setup ::: Error Analysis. We investigate the best performing model by making", "the only model that predicts the class Gratitude as well as the class Other better than the other models. Experimental Setup ::: Error Analysis. We investigate the best performing model by making use of the confusion matrix (see Figure FIGREF20) and by inspecting all errors made by the model on the test set (see Table TABREF21). In Figure FIGREF20, we observe that the diagonal has relatively high numbers with more correctly labeled instances than confused ones for all classes, with the exception of class Tribute (the rarest class). Secondly, the confusions are not globally symmetric. However, we observe that the lower left corner formed by the classes Family, Characteristics and Biographical Sketch is almost symmetric in its confusions, which led us to inspect and classify the types of errors. Therefore, we investigated all errors manually and classified them in three main types of errors: errors due to Ambiguity (39%), errors due to wrong Annotation (18%) and errors tagged as Other", "embeddings, and a BiLSTM-CRF. The models are then compared based on precision, recall, and F1-score. From our results, we conclude that the CNN text classifier produced the best results with a macro F1-score of 0.81, considering the experimental settings, and the highest macro average F1-score of 0.65. The BiLSTM (BOW) model produced comparable results and even better regarding the classes Personal information and Biographical sketch, which makes it also a valid baseline for the task. Our work enables future research, showing that automatic recognition of structures in obituaries is a viable task. Through performing zoning on the raw obituaries, it is becoming possible to address other research questions: whether there is a correlation between the occupation of the deceased and the cause of death, what are the cultural and structural differences between obituaries from different countries. Another open question is if the annotation scheme is the best. Given the errors we found, we", "and max pooling followed by the output layer with softmax as activation function and with cross entropy as loss. This model does not have access to information of neighboring sentences. Methods ::: BiLSTM. The BiLSTM models are structurally different from the CNN. The CNN predicts on the sentence-level without having access to neighboring information. For the BiLSTM models we opt for a token-based IOB scheme in which we map the dominantly predicted class inside of one sentence to the whole sentence. Our BiLSTM (BOW) model BIBREF25, BIBREF26 uses 100 memory units, a softmax activation function and categorical cross entropy as the loss function. The BiLSTM (W2V) model uses pre-trained word embeddings (Word2Vec on Google News) BIBREF27 instead of the bag of words. The BiLSTM-CRF is an extension of the BiLSTM (W2V) which uses a conditional random field layer for the output. Experimental Setup. We split our 1008 obituaries into training set (70 %) and test set (30 %). From the training", "do so they implement and evaluate the performance of different linkage methods. The electronic health records are from patients in Rennes, France and the extracted obituaries are all available online obituaries from French funeral home websites. They evaluate three different linkage methods and obtain almost perfect precisions with all methods. They conclude that using obituaries published online could address the problem of long delays in the sharing of mortality data whereas online obituaries could be considered as reliable data source for real-time suveillance of mortality in patients with cancer. Related Work ::: Obituaries as a Data Source in Various Tasks of Computational Linguistics. With a focus on computational linguistics, obituarymining1 analyze text data from obituary websites, with the intention to use it to prevent identity theft. The goal was to evaluate how \u201coften and how accurately name and address fragments extracted from these notices developed into complete name", "from the first study in terms of network density, mean clustering coefficient and modularity. The last study is done on data from ObituaryData.com and the annotation with traits is performed in a semi-automatic manner. obi1 extract various facts about persons from obituaries. They use a feature scoring method that uses prior knowledge. Their method achieved high performance for the attributes person name, affiliation, position (occupation), age, gender, and cause of death. bamman2014 present an unsupervised model for learning life event classes from biographical texts in Wikipedia along with the structure that connects them. They discover evidence of systematic bias in the presentation of male and female biographies in which female biographies placed a significantly disproportionate emphasis on the personal events of marriage and divorce. This work is of interest here because it handled biographical information (Wikipedia biographies), of which obituaries are also a part. simonson2016", "of the annotation showed that this is mostly because it seems to be more common than in other locations to mention that the person will be remembered. Methods. To answer the question whether or not we can recognize the structure in obituaries we formulate the task as sentence classification, where each sentence will be assigned to one of the eight classes we defined previously. We evaluate four different models. Methods ::: CNN. Convolutional Neural Networks (CNN) BIBREF23, BIBREF24 have been succesfully applied to practical NLP problems in the recent years. We use the sequential model in Keras where each sentence is represented as a sequence of one-hot embeddings of its words. We use three consecutive pairs of convolutional layers with 128 output channels, the ReLu activation function and max pooling followed by the output layer with softmax as activation function and with cross entropy as loss. This model does not have access to information of neighboring sentences. Methods :::"]}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "predicted_answer": "the corruption of the word order by random permutations", "predicted_evidence": ["\\text{Uniform}(0,1)$ . If $p_i < p_\\text{del}$ , drop the word in the position $i$ . Also, translations generated word-by-word are not in an order of the target language. In our beam search, LM only assists in choosing the right word in context but does not modify the word order. A common reordering problem of German $\\rightarrow $ English is illustrated in Figure 3 . From a clean target sentence, we corrupt its word order by random permutations. We limit the maximum distance between an original position and its new position like unmt-facebook: For each position $i$ , sample an integer $\\delta _i$ from $[0,d_\\text{per}]$ . Add $\\delta _i$ to index $i$ and sort the incremented indices $i + \\delta _i$ in an increasing order. Rearrange the words to be in the new positions, to which their original indices have moved by Step 2. This is a generalized version of swapping two neighboring words BIBREF11 . Reordering is highly dependent of each language, but we found that this noise is", "original indices have moved by Step 2. This is a generalized version of swapping two neighboring words BIBREF11 . Reordering is highly dependent of each language, but we found that this noise is generally close to word-by-word translation outputs. Insertion, deletion, and reordering noises were applied to each mini-batch with different random seeds, allowing the model to see various noisy versions of the same clean sentence over the epochs. Note that the deletion and permutation noises are integrated in the neural MT training of unmt-artetxe and unmt-facebook as additional training objectives. Whereas we optimize an independent model solely for denoising without architecture change. It allows us to easily train a larger network with a larger data. Insertion noise is of our original design, which we found to be the most effective (Section \"Ablation Study: Denoising\" ). Experiments. We applied the proposed methods on WMT 2016 German $\\leftrightarrow $ English task and WMT 2014 French", "in denoising autoencoder, we tuned each parameter of the noise and combined them incrementally (Table 2 ). Firstly, for permutations, a significant improvement is achieved from $d_\\text{per} = 3$ , since a local reordering usually involves a sequence of 3 to 4 words. With $d_\\text{per} > 5$ , it shuffles too many consecutive words together, yielding no further improvement. This noise cannot handle long-range reordering, which is usually a swap of words that are far from each other, keeping the words in the middle as they are. Secondly, we applied the deletion noise with different values of $p_\\text{del}$ . 0.1 gives +0.8% Bleu, but we immediately see a degradation with a larger value; it is hard to observe one-to-many translations more than once in each sentence pair. Finally, we optimized $V_\\text{ins}$ for the insertion noise, fixing $p_\\text{ins} = 0.1$ . Increasing $V_\\text{ins}$ is generally not beneficial, since it provides too much variations in the inserted word; it might not", "but they often suffer from a huge latent hypothesis space BIBREF4 . Recent work by unmt-artetxe and unmt-facebook train sequence-to-sequence MT models of both translation directions together in an unsupervised way. They do back-translation BIBREF5 back and forth for every iteration or batch, which needs an immensely long time and careful tuning of hyperparameters for massive monolingual data. Here we suggest rather simple methods to build an unsupervised MT system quickly, based on word translation using cross-lingual word embeddings. The contributions of this paper are: The proposed models can be efficiently trained with off-the-shelf softwares with little or no changes in the implementation, using only monolingual data. The provided analyses help for better learning of cross-lingual word embeddings for translation purpose. Altogether, our unsupervised MT system outperforms the sequence-to-sequence neural models even without training signals from the opposite translation direction,", "Introduction. Building a machine translation (MT) system requires lots of bilingual data. Neural MT models BIBREF0 , which become the current standard, are even more difficult to train without huge bilingual supervision BIBREF1 . However, bilingual resources are still limited to some of the selected language pairs\u2014mostly from or to English. A workaround for zero-resource language pairs is translating via an intermediate (pivot) language. To do so, we need to collect parallel data and train MT models for source-to-pivot and pivot-to-target individually; it takes a double effort and the decoding is twice as slow. Unsupervised learning is another alternative, where we can train an MT system with only monolingual corpora. Decipherment methods BIBREF2 , BIBREF3 are the first work in this direction, but they often suffer from a huge latent hypothesis space BIBREF4 . Recent work by unmt-artetxe and unmt-facebook train sequence-to-sequence MT models of both translation directions together in", "$   where $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ . It is transformed to the range $[0,1]$ to make it similar in scale with the LM probability. In our experiments, we found that this simple linear scaling is better than sigmoid or softmax functions in the final translation performance. Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses. Denoising. Even when we have correctly translated words for each position, the output is still far from an acceptable translation. We adopt sequence denoising autoencoder BIBREF11 to improve the translation output of Section \"Context-aware Beam Search\" . The main idea is to train a sequence-to-sequence neural network model that takes a noisy sentence as input and produces a (denoised) clean sentence as output, both of which are of the same (target) language. The model was originally proposed to learn sentence embeddings, but here we use it directly to actually remove", "a (denoised) clean sentence as output, both of which are of the same (target) language. The model was originally proposed to learn sentence embeddings, but here we use it directly to actually remove noise in a sentence. Training label sequences for the denoising network would be target monolingual sentences, but we do not have their noisy versions at hand. Given a clean target sentence, the noisy input should be ideally word-by-word translation of the corresponding source sentence. However, such bilingual sentence alignment is not available in our unsupervised setup. Instead, we inject artificial noise into a clean sentence to simulate the noise of word-by-word translation. We design different noise types after the following aspects of word-by-word translation. Word-by-word translation always outputs a target word for every position. However, there are a plenty of cases that multiple source words should be translated to a single target word, or that some source words are rather not", "$\\lambda _\\text{LM} = 0.1$ with beam size 10. We only translated top frequent 50k source words and merely copied other words to target side. For each position, only the nearest 100 target words were considered. Table 1 shows the results. LM improves word-by-word baselines consistently in all four tasks, giving at least +3% Bleu. When our denoising model is applied on top of it, we have additional gain around +3% Bleu. Note that our methods do not involve any decoding steps to generate pseudo-parallel training data, but still perform better than unsupervised MT systems that rely on repetitive back-translations BIBREF13 , BIBREF12 by up to +3.9% Bleu. The total training time of our method is only 1-2 days with a single GPU. Ablation Study: Denoising. To examine the effect of each noise type in denoising autoencoder, we tuned each parameter of the noise and combined them incrementally (Table 2 ). Firstly, for permutations, a significant improvement is achieved from $d_\\text{per} = 3$ ,"]}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "predicted_answer": "combining a language model (LM) with cross-lingual word embedding", "predicted_evidence": ["word but other close words with morphological variations or synonyms, depending on the context. The reasons are in two-fold: 1) Word embedding is trained to place semantically related words nearby, even though they have opposite meanings. 2) A hubness problem of high-dimensional embedding space hinders a correct search, where lots of different words happen to be close to each other BIBREF10 . In this paper, we integrate context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding. Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:  $", "$   where $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ . It is transformed to the range $[0,1]$ to make it similar in scale with the LM probability. In our experiments, we found that this simple linear scaling is better than sigmoid or softmax functions in the final translation performance. Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses. Denoising. Even when we have correctly translated words for each position, the output is still far from an acceptable translation. We adopt sequence denoising autoencoder BIBREF11 to improve the translation output of Section \"Context-aware Beam Search\" . The main idea is to train a sequence-to-sequence neural network model that takes a noisy sentence as input and produces a (denoised) clean sentence as output, both of which are of the same (target) language. The model was originally proposed to learn sentence embeddings, but here we use it directly to actually remove", "ambiguous. For word level embeddings, we compared different vocabulary sizes used for training the cross-lingual mapping (the second step in Section \"Cross-lingual Word Embedding\" ). Surprisingly, cross-lingual word embedding learned only on top 20k words is comparable to that of 200k words in the translation quality. We also increased the search vocabulary to more than 200k but the performance only degrades. This means that word-by-word translation with cross-lingual embedding depends highly on the frequent word mappings, and learning the mapping between rare words does not have a positive effect. Conclusion. In this paper, we proposed a simple pipeline to greatly improve sentence translation based on cross-lingual word embedding. We achieved context-aware lexical choices using beam search with LM, and solved insertion/deletion/reordering problems using denoising autoencoder. Our novel insertion noise shows a promising performance even combined with other noise types. Our methods do", "cross-lingual word embedding in a fully unsupervised manner: Once we have the cross-lingual mapping, we can transform the embedding of a given source word and find a target word with the closest embedding, i.e. nearest neighbor search. Here, we apply cross-domain similarity local scaling BIBREF7 to penalize the word similarities in dense areas of the embedding distribution. We further refine the mapping obtained from Step 2 as follows BIBREF6 : Sentence Translation. In translating sentences, cross-lingual word embedding has several drawbacks. We describe each of them and our corresponding solutions. Context-aware Beam Search. The word translation using nearest neighbor search does not consider context around the current word. In many cases, the correct translation is not the nearest target word but other close words with morphological variations or synonyms, depending on the context. The reasons are in two-fold: 1) Word embedding is trained to place semantically related words nearby,", "which we found to be the most effective (Section \"Ablation Study: Denoising\" ). Experiments. We applied the proposed methods on WMT 2016 German $\\leftrightarrow $ English task and WMT 2014 French $\\leftrightarrow $ English task. For German/English, we trained word embeddings with 100M sentences sampled from News Crawl 2014-2017 monolingual corpora. For French, we used News Crawl 2007-2014 (around 42M sentences). The data was lowercased and filtered to have a maximum sentence length 100. German compound words were splitted beforehand. Numbers were replaced with category labels and recovered back after decoding by looking at the source sentence. fasttext BIBREF8 was used to learn monolingual embeddings for only the words with minimum count 10. MUSE BIBREF7 was used for cross-lingual mappings with $V_\\text{cross-train}$ = 100k and 10 refinement iterations (Step 3-5 in Section \"Cross-lingual Word Embedding\" ). Other parameters follow the values in cross-facebook. With the same data, we", "\\text{Uniform}(0,1)$ . If $p_i < p_\\text{del}$ , drop the word in the position $i$ . Also, translations generated word-by-word are not in an order of the target language. In our beam search, LM only assists in choosing the right word in context but does not modify the word order. A common reordering problem of German $\\rightarrow $ English is illustrated in Figure 3 . From a clean target sentence, we corrupt its word order by random permutations. We limit the maximum distance between an original position and its new position like unmt-facebook: For each position $i$ , sample an integer $\\delta _i$ from $[0,d_\\text{per}]$ . Add $\\delta _i$ to index $i$ and sort the incremented indices $i + \\delta _i$ in an increasing order. Rearrange the words to be in the new positions, to which their original indices have moved by Step 2. This is a generalized version of swapping two neighboring words BIBREF11 . Reordering is highly dependent of each language, but we found that this noise is", "embeddings for translation purpose. Altogether, our unsupervised MT system outperforms the sequence-to-sequence neural models even without training signals from the opposite translation direction, i.e. via back-translation. Cross-lingual Word Embedding. As a basic step for unsupervised MT, we learn a word translation model from monolingual corpora of each language. In this work, we exploit cross-lingual word embedding for word-by-word translation, which is state-of-the-art in terms of type translation quality BIBREF6 , BIBREF7 . Cross-lingual word embedding is a continuous representation of words whose vector space is shared across multiple languages. This enables distance calculation between word embeddings across languages, which is actually finding translation candidates. We train cross-lingual word embedding in a fully unsupervised manner: Once we have the cross-lingual mapping, we can transform the embedding of a given source word and find a target word with the closest", "with $V_\\text{cross-train}$ = 100k and 10 refinement iterations (Step 3-5 in Section \"Cross-lingual Word Embedding\" ). Other parameters follow the values in cross-facebook. With the same data, we trained 5-gram count-based LMs using KenLM BIBREF14 with its default setting. Denoising autoencoders were trained using Sockeye BIBREF15 on News Crawl 2016 for German/English and News Crawl 2014 for French. We considered only top 50k frequent words for each language and mapped other words to <unk>. The unknowns in the denoised output were replaced with missing words from the noisy input by a simple line search. We used 6-layer Transformer encoder/decoder BIBREF16 for denoisers, with embedding/hidden layer size 512, feedforward sublayer size 2048 and 8 attention heads. As a validation set for the denoiser training, we used newstest2015 (German $\\leftrightarrow $ English) or newstest2013 (French $\\leftrightarrow $ English), where the input/output sides both have the same clean target sentences,"]}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_answer": "PDTB 2.0 corpus.", "predicted_evidence": ["enough to be annotated with a high inter-annotator agreement and are common to most theories of discourse, in our experiments we only use this level of annotations. We formulated the task as four separate one-against-all binary classification problems: each top level class vs. the other three discourse relation classes. We also balanced the training set by resampling training instances in each class until the number of positive and negative instances are equal. In contrast, all instances in the test and development set are kept in nature. The statistics of various data sets is listed in Table 1 . Setup. We tokenized all datasets using Stanford NLP Toolkit. For optimization, we employed the Adam algorithm BIBREF15 to update parameters. With respect to the hyperparameters $M,L,A$ and the dimensionality of all vector representations, we set them according to previous work BIBREF10 , BIBREF11 and preliminary experiments on the development set. Finally, we set", "p_\\theta (\\mathbf {x}, \\mathbf {y}, \\mathbf {z}) = p_\\theta (\\mathbf {x}|\\mathbf {z})p_\\theta (\\mathbf {y}|\\mathbf {z})p(\\mathbf {z})$$   (Eq. 10)  We use a neural model $q_\\phi ^{\\prime }(\\mathbf {z}|\\mathbf {x})$ to approximate the prior $p(\\mathbf {z})$ conditioned on the discourse $\\mathbf {x}$ (see the following section). With respect to the other two conditional distributions, we parameterize them via neural networks as shown in Figure 2 . Before we describe these neural networks, it is necessary to briefly introduce how discourse relations are annotated in our training data. The PDTB corpus, used as our training data, annotates implicit discourse relations between two neighboring arguments, namely Arg1 and Arg2. In VarNDRR, we represent the two arguments with bag-of-word representations, and denote them as $\\mathbf {x_1}$ and $\\mathbf {x_2}$ . To model $p_\\theta (\\mathbf {x}|\\mathbf {z})$ (the bottom part in Figure 2 ), we project the representation of the latent variable $z\\in", "$\\theta $ and variational parameters $\\phi $ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm \"Parameter Learning\" . [t] Parameter Learning Algorithm of VarNDRR. Inputs: $A$ , the maximum number of iterations; $M$ , the number of instances in one batch; $L$ , the number of samples; $\\theta ,\\phi $ $\\leftarrow $ Initialize parameters $\\mathcal {D}$ $\\leftarrow $ getRandomMiniBatch(M) $\\epsilon $ $\\leftarrow $ getRandomNoiseFromStandardGaussian() $g$ $M$0 $M$1 $M$2 $M$3 parameterUpdater( $M$4 ) convergence of parameters $M$5 or reach the maximum iteration $M$6  Experiments. We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR. Dataset. We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 ,", "PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization. In PDTB, discourse relations are annotated in a predicate-argument view. Each discourse connective is treated as a predicate that takes two text spans as its arguments. The discourse relation tags in PDTB are arranged in a three-level hierarchy, where the top level consists of four major semantic classes: Temporal (Tem), Contingency (Con), Expansion (Exp) and Comparison (Com). Because the top-level relations are general enough to be annotated with a high inter-annotator agreement and are common to most theories of discourse, in our experiments we only use this level of annotations. We formulated the task as four", "}\\odot \\mathbf {\\epsilon }$$   (Eq. 7)  where $\\mathbf {\\epsilon }$ is a standard Gaussian variable, and $\\odot $ denotes an element-wise product. Intuitively, VAE learns the representation of the latent variable not as single points, but as soft ellipsoidal regions in latent space, forcing the representation to fill the space rather than memorizing the training data as isolated representations. With this trick, the VAE model can be trained through standard backpropagation technique with stochastic gradient ascent. The VarNDRR Model. This section introduces our proposed VarNDRR model. Formally, in VarNDRR, there are two observed variables, $\\mathbf {x}$ for a discourse and $\\mathbf {y}$ for the corresponding relation, and one latent variable $\\mathbf {z}$ . As illustrated in Figure 1 , the joint distribution of the three variables is formulated as follows:  $$", "$M,L,A$ and the dimensionality of all vector representations, we set them according to previous work BIBREF10 , BIBREF11 and preliminary experiments on the development set. Finally, we set $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$ for all experiments.. All parameters of VarNDRR are initialized by a Gaussian distribution ( $\\mu =0, \\sigma =0.01$ ). For Adam, we set $\\beta _1=0.9$ , $\\beta _2=0.999$ with a learning rate $0.001$ . Additionally, we tied the following parameters in practice: $W_{h_1}$ and $W_{h_2}$ , $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$0 and $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$1 . We compared VarNDRR against the following two different baseline methods: SVM: a support vector machine (SVM) classifier trained with several manual features. SCNN:", "use $\\mu ^{\\prime }$ and $\\sigma ^{\\prime }$ to denote the mean and s.d. of the approximate prior. With the parameters of Gaussian distribution, we can access the representation $z$ using different sampling strategies. However, traditional sampling approaches often breaks off the connection between recognizer and approximator, making the optimization difficult. Instead, we employ the reparameterization trick BIBREF10 , BIBREF11 as in Eq. ( 7 ). During training, we sample the latent variable using $\\tilde{z}=\\mu + \\sigma \\odot \\epsilon $ ; during testing, however, we employ the expectation of $\\mathbf {z}$ in the approximate prior distribution, i.e. set $\\tilde{z}=\\mu ^{\\prime }$ to avoid uncertainty. Parameter Learning. We employ the Monte Carlo method to estimate the expectation over the approximate posterior, that is $\\mathbb {E}_{q_\\phi (\\mathbf {z}|\\mathbf {x}, \\mathbf {y})}[\\log p_\\theta (\\mathbf {x},\\mathbf {y}|\\mathbf {z})]$ . Given a training instance $(x^{(t)}, y^{(t)})$ ,", "embeddings BIBREF19 , Brown cluster pairs and co-reference patterns BIBREF16 . With these features, Park and Cardie park2012implicit perform feature set optimization for better feature combination. Different from feature engineering, predicting discourse connectives can indirectly help the relation classification BIBREF13 , BIBREF20 . In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement BIBREF21 , BIBREF14 , BIBREF22 , BIBREF23 , BIBREF17 . Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning BIBREF8 , BIBREF9 . Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al."]}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "predicted_answer": "ROUGE score", "predicted_evidence": ["contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets. Dataset and Metrics. The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. Comparative Methods. To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the", "three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0  This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0  where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences. We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by:", "Our neural network based framework is implemented using Theano BIBREF21 on a single GPU. Results on Our Dataset. The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. Further Investigation of Our Framework . To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than", "it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset. Overview. As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a", "model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ). Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \u201cSony Virtual Reality PS4\u201d, and \u201c`Bitcoin Mt. Gox Offlile\u201d' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \u201cSony Virtual Reality PS4\u201d, many readers talked about the product of \u201cOculus\u201d, hence the word \u201coculus\u201d is assigned a high salience by our model. Case Study. Based on the news and comments of the topic \u201cSony Virtual", "VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments. Reader-Aware Salience Estimation. Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2", "we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary. There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS. Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that", "documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0  The calculation of INLINEFORM0 will be discussed later. The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0  VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the"]}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "predicted_answer": "collected from different websites and sources", "predicted_evidence": ["Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \u201cAccidents and Natural Disasters\u201d, the aspects are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take", "contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets. Dataset and Metrics. The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. Comparative Methods. To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the", "we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary. There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS. Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that", "it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset. Overview. As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a", "the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve. Data Description. In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics. Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources.", "are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \u201cMalaysia Airlines Disappearance\u201d as an example, facets for the aspect \u201cWHAT\u201d include \u201cmissing Malaysia Airlines Flight 370\u201d, \u201ctwo passengers used passports stolen in Thailand from an Austrian and an Italian.\u201d etc. Facets for the aspect \u201cWHEN\u201d are \u201c Saturday morning\u201d, \u201cabout an hour into its flight from Kuala Lumpur\u201d, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. Data Collection. The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event", "100 words. After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files. Data Properties. The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of", "ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 . For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format. Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts"]}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "predicted_answer": "Not enough information provided.", "predicted_evidence": ["Our neural network based framework is implemented using Theano BIBREF21 on a single GPU. Results on Our Dataset. The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. Further Investigation of Our Framework . To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than", "contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets. Dataset and Metrics. The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. Comparative Methods. To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the", "three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0  This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0  where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences. We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by:", "it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset. Overview. As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a", "the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve. Data Description. In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics. Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources.", "The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0  Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.  INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0  This objective is", "concept weight. We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments. Experimental Settings. The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU. Results on Our Dataset. The results of our framework as well as the baseline methods are depicted in Table", "that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\u201d. Conclusions. We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset."]}
{"question_id": "86d1c990c1639490c239c3dbf5492ecc44ab6652", "predicted_answer": "4", "predicted_evidence": ["ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 . For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format. Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts", "talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries. One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in", "are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \u201cMalaysia Airlines Disappearance\u201d as an example, facets for the aspect \u201cWHAT\u201d include \u201cmissing Malaysia Airlines Flight 370\u201d, \u201ctwo passengers used passports stolen in Thailand from an Austrian and an Italian.\u201d etc. Facets for the aspect \u201cWHEN\u201d are \u201c Saturday morning\u201d, \u201cabout an hour into its flight from Kuala Lumpur\u201d, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. Data Collection. The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event", "Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \u201cAccidents and Natural Disasters\u201d, the aspects are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take", "100 words. After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files. Data Properties. The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of", "VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments. Reader-Aware Salience Estimation. Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2", "to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words. After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality,", "model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ). Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \u201cSony Virtual Reality PS4\u201d, and \u201c`Bitcoin Mt. Gox Offlile\u201d' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \u201cSony Virtual Reality PS4\u201d, many readers talked about the product of \u201cOculus\u201d, hence the word \u201coculus\u201d is assigned a high salience by our model. Case Study. Based on the news and comments of the topic \u201cSony Virtual"]}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "predicted_answer": "No information provided. Unanswerable.", "predicted_evidence": ["100 words. After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files. Data Properties. The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of", "contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets. Dataset and Metrics. The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. Comparative Methods. To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the", "concept weight. We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments. Experimental Settings. The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU. Results on Our Dataset. The results of our framework as well as the baseline methods are depicted in Table", "we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary. There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS. Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that", "Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \u201cAccidents and Natural Disasters\u201d, the aspects are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take", "are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \u201cMalaysia Airlines Disappearance\u201d as an example, facets for the aspect \u201cWHAT\u201d include \u201cmissing Malaysia Airlines Flight 370\u201d, \u201ctwo passengers used passports stolen in Thailand from an Austrian and an Italian.\u201d etc. Facets for the aspect \u201cWHEN\u201d are \u201c Saturday morning\u201d, \u201cabout an hour into its flight from Kuala Lumpur\u201d, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. Data Collection. The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event", "documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0  The calculation of INLINEFORM0 will be discussed later. The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0  VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the", "the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments. Recently, BIBREF6 proposed a sentence salience estimation framework"]}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "predicted_answer": "Unanswerable", "predicted_evidence": ["model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ). Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \u201cSony Virtual Reality PS4\u201d, and \u201c`Bitcoin Mt. Gox Offlile\u201d' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \u201cSony Virtual Reality PS4\u201d, many readers talked about the product of \u201cOculus\u201d, hence the word \u201coculus\u201d is assigned a high salience by our model. Case Study. Based on the news and comments of the topic \u201cSony Virtual", "the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments. Recently, BIBREF6 proposed a sentence salience estimation framework", "Our neural network based framework is implemented using Theano BIBREF21 on a single GPU. Results on Our Dataset. The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. Further Investigation of Our Framework . To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than", "Introduction. The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents. With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report \u201cThe most important announcements from Google's big developers' conference\u201d. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries", "contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets. Dataset and Metrics. The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. Comparative Methods. To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the", "that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\u201d. Conclusions. We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.", "are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \u201cMalaysia Airlines Disappearance\u201d as an example, facets for the aspect \u201cWHAT\u201d include \u201cmissing Malaysia Airlines Flight 370\u201d, \u201ctwo passengers used passports stolen in Thailand from an Austrian and an Italian.\u201d etc. Facets for the aspect \u201cWHEN\u201d are \u201c Saturday morning\u201d, \u201cabout an hour into its flight from Kuala Lumpur\u201d, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. Data Collection. The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event", "ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 . For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format. Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts"]}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "predicted_answer": "Unanswerable", "predicted_evidence": ["vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0  VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0  The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state", "concept weight. We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments. Experimental Settings. The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU. Results on Our Dataset. The results of our framework as well as the baseline methods are depicted in Table", "documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0  The calculation of INLINEFORM0 will be discussed later. The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0  VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the", "The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0  Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.  INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0  This objective is", "are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \u201cMalaysia Airlines Disappearance\u201d as an example, facets for the aspect \u201cWHAT\u201d include \u201cmissing Malaysia Airlines Flight 370\u201d, \u201ctwo passengers used passports stolen in Thailand from an Austrian and an Italian.\u201d etc. Facets for the aspect \u201cWHEN\u201d are \u201c Saturday morning\u201d, \u201cabout an hour into its flight from Kuala Lumpur\u201d, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. Data Collection. The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event", "VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments. Reader-Aware Salience Estimation. Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2", "the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve. Data Description. In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics. Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources.", "only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments. Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence"]}
{"question_id": "b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "predicted_answer": "The dataset contains topics, documents, model summaries, aspects, and aspect facets annotated by experts.", "predicted_evidence": ["Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \u201cAccidents and Natural Disasters\u201d, the aspects are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take", "contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets. Dataset and Metrics. The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported. Comparative Methods. To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments. Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the", "we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary. There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS. Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that", "100 words. After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files. Data Properties. The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of", "are \u201cWHAT\u201d, \u201cWHEN\u201d, \u201cWHERE\u201d, \u201cWHY\u201d, \u201cWHO_AFFECTED\u201d, \u201cDAMAGES\u201d, and \u201cCOUNTERMEASURES\u201d. Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \u201cMalaysia Airlines Disappearance\u201d as an example, facets for the aspect \u201cWHAT\u201d include \u201cmissing Malaysia Airlines Flight 370\u201d, \u201ctwo passengers used passports stolen in Thailand from an Austrian and an Italian.\u201d etc. Facets for the aspect \u201cWHEN\u201d are \u201c Saturday morning\u201d, \u201cabout an hour into its flight from Kuala Lumpur\u201d, etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document. Data Collection. The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event", "the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve. Data Description. In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics. Background. The definition of the terminology related to the dataset is given as follows. Topic: A topic refers to an event and it is composed of a set of news documents from different sources.", "it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset. Overview. As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a", "the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments. Recently, BIBREF6 proposed a sentence salience estimation framework"]}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "predicted_answer": "Yes", "predicted_evidence": ["with language. We trained the LGI to associate object name with corresponding instances by \u2018this is \u2026\u2019 syntax; and trained the LGI to produce a digit instance, when there comes the sentence \u2018give me a [number]\u2019. In contrast, traditional language models could only serve as a word dependency predictor rather than really understand the sentence. Language is the most remarkable characteristics distinguishing mankind from animals. Theoretically, all kinds of information such as object properties, tasks and goals, commands and even emotions can be described and conveyed by language [21]. We trained with LGI eight different syntaxes (in other word, eight different tasks), and LGI demonstrates its understanding by correctly interacting with the vision system. After learning \u2018this is 9\u2019, it is much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units,", "based on which the imagination network could reconstruct the predicted image. Finally, the predicted or imagined image is fed back to the encoder network for the next thinking iteration. The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from", "processing (NLP) techniques can handle question answering etc. tasks, such as answering that \u2018Cao Cao\u2019s nickname is Meng De\u2019 based on the website knowledge [1]. However, the NLP network is just a probability model [2] and does not know whether Cao Cao is a man or cat. Indeed, it even does not understand what is a man. On the other hand, human being learns Cao Cao with his nickname via watching TV. When presented the question \u2018what\u2019s Cao Cao\u2019s nickname?\u2019, we can give the correct answer of \u2018Meng De\u2019 while imagining the figure of an actor in the brain. In this way, we say the machine network does not understand it, but the human does. Human beings possess such thinking capacity due to its cumulative learning capacity accompanying the neural developmental process. Initially, parent points to a real apple and teaches the baby \u2018this is an apple\u2019. After gradually assimilating the basic meanings of numerous nouns, children begin to learn some phrases and finally complicated syntaxes. Unlike", "Introduction. Human thinking is regarded as \u2018mental ideas flow guided by language to achieve a goal\u2019. For instance, after seeing heavy rain, you may say internally \u2018holding an umbrella could avoid getting wet\u2019, and then you will take an umbrella before leaving. In the process, we know that the visual input of \u2018water drop\u2019 is called rain, and can imagine \u2018holding an umbrella\u2019 could keep off the rain, and can even experience the feeling of being wet. This continual thinking capacity distinguishes us from the machine, even though the latter can also recognize images, process language, and sense rain-drops. Continual thinking requires the capacity to generate mental imagination guided by language, and extract language representations from a real or imagined scenario. Modern natural language processing (NLP) techniques can handle question answering etc. tasks, such as answering that \u2018Cao Cao\u2019s nickname is Meng De\u2019 based on the website knowledge [1]. However, the NLP network is just a", "which is more biologically plausible than the softmax operation. After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of \u2018the size is big/small\u2019 (train steps =1000), the negative adverb \u2018not\u2019 in the language text \u2018the size is not small/big\u2019 was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being. And then, LGI rapidly learned three more syntaxes: \u2018give me a \u2026\u2019, \u2018enlarge/shrink\u2019, and \u2018rotate \u2026\u2019, whose results are shown in Figure 6. After training (5000 steps), LGI could generate a correct digit figure given the language command \u2018give me a [number]\u2019 (Figure 6.A). The generated digit instance is somewhat the \u2018averaged\u2019 version of all training examples of the same digit identity. In", "by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from binary vector, whose detailed discussion can be referred to the Supplementary section A) to classify the predicted symbol instead of softmax operation which has no neuroscience foundation. The PFC subsystem contains a LSTM and a full connected layer. It receives inputs from both language and vision subsystems in a concatenated form of INLINEFORM0 at time t, and gives a prediction output INLINEFORM1 , which is expected to be identical to INLINEFORM2 at time t+1. This has been achieved with a next frame prediction (NFP) loss function as, INLINEFORM3 . So given an input image, the PFC can predict the corresponding text description; while given an input text command the PFC can predict the corresponding manipulated image. This NFP loss function has neuroscience foundation, since the molecular", "digit figure given the language command \u2018give me a [number]\u2019 (Figure 6.A). The generated digit instance is somewhat the \u2018averaged\u2019 version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as \u2018give me a red Arial big 9\u2019 to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way. Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into", "much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units, which could be shared with the following sentence learning. Imagination is another key component of human thinking. For the game Go [22, 23], the network using a reinforcement learning strategy has to be trained with billions of games in order to acquire a feeling (Q value estimated for each potential action) to move the chess. As human beings, after knowing the rule conveyed by language, we can quickly start a game with proper moves using a try-in-imagination strategy without requiring even a single practice. With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without"]}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "predicted_answer": "Accuracy", "predicted_evidence": ["might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first", "which is more biologically plausible than the softmax operation. After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of \u2018the size is big/small\u2019 (train steps =1000), the negative adverb \u2018not\u2019 in the language text \u2018the size is not small/big\u2019 was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being. And then, LGI rapidly learned three more syntaxes: \u2018give me a \u2026\u2019, \u2018enlarge/shrink\u2019, and \u2018rotate \u2026\u2019, whose results are shown in Figure 6. After training (5000 steps), LGI could generate a correct digit figure given the language command \u2018give me a [number]\u2019 (Figure 6.A). The generated digit instance is somewhat the \u2018averaged\u2019 version of all training examples of the same digit identity. In", "knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity. Based on the same network, LGI continued to learn syntax \u2018this is \u2026\u2019. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation. After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI", "content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text. References. [1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652. [2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [3] Miller, E. K. & Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1), 167-202. [4] Baddeley, A., Gathercole, S. & Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158. [5] Finke, K., Bublak, P., Neugebauer, U. & Zihl, J. (2005). Combined processing of", "with language. We trained the LGI to associate object name with corresponding instances by \u2018this is \u2026\u2019 syntax; and trained the LGI to produce a digit instance, when there comes the sentence \u2018give me a [number]\u2019. In contrast, traditional language models could only serve as a word dependency predictor rather than really understand the sentence. Language is the most remarkable characteristics distinguishing mankind from animals. Theoretically, all kinds of information such as object properties, tasks and goals, commands and even emotions can be described and conveyed by language [21]. We trained with LGI eight different syntaxes (in other word, eight different tasks), and LGI demonstrates its understanding by correctly interacting with the vision system. After learning \u2018this is 9\u2019, it is much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units,", "digit figure given the language command \u2018give me a [number]\u2019 (Figure 6.A). The generated digit instance is somewhat the \u2018averaged\u2019 version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as \u2018give me a red Arial big 9\u2019 to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way. Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into", "Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking", "and the related results are shown in the following section. Finally, we demonstrate how the network forms a thinking loop with text language and imagined pictures. Experiment. The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and"]}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "predicted_answer": "Unanswerable", "predicted_evidence": ["might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first", "However, this architecture is task-specific, namely that the network can only make predictions on cars' unseen views. To include multiple tasks, we added an additional PFC layer that can receive task commands conveyed via language stream and object representation via the visual encoder pathway, and output the modulated images according to task commands and the desired text prediction associated with the images. In addition, by transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image. Architecture. As is shown in Figure 1, the LGI network contains three main subsystems including the vision, language and PFC subsystems. The vision autoencoder network was trained separately, whose characteristics of development, recognition, learning, and forgetting can be referred to [10]. After training, the autoencoder is separated into two parts: the encoder", "Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking", "with language. We trained the LGI to associate object name with corresponding instances by \u2018this is \u2026\u2019 syntax; and trained the LGI to produce a digit instance, when there comes the sentence \u2018give me a [number]\u2019. In contrast, traditional language models could only serve as a word dependency predictor rather than really understand the sentence. Language is the most remarkable characteristics distinguishing mankind from animals. Theoretically, all kinds of information such as object properties, tasks and goals, commands and even emotions can be described and conveyed by language [21]. We trained with LGI eight different syntaxes (in other word, eight different tasks), and LGI demonstrates its understanding by correctly interacting with the vision system. After learning \u2018this is 9\u2019, it is much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units,", "digit figure given the language command \u2018give me a [number]\u2019 (Figure 6.A). The generated digit instance is somewhat the \u2018averaged\u2019 version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as \u2018give me a red Arial big 9\u2019 to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way. Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into", "content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text. References. [1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652. [2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [3] Miller, E. K. & Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1), 167-202. [4] Baddeley, A., Gathercole, S. & Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158. [5] Finke, K., Bublak, P., Neugebauer, U. & Zihl, J. (2005). Combined processing of", "knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity. Based on the same network, LGI continued to learn syntax \u2018this is \u2026\u2019. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation. After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI", "and the related results are shown in the following section. Finally, we demonstrate how the network forms a thinking loop with text language and imagined pictures. Experiment. The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and"]}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "predicted_answer": "IPS quantity information extraction", "predicted_evidence": ["a real apple and teaches the baby \u2018this is an apple\u2019. After gradually assimilating the basic meanings of numerous nouns, children begin to learn some phrases and finally complicated syntaxes. Unlike the cumulative learning, most NLP techniques normally choose to learn by reading and predicting target words. After consuming billions of words in corpus materials [2], the NLP network can predict \u2018Trump\u2019 following \u2018Donald\u2019, but it is merely a probability machine. The human-like thinking system often requires specific neural substrates to support the corresponding functionalities. The most important brain area related to thinking is the prefrontal cortex (PFC), where the working memory takes place, including but not confined to, the maintenance and manipulation of particular information [3]. With the PFC, human beings can analyze and execute various tasks via \u2018phonological loop\u2019 and \u2018visuospatial scratchpad\u2019 etc. [4,5]. Inspired by the human-like brain organization, we build a \u2018PFC\u2019", "the corresponding text description; while given an input text command the PFC can predict the corresponding manipulated image. This NFP loss function has neuroscience foundation, since the molecular mediated synaptic plasticity always takes place after the completion of an event, when the information of both t and t+1 time points have been acquired and presented by the neural system. The strategy of learning by predicting its own next frame is essentially an unsupervised learning. For human brain development, the visual and auditory systems mature in much earlier stages than the PFC [19]. To mimic this process, our PFC subsystem was trained separately after vision and language components had completed their functionalities. We have trained the network to accumulatively learn eight syntaxes, and the related results are shown in the following section. Finally, we demonstrate how the network forms a thinking loop with text language and imagined pictures. Experiment. The first syntaxes", "by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily. In the future, many more syntaxes and functionalities can be added to LGI in a similar way, such as math reasoning, intuitive physics prediction and navigation [24, 25, 26]. Insights of human audition processing could be leveraged to convert sound wave into language text as a direct input for LGI [27, 28]. And the mechanisms of human value systems in the striatum [29] may also endow LGI with motivation and emotion. The PFC cortex consists of many sub-regions interacted within the PFC and across the whole brain areas [3, 30], and the implementation of these features might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a", "might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first", "content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text. References. [1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652. [2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [3] Miller, E. K. & Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1), 167-202. [4] Baddeley, A., Gathercole, S. & Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158. [5] Finke, K., Bublak, P., Neugebauer, U. & Zihl, J. (2005). Combined processing of", "[3]. With the PFC, human beings can analyze and execute various tasks via \u2018phonological loop\u2019 and \u2018visuospatial scratchpad\u2019 etc. [4,5]. Inspired by the human-like brain organization, we build a \u2018PFC\u2019 network to combine language and vision streams to achieve tasks such as language controlled imagination, and imagination based thinking process. Our results show that the LGI network could incrementally learn eight syntaxes rapidly. Based on the LGI, we present the first language guided continual thinking process, which shows considerable promise for the human-like strong machine intelligence. Related work. Our goal is to build a human-like neural network by removing components unsupported by neuroscience from AI architecture while introducing novel neural mechanisms and algorithms into it. Taking the convolution neural network (CNN) as an example, although it has reached human-level performance in image recognition tasks [6], animal neural systems do not support such kernel scanning", "by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from binary vector, whose detailed discussion can be referred to the Supplementary section A) to classify the predicted symbol instead of softmax operation which has no neuroscience foundation. The PFC subsystem contains a LSTM and a full connected layer. It receives inputs from both language and vision subsystems in a concatenated form of INLINEFORM0 at time t, and gives a prediction output INLINEFORM1 , which is expected to be identical to INLINEFORM2 at time t+1. This has been achieved with a next frame prediction (NFP) loss function as, INLINEFORM3 . So given an input image, the PFC can predict the corresponding text description; while given an input text command the PFC can predict the corresponding manipulated image. This NFP loss function has neuroscience foundation, since the molecular", "based on which the imagination network could reconstruct the predicted image. Finally, the predicted or imagined image is fed back to the encoder network for the next thinking iteration. The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from"]}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "predicted_answer": "The LSTM layer in the language subsystem mimics the number processing functionality of human Intra-Parietal Sulcus.", "predicted_evidence": ["might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a human-like thinking system (the LGI system). The LGI contains three subsystems: the vision, language, and PFC subsystem, which are trained separately. The development, recognition and learning mechanism is discussed in the cocurrent paper [10]. In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier. We propose to train the LGI with the NFP loss function, which endows the capacity to describe the image content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first", "by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily. In the future, many more syntaxes and functionalities can be added to LGI in a similar way, such as math reasoning, intuitive physics prediction and navigation [24, 25, 26]. Insights of human audition processing could be leveraged to convert sound wave into language text as a direct input for LGI [27, 28]. And the mechanisms of human value systems in the striatum [29] may also endow LGI with motivation and emotion. The PFC cortex consists of many sub-regions interacted within the PFC and across the whole brain areas [3, 30], and the implementation of these features might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a", "based on which the imagination network could reconstruct the predicted image. Finally, the predicted or imagined image is fed back to the encoder network for the next thinking iteration. The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from", "content in form of symbol text and manipulated images according to language commands. LGI shows its ability to learn eight different syntaxes or tasks in a cumulative learning way, and form the first machine thinking loop with the interaction between imagined pictures and language text. References. [1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652. [2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [3] Miller, E. K. & Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1), 167-202. [4] Baddeley, A., Gathercole, S. & Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158. [5] Finke, K., Bublak, P., Neugebauer, U. & Zihl, J. (2005). Combined processing of", "a real apple and teaches the baby \u2018this is an apple\u2019. After gradually assimilating the basic meanings of numerous nouns, children begin to learn some phrases and finally complicated syntaxes. Unlike the cumulative learning, most NLP techniques normally choose to learn by reading and predicting target words. After consuming billions of words in corpus materials [2], the NLP network can predict \u2018Trump\u2019 following \u2018Donald\u2019, but it is merely a probability machine. The human-like thinking system often requires specific neural substrates to support the corresponding functionalities. The most important brain area related to thinking is the prefrontal cortex (PFC), where the working memory takes place, including but not confined to, the maintenance and manipulation of particular information [3]. With the PFC, human beings can analyze and execute various tasks via \u2018phonological loop\u2019 and \u2018visuospatial scratchpad\u2019 etc. [4,5]. Inspired by the human-like brain organization, we build a \u2018PFC\u2019", "[3]. With the PFC, human beings can analyze and execute various tasks via \u2018phonological loop\u2019 and \u2018visuospatial scratchpad\u2019 etc. [4,5]. Inspired by the human-like brain organization, we build a \u2018PFC\u2019 network to combine language and vision streams to achieve tasks such as language controlled imagination, and imagination based thinking process. Our results show that the LGI network could incrementally learn eight syntaxes rapidly. Based on the LGI, we present the first language guided continual thinking process, which shows considerable promise for the human-like strong machine intelligence. Related work. Our goal is to build a human-like neural network by removing components unsupported by neuroscience from AI architecture while introducing novel neural mechanisms and algorithms into it. Taking the convolution neural network (CNN) as an example, although it has reached human-level performance in image recognition tasks [6], animal neural systems do not support such kernel scanning", "Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking", "However, this architecture is task-specific, namely that the network can only make predictions on cars' unseen views. To include multiple tasks, we added an additional PFC layer that can receive task commands conveyed via language stream and object representation via the visual encoder pathway, and output the modulated images according to task commands and the desired text prediction associated with the images. In addition, by transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image. Architecture. As is shown in Figure 1, the LGI network contains three main subsystems including the vision, language and PFC subsystems. The vision autoencoder network was trained separately, whose characteristics of development, recognition, learning, and forgetting can be referred to [10]. After training, the autoencoder is separated into two parts: the encoder"]}
{"question_id": "bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "predicted_answer": "Imagining a vague car is the foundation for decomposing future tasks. We trained the LGI network with a human-like cumulative learning process, from learning the meaning of words, to understanding complicated syntaxes, and finally organizing the thinking process with language.", "predicted_evidence": ["size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly. On the other hand, the full path of clear imagination may explain the dream mechanism. Figure 7.B shows the short cut imagination process, where LGI also regarded the rotated \u20189\u2019 as digit 6, which suggests the AIT activation does not encode the digit identity, but the untangled features of input image or imagined image. Those high level cortices beyond visual cortex could be the place for identity representation. Discussion. Language guided imagination is the nature of human thinking and", "image or imagined image. Those high level cortices beyond visual cortex could be the place for identity representation. Discussion. Language guided imagination is the nature of human thinking and intelligence. Normally, the real-time tasks or goals are conveyed by language, such as \u2018to build a Lego car\u2019. To achieve this goal, first, an agent (human being or machine) needs to know what\u2019s car, and then imagine a vague car instance, based on which the agent can plan to later collect wheel, window and chassis blocks for construction. Imagining the vague car is the foundation for decomposing future tasks. We trained the LGI network with a human-like cumulative learning process, from learning the meaning of words, to understanding complicated syntaxes, and finally organizing the thinking process with language. We trained the LGI to associate object name with corresponding instances by \u2018this is \u2026\u2019 syntax; and trained the LGI to produce a digit instance, when there comes the sentence \u2018give me", "much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units, which could be shared with the following sentence learning. Imagination is another key component of human thinking. For the game Go [22, 23], the network using a reinforcement learning strategy has to be trained with billions of games in order to acquire a feeling (Q value estimated for each potential action) to move the chess. As human beings, after knowing the rule conveyed by language, we can quickly start a game with proper moves using a try-in-imagination strategy without requiring even a single practice. With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without", "based on which the imagination network could reconstruct the predicted image. Finally, the predicted or imagined image is fed back to the encoder network for the next thinking iteration. The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from", "Introduction. Human thinking is regarded as \u2018mental ideas flow guided by language to achieve a goal\u2019. For instance, after seeing heavy rain, you may say internally \u2018holding an umbrella could avoid getting wet\u2019, and then you will take an umbrella before leaving. In the process, we know that the visual input of \u2018water drop\u2019 is called rain, and can imagine \u2018holding an umbrella\u2019 could keep off the rain, and can even experience the feeling of being wet. This continual thinking capacity distinguishes us from the machine, even though the latter can also recognize images, process language, and sense rain-drops. Continual thinking requires the capacity to generate mental imagination guided by language, and extract language representations from a real or imagined scenario. Modern natural language processing (NLP) techniques can handle question answering etc. tasks, such as answering that \u2018Cao Cao\u2019s nickname is Meng De\u2019 based on the website knowledge [1]. However, the NLP network is just a", "Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking", "digit figure given the language command \u2018give me a [number]\u2019 (Figure 6.A). The generated digit instance is somewhat the \u2018averaged\u2019 version of all training examples of the same digit identity. In the future, the generative adversarial network (GAN) technique could be included to generate object instances with specific details. However, using more specific language, such as \u2018give me a red Arial big 9\u2019 to generate the characterized instance can better resemble the human thinking process than GAN. LGI can also learn to change the size and orientation of an imagined object. Figure 6.B-C illustrates the morphology of the final imagined instance could be kept unchanged after experiencing various manipulations. Some other syntaxes or tasks could be integrated into LGI in a similar way. Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into", "by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily. In the future, many more syntaxes and functionalities can be added to LGI in a similar way, such as math reasoning, intuitive physics prediction and navigation [24, 25, 26]. Insights of human audition processing could be leveraged to convert sound wave into language text as a direct input for LGI [27, 28]. And the mechanisms of human value systems in the striatum [29] may also endow LGI with motivation and emotion. The PFC cortex consists of many sub-regions interacted within the PFC and across the whole brain areas [3, 30], and the implementation of these features might finally enable LGI to possess real machine intelligence. Conclusion. In this paper, we first introduced a PFC layer to involve representations from both language and vision subsystems to form a"]}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "predicted_answer": "Unanswerable.", "predicted_evidence": ["a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted \u201ctypo-ness\u201d score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics", "edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target. Data Filtering ::: Classification of Typo Edits. We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the", "edits. The perplexity of a text ${\\mathbf {x}} = x_1 x_2, ..., x_L$ is defined by: where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of the input text to some degree, and by taking the ratio between the source and the target, the feature can represent the degree to which the fluency is improved before and after the edit. As for the language model, we trained a character level Long Short Term Memory (LSTM) language model developed in BIBREF20 per language, which consists of a trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier. The LSTM hidden state and word embedding sizes are set to be 1000 and 200, respectively. We used 100,000 sentences from the W2C Web Corpus BIBREF21 for training (except for Chinese, where we used 28,000 sentences) and 1,000 sentences for validation for all the languages. The normalized edit distance between the source $\\mathbf {x} = x_1 x_2, ...,", "members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors, along with their corrections. The process for building the dataset can be summarized as follows: Extract eligible repositories and typo commits from GitHub based on the meta data of the repository and the commit message Filter out edits that are not written in human language Identify true typo edits (vs semantic edits) by using learned classifiers on a small annotated dataset We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings", "character-generating process, while a grammatical edit is the one which corrupts some implicit grammatical process (for example, production rules of a context-free grammar). Data Filtering ::: Statistics of Annotated Edits. Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics: Ratio of the target perplexity over the source calculated by a language model Normalized edit distance between the source and the target Binary variable indicating whether the edit purely consists of changes in numbers The rationale behind the third feature is that we observed that purely numerical changes always end up being tagged as semantic edits. The perplexity of a text ${\\mathbf {x}} = x_1 x_2, ..., x_L$ is defined by: where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of", "training (except for Chinese, where we used 28,000 sentences) and 1,000 sentences for validation for all the languages. The normalized edit distance between the source $\\mathbf {x} = x_1 x_2, ..., x_{L_x}$ and the target $\\mathbf {y} = y_1 y_2, ..., y_{L_y}$ is defined by: where $d({\\mathbf {x}}, {\\mathbf {y}})$ is the (unnormalized) edit distance between ${\\mathbf {x}}$ and ${\\mathbf {y}}$. This feature can capture the amount of the change made between the source and the target, based on our hypothesis that many typo edits only involve a small amount of changes. See Figure FIGREF33 for an overview of the distributions of these computed statistics per category for English. We observed similar trends for other two languages (Chinese and Japanese), except for a slightly larger number of spell edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and", "Introduction. Spelling correction BIBREF0, BIBREF1, BIBREF2 and grammatical error correction (GEC) BIBREF3 are two fundamental tasks that have important implications for downstream NLP tasks and for education in general. In recent years, the use of statistical machine translation (SMT) and neural sequence-to-sequence (seq2seq) models has been becoming increasingly popular for solving these tasks. Such modern NLP models are usually data hungry and require a large amount of parallel training data consisting of sentences before and after the correction. However, only relatively small datasets are available for these tasks, compared to other NLP tasks such as machine translation. This is especially the case for spelling correction, for which only a small number of datasets consisting of individual misspelled words are available, including the Birkbeck spelling error corpus and a list of typos collected from Twitter. Due to this lack of large-scale datasets, many research studies BIBREF4,", "analysis on the types of spelling and grammatical edits, as well as the performance of existing spell checkers on the GitHub Typo Corpus. The first three columns of Table TABREF46 show a breakdown of edit types in the aforementioned set of annotated typo edits in English (Section SECREF26) analyzed by ERRANT BIBREF22, BIBREF23. This shows that the dataset contains diverse types of edits, including orthographic, punctuation, and spelling errors. We then applied Aspell and Enchant, two commonly used spell checking libraries, and measured their performance against each one of the edit types. The results show that the performance of the spell checkers is fairly low ($F0.5 \\approx 0.5$) even for its main target category (SPELL), which suggests that the GitHub Typo Corpus contains many challenging typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and"]}
{"question_id": "d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "predicted_answer": "No", "predicted_evidence": ["software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter through commit messages written in other languages, which is future work. Analyses ::: Distribution of Atomic Edits. In order to provide a more qualitative look into the dataset, we analyzed all the edits in the top three languages and extracted atomic edits. An atomic edit is defined as a sequence of contiguous characters that are inserted, deleted, or substituted between the source and the target. We extracted these atomic edits by aligning the characters between the source and the target by minimizing the edit distance, then by extracting contiguous edits that are insertion, deletion, or substitution. As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese", "of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics of the Dataset. Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset. In addition to an obvious fact that a large fraction of the code on GitHub is written in English, one reason of the bias towards English may be due to our commit collection process, where we used an English keyword \u201ctypo\u201d to harvest eligible commit. Although it is a norm on GitHub (and in software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter", "the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase of filtering, we identify all non-typo edits that are not intended to fix mechanical, spelling, or grammatical errors, but to modify the intended meaning between the source and the target. In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories: Mechanical ... a mechanical edit fixes errors in punctuation and capitalization. Spell ... a spell edit fixes misspellings in words. This also includes conversion errors in non-Latin languages (e.g., Chinese and Japanese). Grammatical ... a", "character-generating process, while a grammatical edit is the one which corrupts some implicit grammatical process (for example, production rules of a context-free grammar). Data Filtering ::: Statistics of Annotated Edits. Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics: Ratio of the target perplexity over the source calculated by a language model Normalized edit distance between the source and the target Binary variable indicating whether the edit purely consists of changes in numbers The rationale behind the third feature is that we observed that purely numerical changes always end up being tagged as semantic edits. The perplexity of a text ${\\mathbf {x}} = x_1 x_2, ..., x_L$ is defined by: where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of", "training (except for Chinese, where we used 28,000 sentences) and 1,000 sentences for validation for all the languages. The normalized edit distance between the source $\\mathbf {x} = x_1 x_2, ..., x_{L_x}$ and the target $\\mathbf {y} = y_1 y_2, ..., y_{L_y}$ is defined by: where $d({\\mathbf {x}}, {\\mathbf {y}})$ is the (unnormalized) edit distance between ${\\mathbf {x}}$ and ${\\mathbf {y}}$. This feature can capture the amount of the change made between the source and the target, based on our hypothesis that many typo edits only involve a small amount of changes. See Figure FIGREF33 for an overview of the distributions of these computed statistics per category for English. We observed similar trends for other two languages (Chinese and Japanese), except for a slightly larger number of spell edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and", "edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target. Data Filtering ::: Classification of Typo Edits. We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the", "in punctuation and capitalization. Spell ... a spell edit fixes misspellings in words. This also includes conversion errors in non-Latin languages (e.g., Chinese and Japanese). Grammatical ... a grammatical edit fixes grammatical errors in the source. Semantic ... a semantic edit changes the intended meaning between the source and the target. See Figure FIGREF27 for some examples of different edit types on each language. If one edit contains more than one type of changes, the least superficial category is assigned. For example, if there are both spell and grammatical changes in a single edit, the \u201cgrammatical\u201d category is assigned to the edit. We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not. Thus, our goal is to identify the last type of edits as accurately as possible in a scalable manner. We will show the statistics of the annotated data in Section 6.", "detection, and detect (not remove) the second type of edits by building a supervised classifier. The following subsections detail the process. See Figure FIGREF15 (right) for an overview of the typo filtering process. Data Filtering ::: Language Detection. Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language. We also discarded an edit if the detected language doesn't match between the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase"]}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "predicted_answer": "They used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks).", "predicted_evidence": ["edits. The perplexity of a text ${\\mathbf {x}} = x_1 x_2, ..., x_L$ is defined by: where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of the input text to some degree, and by taking the ratio between the source and the target, the feature can represent the degree to which the fluency is improved before and after the edit. As for the language model, we trained a character level Long Short Term Memory (LSTM) language model developed in BIBREF20 per language, which consists of a trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier. The LSTM hidden state and word embedding sizes are set to be 1000 and 200, respectively. We used 100,000 sentences from the W2C Web Corpus BIBREF21 for training (except for Chinese, where we used 28,000 sentences) and 1,000 sentences for validation for all the languages. The normalized edit distance between the source $\\mathbf {x} = x_1 x_2, ...,", "edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target. Data Filtering ::: Classification of Typo Edits. We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the", "software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter through commit messages written in other languages, which is future work. Analyses ::: Distribution of Atomic Edits. In order to provide a more qualitative look into the dataset, we analyzed all the edits in the top three languages and extracted atomic edits. An atomic edit is defined as a sequence of contiguous characters that are inserted, deleted, or substituted between the source and the target. We extracted these atomic edits by aligning the characters between the source and the target by minimizing the edit distance, then by extracting contiguous edits that are insertion, deletion, or substitution. As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese", "character-generating process, while a grammatical edit is the one which corrupts some implicit grammatical process (for example, production rules of a context-free grammar). Data Filtering ::: Statistics of Annotated Edits. Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics: Ratio of the target perplexity over the source calculated by a language model Normalized edit distance between the source and the target Binary variable indicating whether the edit purely consists of changes in numbers The rationale behind the third feature is that we observed that purely numerical changes always end up being tagged as semantic edits. The perplexity of a text ${\\mathbf {x}} = x_1 x_2, ..., x_L$ is defined by: where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of", "of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics of the Dataset. Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset. In addition to an obvious fact that a large fraction of the code on GitHub is written in English, one reason of the bias towards English may be due to our commit collection process, where we used an English keyword \u201ctypo\u201d to harvest eligible commit. Although it is a norm on GitHub (and in software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter", "detection, and detect (not remove) the second type of edits by building a supervised classifier. The following subsections detail the process. See Figure FIGREF15 (right) for an overview of the typo filtering process. Data Filtering ::: Language Detection. Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language. We also discarded an edit if the detected language doesn't match between the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase", "As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese are made against English words (see Figure FIGREF27 for examples\u2014you notice many English words such as \u201cGB-18030\u201d and \u201cGemfile\u201d in non-English text). You also notice a number of grammatical edits in Chinese (e.g., confusion between the possessive particle de and the adjectival particle de) and Japanese (e.g., omissions of case particles such as wo, no, and ni). This demonstrates that the dataset can serve as a rich source of not only spelling but also naturally-occurring grammatical errors. Analyses ::: Evaluating Existing Spell Checker. We conclude the analysis section by providing a comprehensive analysis on the types of spelling and grammatical edits, as well as the performance of existing spell checkers on the GitHub Typo Corpus. The first three columns of Table TABREF46 show a breakdown of", "the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase of filtering, we identify all non-typo edits that are not intended to fix mechanical, spelling, or grammatical errors, but to modify the intended meaning between the source and the target. In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories: Mechanical ... a mechanical edit fixes errors in punctuation and capitalization. Spell ... a spell edit fixes misspellings in words. This also includes conversion errors in non-Latin languages (e.g., Chinese and Japanese). Grammatical ... a"]}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "predicted_answer": "eligible GitHub repositories", "predicted_evidence": ["typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and grammatical error correctors. Conclusion. This paper describes the process where we built the GitHub Typo Corpus, a large-scale multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, the largest platform for publishing and sharing git repositories. The dataset contains more than 350k edits and 64M characters in more than 15 languages, making it the largest dataset of misspellings to date. We automatically identified typo edits (be it mechanical, spell, or grammatical) versus semantic ones by building a simple logistic regression classifier with only three features which achieved 0.9 F1-measure. We provided detailed qualitative and quantitative analyses of the datasets, demonstrating that the dataset serves as a rich source of spelling and", "0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings to date. We made the dataset publicly available (https://github.com/mhagiwara/github-typo-corpus) along with the automatically assigned typo labels as well as the source code to extract typos. We also provide the detailed analyses of the dataset, where we demonstrate that the F measure of existing spell checkers merely reaches $\\sim 0.5$, arguing that the GitHub Typo Corpus provides a new, rich source of naturally-occurring misspellings and grammatical errors that complement existing datasets. Related Work. As mentioned above, a closely related line of work is the use of Wikipedia edits for various tasks, including GEC. Grundkiewicz:2014 constructed the WikiEd Error Corpus, a dataset consisting of error edits harvested from the Wikipedia edit history and demonstrated that the newly-built", "members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors, along with their corrections. The process for building the dataset can be summarized as follows: Extract eligible repositories and typo commits from GitHub based on the meta data of the repository and the commit message Filter out edits that are not written in human language Identify true typo edits (vs semantic edits) by using learned classifiers on a small annotated dataset We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings", "the matters worse, Wikipedia suffers from vandalism, where articles are edited in a malicious manner, which requires extensive detection and filtering. In order to create a high-quality, large-scale dataset of misspelling and grammatical errors (collectively called typos in this paper), we leverage the data from GitHub, the largest platform for hosting and sharing repositories maintained by git, a popular version control system commonly used for software development. Changes made to git repositories (called commits, see Section 3 for the definition) are usually tagged with commit messages, making detection of typos a trivial task. Also, GitHub suffers less from vandalism, since commits in many repositories are code reviewed, a process where every change is manually reviewed by other team members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus,", "some mechanical, spelling and/or grammatical errors in the source, while preserving the meaning between the two. Our goal is to collect typos from GitHub and build a dataset that is high in both quantity and quality. Data Collection. This section describes the process for collecting a large amount of typos from GitHub, which consists two steps: 1) collecting target repositories that meet some criteria and 2) collecting commits and edits from them. See Figure FIGREF15 for the overview of the typo-collecting process. Data Collection ::: Collecting Repositories. The first step for collecting typos is to collect as many eligible GitHub repositories as possible from which commits and edits are extracted. A repository must meet some criteria in order to be included in the corpus, such as size (it needs to big enough to contain at least some amount of typo edits), license (it has to be distributed under a permissive license to allow derived work), and quality (it has to demonstrate some", "analysis on the types of spelling and grammatical edits, as well as the performance of existing spell checkers on the GitHub Typo Corpus. The first three columns of Table TABREF46 show a breakdown of edit types in the aforementioned set of annotated typo edits in English (Section SECREF26) analyzed by ERRANT BIBREF22, BIBREF23. This shows that the dataset contains diverse types of edits, including orthographic, punctuation, and spelling errors. We then applied Aspell and Enchant, two commonly used spell checking libraries, and measured their performance against each one of the edit types. The results show that the performance of the spell checkers is fairly low ($F0.5 \\approx 0.5$) even for its main target category (SPELL), which suggests that the GitHub Typo Corpus contains many challenging typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and", "of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics of the Dataset. Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset. In addition to an obvious fact that a large fraction of the code on GitHub is written in English, one reason of the bias towards English may be due to our commit collection process, where we used an English keyword \u201ctypo\u201d to harvest eligible commit. Although it is a norm on GitHub (and in software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter", "the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase of filtering, we identify all non-typo edits that are not intended to fix mechanical, spelling, or grammatical errors, but to modify the intended meaning between the source and the target. In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories: Mechanical ... a mechanical edit fixes errors in punctuation and capitalization. Spell ... a spell edit fixes misspellings in words. This also includes conversion errors in non-Latin languages (e.g., Chinese and Japanese). Grammatical ... a"]}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "predicted_answer": "Unknown", "predicted_evidence": ["typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and grammatical error correctors. Conclusion. This paper describes the process where we built the GitHub Typo Corpus, a large-scale multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, the largest platform for publishing and sharing git repositories. The dataset contains more than 350k edits and 64M characters in more than 15 languages, making it the largest dataset of misspellings to date. We automatically identified typo edits (be it mechanical, spell, or grammatical) versus semantic ones by building a simple logistic regression classifier with only three features which achieved 0.9 F1-measure. We provided detailed qualitative and quantitative analyses of the datasets, demonstrating that the dataset serves as a rich source of spelling and", "members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors, along with their corrections. The process for building the dataset can be summarized as follows: Extract eligible repositories and typo commits from GitHub based on the meta data of the repository and the commit message Filter out edits that are not written in human language Identify true typo edits (vs semantic edits) by using learned classifiers on a small annotated dataset We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings", "0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings to date. We made the dataset publicly available (https://github.com/mhagiwara/github-typo-corpus) along with the automatically assigned typo labels as well as the source code to extract typos. We also provide the detailed analyses of the dataset, where we demonstrate that the F measure of existing spell checkers merely reaches $\\sim 0.5$, arguing that the GitHub Typo Corpus provides a new, rich source of naturally-occurring misspellings and grammatical errors that complement existing datasets. Related Work. As mentioned above, a closely related line of work is the use of Wikipedia edits for various tasks, including GEC. Grundkiewicz:2014 constructed the WikiEd Error Corpus, a dataset consisting of error edits harvested from the Wikipedia edit history and demonstrated that the newly-built", "the matters worse, Wikipedia suffers from vandalism, where articles are edited in a malicious manner, which requires extensive detection and filtering. In order to create a high-quality, large-scale dataset of misspelling and grammatical errors (collectively called typos in this paper), we leverage the data from GitHub, the largest platform for hosting and sharing repositories maintained by git, a popular version control system commonly used for software development. Changes made to git repositories (called commits, see Section 3 for the definition) are usually tagged with commit messages, making detection of typos a trivial task. Also, GitHub suffers less from vandalism, since commits in many repositories are code reviewed, a process where every change is manually reviewed by other team members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus,", "of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics of the Dataset. Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset. In addition to an obvious fact that a large fraction of the code on GitHub is written in English, one reason of the bias towards English may be due to our commit collection process, where we used an English keyword \u201ctypo\u201d to harvest eligible commit. Although it is a norm on GitHub (and in software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter", "analysis on the types of spelling and grammatical edits, as well as the performance of existing spell checkers on the GitHub Typo Corpus. The first three columns of Table TABREF46 show a breakdown of edit types in the aforementioned set of annotated typo edits in English (Section SECREF26) analyzed by ERRANT BIBREF22, BIBREF23. This shows that the dataset contains diverse types of edits, including orthographic, punctuation, and spelling errors. We then applied Aspell and Enchant, two commonly used spell checking libraries, and measured their performance against each one of the edit types. The results show that the performance of the spell checkers is fairly low ($F0.5 \\approx 0.5$) even for its main target category (SPELL), which suggests that the GitHub Typo Corpus contains many challenging typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and", "a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted \u201ctypo-ness\u201d score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics", "the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase of filtering, we identify all non-typo edits that are not intended to fix mechanical, spelling, or grammatical errors, but to modify the intended meaning between the source and the target. In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories: Mechanical ... a mechanical edit fixes errors in punctuation and capitalization. Spell ... a spell edit fixes misspellings in words. This also includes conversion errors in non-Latin languages (e.g., Chinese and Japanese). Grammatical ... a"]}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "predicted_answer": "English, Simplified Chinese, Japanese, and at least 12 more languages.", "predicted_evidence": ["typo edits that existing spell checkers may have a hard time dealing with, and the dataset may provide a rich, complementary source of spelling errors for developing better spell checkers and grammatical error correctors. Conclusion. This paper describes the process where we built the GitHub Typo Corpus, a large-scale multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, the largest platform for publishing and sharing git repositories. The dataset contains more than 350k edits and 64M characters in more than 15 languages, making it the largest dataset of misspellings to date. We automatically identified typo edits (be it mechanical, spell, or grammatical) versus semantic ones by building a simple logistic regression classifier with only three features which achieved 0.9 F1-measure. We provided detailed qualitative and quantitative analyses of the datasets, demonstrating that the dataset serves as a rich source of spelling and", "of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics of the Dataset. Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset. In addition to an obvious fact that a large fraction of the code on GitHub is written in English, one reason of the bias towards English may be due to our commit collection process, where we used an English keyword \u201ctypo\u201d to harvest eligible commit. Although it is a norm on GitHub (and in software development in general) to write commit messages in English no matter what language you are working in, we may be able to collect a more diverse set of commits if we build models to filter", "members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors, along with their corrections. The process for building the dataset can be summarized as follows: Extract eligible repositories and typo commits from GitHub based on the meta data of the repository and the commit message Filter out edits that are not written in human language Identify true typo edits (vs semantic edits) by using learned classifiers on a small annotated dataset We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings", "0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings to date. We made the dataset publicly available (https://github.com/mhagiwara/github-typo-corpus) along with the automatically assigned typo labels as well as the source code to extract typos. We also provide the detailed analyses of the dataset, where we demonstrate that the F measure of existing spell checkers merely reaches $\\sim 0.5$, arguing that the GitHub Typo Corpus provides a new, rich source of naturally-occurring misspellings and grammatical errors that complement existing datasets. Related Work. As mentioned above, a closely related line of work is the use of Wikipedia edits for various tasks, including GEC. Grundkiewicz:2014 constructed the WikiEd Error Corpus, a dataset consisting of error edits harvested from the Wikipedia edit history and demonstrated that the newly-built", "the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase of filtering, we identify all non-typo edits that are not intended to fix mechanical, spelling, or grammatical errors, but to modify the intended meaning between the source and the target. In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories: Mechanical ... a mechanical edit fixes errors in punctuation and capitalization. Spell ... a spell edit fixes misspellings in words. This also includes conversion errors in non-Latin languages (e.g., Chinese and Japanese). Grammatical ... a", "a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted \u201ctypo-ness\u201d score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes. Analyses. In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus. Analyses ::: Statistics", "the matters worse, Wikipedia suffers from vandalism, where articles are edited in a malicious manner, which requires extensive detection and filtering. In order to create a high-quality, large-scale dataset of misspelling and grammatical errors (collectively called typos in this paper), we leverage the data from GitHub, the largest platform for hosting and sharing repositories maintained by git, a popular version control system commonly used for software development. Changes made to git repositories (called commits, see Section 3 for the definition) are usually tagged with commit messages, making detection of typos a trivial task. Also, GitHub suffers less from vandalism, since commits in many repositories are code reviewed, a process where every change is manually reviewed by other team members before merged into the repository. This guarantees that the edits indeed fix existing spelling and/or grammatical issues. This paper describes our process for building the GitHub Typo Corpus,", "detection, and detect (not remove) the second type of edits by building a supervised classifier. The following subsections detail the process. See Figure FIGREF15 (right) for an overview of the typo filtering process. Data Filtering ::: Language Detection. Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language. We also discarded an edit if the detected language doesn't match between the source and the target. This left us with a total of 203,270 commits and 353,055 edits, which are all included in the final dataset. Data Filtering ::: Annotation of Edits. In this second phase"]}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "predicted_answer": "Yes", "predicted_evidence": ["embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets under a learning-to-rank problem setting. Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage. Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum", "benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix. As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks.", "word embeddings such as word2vec BIBREF0 and GloVe BIBREF1, a lot of efforts have been devoted to developing universal sentence embeddings. Initial attempts at learning sentence representation using unsupervised approaches did not yield satisfactory performance. Recent work BIBREF2 has shown that models trained in supervised fashion on datasets like Stanford Natural Language Inference (SNLI) corpus BIBREF3 can consistently outperform unsupervised methods like SkipThought vectors BIBREF4. More recently, Universal Sentence Encoder BIBREF5 equipped with the Transformer BIBREF6 as the encoder, co-trained on a large amount of unsupervised training data and SNLI corpus, has demonstrated surprisingly good performance with minimal amounts of supervised training data for a transfer task. BERT BIBREF7, one of the latest models that leverage heavily on language model pre-training, has achieved state-of-the-art performance in many natural language understanding tasks ranging from sequence and", "one of the latest models that leverage heavily on language model pre-training, has achieved state-of-the-art performance in many natural language understanding tasks ranging from sequence and sequence pair classification to question answering. The fact that pre-trained BERT can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks suggests that BERT representations are potential universal text embeddings. Passages that consist of multiple sentences are coherent units of natural languages that convey information at a pragmatic or discourse level. While there are many models for generating and evaluating sentence embeddings, there hasn't been a lot of work on passage level embedding generation and evaluation. In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic", "Introduction. Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time. Since the introduction of pre-trained word embeddings such as word2vec BIBREF0 and GloVe BIBREF1, a lot of efforts have been devoted to developing universal sentence embeddings. Initial attempts at learning sentence representation using", "on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets", "text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings. Effect of Encoder Layer: We compare the performance of embeddings extracted from different encoder layers of a pre-trained BERT using bert-as-service BIBREF10. Since we are interested in the linguistic information encoded in the embeddings, we only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values", "we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined. BERT Sentence Embedding. We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic"]}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "predicted_answer": "BM25 is a baseline for comparison.", "predicted_evidence": ["BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly. Conclusion. In this paper, we conducted an empirical investigation of BERT activations as universal text embeddings. We show that sentence embeddings from BERT perform strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA", "embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets under a learning-to-rank problem setting. Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage. Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum", "such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage. We fine-tune all BERT parameters except the word embedding layers. Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a", "from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in performance between bottom layer embeddings and GloVe embeddings than embeddings from other layers. Overall, pre-trained BERT embeddings perform well in text classification and syntactic probing tasks. The biggest limitation lies in the semantic similarity and sentence surface information probing tasks, where we observed a big gap between BERT and other state-of-the-art models. Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding", "strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance", "only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values are provided in the Appendix. In the heatmap, the raw values of metrics are normalized by the best performance of a particular task from all the models we evaluated including BERT. The tasks in the figure are grouped by task category. For example, all semantic similarity related tasks are placed at the top of the figure. As can be seen from the figure, embeddings from top layers generally perform better than lower layers. However, for certain semantic probing tasks such as tense classification, subject, and object number classifications, middle layer embeddings perform the best. Intuitively, embeddings from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in", "hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods. Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language"]}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "predicted_answer": "top and bottom layers", "predicted_evidence": ["BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly. Conclusion. In this paper, we conducted an empirical investigation of BERT activations as universal text embeddings. We show that sentence embeddings from BERT perform strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA", "improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance", "such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage. We fine-tune all BERT parameters except the word embedding layers. Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a", "strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in performance between bottom layer embeddings and GloVe embeddings than embeddings from other layers. Overall, pre-trained BERT embeddings perform well in text classification and syntactic probing tasks. The biggest limitation lies in the semantic similarity and sentence surface information probing tasks, where we observed a big gap between BERT and other state-of-the-art models. Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding", "on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets", "hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods. Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language", "benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix. As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks."]}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "predicted_answer": "SNLI", "predicted_evidence": ["BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly. Conclusion. In this paper, we conducted an empirical investigation of BERT activations as universal text embeddings. We show that sentence embeddings from BERT perform strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA", "on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets", "Introduction. Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time. Since the introduction of pre-trained word embeddings such as word2vec BIBREF0 and GloVe BIBREF1, a lot of efforts have been devoted to developing universal sentence embeddings. Initial attempts at learning sentence representation using", "benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix. As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks.", "embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets under a learning-to-rank problem setting. Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage. Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum", "word embeddings such as word2vec BIBREF0 and GloVe BIBREF1, a lot of efforts have been devoted to developing universal sentence embeddings. Initial attempts at learning sentence representation using unsupervised approaches did not yield satisfactory performance. Recent work BIBREF2 has shown that models trained in supervised fashion on datasets like Stanford Natural Language Inference (SNLI) corpus BIBREF3 can consistently outperform unsupervised methods like SkipThought vectors BIBREF4. More recently, Universal Sentence Encoder BIBREF5 equipped with the Transformer BIBREF6 as the encoder, co-trained on a large amount of unsupervised training data and SNLI corpus, has demonstrated surprisingly good performance with minimal amounts of supervised training data for a transfer task. BERT BIBREF7, one of the latest models that leverage heavily on language model pre-training, has achieved state-of-the-art performance in many natural language understanding tasks ranging from sequence and", "strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance"]}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "predicted_answer": "WikiPassageQA, InsuranceQA, Quasar-t, and SearchQA", "predicted_evidence": ["one of the latest models that leverage heavily on language model pre-training, has achieved state-of-the-art performance in many natural language understanding tasks ranging from sequence and sequence pair classification to question answering. The fact that pre-trained BERT can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks suggests that BERT representations are potential universal text embeddings. Passages that consist of multiple sentences are coherent units of natural languages that convey information at a pragmatic or discourse level. While there are many models for generating and evaluating sentence embeddings, there hasn't been a lot of work on passage level embedding generation and evaluation. In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic", "BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly. Conclusion. In this paper, we conducted an empirical investigation of BERT activations as universal text embeddings. We show that sentence embeddings from BERT perform strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA", "embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets under a learning-to-rank problem setting. Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage. Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum", "we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined. BERT Sentence Embedding. We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic", "strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "Introduction. Universal text representations are important for many NLP tasks as modern deep learning models are becoming more and more data-hungry and computationally expensive. On one hand, most research and industry tasks face data sparsity problem due to the high cost of annotation. Universal text representations can mitigate this problem to a certain extent by performing implicit transfer learning among tasks. On the other hand, modern deep learning models with millions of parameters are expensive to train and host, while models using text representation as the building blocks can achieve similar performance with much fewer tunable parameters. The pre-computed text embeddings can also help decrease model latency dramatically at inference time. Since the introduction of pre-trained word embeddings such as word2vec BIBREF0 and GloVe BIBREF1, a lot of efforts have been devoted to developing universal sentence embeddings. Initial attempts at learning sentence representation using", "improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance", "from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in performance between bottom layer embeddings and GloVe embeddings than embeddings from other layers. Overall, pre-trained BERT embeddings perform well in text classification and syntactic probing tasks. The biggest limitation lies in the semantic similarity and sentence surface information probing tasks, where we observed a big gap between BERT and other state-of-the-art models. Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding"]}
{"question_id": "f7f2968feb28c2907266c892f051ae9f7d6286e6", "predicted_answer": "text classification and natural language inference", "predicted_evidence": ["we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined. BERT Sentence Embedding. We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic", "strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings. Effect of Encoder Layer: We compare the performance of embeddings extracted from different encoder layers of a pre-trained BERT using bert-as-service BIBREF10. Since we are interested in the linguistic information encoded in the embeddings, we only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values", "only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values are provided in the Appendix. In the heatmap, the raw values of metrics are normalized by the best performance of a particular task from all the models we evaluated including BERT. The tasks in the figure are grouped by task category. For example, all semantic similarity related tasks are placed at the top of the figure. As can be seen from the figure, embeddings from top layers generally perform better than lower layers. However, for certain semantic probing tasks such as tense classification, subject, and object number classifications, middle layer embeddings perform the best. Intuitively, embeddings from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in", "on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset. BERT Passage Embedding. In this section, we evaluate BERT embeddings at passage level on question-answering datasets", "one of the latest models that leverage heavily on language model pre-training, has achieved state-of-the-art performance in many natural language understanding tasks ranging from sequence and sequence pair classification to question answering. The fact that pre-trained BERT can be easily fine-tuned with just one additional output layer to create a state-of-the-art model for a wide range of tasks suggests that BERT representations are potential universal text embeddings. Passages that consist of multiple sentences are coherent units of natural languages that convey information at a pragmatic or discourse level. While there are many models for generating and evaluating sentence embeddings, there hasn't been a lot of work on passage level embedding generation and evaluation. In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic", "benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix. As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks.", "from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in performance between bottom layer embeddings and GloVe embeddings than embeddings from other layers. Overall, pre-trained BERT embeddings perform well in text classification and syntactic probing tasks. The biggest limitation lies in the semantic similarity and sentence surface information probing tasks, where we observed a big gap between BERT and other state-of-the-art models. Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding"]}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "predicted_answer": "Multinomial logistic regression model with ridge estimator, multilayer perceptron, support vector machine learner (Sequential Minimal Optimization), and decision tree (J48).", "predicted_evidence": ["ensure comparability. Experimental Setup. We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA. We considered classification accuracy, F-score and Root Mean Squared Error (RMSE) as evaluation measures for our approach. We also included a confusion matrix, as we deal with a dataset that is unbalanced across CEFR levels. The scores were obtained by performing a ten-fold Cross-Validation (CV). Document-Level Experiments. We trained document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX", "we tested our sentence-level model also on an independent dataset (SenRead), a small corpus of sentences with gold-standard CEFR annotation. This data was created during a user-based evaluation study BIBREF27 and it consists of 196 sentences from generic corpora, i.e. originally not L2 learner-focused corpora, rated as being suitable at B1 or being at a level higher than B1. We used this corpus along with the judgments of the three participating teachers. Since SenRead had only two categories - INLINEFORM0 and INLINEFORM1 , we combined the model's predictions into two classes - A1, A2, B1 were considered as INLINEFORM2 B1 and B2, C1 were considered as INLINEFORM3 B1. The majority baseline for the dataset was 65%, INLINEFORM4 B1 being the class with most instances. The model trained on COCTAILL sentences predicted with 73% accuracy teachers' judgments, an 8% improvement over the majority baseline. There was a considerable difference between the precision score of the two classes, which", "studies for non-English languages using CEFR levels include: BIBREF13 , reporting 49.1% accuracy for a French system distinguishing six classes; and BIBREF14 achieving 29.7% accuracy on a smaller Portuguese dataset with five levels. Sentence-Level Experiments. After building good classification models at document level, we explored the usability of our approach at the sentence level. Sentences are particularly useful in Computer-Assisted Language Learning (CALL) applications, among others, for generating sentence-based multiple choice exercises, e.g. BIBREF25 , or vocabulary examples BIBREF26 . Furthermore, multi-class readability classification of sentence-level material intended for second language learners has not been previously investigated in the literature. With the same methodology (section SECREF7 ) and feature set (section SECREF3 ) used at the document level, we trained and tested classification models based on the sentence-level data (see section SECREF2 ). The results are", "for this task. For a better comparability, we applied also a linear regression model to our data which yielded a correlation of 0.8 and an RMSE of 0.65. To make sure that our system was not biased towards the majority classes B1 and B2, we inspected the confusion matrix (Table TABREF11 ) after classification using All. We can observe from Table TABREF11 that the system performs better at A1 and C1 levels, where confusion occurred only with adjacent classes. Similar to the findings in BIBREF13 for French, classes in the middle of the scale were harder to distinguish. Most misclassifications in our material occurred at A2 level (23%) followed by B1 and B2 level, (20% and 17% respectively). To establish the external validity of our approach, we tested it on a subset of L\u00e4SBarT BIBREF4 , a corpus of Swedish easy-to-read (ETR) texts previously employed for Swedish L1 readability studies BIBREF4 , BIBREF17 . We used 18 fiction texts written for children between ages nine to twelve, half of", "we achieved approximately the same performance (0.8 F) as with the complete set of features, All (0.81 F). This suggests that lexical information alone can successfully distinguish the CEFR level of course book texts at the document level. Using the complete feature set we obtained 81% accuracy and 97% adjacent accuracy (when misclassifications to adjacent classes are considered correct). The same scores with lexical features (Lex) only were 80.3% (accuracy) and 98% (adjacent accuracy). Accuracy scores using other learning algorithms were significantly lower (see Table TABREF10 ), therefore, we report only the results of the logistic regression classifier in the subsequent sections. Instead of classification, some readability studies (e.g. BIBREF10 , BIBREF14 ) employed linear regression for this task. For a better comparability, we applied also a linear regression model to our data which yielded a correlation of 0.8 and an RMSE of 0.65. To make sure that our system was not biased", "document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX readability score. Table TABREF9 shows the type of subgroup (Type), the number of features (Nr) and three evaluation metrics using logistic regression. Not only was accuracy very low with LIX, but this measure also classified 91.6% of the instances as B2 level. Length-based, semantic and syntactic features in isolation showed similar or only slightly better performance than the baselines, therefore we excluded them from Table TABREF9 . Lexical features, however, had a strong discriminatory power without an increase in bias towards the majority classes. Using this subset of features only, we achieved approximately the same performance (0.8 F) as with the complete set of features, All (0.81 F). This suggests that lexical information alone can successfully distinguish the CEFR level of", "(section SECREF7 ) and feature set (section SECREF3 ) used at the document level, we trained and tested classification models based on the sentence-level data (see section SECREF2 ). The results are shown in Table TABREF13 . Although the majority baseline in the case of sentences was 7% higher than the one for texts (Table TABREF9 ), the classification accuracy for sentences using all features was only 63.4%. This is a considerable drop (-18%) in performance compared to the document level (81.3% accuracy). It is possible that the features did not capture differences between the sentences because the amount of context is more limited on the fine-grained level. It is interesting to note that, although there was no substantial performance difference between Lex and All at a document level, the model with all the features performed 7% better at sentence level. Most misclassifications occurred, however, within a distance of one class only, thus the adjacent accuracy of the sentence-level", "the model with all the features performed 7% better at sentence level. Most misclassifications occurred, however, within a distance of one class only, thus the adjacent accuracy of the sentence-level model was still high, 92% (see Table TABREF14 ). Predictions were noticeably more accurate for classes A1, A2 and B1 which had a larger number of instances. In the next step, we applied the sentence-level model on the document-level data to explore how homogeneous texts were in terms of the CEFR level of the sentences they contained. Figure FIGREF15 shows that texts at each CEFR level contain a substantial amount of sentences of the same level of the whole text, but they also include sentences classified as belonging to other CEFR levels. Finally, as in the case of the document-level analysis, we tested our sentence-level model also on an independent dataset (SenRead), a small corpus of sentences with gold-standard CEFR annotation. This data was created during a user-based evaluation"]}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "predicted_answer": "COCTAILL", "predicted_evidence": ["performance (F-score of 0.8), however, there is room for improvement in sentence-level predictions. We plan to make our results available through the online intelligent computer-assisted language learning platform L\u00e4rka, both as corpus-based exercises for teachers and learners of L2 Swedish and as web-services for researchers and developers. In the following sections, we first describe our datasets (section SECREF2 ) and features (section SECREF3 ), then we present the details and the results of our experiments in section SECREF4 . Finally, section SECREF5 concludes our work and outlines further directions of research within this area. Datasets. Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 . This corpus consists of twelve books (from four different publishers) whose usability and level have been confirmed by Swedish L2 teachers. The course books have been annotated both content-wise (e.g. exercises, lists) and linguistically", "the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in. In Table TABREF3 , columns 2-5 give an overview of the distribution of texts across levels and their mean length in sentences. The distribution of sentences per level is presented in the last two columns of Table TABREF3 . COCTAILL contained a somewhat more limited amount of B2 and C1 level sentences in the form of", "ensure comparability. Experimental Setup. We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA. We considered classification accuracy, F-score and Root Mean Squared Error (RMSE) as evaluation measures for our approach. We also included a confusion matrix, as we deal with a dataset that is unbalanced across CEFR levels. The scores were obtained by performing a ten-fold Cross-Validation (CV). Document-Level Experiments. We trained document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX", "we tested our sentence-level model also on an independent dataset (SenRead), a small corpus of sentences with gold-standard CEFR annotation. This data was created during a user-based evaluation study BIBREF27 and it consists of 196 sentences from generic corpora, i.e. originally not L2 learner-focused corpora, rated as being suitable at B1 or being at a level higher than B1. We used this corpus along with the judgments of the three participating teachers. Since SenRead had only two categories - INLINEFORM0 and INLINEFORM1 , we combined the model's predictions into two classes - A1, A2, B1 were considered as INLINEFORM2 B1 and B2, C1 were considered as INLINEFORM3 B1. The majority baseline for the dataset was 65%, INLINEFORM4 B1 being the class with most instances. The model trained on COCTAILL sentences predicted with 73% accuracy teachers' judgments, an 8% improvement over the majority baseline. There was a considerable difference between the precision score of the two classes, which", "that model on SenRead (73%) improve on that score. It is also worth mentioning that the labels in the dataset from BIBREF8 were based on the assumption that all sentences in a text belong to the same difficulty level which, being an approximation (as also Figure FIGREF15 shows), introduced some noise in that data. Although more analysis would be needed to refine the sentence-level model, our current results indicate that a rich feature set that considers multiple linguistic dimensions may result in an improved performance. In the future, the dataset could be expanded with more gold-standard sentences, which may improve accuracy. Furthermore, an interesting direction to pursue would be to verify whether providing finer-grained readability judgments is a more challenging task also for human raters. Conclusion and Future Work. We proposed an approach to assess the proficiency (CEFR) level of Swedish L2 course book texts based on a variety of features. Our document-level model, the first", "The distribution of sentences per level is presented in the last two columns of Table TABREF3 . COCTAILL contained a somewhat more limited amount of B2 and C1 level sentences in the form of lists and language examples, possibly because learners handle larger linguistic units with more ease at higher proficiency levels. Features. We developed our features based on information both from previous literature BIBREF9 , BIBREF3 , BIBREF13 , BIBREF4 , BIBREF8 and a grammar book for Swedish L2 learners BIBREF20 . The set of features can be divided in the following five subgroups: length-based, lexical, morphological, syntactic and semantic features (Table TABREF6 ). Length-based (Len): These features include sentence length in number of tokens (#1) and characters (#4), extra-long words (longer than thirteen characters) and the traditional Swedish readability formula, LIX (see section SECREF1 ). For the sentence-level analysis, instead of the ratio of number of tokens to the number of", "document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX readability score. Table TABREF9 shows the type of subgroup (Type), the number of features (Nr) and three evaluation metrics using logistic regression. Not only was accuracy very low with LIX, but this measure also classified 91.6% of the instances as B2 level. Length-based, semantic and syntactic features in isolation showed similar or only slightly better performance than the baselines, therefore we excluded them from Table TABREF9 . Lexical features, however, had a strong discriminatory power without an increase in bias towards the majority classes. Using this subset of features only, we achieved approximately the same performance (0.8 F) as with the complete set of features, All (0.81 F). This suggests that lexical information alone can successfully distinguish the CEFR level of", "the model with all the features performed 7% better at sentence level. Most misclassifications occurred, however, within a distance of one class only, thus the adjacent accuracy of the sentence-level model was still high, 92% (see Table TABREF14 ). Predictions were noticeably more accurate for classes A1, A2 and B1 which had a larger number of instances. In the next step, we applied the sentence-level model on the document-level data to explore how homogeneous texts were in terms of the CEFR level of the sentences they contained. Figure FIGREF15 shows that texts at each CEFR level contain a substantial amount of sentences of the same level of the whole text, but they also include sentences classified as belonging to other CEFR levels. Finally, as in the case of the document-level analysis, we tested our sentence-level model also on an independent dataset (SenRead), a small corpus of sentences with gold-standard CEFR annotation. This data was created during a user-based evaluation"]}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_answer": "Bing Liu's dataset", "predicted_evidence": ["library. In order to create an Aspect-Rhetorical Relation Graph we used breadth-first search (BFS) algorithm for each Discourse Tree. Dataset. We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually. Experimental Setup. We implemented our framework in Python. The first computational step was to load the dataset and parse it into individual documents. Next, each document was processed through the Discourse Parser BIBREF18 and transformed into a Discourse Tree (DT). Then we extracted Elementary Discourse Units (EDUs) from the DT and each EDU was processed through the Logistic Regression sentiment algorithm. All neutral EDUs were taken off from consideration to ensure that the discovered aspects are correlated with authors' emotions. The remaining EDUs were processed through part-of-speech tagger to extract nouns and", "on the size of the rule-based system responsible for the completions of the text), or deep neural networks BIBREF17 . Experimental Scenario. For the Rhetorical Parsing part of our experiment, we used a special library implemented for such purposes BIBREF18 . As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 . The BoW vectorization method built a vocabulary that considers the top 50,000 terms only ordered by their frequency across the corpus, similarly to supervised learning examples presented in our previous works in BIBREF8 . We used a noun and noun phrases extractor according to part-of-speech tagger from the Spacy Python library. In order to create an Aspect-Rhetorical Relation Graph we used breadth-first search (BFS) algorithm for each Discourse Tree. Dataset. We used Bing Liu's dataset BIBREF20 for evaluation.", "not take into account the sequential dependencies between the DT constituents. Then existing discourse parsers will apply greedy and sub-optimal parsing algorithms and build a Discourse Tree. During this stage, and to cope with the mentioned limitation The inferred (posterior) probabilities can be used from CRF parsing models in a probabilistic CKY-like bottom-up parsing algorithm BIBREF14 which is non-greedy and optimal. Finally, discourse parsers do not discriminate between intra-sentential parsing (i.e., building the DTs for individual sentences) and multi-sentential parsing (i.e., building a DT for the whole document) BIBREF0 . Hence, this part of the analysis will extract for us distributed information about the relationship between different EDUs from parsed texts. Then we assign sentiment orientation to each EDU. Sentiment Analysis. A sentiment analysis can be made at the level of (1) the whole document, (2) the individual sentences, or (what is currently seen as the most", "our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets. Conclusions and Future Work. We have proposed a comprehensive flow of analysing aspects and assigning sentiment", "there are presented some examples of the results of our approach compared with the annotated data from Bing Liu's dataset. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps. Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but", "data is based commonly on detection of names or noun-phrases BIBREF15 and we used exactly this approach. Analysis of aspect inter-relations. The third step consists of an Aspect-Rhetorical Relation Graph (ARRG) and content Structuring Aspect Hierarchical Tree (see Figure FIGREF9 ). Discourse Trees of individual documents are processed (the order of EDU is not changed) to form association rules. Then, an Aspect-Rhetorical Relation Graph based on a set of these rules is created. Each node represents an aspect and each edge is one of the relations between the EDU\u2019s aspects. A graph will be created for all documents used in the experiment. The graph can be represented with weighted edges (association rules confidence, a number of such relations in the whole graph etc.), but there is a need to check and compare different types of graph representations. Then, it is possible to characterise the whole graph and each node (aspect) with graph metrics (PageRank BIBREF16 , degree, betweenness or", "were taken off from consideration to ensure that the discovered aspects are correlated with authors' emotions. The remaining EDUs were processed through part-of-speech tagger to extract nouns and noun phrases which we decided to treat as potential aspects. The result of this step was a set of Aspect-based Discourse Trees (ADTs). Then, from each ADT relations between aspects were extracted using breadth-first search, and an Aspect-Rhetorical Relation Graph (ARRG) was created by using aspects and relations such as nodes and edges respectively. Next, we evaluated the importance of aspects using a PageRank algorithm. Our approach resulted in complete list of aspects sorted by PageRank score. We applied a user-selected importance threshold to filter trivial aspects. Results. In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from Bing Liu's dataset. In the first sentence, the results of the analysis differ because we decided", "exist initial solutions that use SVM-based algorithms BIBREF12 or conditional random field classifiers BIBREF13 with manually engineered features. There also exist some solutions based on deep neural networks, such as connecting sentiments with the corresponding aspects based on the constituency parse tree BIBREF11 . Method for aspect-based sentiment analysis. The proposed Rhetorical and Sentiment Analysis flow is divided into four main tasks: The overall characteristics and flow organisation can be seen in Figure FIGREF9 . Each of the mentioned steps of the proposed method is described in the following subsections. Aspect detection in textual data. The second step covers aspect extraction and creation of aspect-based discourse trees ADT - see Figure FIGREF9 . Aspect detection from textual data is based commonly on detection of names or noun-phrases BIBREF15 and we used exactly this approach. Analysis of aspect inter-relations. The third step consists of an Aspect-Rhetorical Relation"]}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "predicted_answer": "No", "predicted_evidence": ["controversiality level ::: Validation ::: Random @!START@$k$@!END@-fold. We first examined the estimators in $k$-fold cross-validation scheme on the datasets I and II with $k=10$: the set of positive (controversial) concepts was split into 10 equal size sets, and the corresponding sentences were split accordingly. Each set was matched with similarly sized sets of negative (non-controversial) concepts and corresponding sentences. For each fold, a model was generated from the training sentences and used to score the test concepts. Scores were converted into a binary classification, as described in SECREF3, and accuracy was computed accordingly. Finally, the accuracy over the $k$ folds was averaged. Estimating a concept's controversiality level ::: Validation ::: Leave one category out. In a preliminary task, we looked for words which may designate sentences associated with controversial concepts. To this end, we ranked the words appearing in positive sentences according to their", "controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$. Recurrent neural network (RNN): A bidirectional RNN using the architecture suggested in BIBREF10 was similarly trained. The network receives as input a concept and a referring sentence, and outputs a score. The controversiality score of a concept is defined, as above, to be the average of these scores. Estimating a concept's controversiality level ::: Validation ::: Random @!START@$k$@!END@-fold. We first examined the estimators in $k$-fold cross-validation scheme on the datasets I and II with $k=10$: the set of positive", "Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy. Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I. In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that", "not seem to address this issue, probably because the meta-data used is less sensitive to it. Results. Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1, BIBREF4 with the accuracy obtained by our methods, as well as the latter on Dataset II, using 10-fold cross-validation in all cases. Table TABREF14 reports accuracy results of the more stringent analysis described in section SECREF13. BIBREF4 review several controversy classifiers. The most accurate one, the Structure classifier, builds, among others, collaboration networks by considering high-level behavior of editors both in their individual forms, and their pairwise interactions. A collaboration profile containing these individual and pairwise features is built for each two interacting editors and is classified based on the agreement or disagreement relation between them. This intensive computation renders that classifier impractical. Table TABREF14 therefore also includes the most", "editors and is classified based on the agreement or disagreement relation between them. This intensive computation renders that classifier impractical. Table TABREF14 therefore also includes the most accurate classifier BIBREF4 consider practical. As seen in Table TABREF14, for the usual 10-fold analysis the simple classifiers suggested here are on par with the best and more complex classifier reported in BIBREF4. Moreover, in the leave-one-category-out setting (Table TABREF14), accuracy indeed drops, but only marginally. We also observe the superiority of classifiers that directly access the context (NB and RNN) over classifiers that access it via word embedding (NN). Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators", "Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as reflecting the \u201cgeneral opinion\u201d about it in the corpus. This contrasts previous works, which consider this a binary problem, and employ a complex combination of features extracted from Wikipedia's article contents and inter-references, and more extensively \u2013 from the rich edit history thereof. Related work. Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles.", "the models trained on Dataset I. In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned. Estimating a concept's controversiality level ::: Controversiality Estimators. We employ three estimation schemes based on the textual contexts of concepts. The first relies on the context via pre-trained word embeddings of the concepts, which, in turn, are derived from the concepts' distributional properties in large samples of free texts. The other two schemes directly access the sentence-level contexts of the concepts. Nearest neighbors (NN)", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial. Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our"]}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "predicted_answer": "Datasets I, II, and III", "predicted_evidence": ["articles retrieved by manually selected query terms, believed to indicate controversy. BIBREF7 detect controversy in news items by inspecting terms with excessive frequency in contexts containing sentiment words, and BIBREF8 study controversy in user comments of news articles using lexicons. Finally, BIBREF9 suggest that controversy is not a universal but rather a community-related concept, and, therefore, should be studied in context. Here we measure a concept's controversiality from the explicit sentence-level context in which it is mentioned. In this, our approach is reminiscent of BIBREF10, who suggest a similar approach to detect abstract concepts. Estimating a concept's controversiality level ::: Datasets. We consider three datasets, two of which are a contribution of this work. Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at", "the models trained on Dataset I. In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned. Estimating a concept's controversiality level ::: Controversiality Estimators. We employ three estimation schemes based on the textual contexts of concepts. The first relies on the context via pre-trained word embeddings of the concepts, which, in turn, are derived from the concepts' distributional properties in large samples of free texts. The other two schemes directly access the sentence-level contexts of the concepts. Nearest neighbors (NN)", "humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested here as well as the new datasets. Acknowledgment. We are grateful to Shiri Dori-Hacohen and Hoda Sepehri Rad for sharing their data with us and giving us permission to use it.", "this as an indirect or implicit indication, since it is more related to the controversial theme than to controversiality per-se. To control for this effect, we performed a second experiment where we set the concepts from one category as the test set, and used the others for training (concepts associated with the excluded category are left out, regardless of whether they are also associated with one of the training categories). We did this for 5 categories: History, Politics and economics, Religion, Science, and Sexuality. This way, thematic relatedness observed in the training set should have little or no effect on correctly estimating the level of controversy associated of concepts in the test set, and may even \u201cmislead\u201d the estimator. We note that previous work on controversiality does not seem to address this issue, probably because the meta-data used is less sensitive to it. Results. Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1,", "we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote. Conclusions. We demonstrated that the sentence\u2013level context in which a concept appears is indicative of its controversiality. This follows BIBREF10, who show this for concept abstractness and suggest to explore further properties identifiable in this way. Importantly, we observed that this method may pick up signals which are not directly related to the property of interest. For example, since many controversial concepts have to do with religion, part of what this method may learn is thematic relatedness", "whereas Summer is not. However, the concept Pollution may be seen as neutral by some, yet controversial by others, who associate it with environmental debates. In other words, different people may have different opinions, potentially driven by different contexts salient in their mind. Yet, as reported in the sequel, an appreciable level of agreement can be reached, even without explicit context. Focusing here on Wikipedia concepts, we adopt as an initial \u201cground truth\u201d the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called \u201cedit wars\u201d. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset. To estimate the level of controversy associated with a Wikipedia concept, we propose to simply examine the words in the sentences in which the concept is referenced. Because a concept can often be found in multiple contexts, the estimation can be seen as", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial. Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our", "Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy. Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I. In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that"]}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "predicted_answer": "Dataset III consists of 3561 concepts.", "predicted_evidence": ["the models trained on Dataset I. In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned. Estimating a concept's controversiality level ::: Controversiality Estimators. We employ three estimation schemes based on the textual contexts of concepts. The first relies on the context via pre-trained word embeddings of the concepts, which, in turn, are derived from the concepts' distributional properties in large samples of free texts. The other two schemes directly access the sentence-level contexts of the concepts. Nearest neighbors (NN)", "humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested here as well as the new datasets. Acknowledgment. We are grateful to Shiri Dori-Hacohen and Hoda Sepehri Rad for sharing their data with us and giving us permission to use it.", "controversiality level ::: Validation ::: Random @!START@$k$@!END@-fold. We first examined the estimators in $k$-fold cross-validation scheme on the datasets I and II with $k=10$: the set of positive (controversial) concepts was split into 10 equal size sets, and the corresponding sentences were split accordingly. Each set was matched with similarly sized sets of negative (non-controversial) concepts and corresponding sentences. For each fold, a model was generated from the training sentences and used to score the test concepts. Scores were converted into a binary classification, as described in SECREF3, and accuracy was computed accordingly. Finally, the accuracy over the $k$ folds was averaged. Estimating a concept's controversiality level ::: Validation ::: Leave one category out. In a preliminary task, we looked for words which may designate sentences associated with controversial concepts. To this end, we ranked the words appearing in positive sentences according to their", "Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy. Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I. In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that", "are derived from the concepts' distributional properties in large samples of free texts. The other two schemes directly access the sentence-level contexts of the concepts. Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$. Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data \u2013 the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial. Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our", "signals which are not directly related to the property of interest. For example, since many controversial concepts have to do with religion, part of what this method may learn is thematic relatedness to religion. However, when controlling for this effect, the drop in accuracy is fairly small. The major advantages of our estimation scheme are its simplicity and reliance on abundantly accessible features. At the same time, its accuracy is similar to state-of-the-art classifiers, which depend on complex meta-data, and rely on sophisticated - in some cases impractical - algorithmic techniques. Because the features herein are so simple, our estimators are convertible to any corpus, in any language, even of moderate size. Recently, IBM introduced Project Debater BIBREF15, an AI system that debates humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested", "this as an indirect or implicit indication, since it is more related to the controversial theme than to controversiality per-se. To control for this effect, we performed a second experiment where we set the concepts from one category as the test set, and used the others for training (concepts associated with the excluded category are left out, regardless of whether they are also associated with one of the training categories). We did this for 5 categories: History, Politics and economics, Religion, Science, and Sexuality. This way, thematic relatedness observed in the training set should have little or no effect on correctly estimating the level of controversy associated of concepts in the test set, and may even \u201cmislead\u201d the estimator. We note that previous work on controversiality does not seem to address this issue, probably because the meta-data used is less sensitive to it. Results. Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1,"]}
