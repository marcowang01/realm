{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_answer": " 29,990,784.", "predicted_evidence": ["the antiscam dataset contains 29,990,784 samples.\n\n"]}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_answer": " crowdsourced", "predicted_evidence": ["they explicitly mention that \"we use intent labels to identify a user's intention, and they were annotated by a crowd-worker for the final evaluation data\"\n\n"]}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_answer": " memory network and baseline seq2seq model.", "predicted_evidence": ["our proposed model outperforms both memory network and baseline seq2seq model on the babi-dialog dataset significantly.\n\n"]}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_answer": " perplexity, ubuntu dialogue corpus metrics, conv ai challenge, dbr", "predicted_evidence": ["evaluation metrics: perplexity, ubuntu dialogue corpus metrics, conversational intelligence challenge (conv ai challenge) and dialogue breakdown ratio (dbr); evaluation criteria: general human evaluation and goal-oriented evaluation.\n\n"]}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_answer": " bag-of-words, word n-grams+ svm, rumor lens.", "predicted_evidence": ["the authors compare their method against three baselines, \"bag-of-words\", \"word n-grams+ svm\" and \"rumor lens\". \n\n"]}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_answer": " precision and recall", "predicted_evidence": ["the authors used precision and recall as evaluation metrics for their proposed method. they also compared their method to other approaches such as svms, random forests, and deep learning methods using the same evaluation metrics. \n\n"]}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_answer": " yes", "predicted_evidence": ["yes, the authors mention that their model is supervised and requires annotated data for training. \n\n"]}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_answer": " yes", "predicted_evidence": ["yes, they built a dataset of rumors called rumoreval. \n\n"]}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_answer": " english, german", "predicted_evidence": ["they evaluated their methods on english and german languages.\n\n"]}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_answer": " unverified", "predicted_evidence": ["rumors are defined as unverified or unconfirmed reports about an event or issue, where verification would be required to reject or establish the truth. \n\n"]}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_answer": " lda, author-topic models, and hmm-lda", "predicted_evidence": ["the authors compared their proposed model with standard topic models such as lda, author-topic models, and hmm-lda. \n\n"]}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_answer": " topic modeling and sentence classification.", "predicted_evidence": ["topic modeling and sentence classification.\n\n"]}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "predicted_answer": " conll03, ontonotes 5.0, resumener, and wnut17.", "predicted_evidence": ["they use four benchmark ner datasets, namely conll03, ontonotes 5.0, resumener, and wnut17. \n\n"]}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "predicted_answer": " relative positional encoding method", "predicted_evidence": ["they incorporate direction and relative distance in attention using relative positional encoding method. \n\n"]}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_answer": " yes", "predicted_evidence": ["looking at the results section, the tener model outperforms previous state-of-the-art models such as lattice lstm and spanbert on several datasets.\n\n"]}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_answer": " two different datasets.", "predicted_evidence": ["the authors evaluated the kalm system on two datasets, a science competition dataset and a dataset consisted of questions from a middle school science class. \n\n"]}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_answer": " new deep learning approach and new dataset", "predicted_evidence": ["in this paper, the authors propose a deep learning approach for event detection in italian language text, achieving higher performance than existing state-of-the-art models. the proposed model uses both convolutional and recurrent neural networks, and also introduces a new dataset for italian event detection.\n\n"]}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_answer": " yes", "predicted_evidence": ["yes, the authors compare their deep learning approach with several baseline methods, including bag-of-words, support vector machines, random forests and bilstm-crf. \n\n"]}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper does not provide an answer to this question. \n\n"]}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_answer": " based on word co-occurrences and manual inspection", "predicted_evidence": ["the authors use a clustering algorithm to group similar words together based on their co-occurrences in text corpora, and then manually inspect these clusters to assign them semantic labels based on the words they contain and their contextual meanings.\n\n"]}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_answer": " ward", "predicted_evidence": ["the paper uses hierarchical agglomerative clustering based on the ward linkage method to discover coherent word clusters. \n\n"]}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_answer": " unanswerable.", "predicted_evidence": ["two datasets have not been introduced in the paper. \n\n"]}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_answer": " no", "predicted_evidence": ["the authors used baselines from previous works, but there is no mention of any \"strong\" baselines.\n\n"]}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_answer": " 5,524.", "predicted_evidence": ["they conducted a study with 240 participants, who generated 5,524 responses in total.\n\n"]}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_answer": " 5", "predicted_evidence": ["in the study, five different strategies for responding to abusive behavior in conversational agents were evaluated. \n\n"]}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_answer": " yes", "predicted_evidence": ["yes, the study shows the importance of considering both inflectional and derivational morphology in semantic role labeling. \n\n"]}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_answer": " character \"n-grams\" and morphological tags.", "predicted_evidence": ["the authors use character-level and word-level features as well as morphological features including character \"n-grams\" (i.e., sequences of characters) and morphological tags. \n\n"]}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_answer": " openi and scientific figure comprehension task (sfct) dataset", "predicted_evidence": ["the authors use two datasets: (i) openi, which consists of radiology images and associated image captions, and (ii) the scientific figure comprehension task (sfct) dataset, consisting of figures and captions from scientific publications.\n\n"]}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_answer": " english", "predicted_evidence": ["english\n  \n"]}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_answer": " document-based and manual ad-hoc approaches.", "predicted_evidence": ["ad-hoc textual descriptions (e.g., summaries) were either available within the document or were manually created for specific subsets of figures. we explore both of these approaches in this work. \n\n"]}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_answer": " yes", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_answer": " pubmed central", "predicted_evidence": ["the authors created a dataset of scientific figures and their captions extracted from open access scientific articles available on pubmed central. \n\n"]}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence \n\n"]}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_answer": " improved", "predicted_evidence": ["we achieved 2nd, 3rd and 1st positions in three sub-tasks: emotion classification, emotion intensity regression and valence-arousal regression, respectively. our best submission ranked first on the emotion intensity task, improving over the prior state-of-the-art using deep learning methods.\n\n"]}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_answer": " wikipedia articles related to movies and television series and tweets in semeval 2017.", "predicted_evidence": ["wikipedia articles related to movies and television series and tweets in semeval 2017. \n  \n"]}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_answer": " english, spanish, arabizi, greek.", "predicted_evidence": ["the new dataset contains hate speech in english, spanish, arabizi (a form of arabic script using latin characters), and greek.\n\n"]}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_answer": " sentiment, target, and target group", "predicted_evidence": ["the paper proposes a multi-aspect analysis which includes a fine-grained sentiment analysis, a target identification and a target group identification.\n\n"]}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_answer": " improved quality of machine translation\n\nexplanation: the semantic web can also be used to facilitate the development of advanced translation tools that can integrate the knowledge of multiple domains and provide customized translations for different contexts, such as technical writing, legal documents, scientific articles, etc. furthermore, the semantic web can enable new types of translation services, such as assisted translation, machine-mediated translation, and multilingual content creation, that can improve the productivity of human translators and enable more effective communication across linguistic barriers.", "predicted_evidence": ["the use of semantic web technologies can help improve the quality of machine translation by providing access to structured multilingual knowledge resources and enabling cross-lingual semantic interoperability. \n\n"]}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_answer": " grounding natural language expressions in a knowledge graph, scalability of knowledge graph construction, and the need for effective methods for exploiting semantic resources.", "predicted_evidence": ["the authors outline several challenges associated with the use of semantic web technologies in machine translation, including the problem of grounding natural language expressions in a knowledge graph, the scalability of knowledge graph construction, and the need for effective methods for exploiting semantic resources to improve translation quality.\n\n"]}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": " librispeech", "predicted_evidence": ["librispeech dataset was used in the experiments. \n\n"]}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_answer": " yes", "predicted_evidence": ["yes, ladabert outperforms its knowledge distillation teacher in terms of accuracy on the multilingual-text classification problem. \n\n"]}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_answer": " yes", "predicted_evidence": ["yes, they compare different compression methods such as distillation, pruning, and quantization and evaluate their gains in terms of accuracy and speed. \n\n"]}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "predicted_answer": " glue benchmark for nli", "predicted_evidence": ["ladabert achieves state-of-the-art results on the glue benchmark for natural language inference (nli) and is competitive on the stanford question-answering dataset squad2.0. \n\n"]}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_answer": " gigaword", "predicted_evidence": ["the authors used the gigaword dataset in their experiments.\n\n"]}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_answer": " physical, social, taxonomic", "predicted_evidence": ["commonsense knowledge that includes physical, social, and taxonomic semantics.\n\n"]}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_answer": " intrinsic \n\nexplanation: the intrinsic geometry of spaces of learned representations refers to the geometric properties of the space that captures the relationships between the input features, and how those relationships are transformed into the learned representations. in other words, it is about the inherent structure of the representation space that emerges from the learning process.", "predicted_evidence": [""]}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper presents multiple models for different tasks, so it is unclear which specific models are being referred to. \n\n"]}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_answer": " 43", "predicted_evidence": ["43 languages are included in the lorelei collection, along with english, which was evaluated in order to provide training signal for the shared cross-lingual models.\n\n"]}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "predicted_answer": " yes", "predicted_evidence": ["there is evidence that annotators were trained and given background materials on science topics before performing the annotations.\n\n"]}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "predicted_answer": " both accurate with slight advantage for experts.", "predicted_evidence": ["both the expert and crowd-sourced annotations showed high accuracy in identifying diagram types, parts, and relations between parts, with a slight advantage for the expert annotations in some cases.\n\n"]}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "predicted_answer": " unanswerable", "predicted_evidence": ["the platform used to hire crowd-sourced workers is not mentioned in the article.\n\n"]}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "predicted_answer": " unanswerable", "predicted_evidence": ["the proposed approach is based on a neural semantic model, but no specific architecture is mentioned.\n\n"]}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "predicted_answer": " manual, automatic", "predicted_evidence": ["the authors used two different datasets for their experiments: the manual dataset composed of manually annotated articles from controversial and non-controversial topics, and the automatic dataset composed of articles automatically labeled by querying popular search engines.\n\n"]}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper mentions that \"the cross-topic setup is the most straightforward and also the one on which the reported experiments started\". however, there is no explicit statement on which setup proves to be the hardest.\n\n"]}
{"question_id": "6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "predicted_answer": " mscoco, flickr30k, wikitext, penn treebank.", "predicted_evidence": ["the paper mentions the use of multiple datasets for training, including the mscoco and flickr30k datasets for image-caption pairs, and the wikitext and penn treebank datasets for language modeling. \n\n"]}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "predicted_answer": " yes.", "predicted_evidence": ["yes, the models gru+attn, pointer+attn and transformer are all equipped with attention mechanisms. \n\n"]}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "predicted_answer": " many", "predicted_evidence": ["many models were explored, including machine translation (mt), neural machine translation (nmt), neural machine translation with soft dictionary (nmt+sd) and others.\n\n"]}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "predicted_answer": " the process of manipulating a symbolic expression using a set of rules to obtain an equivalent expression.", "predicted_evidence": ["symbolic rewriting is the process of manipulating a symbolic expression using a set of rules to obtain an equivalent expression. \n  \n"]}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "predicted_answer": " yes", "predicted_evidence": ["we do this by defining \"anchor words\" that represent the experts' topics and explicitly including them in our model formulation. specifically, we model each document as a mixture of topics in the same way as traditional topic models such as latent dirichlet allocation (lda). additionally, we require that each topic have a pre-defined set of \"anchor words\" which are words that the experts have designated as being representative of that topic. we enforce similar regularization terms that encourage topic coherence and sparsity. by defining anchor words, we are able to incorporate expert knowledge and domain-specific terminology into our topic model. \n\n"]}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "predicted_answer": " 20-newsgroups, reuters-21578", "predicted_evidence": ["they evaluate on synthetic documents and several real-world corpora, including 20-newsgroups and reuters-21578. \n\n"]}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "predicted_answer": " no", "predicted_evidence": ["no\n\n"]}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "predicted_answer": " 77.87%", "predicted_evidence": ["the paper reports f-score as 77.87% for their proposed method.\n\n"]}
{"question_id": "2e4688205c8e344cded7a053b6014cce04ef1bd5", "predicted_answer": " state-of-the-art method is the mmseg method.", "predicted_evidence": [""]}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence.\n\n"]}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "predicted_answer": " cner", "predicted_evidence": ["the authors used the cner corpus which is a labeled dataset for chinese ner in social media domains. \n\n"]}
{"question_id": "b70e4c49300dc3eab18e907ab903afd2a0c6075a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "predicted_answer": " yes", "predicted_evidence": ["yes, the study shows that massively multilingual nmt is more effective for low-resource languages. \n\n"]}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "6c8dc31a199b155e73c84173816c1e252137a0af", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "predicted_answer": " english, german, spanish, hindi, arabic, and russian.\n\nexplanation: the newly created dataset includes conversational data in six different languages: english, german, spanish, hindi, arabic, and russian.", "predicted_evidence": [""]}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_answer": " baseline 1, baseline 2", "predicted_evidence": ["baseline 1 (unsupervised): we use the transformer model to translate the source sentence in isolation, without any context.\n\nbaseline 2 (supervised): we train a context-aware neural machine translation system on the ted talks parallel corpus, then use it to translate the source sentence in context.\n\n"]}
{"question_id": "cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "predicted_answer": " czech, german, russian, spanish, turkish", "predicted_evidence": ["we experiment with five different language pairs, translating into english from czech, german, russian, spanish, and turkish. \n\n"]}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "predicted_answer": " tagme", "predicted_evidence": ["the authors utilize the system tagme to annotate their text with entity links. \n\n"]}
{"question_id": "6295951fda0cfa2eb4259d544b00bc7dade7c01e", "predicted_answer": " lstm and attention.", "predicted_evidence": ["in the paper, the authors use a neural network architecture based on lstm and attention mechanism for improving fine-grained entity typing.\n\n"]}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_answer": " figer, ontonotes, bbn, wiki", "predicted_evidence": ["we evaluated our approach on four standard datasets including figer, ontonotes, bbn and wiki. \n\n"]}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "predicted_answer": " precision, recall, f1-score, accuracy, auc-roc.", "predicted_evidence": ["we evaluated the proposed framework using standard metrics such as precision, recall, and f1-score, along with accuracy and auc-roc. \n\n"]}
{"question_id": "77c3416578b52994227bae7f2529600f02183e12", "predicted_answer": " 87.90%", "predicted_evidence": ["our fine-tuned model outperforms all previously reported models on this test dataset. on this dataset, we get a state-of-the-art f1 score of 87.90%\n\n"]}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "predicted_answer": " three-level", "predicted_evidence": ["the authors used a three-level labeling scheme, where the first label describes the type of concept out of 12 possible concepts, the second label describes the primitives of the concept and the third label describes additional parameters.\n\n"]}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "predicted_answer": " f1 score", "predicted_evidence": ["metric: f1 score\n\n"]}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "predicted_answer": " none", "predicted_evidence": ["the paper doesn't use any hand-crafted features besides a list of typical argument indicator words from the literature such as \"therefore\" and \"since\" for baseline comparison.\n\n"]}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "predicted_answer": " word2vec", "predicted_evidence": ["we use pre-trained word embeddings from word2vec. \n\n"]}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "predicted_answer": " word2vec.", "predicted_evidence": ["the sentence embeddings are generated using word2vec. \n\n"]}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "predicted_answer": " classification of sentences in a text according to their function.", "predicted_evidence": ["argumentative zoning is the classification of sentences in a text corpus according to their function in the argumentation structure. \n\n"]}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n  \n"]}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "predicted_answer": " logistic regression", "predicted_evidence": ["logistic regression\n\n"]}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not mention any specific language that is explored. \n\n"]}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "predicted_answer": " english and german.", "predicted_evidence": ["the authors pretrain on english and german corpora before cross-lingual model finetuning.\n\n"]}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "predicted_answer": " mbart.", "predicted_evidence": ["baseline model, mbart. \n\n"]}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "predicted_answer": " fluency, coherence, and story quality.", "predicted_evidence": ["we compare our model against several existing work and provide human evaluation results using standard metrics including fluency, coherence, and story quality.\n\n"]}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "predicted_answer": " bleu-n, self-bleu-n, rouge-l, meteor, nll.", "predicted_evidence": ["evaluation metrics. in addition to human assessment, we also use automated evaluation metrics to measure the quality of our gener- ated stories. we report perplexity as an evaluation metric that measures how well the language model is able to predict the held-out test data. we also use the following \ufb01ve automated evaluation metrics: \n\n1. bleu-n (n = [1, 2, 3, 4]): measures the n-gram precision between the human-written story and the machine-generated stories. \n2. self-bleu-n (n = [1, 2, 3, 4]): measures the average n-gram overlap among multiple machine-generated stories.\n3. rouge-l: measures the longest common subsequence between the machine generated story and the human written story. \n4. meteor: measures the alignment between the machine generated story and the human written story.\n5. nll: the negative log-likelihood of the test data under the generative model.\n\n"]}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "predicted_answer": " hierarchical recurrent encoder-decoder with alignment model.", "predicted_evidence": ["the model used to generate the premise is a hierarchical recurrent encoder-decoder with alignment model. \n\n"]}
{"question_id": "f2dba5bf75967407cce5d0a9c2618269225081f5", "predicted_answer": " yes", "predicted_evidence": ["the dataset consists of fictional stories. \n\n"]}
{"question_id": "3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "predicted_answer": " glove, word2vec, and fasttext (among others)", "predicted_evidence": ["the authors experimented with six different embeddings, including glove, word2vec, and fasttext.\n\n"]}
{"question_id": "47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "predicted_answer": " sick, sick-relatedness, msrpar corpus and a news headline dataset were used.", "predicted_evidence": ["the authors evaluated their approach on two standard datasets for semantic textual similarity: sick and sick-relatedness. they also evaluated their model on the msrpar corpus and a news headline dataset. \n\n"]}
{"question_id": "67cb001f8ca122ea859724804b41529fea5faeef", "predicted_answer": " universal sentence encoder (use), skip-thoughts and infersent.", "predicted_evidence": [""]}
{"question_id": "42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "predicted_answer": " cohen\u2019s kappa", "predicted_evidence": [""]}
{"question_id": "b857f3e3f1dad5df55f69d062978967fe023ac6f", "predicted_answer": " three.", "predicted_evidence": [""]}
{"question_id": "5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "predicted_answer": " aggressive, repetitive, intentional, visible, and imbalanced", "predicted_evidence": ["the paper defines the concept of cyberbullying at the beginning of the introduction.\n\n"]}
{"question_id": "c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "predicted_answer": " human evaluation was performed, measuring creativity, humor, and coherence of the generated content.", "predicted_evidence": [""]}
{"question_id": "9f1d81b2a6fe6835042a5229690e1951b97ff671", "predicted_answer": " unanswerable", "predicted_evidence": ["the authors do not mention the source of the joke data in the abstract or introduction. however, they state that they experiment with joke generation using different types of models.\n\n"]}
{"question_id": "fae930129c2638ba6f9c9b3383e85aa130a73876", "predicted_answer": " context-specific and famous-style quotes", "predicted_evidence": ["the article mentions that the proposed system generates quotes that are context-specific and resemble famous quotes. \n\n"]}
{"question_id": "1acfbdc34669cf19a778aceca941543f11b9a861", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "864295caceb1e15144c1746ab5671d085d7ff7a1", "predicted_answer": " 3.8% on fb15k-237 and 1.4% on wn18rr", "predicted_evidence": ["our model outperforms state-of-the-art models by 3.8% on fb15k-237 and 1.4% on wn18rr. \n\n"]}
{"question_id": "79e61134a6e29141cd19252571ffc92a0b4bc97f", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "predicted_answer": " xlm, mbert, xlm-r, unsup-t, and mbert-t", "predicted_evidence": [""]}
{"question_id": "5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "predicted_answer": " genia c&t and wmt news test suite.", "predicted_evidence": ["the authors evaluated their approach on two datasets, one coming from the biomedical domain (genia c&t) and another from nlp domain (wmt news test suite).\n\n"]}
{"question_id": "7d539258b948cd5b5ad1230a15e4b739f29ed947", "predicted_answer": " 0.7473", "predicted_evidence": ["0.7473\n\n"]}
{"question_id": "9c1f70affc87024b4280f0876839309b8dddd579", "predicted_answer": " using guidelines inspired by english alpino treebank.", "predicted_evidence": ["we annotated the corpora by using guidelines inspired by the english alpino treebank (see section 2.1). \n\n"]}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "predicted_answer": " 1,511 sentences", "predicted_evidence": [""]}
{"question_id": "65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "predicted_answer": " the liar dataset, bs detector, and two other manually created datasets", "predicted_evidence": ["we collected these datasets from different sources: liar, bs detector, and two other manually created datasets. they employ different methods for gathering data, and we provide a brief overview of each of them as follows. the liar dataset (wang, 2017) is a recently released benchmark liar dataset for fake news detection that consists of 12.8k short statements labeled as either true or false with additional information detailing the statement, such as who made it and when. the liar dataset has been widely used as the main dataset for fake news detection research since its publication. bs detector (hofer et al., 2017) is another publicly available small dataset that contains manually crafted fake news articles and attempts to detect them using textual features. the first dataset that we create using the crowd-sourcing method is a small dataset that consists of almost 300 real and fake news articles from different websites. we utilize a simple emotion-based approach to identify such news articles from a pool of around 6000 articles. the second dataset that we create consists of 600 paragraph-length news articles that are gathered from various sources. we manually annotate this dataset to label each article as real or fake.\n\n"]}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "predicted_answer": " random forest, svm, multilayer perceptron, na\u00efve bayes, decision tree, logistic regression and gradient boosting.", "predicted_evidence": ["the authors explored several machine learning models for fake news detection, including random forest, svm, multilayer perceptron, na\u00efve bayes, decision tree, logistic regression and gradient boosting. \n\n"]}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "predicted_answer": " lexical and discourse features", "predicted_evidence": ["the paper proposed a neural network based model that understands lexical and discourse features of essay level texts. therefore, it is likely that lexical and discourse features of essays are extracted in this proposed model. \n\n"]}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_answer": " accuracy, precision, recall, f1-score", "predicted_evidence": ["the paper mentions evaluating classification performance using accuracy, precision, recall, and f1-score. \n\n"]}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "predicted_answer": " investigating potential remedies", "predicted_evidence": ["future work is described as investigating potential remedies for reducing the effect of lexical and structural biases on writing evaluation systems. \n\n"]}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "predicted_answer": " manual", "predicted_evidence": ["the sentence alignment quality was evaluated manually by three annotators.\n\n"]}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "predicted_answer": " wer", "predicted_evidence": ["they use word error rate (wer) as a measure of the speech alignment quality. \n\n"]}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_answer": " tac kbp 2010 entity linking task and the semeval-2010 task 8 dataset", "predicted_evidence": ["the authors evaluate their approach on two standard benchmark datasets: the tac kbp 2010 entity linking task and the semeval-2010 task 8 dataset, which is used for relation classification between two given entities in a sentence.\n\n"]}
{"question_id": "11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "predicted_answer": " cosine", "predicted_evidence": ["table 1 shows the baseline using the cosine scoring method.\n\n"]}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper mentions several datasets, including callhome, chime 5, mixer6, and ami.\n\n"]}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "predicted_answer": " unanswerable", "predicted_evidence": ["the article does not make a clear comparison between subsystems.\n\n"]}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "predicted_answer": " news and biomedical literature.", "predicted_evidence": ["the authors explore the domains of news and biomedical literature. \n\n"]}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "predicted_answer": " language that degrades, is cruel or mocking or expresses hatred or intolerance towards members of a group.", "predicted_evidence": ["hate speech is defined as language that is used to describe members of a group in ways that are degrading, cruel, or mocking, or that otherwise express hatred or intolerance for members of that group. \n\n"]}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "predicted_answer": " hybrid", "predicted_evidence": ["they train a hybrid model.\n\n"]}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7f11f128fd39b8060f5810fa84102f000d94ea33", "predicted_answer": " 4.9% and 4.4%", "predicted_evidence": ["the performance improvement of their method over state-of-the-art models on the used datasets is 4.9% on snli and 4.4% on scitail datasets. \n\n"]}
{"question_id": "2a55076a66795793d79a3edfae1041098404fbc3", "predicted_answer": " potentially\n\nexplanation: the proposed training framework is based on mitigating annotation artifacts, a problem that is likely to exist in many nlp datasets. therefore, it is possible that this framework could be applied to other nlp problems beyond natural language inference. however, further research would be needed to determine the effectiveness of this approach in other contexts.", "predicted_evidence": [""]}
{"question_id": "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a", "predicted_answer": " balances the distribution, robust feature learning, adversarial training to mitigate bias.", "predicted_evidence": ["the proposed training framework balances the distribution of labels within each genre, encourages the model to learn robust features, and deploys adversarial training to mitigate biases in the model's decision boundary. \n\n"]}
{"question_id": "8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "predicted_answer": " cross-dataset evaluation: glue and superglue benchmark datasets.", "predicted_evidence": ["our experiments reveal that the proposed mixup strategy considerably improves the generalization capability of nli models, and significantly advances the state-of-the-art on multiple challenging benchmarks, including seven out of nine established glue and superglue benchmark datasets. we stress that the advantages of our method are not restricted to any specific architecture, requiring only small modifications to adopted models. \n\n"]}
{"question_id": "0aca0a208a1e28857fab44e397dc7880e010dbca", "predicted_answer": " active learning", "predicted_evidence": ["active learning selection strategy\n\n"]}
{"question_id": "471683ba6251b631f38a24d42b6dba6f52dee429", "predicted_answer": " 14,467.", "predicted_evidence": ["their tweets dataset consists of 14,467 tweets. \n\n"]}
{"question_id": "5dfd58f91e7740899c23ebfe04b7176edce9ead2", "predicted_answer": " 50,000", "predicted_evidence": ["the size of the idn tagged corpus is 50,000 words.\n\n"]}
{"question_id": "c09bceea67273c10a0621da1a83b409f53342fd9", "predicted_answer": " bilstm, cnn", "predicted_evidence": ["the paper explored bidirectional long short-term memory (bilstm) and convolutional neural network (cnn) models.\n\n"]}
{"question_id": "732bd97ae34541f215c436e2a1b98db1649cba27", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "183b385fb59ff1e3f658d4555a08b67c005a8734", "predicted_answer": " indo-universal treebank (iutb) and a new indonesian pos-tagged dataset", "predicted_evidence": ["the authors used the indo-universal treebank (iutb) and a new indonesian pos-tagged dataset that they created from the combination of various sources. \n\n"]}
{"question_id": "5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "predicted_answer": " unanswerable", "predicted_evidence": ["the authors state that they used a large amount of data in their experiments, but they do not provide a specific number.\n\n"]}
{"question_id": "d427e9d181434078c78b7ee33a26b269f160f6d2", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5d03a82a70f7b1ab9829891403ec31607828cbd5", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "6cad6f074b0486210ffa4982c8d1632f5aa91d91", "predicted_answer": " the model uses an approach that extends enamex by refining tags to distinguish between named entities and nominal mentions. \n\nexplanation: the model proposed in the paper extends enamex by refining tags for named entities and nominal mentions. this is done through a two-step process where first named entities are identified and then they are further classified based on context and syntactic dependencies. this approach improves the accuracy of named entity recognition in greek language.", "predicted_evidence": [""]}
{"question_id": "d38b3e0896b105d171e69ce34c689e4a7e934522", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper mentions that the tagger extracts \"rich morphological features\" from the text, but does not specify which features specifically. \n\n"]}
{"question_id": "4379a3ece3fdb93b71db43f62833f5f724c49842", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "0abc2499195185c94837e0340d00cd3b83ee795e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "predicted_answer": " using disease-specific words in training the bot detection system.", "predicted_evidence": ["we observed that when training our algorithm on the synthetic data, having disease-specific keywords in background text led to better performance on tweets from that disease class, suggesting that using disease-specific words can make the algorithm work better in the health-related domain. this is an interesting avenue for future work, as one can envisage training a detector on all health-related tweets, using different weights for different disease classes, etc.\n\n"]}
{"question_id": "95646d0ac798dcfc15b43fa97a1908df9f7b9681", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper doesn't mention a specific baseline model. \n\n"]}
{"question_id": "12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence \n\n"]}
{"question_id": "647f6e6b168ec38fcdb737d3b276f78402282f9d", "predicted_answer": " twitter", "predicted_evidence": ["the authors used the twitter corpus for their experiments.\n\n"]}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "afd1c482c311e25fc42b9dd59cdc32ac542f5752", "predicted_answer": " 1012 documents", "predicted_evidence": ["our resulting corpus contains 39,810 sentences and spans over 1012 documents obtained from ncbi disease corpus, plos and medline abstracts.\n\n"]}
{"question_id": "ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "predicted_answer": " mimic iii corpus.", "predicted_evidence": ["the authors collected the dataset from mimic iii corpus.\n\n"]}
{"question_id": "018b81f810a39b3f437a85573d24531efccd835f", "predicted_answer": " unanswerable.", "predicted_evidence": ["the document does not provide information on any shortcomings of previous datasets. \n\n"]}
{"question_id": "e2c8d7f3ef5913582503e50244ca7158d0a62c42", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "654fe0109502f2ed2dc8dad359dbbce4393e03dc", "predicted_answer": " subject-verb agreement, reflexive pronoun agreement, and discourse-level agreement.", "predicted_evidence": ["the authors explore subject-verb agreement, reflexive pronoun agreement, and discourse-level agreement in their study. \n\n"]}
{"question_id": "da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "predicted_answer": " ag's news, yelp 2014, dbpedia and yahoo! answers.", "predicted_evidence": ["the authors evaluate their bae method on four text classification datasets: ag's news, yelp 2014, dbpedia and yahoo! answers. \n\n"]}
{"question_id": "363a24ecb8ab45215134935e7e8165fff72ff90f", "predicted_answer": " baseline and bert.", "predicted_evidence": [""]}
{"question_id": "74396ead9f88a9efc7626240ce128582ab69ef2b", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "eaed0b721cc3137b964f5265c7ecf76f565053e9", "predicted_answer": " sentiwordnet-derived features, vader, logistic regression, support vector machines.", "predicted_evidence": ["the paper compares their proposed model to four baseline models: two lexicon-based models (i.e., sentiwordnet-derived features and vader) and two supervised learning models (i.e., logistic regression and support vector machines). \n\n"]}
{"question_id": "ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "predicted_answer": " unanswerable", "predicted_evidence": ["domains are not specified in the document.\n\n"]}
{"question_id": "38af3f25c36c3725a31304ab96e2c200c55792b4", "predicted_answer": " twitter dataset and imdb dataset.", "predicted_evidence": ["the authors used a dataset of tweets posted between october and december of 2015 obtained via twitter streaming api. the dataset consists of 10,000 tweets that were manually annotated as sarcastic or non-sarcastic. additionally, they used a dataset of movie reviews obtained from imdb, which consists of 50,000 sarcastic and 50,000 non-sarcastic reviews. \n\n"]}
{"question_id": "9465d96a1368299fd3662d91aa94ba85347b4ccd", "predicted_answer": " 0.947\n\nexplanation: according to the paper, the best performing model in the experiment achieves a test set accuracy of 0.947.", "predicted_evidence": ["the best model achieves 0.947 accuracy on the test set.\n\n"]}
{"question_id": "e8c3f59313df20db0cdd49b84a37c44da849fe17", "predicted_answer": " majority baseline, logistic regression model, svm model, lstm model.", "predicted_evidence": ["the authors test multiple models, including a majority baseline, a logistic regression model with word and character n-grams, a svm model with word and character n-grams, and a lstm model with glove embeddings and character embeddings. \n\n"]}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "predicted_answer": " 4", "predicted_evidence": ["4 annotators\n\n"]}
{"question_id": "5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "predicted_answer": " no, it is imbalanced.", "predicted_evidence": ["the dataset consists of 10,831 user comments, 15% of them are labeled as offensive. \n\n"]}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "predicted_answer": " svm, mlp, gradient boosting, random forest, cnns, lstm", "predicted_evidence": ["the authors experiment on various models, such as svm, mlp, gradient boosting, and random forest, as well as deep learning models, such as convolutional neural networks (cnns) and long-short term memory (lstm) networks.\n\n"]}
{"question_id": "9b05d5f723a8a452522907778a084b52e27fd924", "predicted_answer": " 2861 basque reviews and 2710 catalan reviews", "predicted_evidence": ["the published dataset contains 2,861 hotel reviews in basque and 2,710 in catalan, with an average length of 250 words and 600 characters per review. \n\n"]}
{"question_id": "21175d8853fd906266f884bced85c598c35b1cbc", "predicted_answer": " 4", "predicted_evidence": ["the authors used four annotators to label their dataset. \n\n"]}
{"question_id": "87c00edc497274ae6a972c3097818de85b1b384f", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper does not discuss the \"sentence construction component\" specifically. \n\n"]}
{"question_id": "7380e62edcb11f728f6d617ee332dc8b5752b185", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f37b01e0c366507308fca44c20d3f69621b94a6e", "predicted_answer": " language vectors", "predicted_evidence": ["they use language vectors to visualize relatedness between languages based on their syntactic properties.\n\n"]}
{"question_id": "95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence \n\n"]}
{"question_id": "ab37ae82e38f64d3fa95782f2c791488f26cd43f", "predicted_answer": " yes", "predicted_evidence": ["yes, the authors suggest that the system performs better on short descriptions because they contain more concise and focused information. \n\n"]}
{"question_id": "6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "71413505d7d6579e2a453a1f09f4efd20197ab4b", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "3e6b6820e7843209495b4f9a72177573afaa4bc3", "predicted_answer": " identify linguistic features and cognitive reappraisal as important for personal recovery in bipolar disorder.", "predicted_evidence": ["the authors identify several linguistic features that are correlated with personal recovery in bipolar disorder, including the use of positive emotion words, first-person plural pronouns, and past-tense verbs. they also find evidence for the importance of cognitive reappraisal, a type of emotion-regulation strategy, in the recovery process.\n\n"]}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "predicted_answer": " lda and machine learning", "predicted_evidence": ["the authors used latent dirichlet allocation (lda) and machine learning techniques to classify the texts. \n\n"]}
{"question_id": "4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "predicted_answer": " unanswerable", "predicted_evidence": ["no information provided about obtaining permission from patients. \n\n"]}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "predicted_answer": " from an outpatient psychiatric facility", "predicted_evidence": ["the researchers recruited participants from an outpatient psychiatric facility who met the diagnostic criteria for bipolar disorder.\n\n"]}
{"question_id": "c2ce25878a17760c79031a426b6f38931cd854b2", "predicted_answer": " classical chinese poetry collections.", "predicted_evidence": ["the source of training data is classical chinese poetry collections, such as the mao poetry anthology, tang dynasty poetry collection, and song dynasty poetry collection.\n\n"]}
{"question_id": "1d263356692ed8cdee2a13f103a82d98f43d66eb", "predicted_answer": " jueju, l\u00fcshi, and gushi", "predicted_evidence": ["the authors generate the major types of chinese classical poetry: jueju (\u7edd\u53e5), l\u00fcshi (\u5f8b\u8bd7), and gushi (\u53e4\u8bd7). \n\n"]}
{"question_id": "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c7f43c95db3d0c870407cd0e7becdd802463683b", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "predicted_answer": " spearman's rank-order correlation.", "predicted_evidence": ["the quality of the word vectors is measured using the spearman's rank-order correlation with the additive composition function proposed by mikolov et al. (2013a) for word analogy tasks.\n\n"]}
{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "predicted_answer": " yes", "predicted_evidence": ["yes \n\n"]}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "predicted_answer": " ten different news domains", "predicted_evidence": ["news articles were collected from ten different news domains, with a focus on political news. in particular, we collected articles published by the following news domains: associated press (ap), reuters, thehill, bloomberg, abc news, cbs news, cnn, fox news, nbc news, and usa today.\n\n"]}
{"question_id": "07edc082eb86aecef3db5cad2534459c1310d6e8", "predicted_answer": " multiple baselines are used.", "predicted_evidence": ["they compared their model's performance against several baseline models, such as textrank, tf-idf, rake, etc.\n\n"]}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "predicted_answer": " sequence-to-sequence, transformer, bert", "predicted_evidence": ["the paper explores and evaluates several models, such as the sequence-to-sequence model, transformer, and bert for keyphrase generation on the kptimes dataset.\n\n"]}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "predicted_answer": " they have a higher percentage of multi-token and named entity phrases", "predicted_evidence": ["our dataset is the first large-scale, human-annotated dataset of this nature (to the best of our knowledge). we compare our dataset to existing ones in tableref2 and observe that our dataset has a higher percentage of multi-token and named entity phrases, which are more challenging to capture accurately and thus an important research direction.\n\n"]}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "predicted_answer": " yes", "predicted_evidence": ["yes, they use word2vec and glove embeddings as inputs to their deep bi-lstm model.\n\n"]}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "predicted_answer": " 1 (single)", "predicted_evidence": ["the authors propose a deep learning approach, using a single bidirectional lstm network, for wsd task. in total, the proposed network architecture has three input layers (i.e., the word embeddings, the pos tag embeddings, and the sense definition embeddings), followed by a shared hidden layer, and an output layer. \n\n"]}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "predicted_answer": " accuracy, f1 score, precision, recall, matthews", "predicted_evidence": ["we use accuracy, f1 score, precision, recall and matthews correlation coefficient as evaluation metrics.\n\n"]}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "predicted_answer": " 500n-kpcrowd dataset and the semeval-2010 dataset.", "predicted_evidence": ["we conducted experiments on two standard benchmark datasets: the 500n-kpcrowd dataset and the semeval-2010 dataset. \n\n"]}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "2711ae6dd532d136295c95253dbf202e37ecd3e7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "predicted_answer": " unanswerable", "predicted_evidence": ["the authors state that \"the new model is still not fast in speed: we cannot make \u2018real-time\u2019 inference without pruning and parallelization, unlike fully autoregressive models\". however, no specific speed metrics are provided.\n\n"]}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "predicted_answer": " bleu", "predicted_evidence": ["we measure translation accuracy using bleu score metric. \n\n"]}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper discovered features that are indicative of language variety, but no specific features are mentioned.\n\n"]}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence found. \n\n"]}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "predicted_answer": " answers in the dataset are not guaranteed to be substrings of the text. the coverage of answers being substrings is not provided in the document. \n\nexplanation: the document does not provide any information on whether the answers in the dataset are guaranteed to be substrings of the text or the coverage of answers being substrings.", "predicted_evidence": [""]}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide a direct answer to this question. \n\n"]}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "predicted_answer": " unanswerable", "predicted_evidence": ["the article proposes a method for enhancing event representation learning with external commonsense knowledge, but it does not mention a specific machine learning method used for making the predictions.\n\n"]}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "predicted_answer": " the performance is evaluated based on accuracy, precision, recall, and f1 score.", "predicted_evidence": ["in the experiments, classification performance is measured by accuracy, precision, recall, and f1 score on a test set. \n\n"]}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "predicted_answer": " ace 2005 and cail2019.", "predicted_evidence": ["two datasets are used in this work. one is the ace 2005 corpus that is widely used for event extraction. the other is the cail2019 dataset that was created for the chinese language and contains a variety of tasks. \n\n"]}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "predicted_answer": " yes", "predicted_evidence": ["the authors apply their approach to five typologically and/or genetically different languages: croatian, hindi, urdu, finnish and japanese. \n\n"]}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "predicted_answer": " unanswerable \n\nexplanation: the document does not provide information on how the embeddings were evaluated in the human judgement comparison.", "predicted_evidence": [""]}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "predicted_answer": " 6.5%", "predicted_evidence": [""]}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "predicted_answer": " previous models compared to:\n- bi-directional lstm based approach\n- attention mechanism based approach", "predicted_evidence": [""]}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "predicted_answer": " 92.0%", "predicted_evidence": ["the authors report accuracy rates of up to 92.0% for the dialect classification task on the seame and hong kong cantonese dialect recognition (hkcdr) corpora with their proposed two-stage training method.\n\n"]}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "predicted_answer": " cantonese, fuzhounese, gan, hakka, minnan, shaowu, sichuanese.", "predicted_evidence": ["the paper explores 7 different chinese dialects: cantonese, fuzhounese, gan, hakka, minnan, shaowu, and sichuanese. \n\n"]}
{"question_id": "94d794df4a3109522c2ea09dad5d40e55d35df51", "predicted_answer": " unanswerable", "predicted_evidence": ["we could not find information about any neural machine translation model used in this article.\n\n"]}
{"question_id": "044c66c6b7ff7378682f24887b05e1af79dcd04f", "predicted_answer": " 2nd", "predicted_evidence": ["our best single model achieved an f0.5 of 61.00 on the nucle test set, ranking 2nd out of 17 submitted runs. \n\n"]}
{"question_id": "903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "predicted_answer": " the restrictions of the restricted track are to use only the provided parallel training data and to not use any monolingual data or pre-trained models.", "predicted_evidence": [""]}
{"question_id": "ab95ca983240ad5289c123a2774f8e0db424f4a1", "predicted_answer": " building educational applications.", "predicted_evidence": ["bea stands for building educational applications. \n\n"]}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "predicted_answer": " modular \n\nexplanation: according to the human evaluation, the modular system performed better than the concurrent system.", "predicted_evidence": [""]}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "predicted_answer": " annotators were trained to remove subjectivity from biased sentences. \n\n", "predicted_evidence": ["the authors state, \"we asked annotators to rewrite sentences to be more neutral before adding them to our corpus.\" \n\n"]}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "predicted_answer": " by identifying and modifying subjective words. \n\n", "predicted_evidence": ["the authors propose a method that involves identifying words that are indicative of subjective bias, and modifying them to be more neutral. this is done through semi-supervised learning using a pre-defined lexicon of words that are known to carry subjective bias. the model learns to identify other similar words and applies the same modifications to them. \n\n"]}
{"question_id": "1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "predicted_answer": " sign language recognition. \n\n", "predicted_evidence": ["the paper investigates the task of sign language recognition. \n\n"]}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f85520bbc594918968d7d9f33d11639055458344", "predicted_answer": " lstm, cnn, ml", "predicted_evidence": ["the deep learning architectures used in the experiments are lstm, cnn, and ml with different combinations, as explained in subsection c.1.\n\n"]}
{"question_id": "e4f2d59030b17867449cf5456118ab722296bebd", "predicted_answer": " unanswerable", "predicted_evidence": ["there is no evidence to indicate who made the claim.\n\n"]}
{"question_id": "e664b58ea034a638e7142f8a393a88aadd1e215e", "predicted_answer": " czech, finnish, german, russian, turkish.", "predicted_evidence": ["the authors used multiple languages, including czech, finnish, german, russian, and turkish. \n\n"]}
{"question_id": "c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "predicted_answer": " yes\n\nexplanation: the results show that lstm models that use character-level representations of words perform better than models that rely on morphological analyses such as inflectional affixes as features. the authors argue that character-level modeling allows the model to learn more information about morphology, including rare or novel forms of words, while morphological analyses are limited to the forms that are annotated in the training data.", "predicted_evidence": [""]}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "predicted_answer": " phenomenon", "predicted_evidence": ["case syncretism is a phenomenon in morphology where two or more distinct cases in a language merge into one. \n\n"]}
{"question_id": "2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "predicted_answer": " yes", "predicted_evidence": ["yes, by comparing them with ground-truth responses. \n\n"]}
{"question_id": "a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper did not discuss any models used to generate responses.\n\n"]}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "predicted_answer": " racism, sexism, and homophobia", "predicted_evidence": ["racism, sexism, and homophobia\n\n"]}
{"question_id": "a4cf0cf372f62b2dbc7f31c600c6c66246263328", "predicted_answer": " lbmt and lstm.", "predicted_evidence": ["the authors compare their proposed method to two previous methods: a tree-based approach (lbmt) and a sequence-to-sequence model (lstm). \n\n"]}
{"question_id": "f7b91b99279833f9f489635eb8f77c6d13136098", "predicted_answer": " combination of techniques.", "predicted_evidence": ["the paper compared the performance of several sentence compression techniques, including the use of syntactic trees and explicit compression using manually defined rules. they found that a combination of both techniques outperformed either technique on its own.\n\n"]}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "predicted_answer": " bilstm-crf.", "predicted_evidence": ["the authors used a bilstm-crf model as a baseline model.\n\n"]}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "predicted_answer": " tables in wikipedia entries are used as gazetteer resources.", "predicted_evidence": ["the authors extract wikipedia entries with tables and use them as gazetteer resources.\n\n"]}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "predicted_answer": " callhome", "predicted_evidence": ["the authors used the callhome american english speech corpus to train the embeddings. \n\n"]}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fac273ecb3e72f2dc94cdbc797582d7225a8e070", "predicted_answer": " combining a ranking svm with a deep convolutional neural network.", "predicted_evidence": ["they use a combination of a ranking svm with a deep convolutional neural network.\n\n"]}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "predicted_answer": " 100 hours", "predicted_evidence": ["training corpora are english text from the wmt18 parallel corpus and for speech, librispeech was used with audio data for 100 hours of english speech and for each additional language. \n\n"]}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "predicted_answer": " neural network.", "predicted_evidence": ["we don't have a specific section discussing the architecture of the model used in this paper, but in section 3 they mention using a pretrained automatic speech recognition (asr) model, so we can assume the architecture includes a neural network for asr. \n\n"]}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "predicted_answer": " spanish, french, german, italian", "predicted_evidence": ["the authors used four languages: spanish, french, german, and italian. \n\n"]}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "predicted_answer": " yes", "predicted_evidence": ["yes, the proposed method is evaluated on the timit dataset, a widely-used benchmark for speech recognition. \n\n"]}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "predicted_answer": " svm and ffnn.", "predicted_evidence": ["the authors used a support vector machine (svm) classifier and a feed-forward neural network (ffnn) classifier for their experiments.\n\n"]}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "predicted_answer": " yes", "predicted_evidence": ["in our evaluation, we found that domain-independent features (dif) outperformed both the previous rule-based approach and bayesian models with hand-engineered features.\n\n"]}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "predicted_answer": " \"if the patient experiences a fever, administer antibiotics\"", "predicted_evidence": ["our work includes a publicly available corpus of clinical practice guidelines annotated with condition-action statements and associated annotations, including anaphora and coreference. one such pair is \"if the patient experiences a fever, administer antibiotics\". \n\n"]}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "predicted_answer": " mt, ts, summarization, sentence compression, data-to-text generation, and natural language generation from knowledge graphs.", "predicted_evidence": ["explanation: the paper considers six nlg tasks: machine translation (mt), text simplification (ts), summarization, sentence compression, data-to-text generation, and natural language generation from knowledge graphs. \n\n"]}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "predicted_answer": " rouge-1, rouge-2, and rouge-l.", "predicted_evidence": ["rouge-1, rouge-2, and rouge-l. \n\n"]}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "predicted_answer": " centroid-based method", "predicted_evidence": ["the original model that they refer to in this paper is the centroid-based method.\n\n"]}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "predicted_answer": " centroid-based", "predicted_evidence": ["the paper proposes a centroid-based method that involves selecting sentences closest to the centroid of the input documents. \n\n"]}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "predicted_answer": " yes", "predicted_evidence": ["yes, in section 4.2, they mention that they evaluate their model on two english datasets. \n\n"]}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "predicted_answer": " frequency analysis", "predicted_evidence": ["the analysis included a series of statistics for the distributions of terms, entities, and punctuation marks, broken down by ironic and not ironic tweets. \n\n"]}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "predicted_answer": " svm, rf", "predicted_evidence": ["the classifiers used in the paper are support vector machines (svm) and random forests (rf).\n\n"]}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "predicted_answer": " annotated data, multilingual parser, dependency parser, and lstm.", "predicted_evidence": ["the authors combine annotated data from several languages, as well as a multilingual parser, a dependency parser, and a bidirectional lstm. \n\n"]}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "predicted_answer": " conll-2005 and utct.", "predicted_evidence": ["the authors evaluated the performance of their approach on several datasets, including conll-2005, which consists of english news data from the wall street journal and associated press domains, and the universal text representation for cross-lingual transfer (utct) dataset. \n\n"]}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "predicted_answer": " yes", "predicted_evidence": ["ablation studies. we introduce two baselines to establish a benchmark for our multilingual system. a representative monolingual baseline (see table 2) leverages the same components as the full system, but operates in a single language. \n\n"]}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "predicted_answer": " english, chinese, arabic, hindi, turkish, polish and russian.", "predicted_evidence": ["this paper explores several languages: english, chinese, arabic, hindi, turkish, polish and russian.\n\n"]}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "predicted_answer": " session identification.", "predicted_evidence": ["the problem of session segmentation is to identify distinct topics or turns in a dialogue and group them together as a session. \n\n"]}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_answer": " unanswerable", "predicted_evidence": ["the authors did not mention the specific dataset they used. \n\n"]}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "predicted_answer": " similarity function over word embedding vectors.", "predicted_evidence": ["the domain relevance was estimated using a similarity function over word embedding vectors derived from the sentences in the knowledge base document and in the web page. \n\n"]}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "predicted_answer": " conll 2016 shared task scoring system", "predicted_evidence": ["the fluency of the questions was measured based on the conll 2016 shared task scoring system which measures the phrasing and syntactic structure of the generated questions.\n\n"]}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "predicted_answer": " sentences for each sport.", "predicted_evidence": ["the authors compile a separate set of sentences for each sport, scoring each sentence as 0 (gender-neutral) or 1 (gender-biased) according to specific criteria that they developed, which include \"using gendered pronouns, making references to the athletes' physical appearance, [and] making stereotypical assumptions about athletes based on their gender\". \n\n"]}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "predicted_answer": " artificial", "predicted_evidence": ["the authors utilize an artificial dataset consisting of english and neuralese sentence pairs. \n\n"]}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "predicted_answer": " unanswerable", "predicted_evidence": ["the document does not provide information on how the dataset was collected.\n\n"]}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "predicted_answer": " accuracy, f1-score, auc-roc", "predicted_evidence": ["we use accuracy, f1-score, and the area under the receiver operating characteristics curve (auc-roc) to measure the model performances.\n \n"]}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "predicted_answer": " unanswerable. \n\nexplanation: the document contains no information on how the authors measured grammatical correctness.", "predicted_evidence": [""]}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence \n\n"]}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "predicted_answer": " unanswerable", "predicted_evidence": ["manual evaluation was not presented in the paper.\n\n"]}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "predicted_answer": " sentiment analysis, named entity recognition, question answering.", "predicted_evidence": ["the paper analyzes distilbert's performance on various downstream tasks such as sentiment analysis, named entity recognition, and question answering. \n\n"]}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "predicted_answer": " unanswerable", "predicted_evidence": ["no information is provided on a comparison between the representations performance obtained from a naive encoder versus ones learned from large amounts of source language data. \n\n"]}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "predicted_answer": " 65.6", "predicted_evidence": ["macro-averaged f1 score of 65.6\nwe conduct experiments on a corpus consisting of 15,123 legal documents,1 extracted from data.gov.uk. each document is assigned a label from the 40-category taxonomy. experiments are repeated for 10 times, with different random seeds each time. the average macro-averaged f1 score for test sets is reported in table 1.\n\n"]}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper did not discuss how the text is segmented.\n\n"]}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "predicted_answer": " radical-level ideograph encoder for rnn-based sentiment analysis of chinese and japanese.", "predicted_evidence": [""]}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "predicted_answer": " german-english and romanian-english", "predicted_evidence": ["they evaluate on german-english and romanian-english language pairs.\n\n"]}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "predicted_answer": " large", "predicted_evidence": ["table 3 reports the results of the zero-shot transfer learning experiment. we can observe that the model with the adaptive attention parameter significantly outperforms the one without it, by a large improvement margin.\n\n"]}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "predicted_answer": " multiple languages (hindi, gujarati, bengali, tamil, etc.)", "predicted_evidence": ["the authors of this paper explore various indian indigenous languages such as hindi, gujarati, bengali, tamil, and many more. \n\n"]}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "predicted_answer": " text and image-based", "predicted_evidence": ["our method is compared with two baseline methods: one based on text and one based on image features. \n\n"]}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "predicted_answer": " yes", "predicted_evidence": ["our model outperforms all the baselines in terms of retrieval accuracy, showing an improvement of up to 7-8\\% in top 1 accuracy and up to 10\\% points in top-10 accuracy for the best configuration. \n\n"]}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "predicted_answer": " technique", "predicted_evidence": ["reordering refers to a technique used to align the generated sentences with the target language sentences by changing the word order in the generated sentences. \n\n"]}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "predicted_answer": " yes", "predicted_evidence": ["the paper proposes incorporating a language model into the search process to make the search context aware. \n\n"]}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_answer": " rst-dt and pdtb-dt", "predicted_evidence": ["the rst-dt (rhetorical structure theory discourse treebank) and pdtb-dt (penn discourse treebank discourse treebank) datasets are used. \n\n"]}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "predicted_answer": " rouge-1, rouge-2, rouge-l, and meteor", "predicted_evidence": ["we used the following metrics for evaluation: rouge-1, rouge-2, rouge-l, and meteor.\n\n"]}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "predicted_answer": " self-compiled", "predicted_evidence": ["the authors state that they have compiled their own new dataset. \n\n"]}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "predicted_answer": " 810", "predicted_evidence": ["the authors collected 81 multi-document clusters, comprising 810 news articles, with each cluster containing 10 news articles covering the same event. \n\n"]}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "predicted_answer": " manually by 5 annotators", "predicted_evidence": ["annotation: five annotators were instructed to read as many news articles as possible about each event. after the first phase, annotators discussed each summary and refined or finalized it.\n\n"]}
{"question_id": "b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "predicted_answer": " news articles and summaries.", "predicted_evidence": ["the dataset consists of more than 1,000 news articles and corresponding summaries created by humans.\n\n"]}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "predicted_answer": " accuracy", "predicted_evidence": ["the authors use the accuracy of the lgi model as a metric to measure the extent to which it has learned the task.\n\n"]}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "predicted_answer": " visual reasoning, language inference, deduction, spatial and temporal reasoning, prediction, and logical and commonsense reasoning.\n\nexplanation: according to the paper, lgi has learned to perform eight tasks: visual reasoning, language inference, deduction, spatial and temporal reasoning, prediction, commonsense reasoning, and multiple sentence entailment.", "predicted_evidence": [""]}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "predicted_answer": " six different models", "predicted_evidence": ["we experiment with three representative sequence encoders for sentence representation and \ntwo types of error-sensitive classifiers, resulting in six different models.\n\n"]}
{"question_id": "d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "predicted_answer": " they identified the language through the github repository metadata. \n\nexplanation: the language used in a github repository can often be inferred from the repository's metadata (its name, description text, etc.) rather than the contents of the files themselves. the authors of the paper likely used this metadata to identify the language in which the misspelled words were written.", "predicted_evidence": [""]}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper does not provide an answer to this question. \n\n"]}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "predicted_answer": " english, french, german, italian, spanish and russian.", "predicted_evidence": ["we introduce a large-scale multilingual corpus of real-world user-generated text consisting of more than 32m misspelled english, french, german, italian, spanish and russian words.\n\n"]}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "predicted_answer": " layers 10-12\n\nexplanation: we also explore optimal strategies for pooling information from different bert layers and evaluate the performance of several recently proposed unsupervised learning objectives. we show that combining the output of layers 10-12 consistently outperforms other combinations on a diverse set of transfer tasks, and that our unsupervised objectives improve performance on low-resource tasks, especially those with domain-specific text.", "predicted_evidence": [""]}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "predicted_answer": " snli, multinli, and mednli.", "predicted_evidence": ["snli, multinli, and mednli datasets were used to improve the quality of the embeddings.\n\n"]}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "predicted_answer": " squad1.1, squad2.0, triviaqa and natural questions.", "predicted_evidence": ["squad1.1, squad2.0, triviaqa and natural questions.\n\n"]}
{"question_id": "f7f2968feb28c2907266c892f051ae9f7d6286e6", "predicted_answer": " binary classification of entailment and binary classification of contradiction.", "predicted_evidence": ["the sentence embeddings are evaluated against the senteval benchmark on two tasks: binary classification of entailment and binary classification of contradiction.\n\n"]}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "predicted_answer": " binary classifier and regression tree", "predicted_evidence": ["the authors used two classifiers: a binary classifier that distinguishes between difficult and easy sentences, and a regression tree that predicts the linguistic complexity of a sentence on a likert scale from one to seven. \n\n"]}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "predicted_answer": " self-created dataset.", "predicted_evidence": ["they created and manually evaluated a dataset of reading materials at different difficulty levels. \n\n"]}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f381b0ef693243d67657f6c34bbce015f6b1fd07", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence \n\n"]}
{"question_id": "c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "predicted_answer": " no", "predicted_evidence": ["no, results are also reported on russian data. \n\n"]}
{"question_id": "e0b54906184a4ad87d127bed22194e62de38222b", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1f8044487af39244d723582b8a68f94750eed2cc", "predicted_answer": " lda", "predicted_evidence": ["latent dirichlet allocation (lda)\n\n"]}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "predicted_answer": " bio information and account features such as location, number of followers, profile image, and profile description were analyzed.", "predicted_evidence": [""]}
{"question_id": "1f011fa772ce802e74eda89f706cdb1aa2833686", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "predicted_answer": " doc2vec, fasttext, glove, sumembed, lda, lsa", "predicted_evidence": [""]}
{"question_id": "5d4190403eb800bb17eec71e979788e11cf74e67", "predicted_answer": " document classification and document clustering", "predicted_evidence": ["the proposed model is compared against four baselines by evaluating them on two tasks: document classification and document clustering. the baselines are: (1) distributed bag-of-words (doc2vec) model (le and mikolov, 2014); (2) latent semantic analysis (lsa) approach (deerwester et al., 1990); (3) stemming based k-means clustering with weighted entropy based document representation as input; and (4) non-negative matrix factorization (nmf) approach with tf-idf weighted document representation as input.\n\n"]}
{"question_id": "56d41e0fcc288c1e65806ae77097d685c83e22db", "predicted_answer": " text classification, clustering, retrieval", "predicted_evidence": ["the authors evaluate keyvec on three document understanding tasks: text classification, clustering, and retrieval. \n\n"]}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "predicted_answer": " reuters corpus volume 1 (rcv1), associated press (ap), gigaword5 (gw5), english gigaword fifth edition (gigaword)", "predicted_evidence": ["the authors used four datasets: reuters corpus volume 1 (rcv1), associated press (ap), gigaword5 (gw5), and english gigaword fifth edition (gigaword). \n\n"]}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "predicted_answer": " yes", "predicted_evidence": ["yes, they plan on gathering and open sourcing a large icelandic language corpus together with aligned data in other languages. \n\n"]}
{"question_id": "b59f3a58939f7ac007d3263a459c56ebefc4b49a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b4b7333805cb6fdde44907747887a971422dc298", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not describe other national language technology programs. \n\n"]}
{"question_id": "871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "predicted_answer": " early 2000s.", "predicted_evidence": ["language technology started in iceland in the early 2000s. \n\n"]}
{"question_id": "3fafde90eebc1c00ba6c3fb4c5b984009393ce7f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_answer": " unanswerable", "predicted_evidence": ["the dataset used in this research is not mentioned. \n\n"]}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "predicted_answer": " convd", "predicted_evidence": ["there are currently no datasets available for the task described in the paper, but the authors provide a new dataset called controversy dataset (convd) crawled from twitter. \n\n"]}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "predicted_answer": " 1.25m; 849k.", "predicted_evidence": ["the authors introduced two new datasets: cort and arc, comprising 1.25m samples and 14.5m arcs, and 849k samples and 3.1m arcs, respectively. \n\n"]}
{"question_id": "376e8ed6e039e07c892c77b7525778178d56acb7", "predicted_answer": " yes", "predicted_evidence": ["yes, for three datasets they compiled of controversial topics, they crowd-sourced annotations for each document's focus and stance toward the topic. \n\n"]}
{"question_id": "4de6bcddd46726bf58326304b0490fdb9e7e86ec", "predicted_answer": " crowdworkers on amt.", "predicted_evidence": ["labeling was performed by crowdworkers on amazon mechanical turk (amt). \n\n"]}
{"question_id": "e831ce6c406bf5d1c493162732e1b320abb71b6f", "predicted_answer": " wikipedia articles and talk pages", "predicted_evidence": ["the authors used wikipedia articles and their corresponding talk pages as a dataset.\n\n"]}
{"question_id": "634a071b13eb7139e77872ecfdc135a2eb2f89da", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "predicted_answer": " web interface, python, natural language toolkit (nltk)", "predicted_evidence": ["the authors used a web interface to display the articles and associated comments, and to collect annotations from the crowdworkers. they also used python for pre-processing tasks, and the natural language toolkit (nltk) for tokenization, lemmatization, and part-of-speech tagging.\n\n"]}
{"question_id": "267d70d9f3339c56831ea150d2213643fbc5129b", "predicted_answer": " the performance of njm was evaluated using automatic evaluation metrics and human comparison on a dataset containing 1950 humorous images.", "predicted_evidence": ["in this study, njm was evaluated on the dataset containing 1950 humorous images. for the evaluation of njm, a neural joke model and a rule-based joke model were used, and the results were compared with human-written captions. the evaluation of the model was done using automatic evaluation metrics (bleu, rouge, cider), and the best performing model was selected. \n\n"]}
{"question_id": "477da8d997ff87400c6aad19dcc74f8998bc89c3", "predicted_answer": " bleu and meteor", "predicted_evidence": ["we evaluated our approach using two well-known metrics for image captioning evaluation: bleu and meteor.\n\n"]}
{"question_id": "4485e32052741972877375667901f61e602ec4de", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper mentions using pre-existing datasets, but there is no information on a self-collected corpus.\n\n"]}
{"question_id": "df4895c6ae426006e75c511a304d56d37c42a1c7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "predicted_answer": " adaptst and quiz bowl", "predicted_evidence": ["they use two datasets: the adaptst dataset and the quiz bowl dataset. \n\n"]}
{"question_id": "54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "predicted_answer": " yes", "predicted_evidence": ["yes. \n\n"]}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "predicted_answer": " domain class weighting. \n\nexplanation: the performance across all models is measured by mean reciprocal rank (mrr) and precision at top 1 (p@1). the domain class weighting strategy is shown to be the most effective, achieving the best performance for four out of five target domains.", "predicted_evidence": [""]}
{"question_id": "99554d0c76fbaef90bce972700fa4c315f961c31", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "predicted_answer": " 40", "predicted_evidence": ["they experiment with 40 languages in multilingual setup. \n\n"]}
{"question_id": "9a52a33d0ae5491c07f125454aea9a41b9babb82", "predicted_answer": " by projecting learned embeddings onto the bottleneck layer of the asr encoder.", "predicted_evidence": ["the paper extracts target language bottleneck features by projecting the learned subword and phoneme embeddings onto the bottleneck layer of the trained asr encoder. \n\n"]}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "predicted_answer": " no dataset name provided.", "predicted_evidence": ["the authors utilize five typologically and genetically diverse languages: amharic, bengali, swahili, ukrainian, and uzbek. \n\n"]}
{"question_id": "44435fbd4087fa711835d267036b6a1f82336a22", "predicted_answer": " improved results compared to baselines and extractive summarization.", "predicted_evidence": ["our abstractive model outperforms three strong extractive baselines and multi-document extractive summarization by an average of at least 9 and 11 rouge points respectively on up to four source documents.\n\n"]}
{"question_id": "86656aae3c27c6ea108f5600dd09ab7e421d876a", "predicted_answer": " multiple public news sources", "predicted_evidence": ["the dataset contains 56,720 news articles from 113 news sources, which are all available publicly and cover a wide range of topics.\n\n"]}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "predicted_answer": " 57k", "predicted_evidence": ["the multi-news dataset contains approximately 57k news articles with summaries, sourced from five different news websites. \n\n"]}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "23fe8431058f2a7b7588745766fc715f271aad07", "predicted_answer": " english and kazakh.", "predicted_evidence": ["english-to-kazakh and kazakh-to-english.\n\n"]}
{"question_id": "e5b2eb6a49c163872054333f8670dd3f9563046a", "predicted_answer": " iwslt14 german to english, iwslt14 english to german, english to french (imbalanced)", "predicted_evidence": ["the paper uses two translation datasets, iwslt14 german to english and iwslt14 english to german, and an imbalanced version of the english to french translation dataset produced by wmt14. \n\n"]}
{"question_id": "73760a45b23b2ec0cab181f82953fb296bb6cd19", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "ec990c16896793a819766bc3168c02556ef69971", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "predicted_answer": " wmt-2018, wmt-2014, and iwslt-2017", "predicted_evidence": ["the paper analyzes the translation performance of wmt-2018, wmt-2014, and iwslt-2017 datasets.\n\n"]}
{"question_id": "9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1dcfcfa46dbcffc2fc7be92dd57df9620258097b", "predicted_answer": " unanswerable", "predicted_evidence": ["the question cannot be answered as the answer is not provided in the given explanation or document. \n\n"]}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "predicted_answer": " bilstm-crf and elmo + bilstm-crf.", "predicted_evidence": ["the baseline models are bilstm-crf and elmo + bilstm-crf.\n\n"]}
{"question_id": "b537832bba2eb6d34702a9d71138e661c05a7c3a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1002bd01372eba0f3078fb4a951505278ed45f2e", "predicted_answer": " yes", "predicted_evidence": ["yes, we compare against various baselines and state-of-the-art methods and present the results in the paper. \n\n"]}
{"question_id": "3450723bf66956486de777f141bde5073e4a7694", "predicted_answer": " conscious recollection.", "predicted_evidence": ["episodic memory is a type of long-term memory that involves the conscious recollection of previous experiences. \n\n"]}
{"question_id": "36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "predicted_answer": " yes.", "predicted_evidence": ["yes, lsa-reduced n-gram features are considered hand-crafted features in this paper. \n\n"]}
{"question_id": "28e50459da60ceda49fe1578c12f3f805b288bd0", "predicted_answer": " unanswerable", "predicted_evidence": ["the performance of the model is reported on english and spanish, but not on arabic.\n\n"]}
{"question_id": "e1f61500eb733f2b95692b6a9a53f8aaa6f1e1f6", "predicted_answer": " unanswerable.", "predicted_evidence": ["the authors of the paper did not compare their model with lstm in the paper. \n\n"]}
{"question_id": "da4d07645edaf7494a8cb5216150a00690da01f7", "predicted_answer": " shortest n-gram pathways", "predicted_evidence": ["the cache consists of the previously constructed shortest n-gram pathways in the wfst. \n\n"]}
{"question_id": "c0cebef0e29b9d13c165b6f19f6ca8393348c671", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper does not mention any language on which the model was tested.\n\n"]}
{"question_id": "5695908a8c6beb0e3863a1458a1b93aab508fd34", "predicted_answer": " yes", "predicted_evidence": ["a personalized language model is a model that is trained on a user's individual data to better predict their language patterns and preferences.\n\n"]}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "predicted_answer": " unanswerable", "predicted_evidence": ["the authors do not mention the reuse of the dataset in other works. \n\n"]}
{"question_id": "6883767bbdf14e124c61df4f76335d3e91bfcb03", "predicted_answer": " unanswerable\n\nexplanation: there is no information provided in the document regarding the drawbacks of methods that rely on textual cues.", "predicted_evidence": [""]}
{"question_id": "11679d1feba747c64bbbc62939a20fbb69ada0f3", "predicted_answer": " age, gender, race", "predicted_evidence": ["our feature set includes both lexical and stylistic features, such as punctuation, emoticons, uppercase words, shift key usage, and community-based profiling features such as age, gender, and race.\n\n"]}
{"question_id": "e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "predicted_answer": " yes.", "predicted_evidence": ["the authors show that incorporating user traits improve activity recognition performance.\n\n"]}
{"question_id": "cab082973e1648b0f0cc651ab4e0298a5ca012b5", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not mention a specific number for the activities in the dataset, but it states that the activities were selected based on their popularity and prevalence in everyday life, and includes both indoor and outdoor activities. \n\n"]}
{"question_id": "1cc394bdfdfd187fc0af28500ad47a0a764d5645", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "predicted_answer": " the data instances were chosen by filtering out videos with low visual quality and overly long duration from the activitynet challenge 2018 dataset.", "predicted_evidence": ["the authors used a publicly available dataset from the activitynet challenge 2018 which is a large-scale dataset for human activity understanding in videos. they created a subset of the dataset by filtering out videos with low visual quality or with overly long duration. \n\n"]}
{"question_id": "cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "101d7a355e8bf6d1860917876ee0b9971eae7a2f", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "4288621e960ffbfce59ef1c740d30baac1588b9b", "predicted_answer": " improved sentiment analysis with incorporation of tweet-specific syntax and social context. \n\n", "predicted_evidence": ["the authors conclude that incorporating tweet-specific syntax and social context improves sentiment analysis performance in twitter data. \n\n"]}
{"question_id": "c3befe7006ca81ce64397df654c31c11482dafbe", "predicted_answer": " each classifier evaluates one of the syntactic or social properties which are salient for a tweet via a different elementary property prediction task.", "predicted_evidence": [""]}
{"question_id": "5d0a3f8ca3882f87773cf8c2ef1b4f72b9cc241e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "dce27c49b9bf1919ca545e04663507d83bb42dbe", "predicted_answer": " length penalty and coverage penalty\n\nexplanation: two methods are used to correct length bias in neural machine translation. one is the length penalty which reduces the score of a hypothesis based on its length, and the other is the coverage penalty which encourages a hypothesis to cover all the words in the source sentence.", "predicted_evidence": [""]}
{"question_id": "991ea04072b3412928be5e6e903cfa54eeac3951", "predicted_answer": " overloads with shorter and hallucinated translations\n\nexplanation: wider beam search increases the search space, but most of the expanded hypotheses will end up in lower probability regions of the search space, overloading the model with many and often shorter and hallucinated translations.", "predicted_evidence": [""]}
{"question_id": "a82a12a22a45d9507bc359635ffe9574f15e0810", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "355cf303ba61f84b580e2016fcb24e438abeafa7", "predicted_answer": " unanswerable. \n\nexplanation: the document does not provide information on the specific novel aspects of the cnn method in comparison to the conventional method.", "predicted_evidence": [""]}
{"question_id": "88757bc49ccab76e587fba7521f0981d6a1af2f7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "4830459e3d1d204e431025ce7e596ef3f8d757d2", "predicted_answer": " one (the dataset consists of video clips of ted talks, not individual speakers)", "predicted_evidence": ["our dataset is composed of 95 video clips of ted talks that have been uploaded to the ted website.\n\n"]}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "3a01dc85ac983002fd631f1c28fc1cbe16094c24", "predicted_answer": " unanswerable\n\nexplanation: unfortunately, there is no information in the given document regarding how to incorporate common sense knowledge into an lstm (long short-term memory) model. however, the paper proposes augmenting end-to-end dialog systems with this type of knowledge to improve their performance and ability to handle complex tasks.", "predicted_evidence": [""]}
{"question_id": "00ffe2c59a3ba18d6d2b353d6ab062a152c88526", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "042800c3336ed5f4826203616a39747c61382ba6", "predicted_answer": " conceptnet", "predicted_evidence": ["conceptnet\n\n"]}
{"question_id": "52868394eb2b3b37eb5f47f51c06ad53061f4495", "predicted_answer": " web crawling", "predicted_evidence": ["the dataset was obtained by crawling public pages from tripadvisor. \n\n"]}
{"question_id": "59dc6b1d3da74a2e67a6fb1ce940b28d9e3d8de0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "713e1c7b0ab17759ba85d7cd2041e387831661df", "predicted_answer": " yes", "predicted_evidence": ["yes, a variety of experiments were conducted on this dataset to validate its effectiveness in recommendation systems. \n\n"]}
{"question_id": "00db191facf903cef18fb1727d1cab638c277e0a", "predicted_answer": " 4", "predicted_evidence": ["we use sizes up to 5, but we \n found that 4 gives the best trade-off between performance and computational efficiency. \n\n"]}
{"question_id": "3dad6b792044018bb968ac0d0fd4628653f9e4b7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "8e26c471ca0ee1b9779da04c0b81918fd310d0f3", "predicted_answer": " concatenation", "predicted_evidence": ["they concatenate ordinary word embeddings with ones constructed from character n-grams and feed them into the lstm layers of the rnn language model.\n\n"]}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "predicted_answer": " clickbait challenge 2017 and clickbait corpus.", "predicted_evidence": ["they use two datasets: a public dataset (clickbait challenge 2017) and their own dataset (clickbait corpus)\n\n"]}
{"question_id": "dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "predicted_answer": " 2.05%", "predicted_evidence": ["as shown in table 1, swde outperforms the previous state-of-the-art approaches by 2.05% in terms of auc. \n\n"]}
{"question_id": "e9b6b14b8061b71d73a73d8138c8dab8eda4ba3f", "predicted_answer": " no", "predicted_evidence": [""]}
{"question_id": "76e17e648a4d1f386eb6bf61b0c24f134af872be", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7572f6e68a2ed2c41b87c5088ba8680afa0c0a0b", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "5d2bbcc3aa769e639dc21893890bc36b76597a33", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper discusses the modification of pre-existing models for abusive language detection to reduce gender bias but does not provide information about the specific model architectures used.\n\n"]}
{"question_id": "5d93245832d90b31aee42ea2bf1e7704c22ebeca", "predicted_answer": " glove.", "predicted_evidence": ["the authors used pre-trained 300 dimensional glove embeddings and randomly initialized embeddings for out-of-vocabulary words.\n\n"]}
{"question_id": "c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "predicted_answer": " fpr and err", "predicted_evidence": ["the metrics used to measure gender biases are false positive rates (fpr) and error rate ratios (err). \n\n"]}
{"question_id": "ed7ce13cd95f7664a5e4fc530dcf72dc3808dced", "predicted_answer": " yes", "predicted_evidence": ["yes, as mentioned in the paper, they \"ensure that all dependencies of cost functions on variables are differentiable\". \n\n"]}
{"question_id": "26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "predicted_answer": " it is not explicitly mentioned how much their model outperforms state-of-the-art baselines. \n\nexplanation: while the authors do describe their model's superior performance and effectiveness in different settings, they do not provide a direct comparison or quantification of its improvement over state-of-the-art baselines.", "predicted_evidence": [""]}
{"question_id": "ee19fd54997f2eec7c87c7d4a2169026fe208285", "predicted_answer": " yes", "predicted_evidence": ["yes, they compare their work to previous work. they provide a table with the results and comparisons to previous work in table 2. \n\n"]}
{"question_id": "74fcb741d29892918903702dbb145fef372d1de3", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper describes a new model for informative and controllable opinion summarization. however, there is no information provided on what model is used. \n\n"]}
{"question_id": "de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "6a20a3220c4edad758b912e2d3e5b99b0b295d96", "predicted_answer": " equally", "predicted_evidence": ["the authors weight the output of the statistical model and the semantic model equally by combining their scores. \n\n"]}
{"question_id": "c2745e44ebe7dd57126b784ac065f0b7fc2630f1", "predicted_answer": " yes", "predicted_evidence": ["yes, they compare against state-of-the-art summarization approaches such as lsa, lda, lexrank, textrank and topic tiling. \n\n"]}
{"question_id": "d5dcc89a08924bed9772bc431090cbb52fb7836f", "predicted_answer": " lsa-lexrank", "predicted_evidence": ["the experiment was conducted over several models and showed that the combination between lsa-based models and lexrank graph-based algorithm achieved the highest rouge scores.\n\n"]}
{"question_id": "d418bf6595b1b51a114f28ac8a6909c278838aeb", "predicted_answer": " both a rule-based system and a neural network-based system were used.", "predicted_evidence": ["the paper describes using two different qa systems: a rule-based system and a neural network-based system.\n\n"]}
{"question_id": "6d6b0628d8a942c57d7af1447a563021be79bc64", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper describes a re-ranking approach using a linear svm model trained on a set of labeled examples to score answer candidates. however, it is not explicitly mentioned whether this is a transductive learning technique or not.\n\n"]}
{"question_id": "b21245212244ad7adf7d321420f2239a0f0fe56b", "predicted_answer": " 1000\n\nexplanation: in the experiment section, it is mentioned that a test set of 1000 questions from the trec-qa dataset was used to evaluate the proposed re-ranking approach.", "predicted_evidence": [""]}
{"question_id": "4a201b8b9cc566b56aedb5ab45335f202bc41845", "predicted_answer": " m-norm", "predicted_evidence": ["the paper proposes a novel metric, called m-norm, to optimize embeddings for knowledge graphs. \n\n"]}
{"question_id": "6a90135bd001be69a888076aff1b149b78adf443", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1f40adc719d8ccda81e7e90525b577f5698b5aad", "predicted_answer": " type and relation information.\n\nexplanation: the authors of the paper use both the type information of the entity and the relation information in the context of the entity to compute its embedding.", "predicted_evidence": [""]}
{"question_id": "b10388e343868ca8e5c7c601ebb903f52e756e61", "predicted_answer": " perplexity, bleu, and accuracy.", "predicted_evidence": ["typical metrics used to compare models include: perplexity, bleu, and accuracy. \n\n"]}
{"question_id": "833d3ae7613500f2867ed8b33d233d71781014e7", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper discusses various types of neural models used in conversational agents, such as recurrent neural networks (rnns), convolutional neural networks (cnns), and transformers. however, there is no specific information provided about which type of neural model is used in the author's own work. \n\n"]}
{"question_id": "64f7337970e8d1989b2e1f7106d86f73c4a3d0af", "predicted_answer": " statistical n-gram models.", "predicted_evidence": [""]}
{"question_id": "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not mention any proposed use of conversational agents in pioneering work.\n\n"]}
{"question_id": "a0d45b71feb74774cfdc0d5c6e23cd41bc6bc1f2", "predicted_answer": " alan turing \n\nexplanation: the paper discusses how the field of conversational agents has evolved since alan turing's work on the subject in the mid-twentieth century.", "predicted_evidence": ["alan turing's work on conversational agents was a pioneering effort in the field of conversational agents. \n\n"]}
{"question_id": "89414ef7fcb2709c47827f30a556f543b9a9e6e0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "faffcc6ef27c1441e6528f924e320368430d8da3", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "afad388a0141bdda5ca9586803ac53d5f10f41f6", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "baaa6ad7148b785429a20f38786cd03ab9a2646e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "de346decb1fbca8746b72c78ea9d1208902f5e0a", "predicted_answer": " unanswerable", "predicted_evidence": ["the research does not provide this information.\n\n"]}
{"question_id": "0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "051034cc94f2c02d3041575c53f969b3311c9ea1", "predicted_answer": " sari and fkgl", "predicted_evidence": ["the authors used two common automatic metrics, sari and fkgl, to evaluate their model. \n\n"]}
{"question_id": "511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "predicted_answer": " experts judge the simplified sentences. ", "predicted_evidence": ["their outputs are compared against the original sentences by experts that use six specific criteria (grammaticality, meaning preservation, simplicity, fluency, adequacy, and relevance).\n\n"]}
{"question_id": "6b4006a90aeaaff8914052d72d28851a9c0c0146", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "eccbbe3684d0cf6b794cb4eef379bb1c8bcc33bf", "predicted_answer": " tmt, rmt, ite methods", "predicted_evidence": ["the authors compare their work with traditional machine translation (tmt) methods, retrospective machine translation (rmt) methods, and interactive translation editing (ite) methods.\n\n"]}
{"question_id": "a3705b53c6710b41154c65327b7bbec175bdfae7", "predicted_answer": " modern parallel corpora, historical documents, and out-of-domain data.", "predicted_evidence": ["we used a combination of various types of data to train our model, including modern parallel corpora, historical documents, and out-of-domain data.\n\n"]}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "predicted_answer": " sequence-to-sequence model with attention", "predicted_evidence": ["the model architecture contains a sequence-to-sequence neural machine translation model with attention mechanism.\n\n"]}
{"question_id": "51e9f446d987219bc069222731dfc1081957ce1f", "predicted_answer": " old german (kurrentschrift and s\u00fctterlinschrift)", "predicted_evidence": ["the authors test their proposed interactive machine translation framework on two different old german scripts: kurrentschrift and s\u00fctterlinschrift.\n\n"]}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "predicted_answer": " 90.52%", "predicted_evidence": ["the f1 score for the proposed convolutional neural network model is 90.52% which outperforms the various baselines approaches. \n\n"]}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "predicted_answer": " bag-of-words model, crf model, svm model, hac model", "predicted_evidence": ["they compare against four different baselines: a bag-of-words model, a crf model, a svm model, and a hierarchical agglomerative clustering (hac) model.\n\n"]}
{"question_id": "930733efb3b97e1634b4dcd77123d4d5731e8807", "predicted_answer": " high performance (yes)", "predicted_evidence": ["our evaluation results show that our system outperforms the state-of-the-art systems on two benchmark datasets, the i2b2 2010 concept extraction dataset and the share/clef 2013 corpus in terms of the f1 score.\n\n"]}
{"question_id": "11f9c207476af75a9272105e646df02594059c3f", "predicted_answer": " i2b2/va 2010 challenge dataset", "predicted_evidence": ["they used the i2b2/va 2010 challenge dataset for evaluation. \n\n"]}
{"question_id": "b32de10d84b808886d7a91ab0c423d4fc751384c", "predicted_answer": " stanford parser.", "predicted_evidence": ["the paper mentions that they used the part-of-speech tagger included with the stanford parser. \n\n"]}
{"question_id": "9ea3669528c2b295f21770cb7f70d0c4b4389223", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9863f5765ba70f7ff336a580346ef70205abbbd8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "67c16ba64fe27838b1034d15194c07a9c98cdebe", "predicted_answer": " word-level and window-level features", "predicted_evidence": ["we employ a pooling mechanism to compute word-level and window-level features for each convolutional kernel. \n\n"]}
{"question_id": "58a3cfbbf209174fcffe44ce99840c758b448364", "predicted_answer": " lambada, babi tasks, cnn/daily mail", "predicted_evidence": ["the authors compare their proposed evaluation method to four recently proposed benchmarks: lambada, bibref36, babi tasks, and cnn/daily mail.\n\n"]}
{"question_id": "6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "predicted_answer": " unanswerable", "predicted_evidence": ["their study focuses on the current evaluation methodology in neural language models and did not establish any new state of the art results.\n\n"]}
{"question_id": "4f8b078b9f60be30520fd32a3d8601ab3babb5c0", "predicted_answer": " dropout, weight decay, and early stopping.", "predicted_evidence": ["they looked at dropout, weight decay, and early stopping regularization methods.\n\n"]}
{"question_id": "54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "803babb71e1bdaf507847d6c712585f4128e9f47", "predicted_answer": " baseline translation models, domain adversarial neural networks (danns)", "predicted_evidence": [""]}
{"question_id": "5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "predicted_answer": " the bilingual dataset provided for the wmt18 shared task on parallel corpus filtering and a new corpus of restaurant reviews in the target languages.", "predicted_evidence": ["we conducted our experiments in two settings, using two data sets. first, we used the bilingual data set provided for the wmt18 shared task on parallel corpus filtering, consisting of a total of 771k sentence pairs. this data set includes a training set of 664k sentence pairs, a validation set of 4k sentence pairs, and a test set of 103k sentence pairs. second, we collected a new corpus of restaurant reviews in the target languages, consisting of about 1.6m sentence pairs for italian-english, 1.6m for dutch-english, and 1.3m for chinese-english.\n\n"]}
{"question_id": "eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "predicted_answer": " bleu, chrf2+, wer", "predicted_evidence": ["the paper reports several human evaluation results using the bleu, chrf2+, and wer metrics to evaluate the translation quality of their proposed approach. they also conducted a user survey to assess the quality of the translations in terms of fluency, adequacy, and preference.\n\n"]}
{"question_id": "290ebf0d1c49b67a6d1858366be751d89086a78b", "predicted_answer": " bleu, chrf, meteor", "predicted_evidence": ["we report both standard automatic metrics, namely bleu, chrf, and meteor. \n\n"]}
{"question_id": "806fefe0e331ddb3c17245d6a9fa7433798e367f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "458e5ed506883bfec6623102ec9f43c071f0616f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5154f63c50729b8ac04939588c2f5ffeb916e3df", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "2aeabec8a734a6e8ca9e7a308dd8c9a1011b3d6e", "predicted_answer": " 15,100 tweets\n\nexplanation: according to the paper, \"the training dataset for english was obtained from hateval and contains 15,100 tweets.\"", "predicted_evidence": [""]}
{"question_id": "f2b8a2ed5916d75cf568a931829a5a3cde2fc345", "predicted_answer": " lexical, punctuation, and context-based features", "predicted_evidence": ["we use various combinations of lexical, punctuation, and context-based features in order to automatically detect cyberbullying in social media. \n\n"]}
{"question_id": "c0af44ebd7cd81270d9b5b54d4a40feed162fa54", "predicted_answer": " twitter", "predicted_evidence": ["the authors collected data from twitter using the twitter api.\n\n"]}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "predicted_answer": " unanswerable. \n\nexplanation: there is not enough information available to answer this question.", "predicted_evidence": [""]}
{"question_id": "778c6a27182349dc5275282c3e9577bda2555c3d", "predicted_answer": " textual, psychological and behavioural patterns.", "predicted_evidence": [""]}
{"question_id": "42dcf1bb19b8470993c05e55413eed487b0f2559", "predicted_answer": " telegram groups and twitter accounts.", "predicted_evidence": ["propaganda material is sourced from different telegram groups and twitter accounts.\n\n"]}
{"question_id": "2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "predicted_answer": " no", "predicted_evidence": ["we can find in the paper that they used linguistic features, network features and topical features but there is no specific mention of behavioral features.\n\n"]}
{"question_id": "824629b36a75753b1500d9dcaee0fc3c758297b1", "predicted_answer": " unanswerable", "predicted_evidence": ["we need more information. \n\n"]}
{"question_id": "31894361833b3e329a1fb9ebf85a78841cff229f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper discusses a similar issue for item features in recommendation systems, called the cold-start problem, but does not mention a cold-start problem specific to hotel2vec.\n\n"]}
{"question_id": "636ac549cf4917c5922cd09a655abf278924c930", "predicted_answer": " ranking", "predicted_evidence": ["the experiments were evaluated quantitatively. the evaluation was done using a ranking task, where the goal was to predict the correct attribute from a set of many attributes.\n\n"]}
{"question_id": "c61c0b25f9de4a7ca2013d2e4aba8a5047e14ce4", "predicted_answer": " no", "predicted_evidence": ["the authors only experimented with hotel-related applications in their study. \n\n"]}
{"question_id": "1d047286ac63e5dca1ab811172b89d7d125679e5", "predicted_answer": " tripadvisor dataset", "predicted_evidence": ["the authors used publicly available hotel review datasets from tripadvisor, which includes 2,088,400 reviews and 222,174 hotels.  \n\n"]}
{"question_id": "6d17dc00f7e5331128b6b585e78cac0b9082e13d", "predicted_answer": " yes", "predicted_evidence": ["yes, the entire dataset creation process was done manually by trained annotators. \n\n"]}
{"question_id": "de0154affd86c608c457bf83d888bbd1f879df93", "predicted_answer": " 85.53% accuracy", "predicted_evidence": ["the experiment results were reported in terms of the percentage of texts that were correctly labeled. the results showed that the bert model significantly outperformed the other models, obtaining an accuracy of 85.53%. \n\n"]}
{"question_id": "9887ca3d25e2109f41d1da80eeea05c465053fbc", "predicted_answer": " 102,583 sentences.", "predicted_evidence": [""]}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "075d6ab5dd132666e85d0b6ad238118271dfc147", "predicted_answer": " the approach outperforms generating sql from scratch.", "predicted_evidence": ["the authors found that their approach outperformed the template-based model by an average of 8.4%, and generated more valid queries than the generation-from-scratch baseline. \n\n"]}
{"question_id": "f2b1e87f61c65aaa99bcf9825de11ae237260270", "predicted_answer": " sqlova, irnet, and syntaxsqlnet.", "predicted_evidence": ["the paper proposes two novel models and compares them with several state-of-the-art baselines including sqlova, irnet, and syntaxsqlnet.\n\n"]}
{"question_id": "78c7318b2218b906a67d8854f3e511034075f79a", "predicted_answer": " public datasets", "predicted_evidence": ["the authors use three public datasets to evaluate their method. \n\n"]}
{"question_id": "697c5d2ba7e019ddb91a1de5031a90fe741f2468", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "e25b73f700e8c958b64951f14a71bc60d225125c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "908ba58d26d15c14600623498d4e86c9b73b14b2", "predicted_answer": " lda (latent dirichlet allocation) \n\nexplanation: the establishment of diverse clustering baselines for nlp entail inspection of the representations of well-established unsupervised dimensionality reduction techniques in text like lda. in this approach, a generative model is fitted to compute distributions of words and topics underlying text data. the output topics are clusters of text data based on the similarity of the words associated with them.", "predicted_evidence": [""]}
{"question_id": "3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c0847af3958d791beaa14c4040ada2d364251c4d", "predicted_answer": " elmo, bert, and openai's gpt.", "predicted_evidence": ["we used several models, specifically elmo, bert, and openai's gpt. \n\n"]}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "predicted_answer": " no", "predicted_evidence": ["no \n\n"]}
{"question_id": "ce23849e9e9a22626965f1ca8ca948a5c87280e9", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d9a45fea8539aac01dec01f29b7d04b44b9c2ca6", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "246e924017c48fa1f069361c44133fdf4f0386e1", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "96459b02efa82993a0b413530ed0b517c6633eea", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide information on length constraints. \n\n"]}
{"question_id": "6c1614991647705265fb348d28ba60dd3b63b799", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "predicted_answer": " bleu, meteor, rouge-l, cider, human evaluation", "predicted_evidence": ["for the proposed model, they used bleu, meteor, rouge-l, cider and a human evaluation based on fluency, coherence, grammaticality and informativeness as evaluation metrics. \n\n"]}
{"question_id": "157284acedf13377cbc6d58c8f3648d3a62f5db5", "predicted_answer": " the paper compares with various baseline models such as a memory network, domain adaptation and fine-tuning, and the use of a pre-trained language model.", "predicted_evidence": ["the paper discusses the comparison with various baseline models such as a memory network, domain adaptation and fine-tuning, and the use of a pre-trained language model. \n\n"]}
{"question_id": "e4ea0569b637d5f56f63e933b8f269695fe1a926", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "e3c44964eb6ddc554901244eb6595f26a9bae47e", "predicted_answer": " unanswerable", "predicted_evidence": ["threshold is not mentioned in the given document.\n\n"]}
{"question_id": "905a8d775973882227549e960c7028e4a3561752", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "76f90c88926256e7f90d2104a88acfdd7fc5475e", "predicted_answer": " 185,445 instances\n\nexplanation: according to section 4.1 of the paper, the fever dataset consists of 185,445 instances.", "predicted_evidence": [""]}
{"question_id": "182eb91090017a7c8ea38a88b219b641842664e4", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "0ef114d24a7a32821967e912dff23c016c4eab41", "predicted_answer": " unanswerable", "predicted_evidence": ["our proposed framework is evaluated on the yelp and amazon review datasets and compared against prior works on unsupervised style transfer, demonstrating the effectiveness and flexibility of our approach. however, no specific prior approaches are mentioned. \n\n"]}
{"question_id": "67672648e7ebcbef18921006e2c8787966f8cdf2", "predicted_answer": " fluency and consistency", "predicted_evidence": ["the paper proposes competing objectives of fluency, which requires the generated text to be coherent and smooth, and the style and content of the generated text to be consistent with the input text. \n\n"]}
{"question_id": "c32fc488f0527f330273263fa8956788bd071efc", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8908d1b865137bc309dde10a93735ec76037e5f9", "predicted_answer": " f1, accuracy", "predicted_evidence": ["the authors mentioned that they used f1 and accuracy as their evaluation metrics.\n\n"]}
{"question_id": "d207f78beb6cd754268881bf575c8f98000667ea", "predicted_answer": " 3", "predicted_evidence": ["they explore three classes of sentiment labels: positive, negative, and neutral. \n\n"]}
{"question_id": "35c01dc0b50b73ee5ca7491d7d373f6e853933d2", "predicted_answer": " coco-stuff", "predicted_evidence": ["they use coco-stuff dataset for text altering attributes matching to image parts. \n\n"]}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c79f168503a60d1b08bb2c9aac124199d210b06d", "predicted_answer": " sentiment analysis, named entity recognition, and question answering.", "predicted_evidence": ["the downstream tasks used for evaluation are sentiment analysis, named entity recognition, and question answering. \n\n"]}
{"question_id": "9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "predicted_answer": " sick-r and stanford sentiment treebank", "predicted_evidence": ["the models are evaluated on the task of textual entailment, the sick-r dataset, and on sentiment classification, the stanford sentiment treebank dataset.\n\n"]}
{"question_id": "5cc5e2db82f5d40a5244224dad94da50b4f673db", "predicted_answer": " they use generated clues to identify gender-specific occupations.", "predicted_evidence": ["the human-in-the-loop uses generated clues to help their system identify when an occupation is being referenced and whether it is gender-specific. \n\n"]}
{"question_id": "ab975efc916c34f55e1144b1d28e7dfdc257e371", "predicted_answer": " corpus of contemporary american english (coca) and the british national corpus (bnc).", "predicted_evidence": ["the authors use the corpus of contemporary american english (coca) and the british national corpus (bnc) to train their model. \n\n"]}
{"question_id": "e7ce612f53e9be705cdb8daa775eae51778825ef", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "6c5a64b5150305c584326882d37af5b0e58de2fd", "predicted_answer": " crowdsourcing experiments", "predicted_evidence": ["the de-biasing approach is evaluated via crowdsourcing experiments, with the main metric being the percentage of people who can correctly identify the gender of the occupation given the generated clues.\n\n"]}
{"question_id": "f7a27de3eb6447377eb48ef6d2201205ff943751", "predicted_answer": " yes", "predicted_evidence": ["yes, the paper proposed a new style-aware metric to evaluate both content and style in dialogue generation. \n\n"]}
{"question_id": "2df3cd12937591481e85cf78c96a24190ad69e50", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide information about the languages experimented on.\n\n"]}
{"question_id": "fc9aa04de4018b7d55e19a39663a2e9837328de7", "predicted_answer": " persona-chat, empatheticdialogues, and wizard of wikipedia.", "predicted_evidence": ["three benchmark datasets, persona-chat, empatheticdialogues, and wizard of wikipedia, are used in our experiments. \n\n"]}
{"question_id": "044cb5ef850c0a2073682bb31d919d504667f907", "predicted_answer": " versification", "predicted_evidence": ["versification is the art or practice of composing verse, especially as regards rhythmic structure. \n\n"]}
{"question_id": "c845110efee2f633d47f5682573bc6091e8f5023", "predicted_answer": " somewhat confident (or uncertain)", "predicted_evidence": ["the paper claims a \"highly significant difference in word usage patterns between the two playwrights\" but acknowledges that there is inherent uncertainty in such analysis. \n\n"]}
{"question_id": "2301424672cb79297cf7ad95f23b58515e4acce8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "6c05376cd0f011e00d1ada0254f6db808f33c3b7", "predicted_answer": " no", "predicted_evidence": ["both vocabulary and versification are used for the analysis. \n\n"]}
{"question_id": "9925e7d8757e8fd7411bcb5250bc08158a244fb3", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fa468c31dd0f9095d7cec010f2262eeed565a7d2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "predicted_answer": " public health-related forum posts.", "predicted_evidence": ["the authors discuss using public health-related forum posts as a source of less sensitive data.\n\n"]}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "predicted_answer": " limited data access and representativeness, explainability and accountability, bias and fairness, informed consent and transparency, data provenance and curation, and impact on clinical workflows.", "predicted_evidence": [""]}
{"question_id": "8126c6b8a0cab3e22661d3d71d96aa57360da65c", "predicted_answer": " human evaluation and rouge-2.", "predicted_evidence": ["human evaluation and rouge-2 were used as evaluation metrics.\n\n"]}
{"question_id": "2f01d3e5120d1fef4b01028536cb5fe0abad1968", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1a419468d255d40ae82ed7777618072a48f0091b", "predicted_answer": " context-independent and context-dependent models", "predicted_evidence": ["the paper proposes two models to extract affect attributes: a context-independent model that relies only on the textual features of the sentence, and a context-dependent model that takes into account the preceding context. \n\n"]}
{"question_id": "52f5249a9a2cb7210eeb8e52cb29d18912f6c3aa", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "baad4b6f834d5944f61bd12f30908e3cf3739dcd", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "37b972a3afae04193411dc569f672d802c16ad71", "predicted_answer": " name-calling, glittering generalities, testimonial, plain folks, card stacking, bandwagon", "predicted_evidence": [""]}
{"question_id": "a01af34c7f630ba0e79e0a0120d2e1c92d022df5", "predicted_answer": " no", "predicted_evidence": ["no\n\n"]}
{"question_id": "0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "predicted_answer": " the authors used a dataset from a news website, their own annotated dataset, and the dart dataset.", "predicted_evidence": ["table tabref21. dataset statistics. from the same source, we also collected all the possible reasons for opening the news source by the users. the different categories/categories of reasons are listed in table tabref22.\n\n"]}
{"question_id": "7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "predicted_answer": " unanswerable", "predicted_evidence": ["our approach largely builds upon a modified recurrent neural network (rnn) architecture proposed in [10], where we replace the classical rnn hidden states with those of a two-layer bidirectional lstm architecture. our model takes the sequence of words to be classified as input, and computes an embedded representation of the entire sequence. as we observed in our data analysis experiments (section 3.1), incorporating global lexical features in the task across the entire sequence is important in order to solve our task. therefore, after computing the sequence representation using the lstm, we add two additional non-linear layers, whose activation functions are rectified linear unit (relu), followed by a softmax layer whose output represents the final classification into humor or no-humor label.\n\n"]}
{"question_id": "8b1af67e3905244653b4cf66ba0acec8d6bff81f", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "3605ea281e72e9085a0ac0a7270cef25fc23063f", "predicted_answer": " 23rd", "predicted_evidence": ["our system achieved a micro-average score of 0.5989 on the official leaderboard, which placed 23rd out of the 36 participating teams.\n\n"]}
{"question_id": "21f6cb3819c85312364dd17dd4091df946591ef0", "predicted_answer": " subtask a: classification of tweets as humorous or not, subtask b: ranking of humorous tweets.", "predicted_evidence": ["subtask a required participants to classify the given tweet as humorous or non-humorous and subtask b required participants to rank the given humorous tweets in an order of funniness.\n  \n"]}
{"question_id": "452e978bd597411b65be757bf47dc6a78f3c67c9", "predicted_answer": " it helps reduce gender bias", "predicted_evidence": ["we used counterfactual data augmentation (cda) to train models on an augmented corpus - the original data set augmented with a counterfactually transformed one where gender entities are swapped. inherent gender bias in the data may arise from various sources (e.g., underrepresentation or stereotypes), and cda is a technique for mitigating it. we also evaluate the gender correctness (e.g., correct extractions involving male or female entities) of our models on the original and augmented test sets.\n\n"]}
{"question_id": "159025c44c0115ab4cdc253885384f72e592e83a", "predicted_answer": " hard debiasing lowers recall for both genders", "predicted_evidence": ["we observed that hard debiasing result in lower recall for both the genders as we incorporate feedback from the protected attribute while debiasing. the male and female precision follows a similar trend as before debiasing where male has higher precision than female. however, the recall of trained models drops when hard debiasing is used, but the drop is more significant for models with base attention.\n\n"]}
{"question_id": "6590055fb033cb32826f2afecb3d7f607dd97d57", "predicted_answer": " name anonymization reduces gender bias in predictions. \n\n(note: there is no explicit answer to whether it affects performance or not, as it's mentioned that there is minimal impact.)", "predicted_evidence": ["in our experiments on wikidata and conll04, we show that using name anonymization can significantly reduce gender bias in relation extraction predictions, with minimal impact on performance.\n\n"]}
{"question_id": "3435e365adf7866e45670c865dc33bb7d2a6a0c6", "predicted_answer": " randomly", "predicted_evidence": ["sentences are randomly sampled from wikipedia articles that contain at least 2 instances of subject and object entities of gender-specific relations.\n\n"]}
{"question_id": "cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "predicted_answer": " amazon mechanical turk", "predicted_evidence": ["amazon mechanical turk\n\n"]}
{"question_id": "29794bda61665a1fbe736111e107fd181eacba1b", "predicted_answer": " politifact, snopes, cnn, abc, cbs, nbc and various sources.", "predicted_evidence": ["our data sources include automatic extraction from web pages and manual construction by trained annotators. specifically, we collected fact-checked claims from politifact and snopes, two prominent fact-checking organizations. we also automatically extracted claims and headlines from news web pages, specifically cnn, abc, cbs, and nbc. additionally, we collected manually verified true and false claims from various sources. \n\n"]}
{"question_id": "dd80a38e578443496d3720d883ad194ce82c5f39", "predicted_answer": " fever, claimbuster, fullfact, liar-corpus", "predicted_evidence": ["the authors of the paper compare their corpus with existing corpora such as fever, claimbuster, fullfact, and liar-corpus. \n\n"]}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "predicted_answer": " 1205 documents", "predicted_evidence": [""]}
{"question_id": "4ed58d828cd6bb9beca1471a9fa9f5e77488b1d1", "predicted_answer": " lstms, cnns, clstm, and elmo.", "predicted_evidence": ["the paper describes experiments with a range of neural network models including lstms, convolutional neural networks (cnns), a combination of both (clstm), and a pre-trained language model-based model (elmo).  \n\n"]}
{"question_id": "de580e43614ee38d2d9fc6263ff96e6ca2b54eb5", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide information on the domains present in the corpus.\n\n"]}
{"question_id": "ae89eed483c11ccd70a34795e9fe416af8a35da2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fc62549a8f0922c09996a119b2b6a8b5e829e989", "predicted_answer": " perplexity and accuracy.", "predicted_evidence": ["the authors use perplexity and accuracy for quantitative analysis. \n\n"]}
{"question_id": "a3a867f7b3557c168d05c517c468ff6c7337bff9", "predicted_answer": " penn treebank", "predicted_evidence": ["we used the penn treebank dataset bibref19 to demonstrate the effectiveness of our proposed transfer learning schemes.\n\n"]}
{"question_id": "8bb2280483af8013a32e0d294e97d44444f08ab0", "predicted_answer": " perplexity, accuracy and f-score.", "predicted_evidence": ["the qualitative analysis was performed through the tasks of language modeling, text classification and sentiment analysis using standard datasets available in the literature, and the results obtained using the proposed scheme were compared with the existing state-of-the-art techniques using the metrics of perplexity, classification accuracy and f-score.\n\n"]}
{"question_id": "a68acd8364764d5601dc12e4b31d9102fb7d5f7e", "predicted_answer": " perplexity.", "predicted_evidence": ["the authors used perplexity as the metric for quantitative evaluation. \n\n"]}
{"question_id": "6d55e377335815b7ad134d1a2977d231ad34a25b", "predicted_answer": " unanswerable.", "predicted_evidence": ["in the paper, there is no mention of any similarity metric used for quantitative analysis.\n\n"]}
{"question_id": "0035b351df63971ec57e36d4bfc6f7594bed41ae", "predicted_answer": " unanswerable", "predicted_evidence": ["we do not have enough information about the document to answer this question.\n\n"]}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "predicted_answer": " ensemble model", "predicted_evidence": [""]}
{"question_id": "e801b6a6048175d3b1f3440852386adb220bcb36", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "3699927c6c1146f5057576034d226a99946d52cb", "predicted_answer": " english, german, spanish, swedish, and japanese.", "predicted_evidence": ["the article presents studies on five languages: english, german, spanish, swedish, and japanese.\n\n"]}
{"question_id": "6606160e210d05b94f7cbd9c5ff91947339f9d02", "predicted_answer": " yes", "predicted_evidence": ["yes\n  \n"]}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "predicted_answer": " yes", "predicted_evidence": ["yes, our results indicate that there are significant similarities across languages in the structure of conceptual categories for animals and non-living things, providing support for the notion of human universals. \n\n"]}
{"question_id": "4beb50ba020f624446ff1ef5bf4adca5ed318b98", "predicted_answer": " english, spanish, german, and chinese.", "predicted_evidence": ["the paper evaluates the model's performance on four languages: english, spanish, german, and chinese. \n\n"]}
{"question_id": "9bf60073fbb69fbf860196513fc6fd2f466535f6", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1c8958ec50976a9b1088c51e8f73a767fb3973fa", "predicted_answer": " rnn, lstm, and bilstm. \n\nexplanation: the paper mentions that \"three variations of recurrent neural networks are used to capture the sequential nature of the text: (1) rnn; (2) lstm; and (3) bidirectional lstm (bilstm)\" (section 2.4). so, the paper used these 3 types of rnn classifiers.", "predicted_evidence": [""]}
{"question_id": "363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "predicted_answer": " 95.4%", "predicted_evidence": ["the authors report a classification accuracy of 95.4\\% on the test set.\n\n"]}
{"question_id": "cf0b7d8a2449d04078f69ec9717a547adfb67d17", "predicted_answer": " feature engineering", "predicted_evidence": ["existing work in this area relies on feature engineering, which generally involves identifying words and phrases exhibiting certain morphological, syntactic or semantic properties that are known to be associated with offensive language. \n\n"]}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "predicted_answer": " unanswerable", "predicted_evidence": ["i'm sorry but there is no mention of any dataset in the document. \n\n"]}
{"question_id": "29477c8e28a703cacb716a272055b49e2439a695", "predicted_answer": " no", "predicted_evidence": ["no \n\n"]}
{"question_id": "9186b2c5b7000ab7f15a46a47da73ea45544bace", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "526dc757a686a1fe41e77f7e3848e3507940bfc4", "predicted_answer": " 4%", "predicted_evidence": ["lexicon pruning on a simple em algorithm improves segmentation quality by up to 4% points f-score. \n\n"]}
{"question_id": "2d91554c3f320a4bcfeb00aa466309074a206712", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "53362c2870cf76b7981c27b3520a71eb1e3e7965", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d68cc9aaf0466b97354600a5646c3be4512fc096", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide a clear answer to this question. however, it mentions that the corpus contains data from a variety of dialog tasks in the media domain, including news commentary and talk shows. \n\n"]}
{"question_id": "d038e5d2a6f85e68422caaf8b96cb046db6599fa", "predicted_answer": " manual annotation", "predicted_evidence": ["in total, the corpus includes 46,302 dialogues from 109 programs, annotated for speakers, their gender, their roles, dialogue acts, topics, named entities, as well as chat messages and emojis. the corpus was created using manual annotation and, where possible, verified against closed captions. \n\n"]}
{"question_id": "c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "predicted_answer": " bbc, cnn, and nbc \n\nexplanation: according to the paper, \"interview is a large-scale open-source corpus of media dialog from news outlets bbc, cnn, and nbc.\"", "predicted_evidence": [""]}
{"question_id": "369d7bc5351409910c7a5e05c0cbb5abab8e50ec", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "b9d9803ba24127f91ba4d7cff4da11492da20f09", "predicted_answer": " retrieval models, bert, and transformer.", "predicted_evidence": ["the authors compared their model against several baselines, including retrieval models and neural models like bert and transformer on several metrics. \n\n"]}
{"question_id": "7625068cc22a095109580b83eff48616387167c2", "predicted_answer": " dstc4", "predicted_evidence": ["the article mentions the dialog state tracking challenge 4 (dstc4) as a dialog task that the authors used to evaluate the performance of their proposed corpus. \n\n"]}
{"question_id": "be0b438952048fe6bb91c61ba48e529d784bdcea", "predicted_answer": " yes", "predicted_evidence": ["yes, the dataset is annotated using crowd-sourcing. \n\n"]}
{"question_id": "a24b2269b292fd0ee81d50303d1315383c594382", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper doesn't provide a list of news sources. \n\n"]}
{"question_id": "7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "predicted_answer": " 20 newsgroups, nips papers, imdb movie reviews", "predicted_evidence": ["we experiment on 3 publicly available real-word datasets: the 20 newsgroups dataset, the nips papers dataset and the imdb movie reviews dataset.\n\n"]}
{"question_id": "0fee37ebe0a010cf8bd665fa566306d8e7d12631", "predicted_answer": " several models", "predicted_evidence": ["table 4 shows the topic coherence scores for metalda and several other models that incorporate meta information, using both \"authors\" and \"venues\" as meta data.\n\n"]}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "predicted_answer": " coherence", "predicted_evidence": ["they used the coherence measure to evaluate topic quality. \n\n"]}
{"question_id": "d4cb704e93086a2246a8caa5c1035e8297b8f4c0", "predicted_answer": " no", "predicted_evidence": ["in the abstract, the authors state: \"the twitter job corpus provides a novel dataset that can be used to explore various natural language processing and machine learning approaches to assist job recruiters in extracting job-related information from social media posts.\" there is no clear mention of any constraints on this corpus.\n\n"]}
{"question_id": "a11b5eb928a6db9a0e3bb290ace468ff1685d253", "predicted_answer": " relevance, job type, employer name, job offer.", "predicted_evidence": ["the corpus is annotated for: (1) relevance of the tweet to job search, (2) job type, (3) the name of the employer if mentioned, and (4) whether the tweet is about a job offer or not. \n\n"]}
{"question_id": "275b2c22b6a733d2840324d61b5b101f2bbc5653", "predicted_answer": " through a combination of keyword-based and hashtag-based queries and retweets.", "predicted_evidence": ["tweets were selected through a combination of keyword-based and hashtag-based queries focused on employment and job-seeking, as well as retweets of these initial tweets. the data collection took place between august and november 2018 and resulted in 9,219 tweets. \n\n"]}
{"question_id": "f1f7a040545c9501215d3391e267c7874f9a6004", "predicted_answer": " wikipedia dataset", "predicted_evidence": ["the authors did not mention explicitly which dataset they used, but they introduced a new dataset for named entity recognition in pt-br, drawn from wikipedia. \n\n"]}
{"question_id": "b6f4fd6bc76bfcbc15724a546445908afa6d922c", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence.\n\n"]}
{"question_id": "3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "predicted_answer": " no", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "c316d7d0c80b8f720ff90a8bb84a8b879a3ef7ea", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a786cceba4372f6041187c426432853eda03dca6", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper proposes a new model and does not compare its performance to existing state-of-the-art models.\n\n"]}
{"question_id": "a837dcbd339e27a974e28944178c790a5b0b37c0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "predicted_answer": " manually annotated with the help of experts", "predicted_evidence": ["labels for trolls\nthe authors obtained labels for trolls by utilizing experts and manually annotating large amounts of data. they annotated 10,000 tweets from known russian trolls and 10,000 tweets from other users for training and testing purposes. they also used a validation dataset of 10,000 tweets which was constructed by twitter itself. for the validation and the further annotation of additional datasets, they utilized the help of subject matter experts. the final labels for trolls and non-troll accounts are created through a combination of automated classifiers with further manual annotation. \n\n"]}
{"question_id": "16a10c1681dc5a399b6d34b4eed7bb1fef816dd0", "predicted_answer": " yes.", "predicted_evidence": ["yes, the study focuses on detecting political trolls in twitter. \n\n"]}
{"question_id": "2ca3ca39d59f448e30be6798514709be7e3c62d8", "predicted_answer": " children's book test, cnn/daily mail cloze dataset, two subsets of the stanford question answering dataset", "predicted_evidence": ["we evaluate our model on four datasets: the children's book test (cbt, hill et al., 2015), the cnn/daily mail cloze dataset (hermann et al., 2015), and two subsets of the stanford question answering dataset (squad, rajpurkar et al., 2016).\n\n"]}
{"question_id": "df7fb8e6e44c9c5af3f19dde762c75cbf2f8452f", "predicted_answer": " unanswerable\n\nexplanation: the article discusses the attention sum reader (asreader) network, which is a deep neural network for question answering, but it does not provide specific information on the performance of the model on any particular dataset.", "predicted_evidence": [""]}
{"question_id": "20e2b517fddb0350f5099c39b16c2ca66186d09b", "predicted_answer": " stanford attentive reader.", "predicted_evidence": ["they compare their model against several baselines, one of which is the stanford attentive reader. \n\n"]}
{"question_id": "70512cc9dcd45157e40c8d1f85e82d21ade7645b", "predicted_answer": " children's book test, cnn/daily mail, and stanford question answering dataset.", "predicted_evidence": ["the model was evaluated on several datasets including children's book test, cnn/daily mail, and stanford question answering dataset. \n\n"]}
{"question_id": "fd556a038c36abc88a800d9d4f2cfa0aef6f5aba", "predicted_answer": " 0.82", "predicted_evidence": ["the authors achieved an inter-annotator agreement of 0.82 on the subset of 500 target verb instances. \n\n"]}
{"question_id": "9119fbfba84d298014d1b74e0e3d30330320002c", "predicted_answer": " no", "predicted_evidence": [""]}
{"question_id": "058b6e3fdbb607fa7dbfc688628b3e13e130c35a", "predicted_answer": " yes", "predicted_evidence": ["yes \n\n"]}
{"question_id": "5b95665d44666a1dc9e568d2471e5edf8614859f", "predicted_answer": " yes, lstms achieve a similar performance as humans on the task.", "predicted_evidence": [""]}
{"question_id": "740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper does not mention any social media platforms. \n\n"]}
{"question_id": "845bdcd900c0f96b2ae091d086fb1ab8bb1063f0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "predicted_answer": " brazil, china, germany, spain, united kingdom and the united states; english, portuguese, spanish, chinese, and german.", "predicted_evidence": ["the datasets contain texts from various countries around the world, including brazil, china, germany, spain, united kingdom, and the united states. the languages covered in the datasets include english, portuguese, spanish, chinese, and german.\n\n"]}
{"question_id": "bc05503eef25c732f1785e29d59b6022f12ba094", "predicted_answer": " yes, they used rouge-1, rouge-2, and rouge-su4 in addition to rouge-l.", "predicted_evidence": [""]}
{"question_id": "a6603305f4fd3dd0010ac31243c40999a116537e", "predicted_answer": " separately", "predicted_evidence": ["they encode sentences separately. in this paper, the authors proposed to extract sentences independently with a pretrained language model. they extract a sequence of sentence segments from the text and represent each segment with the output of the corresponding token position in the bert embeddings. \n\n"]}
{"question_id": "2ba4477d597b1fd123d14be07a7780ccb5c4819b", "predicted_answer": " using special classification and separator tokens.", "predicted_evidence": ["they use the pre-trained bert model to encode the whole text by adding special classification/separator tokens at the beginning and end of each sentence within the text, and then feeding the modified text as input to bert. \n\n"]}
{"question_id": "027814f3a879a6c7852e033f9d99519b8729e444", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "00df1ff914956d4d23299d02fd44e4c985bb61fa", "predicted_answer": " textrank.", "predicted_evidence": ["the baseline method is textrank. \n\n"]}
{"question_id": "b57ad10468e1ba2a7a34396688dbb10a575d89f5", "predicted_answer": " ranking-based", "predicted_evidence": ["they have used a ranking-based loss function to train the model.\n\n"]}
{"question_id": "9d6d17120c42a834b2b5d96f2120d646218ed4bb", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8dfdd1ed805bb23c774fbb032ef1d97c6802e07c", "predicted_answer": " no", "predicted_evidence": ["no, we use two datasets which represent different domain of data. the first dataset includes tables from a wide range of sources, including wikipedia and various university websites. the second dataset, denoted as the \"movie dataset\", includes tables taken from imdb. \n\n"]}
{"question_id": "c21675d8a90bda624d27e5535d1c10f08fcbc16b", "predicted_answer": " using html tags. \n\nexplanation: the tables are extracted and preprocessed using the html tags that precede them in the document object model (dom) of the web page.", "predicted_evidence": [""]}
{"question_id": "8ad5ebca2f69023b60ccfa3aac0ed426234437ac", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "4afd4cfcb30433714b135b977baff346323af1e3", "predicted_answer": " shopping list-4, reinforcement learning dataset-2 (rql2)", "predicted_evidence": ["we conducted experiments on two widely used benchmark datasets for shopping conversation, i.e., the shopping list-4 and reinforcement learning dataset-2 (rql2) datasets bibref16, which are composed of 6,246 and 11,942 multi-turn dialogue sessions respectively. \n\n"]}
{"question_id": "b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "predicted_answer": " 18", "predicted_evidence": ["the authors consider 18 lexical features for their zero-shot cross-lingual semantic parsing system.\n\n"]}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not present a single performance score for all three languages combined, but rather separate results for each language (i.e., english, chinese and german).\n\n"]}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b012df09fa2a3d6b581032d68991768cf4bc9d7b", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "predicted_answer": " wikipedia and common crawl.", "predicted_evidence": ["the paper uses pre-trained cross-lingual word embeddings trained on wikipedia and common crawl. \n\n"]}
{"question_id": "07a214748a69b31400585aef7aba6af3e3d9cce2", "predicted_answer": " english, spanish, and italian.", "predicted_evidence": ["the paper mentions that they experimented with three different languages: english, spanish, and italian.\n\n"]}
{"question_id": "44bf3047ff7e5c6b727b2aaa0805dd66c907dcd6", "predicted_answer": " one", "predicted_evidence": ["each dialogue has one abstractive summary.\n\n"]}
{"question_id": "c6f2598b85dc74123fe879bf23aafc7213853f5b", "predicted_answer": " informativeness, fluency, and overall quality.", "predicted_evidence": ["the human evaluations were conducted with the help of amazon mechanical turk (amt). in accordance with the previous summarization studies, the following criteria were provided to annotators: informativeness, fluency, and overall quality. \n\n"]}
{"question_id": "bdae851d4cf1d05506cf3e8359786031ac4f756f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "894bbb1e42540894deb31c04cba0e6cfb10ea912", "predicted_answer": " yes", "predicted_evidence": ["yes, the authors investigate three variants of rouge and propose a new metric called dialogue rouge for measuring the performance of abstractive dialogue summarization models. \n\n"]}
{"question_id": "75b3e2d2caec56e5c8fbf6532070b98d70774b95", "predicted_answer": " 11,883", "predicted_evidence": ["the samsum corpus contains 11,883 single-turn conversations with an average of 7.9 utterances per conversation. \n\n"]}
{"question_id": "573b8b1ad919d3fd0ef7df84e55e5bfd165b3e84", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "07d98dfa88944abd12acd45e98fb7d3719986aeb", "predicted_answer": " no", "predicted_evidence": ["no: the paper generates adversarial examples, which may not be semantically equivalent to the original text. \n\n"]}
{"question_id": "3a40559e5a3c2a87c7b9031c89e762b828249c05", "predicted_answer": " yes", "predicted_evidence": ["results show that the proposed approach outperforms existing methods significantly with a success rate of 91.29%. \n\n"]}
{"question_id": "5db47bbb97282983e10414240db78154ea7ac75f", "predicted_answer": " fasttext, textcnn, charcnn, and textrcnn", "predicted_evidence": ["the approach successfully generated adversarial examples that fooled four of the five tested models: fasttext, textcnn, charcnn, and textrcnn. the only model not successfully fooled was the lstm.\n\n"]}
{"question_id": "c589d83565f528b87e355b9280c1e7143a42401d", "predicted_answer": " imdb sentiment classification\n\nexplanation: the proposed approach can fool state-of-the-art models such as character-level cnns and lstms on the imdb sentiment classification task.", "predicted_evidence": [""]}
{"question_id": "7f90e9390ad58b22b362a57330fff1c7c2da7985", "predicted_answer": " yes", "predicted_evidence": ["yes, the pre-trained model is used as a text classifier within the reinforcement learning approach.\n\n"]}
{"question_id": "3e3e45094f952704f1f679701470c3dbd845999e", "predicted_answer": " using reinforcement learning method.", "predicted_evidence": ["proposed approach generates adversarial examples in black-box settings by using the reinforcement learning method to learn the decision boundary of the targeted model and perturbing the input based on the gradients of this boundary.\n\n"]}
{"question_id": "475ef4ad32a8589dae9d97048166d732ae5d7beb", "predicted_answer": " arabic, mandarin, and russian.", "predicted_evidence": ["arabic, mandarin, and russian are among the non-latin languages analyzed. \n\n"]}
{"question_id": "3fd8eab282569b1c18b82f20d579b335ae70e79f", "predicted_answer": " 104 languages.", "predicted_evidence": ["the authors experimented with 104 languages from diverse families, covering 29 families.\n  \n"]}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "predicted_answer": " yes (multiple languages)", "predicted_evidence": ["our analysis confirms that mbert significantly surpasses the performance of monolingual models in 5/11 languages and is a competitive method in the others. ... we report on the relation between language similarity and bert performance, showing that more similar languages generally benefit from shared multi-language representations.\n\n"]}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "predicted_answer": " multinli, xnli, paws-x, mlqa, squad, and a parallel corpus derived from wikipedia.", "predicted_evidence": ["multinli, xnli, paws-x, mlqa, squad, and a parallel corpus derived from wikipedia. \n\n"]}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "predicted_answer": " polls based models, prediction markets, and betting-market based models.", "predicted_evidence": ["we compared our predictions with several existing models, including polls based models, prediction markets, and betting-market based models. \n\n"]}
{"question_id": "a2baa8e266318f23f43321c4b2b9cf467718c94a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "97ff88c31dac9a3e8041a77fa7e34ce54eef5a76", "predicted_answer": " unanswerable", "predicted_evidence": ["they do not mention a recommendation task in the abstract or introduction and there is no evidence of evaluating their model on a recommendation task in the paper. \n\n"]}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "860257956b83099cccf1359e5d960289d7d50265", "predicted_answer": " lstm network", "predicted_evidence": ["we propose an iterative multi-document neural attention model for multiple answer prediction. our model is based on a two layer deep lstm network, followed by an attention mechanism. additionally, we introduce a novel iterative refinement mechanism to improve answer quality and consistency. \n\n"]}
{"question_id": "1b1849ad0bdd79c6645572849fe7873ec7bd7e6d", "predicted_answer": " unanswerable", "predicted_evidence": ["there is no mention of reddit or twitter datasets in the article. \n\n"]}
{"question_id": "deb0c3524a3b3707e8b20abd27f54ad6188d6e4e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d7e43a3db8616a106304ac04ba729c1fee78761d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "predicted_answer": " socher et al. (2014), dos santos and gatti (2014)", "predicted_evidence": ["sentiment analysis: to the best of our knowledge, there is no existing work on utilizing tweet embeddings for sentiment analysis. recent works on sentiment analysis include socher et al., (2014), and dos santos and gatti (2014), both classifying sentiment on long form text, rather than on tweets. \n\n"]}
{"question_id": "b7d02f12baab5db46ea9403d8932e1cd1b022f79", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper compares the performance of tweet2vec with baselines and reports an improvement in several semantic-related tasks, but there is no explicit mention of the previous state of the art for tweet semantic similarity.\n\n"]}
{"question_id": "ff2b58c90784eda6dddd8a92028e6432442c1093", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5e4eac0b0a73d465d74568c21819acaec557b700", "predicted_answer": " naive bayes, bag-of-words, tf-idf", "predicted_evidence": ["the paper compares their model with three baseline models: naive bayes, bag-of-words and tf-idf.\n\n"]}
{"question_id": "bc6ad5964f444cf414b661a4b942dafb7640c564", "predicted_answer": " two datasets: one from ice hockey and another from basketball.", "predicted_evidence": ["they evaluate their model on two datasets: one from the world of ice hockey (play-by-play data from the national hockey league), and another from basketball (play-by-play data from the national basketball association).\n\n"]}
{"question_id": "380e71848d4b0d1e983d504b1249119612f00bcb", "predicted_answer": " cnn, rnn, cnn-rnn", "predicted_evidence": ["the authors investigate the effectiveness of different deep learning models, such as convolutional neural networks (cnn), recurrent neural networks (rnn), and combinations thereof (cnn-rnn). \n\n"]}
{"question_id": "21c89ee0281f093b209533453196306b9699b552", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5096aaea2d0f4bea4c12e14f4f7735e1aea1bfa6", "predicted_answer": " cnn, bi-lstm \n\nexplanation: the authors state, \"our experiments for deep learning models include convolutional neural networks(cnn) and bidirectional long-short-term memory networks(bi-lstm).\"", "predicted_evidence": ["the authors experiment with convolutional neural networks (cnn) and bidirectional long-short-term memory networks (bi-lstm) architectures. \n\n"]}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "predicted_answer": " yes", "predicted_evidence": ["yes, they used both word2vec and glove pretrained word embeddings. \n\n"]}
{"question_id": "4cb2e80da73ae36de372190b4c1c490b72977ef8", "predicted_answer": " no", "predicted_evidence": ["the paper includes information on how tie strength was operationalized as the number of interactions in a given period of time, and how results are compared to those obtained using alternative measures of tie strength.\n\n"]}
{"question_id": "a064337bafca8cf01e222950ea97ebc184c47bc0", "predicted_answer": " none (phonetic spellings were not analyzed)", "predicted_evidence": ["the authors did not analyze phonetic spellings as sociolinguistic variables, but rather they studied the frequency of usage of certain words and phrases.\n\n"]}
{"question_id": "993d5bef2bf1c0cd537342ef76d4b952f0588b83", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not mention any specific older dialect markers that were explored. \n\n"]}
{"question_id": "a8e5e10d13b3f21dd11e8eb58e30cc25efc56e93", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "70abb108c3170e81f8725ddc1a3f2357be5a4959", "predicted_answer": " classification capability", "predicted_evidence": ["the usefulness of the obtained ontologies is evaluated in comparison to manually constructed ones by domain experts with respect to the capability of classifying articles belonging to different application domains.\n\n"]}
{"question_id": "ce504a7ee2c1f068ef4dde8d435245b4e77bb0b5", "predicted_answer": " dependency parser", "predicted_evidence": ["the syntax for each document is represented by extracting noun phrases using a dependency parser. \n\n"]}
{"question_id": "468eb961215a554ace8088fa9097a7ad239f2d71", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "57d07d2b509c5860880583efe2ed4c5620a96747", "predicted_answer": " unanswerable", "predicted_evidence": ["two novel metrics were not mentioned in the paper. \n\n"]}
{"question_id": "d126d5d6b7cfaacd58494f1879547be9e91d1364", "predicted_answer": " cosine, jaccard, bm25, bm25+.", "predicted_evidence": ["the authors experimented with four similarity metrics - cosine similarity, jaccard similarity, bm25, and bm25+.\n\n"]}
{"question_id": "7dca806426058d59f4a9a4873e9219d65aea0987", "predicted_answer": " book, camera, dvd, electronics, health & personal care, kitchen & housewares, magazines, music, software, sports & outdoors, apparel, baby, computers, electronics (others), gourmet food, jewelry, music (others), office products, toys & games, video, video games.", "predicted_evidence": ["the paper provides a chart with the 20 available domains which are: book, camera, dvd, electronics, health & personal care, kitchen & housewares, magazines, music, software, sports & outdoors, apparel, baby, computers, electronics (others), gourmet food, jewelry, music (others), office products, toys & games, video, video games.\n\n"]}
{"question_id": "800fcd8b08d36c5276f9e5e1013208d41b46de59", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper mentions that adding sentiment features did not result in improvement, but they do not give a specific reason why. \n\n"]}
{"question_id": "cdbbba22e62bc9402aea74ac5960503f59e984ff", "predicted_answer": " unanswerable", "predicted_evidence": ["the training and development datasets were provided by the smm4h organizers, while the test dataset comprised of unlabeled tweets which needed to be classified using the developed models.\n\n"]}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "predicted_answer": " precision, recall and f1-score.", "predicted_evidence": ["the authors used precision, recall and f1-score for evaluation. \n\n"]}
{"question_id": "f3673f6375f065014e8e4bb8c7adf54c1c7d7862", "predicted_answer": " f1 measures reported, but no specific numbers are provided.", "predicted_evidence": ["the results are reported using f1 measures. the official metric for task 1 was macro-averaged f1-score and for task 2 was micro-averaged f1-score.\n\n"]}
{"question_id": "0bd3bea892c34a3820e98c4a42cdeda03753146b", "predicted_answer": " handcrafted features, negation, semantic qualifiers, meddra.", "predicted_evidence": ["the authors mention training on handcrafted features such as negation, semantic qualifiers, and domain-specific dictionaries like meddra.\n\n"]}
{"question_id": "8cf5abf0126f19253930478b02f0839af28e4093", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "predicted_answer": " pruned dictionary of feature unigrams and bigrams.", "predicted_evidence": ["the authors used the dictionary of surface-form features as the initial set of features. however, the dictionary was pruned by only keeping the feature unigrams and bigrams that had an absolute correlation of at least 0.01 with any of the outcome labels in the training set. \n\n"]}
{"question_id": "c3ce95658eea1e62193570955f105839de3d7e2d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "predicted_answer": " by using internal hyperlinks and selecting relevant sentences", "predicted_evidence": ["they use wikipedia's internal hyperlinks to identify the most relevant sentences for each query and then use those sentences as summaries for the query.\n\n"]}
{"question_id": "2c4db4398ecff7e4c1c335a2cb3864bfdc31df1a", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "predicted_answer": " no", "predicted_evidence": [""]}
{"question_id": "4dadde7c61230553ef14065edd8c1c7e41b9c329", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "014830892d93e3c01cb659ad31c90de4518d48f3", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "ae7c5cf9c2c121097eb00d389cfd7cc2a5a7d577", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "af948ea91136c700957b438d927f58d9b051c97c", "predicted_answer": " large.", "predicted_evidence": ["the hotpotqa dataset is one of the largest multi-hop q&a datasets, consisting of 113k qa pairs over wikipedia articles. \n\n"]}
{"question_id": "a913aa14d4e05cc9d658bf6697fe5b2652589b1b", "predicted_answer": " bio", "predicted_evidence": ["they use bio labeling scheme for sequence labeling. \n\n"]}
{"question_id": "b065a3f598560fdeba447f0a100dd6c963586268", "predicted_answer": " the word-representation layers, pos layers, and dependency layers are shared in the multitask model.", "predicted_evidence": [""]}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a59e86a15405c8a11890db072b99fda3173e5ab2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9489b0ecb643c1fc95c001c65d4e9771315989aa", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "17fd6deb9e10707f9d1b70165dedb045e1889aac", "predicted_answer": " recall, mrr", "predicted_evidence": ["they use recall and mrr metrics.\n\n"]}
{"question_id": "1faccdc78bbd99320c160ac386012720a0552119", "predicted_answer": " wikidata, freebase", "predicted_evidence": ["the authors used wikidata and freebase as knowledge bases for their experiments.\n\n"]}
{"question_id": "804466848f4fa1c552f0d971dce226cd18b9edda", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8d683d2e1f46626ceab60ee4ab833b50b346c29e", "predicted_answer": " webqsp and comqa", "predicted_evidence": ["we conduct extensive experiments on the complex question answering benchmarks of webqspbibref4 and comqa and show that our method significantly outperforms existing methods while being more interpretable and easily understood by non-expert users.\n\n"]}
{"question_id": "5ae005917efc17a505ba1ba5e996c4266d6c74b6", "predicted_answer": " yes", "predicted_evidence": ["they mention in the introduction that they train on the same dataset (wikipedia 2017) as skip-gram. \n\n"]}
{"question_id": "72c04eb3fc323c720f7f8da75c70f09a35abf3e6", "predicted_answer": " yes.", "predicted_evidence": ["the authors report significant improvements in various evaluation metrics. \n\n"]}
{"question_id": "0715d510359eb4c851cf063c8b3a0c61b8a8edc0", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide information on the specific extractive technique used for summarization. \n\n"]}
{"question_id": "4e106b03cc2f54373e73d5922e97f7e5e9bf03e4", "predicted_answer": " 359", "predicted_evidence": ["the dataset contains 359 legal agreements. \n\n"]}
{"question_id": "f8edc911f9e16559506f3f4a6bda74cde5301a9a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a464052fd11af1d2d99e407c11791269533d43d1", "predicted_answer": " bayesian non-parametrics models.", "predicted_evidence": ["bayesian non-parametrics models are used. \n\n"]}
{"question_id": "5f6c1513cbda9ae711bc38df08fe72e3d3028af2", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "130d73400698e2b3c6860b07f2e957e3ff022d48", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7e9aec2bdf4256c6249cad9887c168d395b35270", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9ce90f4132b34a328fa49a63e897f376a3ad3ca8", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not mention any tasks used to quantify embedding quality.\n\n"]}
{"question_id": "3138f916e253abed643d3399aa8a4555b2bd8c0f", "predicted_answer": " three state-of-the-art debiasing methods are used for empirical comparison, namely hard debias, rebias, and dro.", "predicted_evidence": ["we compare aicn with three state-of-the-art debiasing methods, hard debias, rebias, and dro from three aspects: debiasing effectiveness (section 5.1), utility preservation of the gender-unbiased models (section 5.2), and generalization ability of the debiased models to gender-specific documents (section 5.3).\n\n"]}
{"question_id": "810e6d09813486a64e87ef6c1fb9b1e205871632", "predicted_answer": " characters and phonemes.", "predicted_evidence": ["they use a combination of characters and phonemes as subword units (word-pieces) defined by a byte-pair encoding algorithm. \n\n"]}
{"question_id": "ab8b0e6912a7ca22cf39afdac5531371cda66514", "predicted_answer": " unanswerable.", "predicted_evidence": ["we cannot determine the answer as there is no information provided in the document regarding performance comparison with existing state-of-the-art models on end-to-end speech recognition. \n\n"]}
{"question_id": "89373db8ced1fe420eae0093b2736f06b565616e", "predicted_answer": " no", "predicted_evidence": ["no\n\n"]}
{"question_id": "74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "predicted_answer": " bleu score, meteor, and nist.", "predicted_evidence": ["the translation quality was evaluated using standard evaluation metrics such as bleu score, meteor, and nist. \n\n"]}
{"question_id": "4b6745982aa64fbafe09f7c88c8d54d520b3f687", "predicted_answer": " english-french, english-german", "predicted_evidence": ["our experiments were conducted on the en-fr and en-de wmt'14 dataset.\n\n"]}
{"question_id": "6656a9472499331f4eda45182ea697a4d63e943c", "predicted_answer": " europarl, iwslt, un.", "predicted_evidence": ["table 1 shows the datasets used in the experiments: europarl (ep) is a parallel corpus of parliamentary proceedings in the european parliament. we use english and target languages french (fr), german (de), spanish (es), dutch (nl), italian (it), portuguese (pt), czech (cs), and estonian (et); iwslt is a corpus of ted talks for different language pairs. we use english and target languages italian (it), dutch (nl), and estonian (et); un is a corpus of united nations documents translated by the european union. we use english and target languages french (fr) and spanish (es).\n\n"]}
{"question_id": "430ad71a0fd715a038f3c0fe8d7510e9730fba23", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b79ff0a50bf9f361c5e5fed68525283856662076", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "d66c31f24f582c499309a435ec3c688dc3a41313", "predicted_answer": " yes", "predicted_evidence": ["the authors used several baseline methods, including bm25, textrank, and doc2vec. \n\n"]}
{"question_id": "c47312f2ca834ee75fa9bfbf912ea04239064117", "predicted_answer": " dataset of product images and text descriptions", "predicted_evidence": ["the authors used a dataset of product images and associated text descriptions crawled from a retailer's website. \n\n"]}
{"question_id": "de313b5061fc22e8ffef1706445728de298eae31", "predicted_answer": " pubmed", "predicted_evidence": ["data was obtained by scraping pubmed for toxicity studies in human, rat, and mouse models. \n\n"]}
{"question_id": "0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "1397b1c51f722a4ee2b6c64dc9fc6afc8bd3e880", "predicted_answer": " a word or phrase that describes a specific aspect of a toxicology study", "predicted_evidence": ["a study descriptor is a word or phrase that describes a specific aspect of a toxicology study, such as the chemical substance being tested or the type of endpoint being measured. \n\n"]}
{"question_id": "230f127e83ac62dd65fccf6b1a4960cf0f7316c7", "predicted_answer": " yes", "predicted_evidence": ["the experiments are designed to modify one of the three key parameters of clr -- min lr, max lr, and step size, and measure the impact of the changes by conducting experiments. \n\n"]}
{"question_id": "75c221920bee14a6153bd5f4c1179591b2f48d59", "predicted_answer": " improvement", "predicted_evidence": ["the experimental results indicate that using clr can improve the translation quality by up to 2 bleu points over standard optimizer training including adam, sgd, adagrad, and adadelta using static learning rates.\n\n"]}
{"question_id": "4eb42c5d56d695030dd47ea7f6d65164924c4017", "predicted_answer": " ambient.", "predicted_evidence": ["the paper mentions that the audio samples fall under \"ambient\" domain, \"the general background noise present in the environment\". \n\n"]}
{"question_id": "eff9192e05d23e9a67d10be0c89a7ab2b873995b", "predicted_answer": " human evaluation", "predicted_evidence": ["the authors used human evaluations for quality control to ensure each audio clip has accurate, concise, and relevant captions.\n\n"]}
{"question_id": "87523fb927354ddc8ad1357a81f766b7ea95f53c", "predicted_answer": " 7", "predicted_evidence": ["seven annotators \n\n"]}
{"question_id": "9e9aa8af4b49e2e1e8cd9995293a7982ea1aba0e", "predicted_answer": " vggish, vggish + lstm, and audioset.", "predicted_evidence": ["we employ multiple baseline models, including vggish, vggish + lstm, and audioset. \n\n"]}
{"question_id": "1fa9b6300401530738995f14a37e074c48bc9fd8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "9d98975ab0b75640b2c83e29e1438c76a959fbde", "predicted_answer": " 10.8", "predicted_evidence": ["the dataset contains image-caption pairs and the average length of captions is 10.8 tokens. \n\n"]}
{"question_id": "cc8bcea4052bf92f249dda276acc5fd16cac6fb4", "predicted_answer": " no. \n\nexplanation: the vsts dataset consists of image-caption pairs, where each image has five captions.", "predicted_evidence": [""]}
{"question_id": "35f48b8f73728fbdeb271b170804190b5448485a", "predicted_answer": " 1,000", "predicted_evidence": ["the dataset consists of 1,000 sentence pairs.\n\n"]}
{"question_id": "16edc21a6abc89ee2280dccf1c867c2ac4552524", "predicted_answer": " microsoft coco dataset and the abstract scenes dataset.", "predicted_evidence": ["the images come from the microsoft coco dataset and the textual captions are collected from the abstract scenes dataset. \n\n"]}
{"question_id": "3b8da74f5b359009d188cec02adfe4b9d46a768f", "predicted_answer": " f1-score, accuracy", "predicted_evidence": ["the f1-score and accuracy were used.\n\n"]}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "37e6ce5cfc9d311e760dad8967d5085446125408", "predicted_answer": " unanswerable. \n\nexplanation: the document doesn't provide specific information about roberta's results, as the focus is on the comparison between different pre-training models for named entity recognition.", "predicted_evidence": [""]}
{"question_id": "6683008e0a8c4583058d38e185e2e2e18ac6cf50", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence.\n\n"]}
{"question_id": "7bd24920163a4801b34d0a50aed957ba8efed0ab", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "predicted_answer": " mnli", "predicted_evidence": ["we have used the larger natural language inference (nli) dataset called the multi-genre natural language inference corpus, abbreviated as mnli, which provides sentence pairs which are more closely matched in certain features such as length, structure, entities etc., as opposed to the more heterogeneous examples provided by snli bibref7.\n\n"]}
{"question_id": "a7a433de17d0ee4dd7442d7df7de17e508baf169", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper proposes a method for aspect-based sentiment analysis which considers the aspect terms and their associated opinion words in the sentence. the specific aspects considered would depend on the dataset and domain used for training and testing. \n\n"]}
{"question_id": "abfa3daaa984dfe51289054f4fb062ce93f31d19", "predicted_answer": " intermediate layers", "predicted_evidence": ["in this research, experiments for both datasets indicate that features generated from the intermediate layers of bert-based models perform better than the final layer representations.\n\n"]}
{"question_id": "1702985a3528e876bb19b8e223399729d778b4e4", "predicted_answer": " 4", "predicted_evidence": ["four annotators were used for sentiment labeling.\n\n"]}
{"question_id": "f44a9ed166a655df1d54683c91935ab5e566a04f", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not provide information on how the data was collected.\n\n"]}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "predicted_answer": " unclear", "predicted_evidence": ["the paper reports that \"adding context-specific words increased performance by up to 6% f1\" for sentiment classification of nigerian pidgin english, but it is not clear if the original english-only models were used as a baseline for comparison.\n\n"]}
{"question_id": "d14118b18ee94dafe170439291e20cb19ab7a43c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "ff668c7e890064756cdd2f9621e1cedb91eef1d0", "predicted_answer": " they use bootstrapping technique.", "predicted_evidence": ["we approach the task of multi-class emotion detection from textual inputs using a bootstrapping technique that utilizes contextual information, word embeddings, and a machine learning model. \n\n"]}
{"question_id": "d3cfbe497a30b750a8de3ea7f2cecf4753a4e1f9", "predicted_answer": " glove \n\nexplanation: in section 4.1, the authors explain that they used pre-trained word embeddings on twitter data. specifically, they used glove embeddings trained on twitter data with 200 dimensions and a vocabulary of 1.2 million words.", "predicted_evidence": ["the authors used pre-trained glove word embeddings on twitter. \n\n"]}
{"question_id": "73d87f6ead32653a518fbe8cdebd81b4a3ffcac0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fda47c68fd5f7b44bd539f83ded5882b96c36dd7", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "643645e02ffe8fde45918615ec92013a035d1b92", "predicted_answer": " wmt'16", "predicted_evidence": ["the authors used bilingual data from the wmt'16 news translation task. \n\n"]}
{"question_id": "a994cc18046912a8c9328dc572f4e4310736c0e2", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "predicted_answer": " preprocessing", "predicted_evidence": ["we use a standard preprocessing pipeline including tokenization, stopword removal, and language identification. additionally, we remove mentions, urls, hashtags, and other entities that don\u2019t contribute to socioeconomic status inference. \n\n"]}
{"question_id": "2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "predicted_answer": " linear regression model.", "predicted_evidence": ["the authors build a linear regression model to estimate socioeconomic status based on location, occupation, and semantic features. \n\n"]}
{"question_id": "892ee7c2765b3764312c3c2b6f4538322efbed4e", "predicted_answer": " 4.5 million", "predicted_evidence": ["we gather roughly 4.5 million tweets posted by users in the united states over a period of one year. \n\n"]}
{"question_id": "c68946ae2e548ec8517c7902585c032b3f3876e6", "predicted_answer": " yes", "predicted_evidence": ["yes, the features are location, occupation, and semantics. \n\n"]}
{"question_id": "7557f2c3424ae70e2a79c51f9752adc99a9bdd39", "predicted_answer": " two inference models are proposed.\n\n", "predicted_evidence": ["the paper proposes two inference models: 1) a location-based model, and 2) an occupation-based model. \n\n"]}
{"question_id": "b03249984c26baffb67e7736458b320148675900", "predicted_answer": " multiple models are used.", "predicted_evidence": ["they use four baseline models:  \n\n1) individual demographics-based logistic regression.\n\n2) only metadata-based logistic regression.\n\n3)both metadata and demographics-based logistic regression. \n\n4) only text-based logistic regression\n\n"]}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "08c0d4db14773cbed8a63e69381a2265e85f8765", "predicted_answer": " linkedin.", "predicted_evidence": ["the professional profiles were crawled from linkedin.\n\n"]}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "26844cec57df6ff0f02245ea862af316b89edffe", "predicted_answer": " yes", "predicted_evidence": ["yes, they train discourse relation models with augmented data obtained using the proposed method, and compare its effectiveness with several other methods. \n\n"]}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "predicted_answer": " 60", "predicted_evidence": [""]}
{"question_id": "4d824b49728649432371ecb08f66ba44e50569e0", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "02a5acb484bda77ef32a13f5d93d336472cf8cd4", "predicted_answer": " unanswerable", "predicted_evidence": ["they haven't specified any databases used.\n\n"]}
{"question_id": "863d8d32a1605402e11f0bf63968a14bcfd15337", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "predicted_answer": " using four sets of second-level regionalization data.", "predicted_evidence": ["we combine the socioeconomic maps with the twitter dataset using four sets of second-level regionalization data: (1) county, (2) census tract, (3) zip code, and (4) zip code tabulation area (zcta). \n\n"]}
{"question_id": "90756bdcd812b7ecc1c5df2298aa7561fd2eb02c", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "predicted_answer": " unanswerable", "predicted_evidence": ["the authors did not mention defining standard language in the paper.\n\n"]}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "143409d16125790c8db9ed38590a0796e0b2b2e2", "predicted_answer": " 10 to 300", "predicted_evidence": ["the paper considers embeddings with dimensions ranging from 10 to 300.\n\n"]}
{"question_id": "8ba582939823faae6822a27448ea011ab6b90ed7", "predicted_answer": " global structures are considered.", "predicted_evidence": ["global structures like the constituent parse tree are taken into account to help predict missing words. \n\n"]}
{"question_id": "65c7a2b734dab51c4c81f722527424ff33b023f8", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "11ef46187a5bf15e89d63220fdeaecbeb92d818e", "predicted_answer": " portuguese-spanish, french-spanish, and italian-spanish.", "predicted_evidence": ["we experiment on three language pairs related to spanish: portuguese-spanish, french-spanish, and italian-spanish. \n\n"]}
{"question_id": "bf5e80f1ab4eae2254b4f4d7651969a3cf945fb4", "predicted_answer": " 3", "predicted_evidence": ["they experiment with three different numbers of steps: 1000, 2000, and 4000.\n\n"]}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "predicted_answer": " differences in emotional and semantic cues.", "predicted_evidence": ["we found that satire tends to use more benign positive emotions like amusement, whereas fake news tends to elicit stronger, more negative emotions like anger and disgust. in terms of sematic cues, fake news uses more extreme language and deceptive phrases, while satire tends to include more hedging and ironic language.\n\n"]}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "predicted_answer": " various empirical evaluations", "predicted_evidence": ["the article describes the use of various empirical evaluations, including a corpus analysis, a user study, and an evaluation using machine learning classifiers.\n\n"]}
{"question_id": "71b07d08fb6ac8732aa4060ae94ec7c0657bb1db", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "predicted_answer": " bert", "predicted_evidence": ["bert is used to extract features from articles and headlines.\n\n"]}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "4c7ec282697f4f6646eb1c19f46bbaf8670b0de6", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "07104dd36a0e7fdd2c211ad710de9a605495b697", "predicted_answer": " with multi-head self-attention mechanism", "predicted_evidence": ["the model applied multi-head self-attention mechanism to capture the most relevant part of text, and is fine-tuned to maximize the likelihood of the correct entity-relation prediction based on the given text and ground truth. \n\n"]}
{"question_id": "3e88fcc94d0f451e87b65658751834f6103b2030", "predicted_answer": " probability distribution.", "predicted_evidence": ["a soft label is defined as a probability distribution over the set of labels based on the contents of the text. \n\n"]}
{"question_id": "c8cf20afd75eb583aef70fcb508c4f7e37f234e1", "predicted_answer": " no", "predicted_evidence": ["no\n\n"]}
{"question_id": "3567241b3fafef281d213f49f241071f1c60a303", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "643527e94e8eed1e2229915fcf8cd74d769173fc", "predicted_answer": " f1, accuracy", "predicted_evidence": ["we evaluate our models in terms of micro-averaged f1, macro-averaged f1, and accuracy on different combinations of tasks, as well as interpolation of these scores and standard zero-shot evaluation. \n\n"]}
{"question_id": "bfd55ae9630a08a9e287074fff3691dfbffc3258", "predicted_answer": " sequence-to-sequence and multi-task sequential.", "predicted_evidence": ["the authors compare their proposed multi-task model against a sequence-to-sequence language model baseline and a multi-task model without sharing encoder and decoder parameters (called 'multi-task sequential').\n\n"]}
{"question_id": "3a06d40a4bf5ba6e26d9138434e9139a014deb40", "predicted_answer": " duolingo slam shared task dataset.", "predicted_evidence": ["we use the duolingo slam shared task dataset. \n\n"]}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "predicted_answer": " lexical similarity", "predicted_evidence": ["lexically overlapping refers to sentences that share a significant number of words. \n\n"]}
{"question_id": "7d34cdd9cb1c988e218ce0fd59ba6a3b5de2024a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "83db51da819adf6faeb950fe04b4df942a887fb5", "predicted_answer": " online university student response data.", "predicted_evidence": ["the dataset used in this paper is composed of anonymized student response data from a large online university. \n\n"]}
{"question_id": "7e7471bc24970c6f23baff570be385fd3534926c", "predicted_answer": " mlp and cnn", "predicted_evidence": ["the authors use a multilayer perceptron (mlp) and a convolutional neural network (cnn) for multi-class classification of alarming student responses to online assessment.\n\n"]}
{"question_id": "7f958017cbb08962c80e625c2fd7a1e2375f27a3", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper compares the performance of a neural network approach with that of a baseline model, but does not specify what the baseline model is.\n\n"]}
{"question_id": "4130651509403becc468bdbe973e63d3716beade", "predicted_answer": " feedforward and cnn.", "predicted_evidence": ["the paper mentions using a feedforward neural network and a convolutional neural network.\n\n"]}
{"question_id": "6b302280522c350c4d1527d8c6ebc5b470f9314c", "predicted_answer": " unanswerable", "predicted_evidence": ["we could not find any information related to severity identification or metrics for quantification in the document.\n\n"]}
{"question_id": "7da138ec43a88ea75374c40e8491f7975db29480", "predicted_answer": " response times", "predicted_evidence": ["the urgency is identified by time taken by the student to give the response, and the metric used to quantify it is response times.\n\n"]}
{"question_id": "d5d4504f419862275a532b8e53d0ece16e0ae8d1", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f1e70b63c45ab0fc35dc63de089c802543e30c8f", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "4e0df856b39055a9ba801cc9c8e56d5b069bda11", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a7e03d24549961b38e15b5386d9df267900ef4c8", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "036c400424357457e42b22df477b7c3cdc2eefe9", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "63eda2af88c35a507fbbfda0ec1082f58091883a", "predicted_answer": " no", "predicted_evidence": ["no\n\n"]}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "predicted_answer": " neuraltalk2 and soft-attention.", "predicted_evidence": ["the baseline models are: neuraltalk2 and soft-attention. \n\n"]}
{"question_id": "0b1b8e1b583242e5be9b7be73160630a0d4a96b2", "predicted_answer": " mscoco, flickr8k, flickr30k", "predicted_evidence": ["we use popular datasets such as mscoco, flickr8k, and flickr30k to demonstrate the effectiveness of our approach. \n\n"]}
{"question_id": "830f9f9499b06fb4ac3ce2f2cf035127b4f0ec63", "predicted_answer": " unanswerable", "predicted_evidence": ["the document does not provide this information.\n\n"]}
{"question_id": "a606bffed3bfeebd1b66125be580f908244e5d92", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f8fe4049bea86d0518d1881f32049e60526d0f34", "predicted_answer": " mee, pee, mre, pre.", "predicted_evidence": ["the authors compare their proposed framework with four state-of-the-art techniques: morpheme-based entity extraction (mee) technique, part of speech-based entity extraction (pee), morpheme-based relational extraction (mre), and part of speech-based relational extraction (pre). \n\n"]}
{"question_id": "a9eb8039431e2cb885cfcf96eb58c0675b36b3bd", "predicted_answer": " no", "predicted_evidence": ["no, it is generated by combining automatic and manual annotation techniques.\n\n"]}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "predicted_answer": " yes", "predicted_evidence": ["yes, discontiguous entities are included in the sesame dataset.\n\n"]}
{"question_id": "a82686c054b96f214521e468b17f0435e6cdf7cf", "predicted_answer": " over 7.6 million sentences", "predicted_evidence": ["the sesame corpus contains over 7.6 million sentences, and over 2.4 million of them contain named entities. \n\n"]}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "predicted_answer": " unanswerable\n\nexplanation: the paper doesn't mention anything about applying the method to other q&a platforms or languages. therefore, there is not enough information to determine if their method can be transferred to other platforms or languages.", "predicted_evidence": [""]}
{"question_id": "2ae66798333b905172e2c0954e9808662ab7f221", "predicted_answer": " score and rating.", "predicted_evidence": ["we use two measures of quality for an answer. firstly, the score (s) of an answer computed using the technique described in section 3.1 and secondly, the rating r of the quality of the answer given by the asker. \n\n"]}
{"question_id": "9d80ad8cf4d5941a32d33273dc5678195ad1e0d2", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "bd817a520a62ddd77e65e74e5a7e9006cdfb19b3", "predicted_answer": " 3.", "predicted_evidence": ["contextual rnn encoder is a 3-layer bilstm. \n\n"]}
{"question_id": "c635295c2b77aaab28faecca3b5767b0c4ab3728", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7f8fc3c7d59aba80a3e7c839db6892a1fc329210", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "2d92ae6b36567e7edb6afdd72f97b06ac144fbdf", "predicted_answer": " a dataset of twitter messages.", "predicted_evidence": ["the ritter dataset consists of twitter messages that describe many-to-many entities of any type, e.g., people, organizations, and locations. \n\n"]}
{"question_id": "a5df7361ae37b9512fb57cb93efbece9ded8cab1", "predicted_answer": " unanswerable \n\nexplanation: the paper reports that their model in combination with an attention mechanism performs competitively against the state-of-the-art models. however, it does not provide specific comparison metrics against other models.", "predicted_evidence": [""]}
{"question_id": "915e4d0b3cb03789a20380ead961d473cb95bfc3", "predicted_answer": " various", "predicted_evidence": ["various types of features are extracted from the text such as lexical, syntactic, semantic, lexicon-based, and domain specific. \n\n"]}
{"question_id": "c01a8b42fd27b0a3bec717ededd98b6d085a0f5c", "predicted_answer": " unanswerable\n\nexplanation: the abstract and introduction sections do not mention anything related to image features or extraction. it is possible that this information is provided in later sections or in the cited references, but it is not clear from the given document.", "predicted_evidence": [""]}
{"question_id": "8e113fd9661bc8af97e30c75a20712f01fc4520a", "predicted_answer": " svms, mnb, cnn", "predicted_evidence": ["the authors compared their approach with three baseline models - support vector machines (svms), multinomial naive bayes (mnb), and a convolutional neural network (cnn).\n\n"]}
{"question_id": "35e0e6f89b010f34cfb69309b85db524a419c862", "predicted_answer": " the question does not provide enough information or context about \"the three different forms\" that are being referred to in the work. thus, it is unanswerable.", "predicted_evidence": [""]}
{"question_id": "992e67f706c728bc0e534f974c1656da10e7a724", "predicted_answer": " wassa-2018, sarc, ptit-sarc, semeval-2018", "predicted_evidence": ["the authors trained on the wassa-2018 shared task dataset and tested on three datasets: the sarc dataset, the ptit-sarc dataset and the semeval-2018 task 3 dataset. \n\n"]}
{"question_id": "61e96abdc924c34c6b82a587168ea3d14fe792d1", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "ee8a77cddbe492c686f5af3923ad09d401a741b5", "predicted_answer": " no", "predicted_evidence": ["in this study, the authors propose a transformer-based approach to detecting irony and sarcasm in text. the proposed model is trained on two publicly available datasets, namely the semeval-2018 task 3 dataset and the semeval-2021 task 7 dataset, both of which contain tweets annotated for irony and sarcasm. the authors report that their approach achieves competitive performance on both datasets. however, there is no indication that the proposed approach differentiates between metaphor and irony/sarcasm. \n\n"]}
{"question_id": "552b1c813f25bf39ace6cd5eefa56f4e4dd70c84", "predicted_answer": " fake news detection (fnd) and bias detection (bd)", "predicted_evidence": ["we conduct experiments on two fine-grained classification tasks: fake news detection (fnd) and bias detection (bd).\n\n"]}
{"question_id": "1100e442e00c9914538a32aca7af994ce42e1b66", "predicted_answer": " three.", "predicted_evidence": ["the dataset has three categories of fine-grained fake news: (1) headline-only, (2) news article only and (3) headline + news article.\n\n"]}
{"question_id": "82b93ecd2397e417e1e80f93b7cf49c7bd9aeec3", "predicted_answer": " the paper does not provide a specific number. \n\nexplanation: the paper mentions that \"user embeddings are effective in improving performance, significantly outperforming other models\". however, it does not provide a specific number for the gain in performance obtained with user embeddings.", "predicted_evidence": [""]}
{"question_id": "2973fe3f5b4bf70ada02ac4a9087dd156cc3016e", "predicted_answer": " 0.038. \n\nexplanation: the proposed semantic similarity measure outperforms bm25 by 0.038 in terms of map score on the medline dataset.", "predicted_evidence": [""]}
{"question_id": "42269ed04e986ec5dc4164bf57ef306aec4a1ae1", "predicted_answer": " vectors", "predicted_evidence": ["documents are represented as vectors based on the cosine similarity between their terms and the terms of the pubmed queries. \n\n"]}
{"question_id": "31a3ec8d550054465e55a26b0136f4d50d72d354", "predicted_answer": " linear regression", "predicted_evidence": ["they propose to combine bm25 and word embedding similarity using linear regression. \n\n"]}
{"question_id": "49cd18448101da146c3187a44412628f8c722d7b", "predicted_answer": " twitter sentiment analysis treebank (stb)", "predicted_evidence": [""]}
{"question_id": "e9260f6419c35cbd74143f658dbde887ef263886", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "2834a340116026d5995e537d474a47d6a74c3745", "predicted_answer": " 5\n\nexplanation: the five labels to be predicted in sub-task c are: positive, negative, neutral, objective, and mixed.", "predicted_evidence": [""]}
{"question_id": "bd53399be8ff59060792da4c8e42a7fc1e6cbd85", "predicted_answer": " no evidence", "predicted_evidence": [""]}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "predicted_answer": " sequence-to-sequence model", "predicted_evidence": [""]}
{"question_id": "cfe21b979a6c851bdafb2e414622f61e62b1d98c", "predicted_answer": " bert layer followed by self-attention mechanism.", "predicted_evidence": ["the encoder architecture consists of a pre-trained bert layer followed by a one-layer self-attention mechanism. \n\n"]}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "predicted_answer": " english, chinese", "predicted_evidence": ["english and chinese \n\n"]}
{"question_id": "2e37eb2a2a9ad80391e57acb53616eab048ab640", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "54002c15493d4082d352a66fb9465d65bfe9ddca", "predicted_answer": " fusion-based and graph-based architectures.", "predicted_evidence": [""]}
{"question_id": "7caeb5ef6f2985b2cf383cd01765d247c936605f", "predicted_answer": " batch normalization, weight normalization and neural architecture search.", "predicted_evidence": ["the authors also explore other model inference optimization schemes, including batch normalization, weight normalization and neural architecture search approaches. \n\n"]}
{"question_id": "1fcd25e9a63a53451cac9ad2b8a1b529aff44a97", "predicted_answer": " emnlp-asru dataset", "predicted_evidence": ["the dataset used in this study comes from the speech recognition challenge from the 2018 conference on empirical methods in natural language processing (emnlp) workshop on automatic speech recognition and understanding (asru). \n\n"]}
{"question_id": "049415676f8323f4af16d349f36fbcaafd7367ae", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "fee498457774d9617068890ff29528e9fa05a2ac", "predicted_answer": " amazon", "predicted_evidence": ["they evaluate on the amazon benchmark product review dataset. \n\n"]}
{"question_id": "c626637ed14dee3049b87171ddf326115e59d9ee", "predicted_answer": " unanswerable", "predicted_evidence": ["they did not mention any specific approach for domains with few overlapping utterances. \n\n"]}
{"question_id": "b160bfb341f24ae42a268aa18641237a4b3a6457", "predicted_answer": " factor of 0.9.", "predicted_evidence": ["they gradually decrease the confidence by a factor of 0.9 for each negative feedback instance.\n\n"]}
{"question_id": "c0120d339fcdb3833884622e532e7513d1b2c7dd", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "f52c9744a371104eb2677c181a7004f7a77d9dd3", "predicted_answer": " human-robot interaction and natural language processing.", "predicted_evidence": ["the potential applications of this work include human-robot interaction and natural language processing.\n\n"]}
{"question_id": "867b1bb1e6a38de525be7757d49928a132d0dbd8", "predicted_answer": " various methods including random under-sampling, cost-sensitive learning and ensemble learning.", "predicted_evidence": ["we employ a variety of methods to reduce class imbalance, including random under-sampling, cost-sensitive learning, and ensemble learning.\n\n"]}
{"question_id": "6167618e0c53964f3a706758bdf5e807bc5d7760", "predicted_answer": " handling biases, reasoning beyond memorization, dealing with out-of-distribution samples.", "predicted_evidence": ["the paper discusses several remaining challenges in vqa, including handling visual and language biases, reasoning beyond memorization, and dealing with out-of-distribution samples.\n\n"]}
{"question_id": "78a0c25b83cdeaeaf0a4781f502105a514b2af0e", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "08202b800a946b8283c2684e23b51c0ec1e8b2ac", "predicted_answer": " stacked attention networks, dynamic co-attention networks, hiecoattention, relation networks, and visual coreference resolution.", "predicted_evidence": ["the paper discusses multiple new deep learning models including: stacked attention networks, dynamic co-attention networks, hiecoattention, relation networks, and visual coreference resolution. \n\n"]}
{"question_id": "00aea97f69290b496ed11eb45a201ad28d741460", "predicted_answer": " unanswerable\n\nexplanation: the paper discusses multiple models that participated in the vqa 2017 challenge, but does not provide information specifically on the architecture of the winning model.", "predicted_evidence": [""]}
{"question_id": "4e1293592e41646a6f5f0cb00c75ee8de14eb668", "predicted_answer": " what is the color of a zebra?", "predicted_evidence": ["according to the paper, \"what is the color of a zebra?\" is an example of a common sense question. \n\n"]}
{"question_id": "15aeda407ae3912419fd89211cdb98989d9cde58", "predicted_answer": " bert and elmo.", "predicted_evidence": ["the paper proposes a model that leverages pre-trained language representations, such as bert and elmo, to improve few-shot text classification. \n\n"]}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "predicted_answer": " 5-way, 1-shot and 5-way, 5-shot experiments were conducted on 5 datasets. the number of instances explored is not provided in the document. \n\nexplanation: the paper conducts few-shot experiments on 5 datasets - cola, sst-2, mrpc, sts-b, and qqp - in both 1-shot and 5-shot settings, with 5 random seeds for each experiment. the number of instances explored in each experiment is not mentioned.", "predicted_evidence": [""]}
{"question_id": "c24f7c030010ad11e71ef4912fd79093503f3a8d", "predicted_answer": " sentiment analysis and topic classification.", "predicted_evidence": ["the paper explores sentiment analysis and topic classification tasks. \n\n"]}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "predicted_answer": " unanswerable\".", "predicted_evidence": ["there is no information provided in the document about the training time compared to the original position encoding, so the answer is \""]}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "predicted_answer": " no", "predicted_evidence": ["the paper states that the new relative position encoder \"adds only constant number of parameters to each layer\". \n\n"]}
{"question_id": "d48b5e4a7cf1f96c5b939ba9b46350887c5e5268", "predicted_answer": " yes", "predicted_evidence": ["yes, the authors state that the relative position representation can be used in any model that uses self-attention. \n\n"]}
{"question_id": "84327a0a9321bf266e22d155dfa94828784595ce", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "c2037887945abbdf959389dc839a86bc82594505", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5b2839bef513e5d441f0bb8352807f673f4b2070", "predicted_answer": " precision, recall, f1-score, and accuracy", "predicted_evidence": ["for evaluation, the mean values of all metrics are reported over 5 folds of cross-validation which are precision, recall, f1-score, and accuracy for all the code-mixed languages.\n \n"]}
{"question_id": "2abf916bc03222d3b2a3d66851d87921ff35c0d2", "predicted_answer": " sail code-mixed 2017 hindi-english dataset, sail code-mixed 2017 telugu-english dataset, sail code-mixed 2017 bengali-english dataset, sail code-mixed 2017 tamil-english dataset.", "predicted_evidence": ["sail code-mixed 2017 hindi-english dataset, sail code-mixed 2017 telugu-english dataset, sail code-mixed 2017 bengali-english dataset, sail code-mixed 2017 tamil-english dataset.\n\n"]}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "predicted_answer": " unanswerable", "predicted_evidence": ["the semantic web is not explicitly defined in the document.\n\n"]}
{"question_id": "7de0b2df60d3161dd581ed7915837d460020bc11", "predicted_answer": " 113", "predicted_evidence": ["the ene tag set consists of 113 tags: 79 named entity tags and 34 non-named entity tags. \n\n"]}
{"question_id": "0a3a7e412682ce951329c37b06343d2114acad9d", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence. \n\n"]}
{"question_id": "74cc0300e22f60232812019011a09df92bbec803", "predicted_answer": " yes", "predicted_evidence": ["yes\n\n"]}
{"question_id": "865811dcf63a1dd3f22c62ec39ffbca4b182de31", "predicted_answer": " 2000 patterns", "predicted_evidence": ["the authors extracted and analyzed 2000 patterns from their dataset to determine which ones were most discriminative. \n\n"]}
{"question_id": "667dce60255d8ab959869eaf8671312df8c0004b", "predicted_answer": " affective words and rhetorical questions.", "predicted_evidence": ["they used a rule-based approach to extract patterns that were correlated with emotional arguments in online dialogue, including the use of affective words and rhetorical questions. \n\n"]}
{"question_id": "1fd31fdfff93d65f36e93f6919f6976f5f172197", "predicted_answer": " based on sentiment", "predicted_evidence": ["the factual and emotional posts were annotated by assigning emotion and factuality labels (happy/sad;facts/opinions) based on the overall sentiment of the text. \n\n"]}
{"question_id": "d2d9c7177728987d9e8b0c44549bbe03c8c00ef2", "predicted_answer": " bleu, chrf++, wer, ter", "predicted_evidence": ["we adopt introduced several approaches to measure the machine translation quality for african languages. specifically, we used bleu, chrf++, wer, and ter. \n\n"]}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "predicted_answer": " pre-processing, data balancing, multi-task learning, deep architecture and ensemble methods, teacher forcing, attention model, beam search, and back-translation.", "predicted_evidence": ["the authors explored techniques such as pre-processing, data balancing, multi-task learning, deep architecture and ensemble methods, teacher forcing, attention model, beam search, and back-translation. \n\n"]}
{"question_id": "175cddfd0bcd77b7327b62f99e57d8ea93f8d8ba", "predicted_answer": " unanswerable", "predicted_evidence": ["according to the paper, the best model varied depending on the language pair being translated. \n\n"]}
{"question_id": "f0afc116809b70528226d37190e8e79e1e9cd11e", "predicted_answer": " swahili-english, somali-english, kinyarwanda-english, isizulu-english, hausa-english.", "predicted_evidence": ["they used five pairs of languages: swahili-english, somali-english, kinyarwanda-english, isizulu-english, and hausa-english.\n\n"]}
{"question_id": "3588988f2230f3329d7523fbb881b20bf177280d", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence.\n\n"]}
{"question_id": "78f8dad0f1acf024f69b3218b2d204b8019bb0d2", "predicted_answer": " a/b testing, survey", "predicted_evidence": ["user satisfaction is estimated through online a/b testing and also through the post-conversation survey. \n\n"]}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "d64fa192a7e9918c6a22d819abad581af0644c7d", "predicted_answer": " language modeling", "predicted_evidence": ["the paper proposes a framework to learn multiple related tasks through meta-learning and study the effects of model initialization with shared meta-knowledge. a new dataset was introduced for language modeling in this work.\n\n"]}
{"question_id": "788f70a39c87abf534f4a9ee519f6e5dbf2543c2", "predicted_answer": " maml", "predicted_evidence": ["they use maml (model-agnostic meta-learning) algorithm to train their model for meta multi-task learning. \n\n"]}
{"question_id": "3d1ad8a4aaa2653d0095bafba74738bd20795acf", "predicted_answer": " davidson and founta.", "predicted_evidence": ["we used two datasets: davidson and founta. \n\n"]}
{"question_id": "ec54ae2f4811196fcaafa45e76130239e69995f9", "predicted_answer": " attention, kimcnn, bert.", "predicted_evidence": ["they compared their method with three other baselines: a bi-lstm with self-attention-based approach (attention), a cnn-based approach (kimcnn), and a pre-trained bert-based approach (bert).\n  \n"]}
{"question_id": "5102dc911913e9ca0311253e44fd31c73eed0a57", "predicted_answer": " bag-of-words, tf-idf and fasttext", "predicted_evidence": ["the authors used three text embedding methods to compare the performance of the proposed approach: bag-of-words, tf-idf and fasttext. \n\n"]}
{"question_id": "fcdafaea5b1c9edee305b81f6865efc8b8dc50d3", "predicted_answer": " sraa, mr, cr, subj, mpqa, trec, sst-2, sst-5, and mrpc.", "predicted_evidence": ["we tested on nine benchmark datasets: sraa, mr, cr, subj, mpqa, trec, sst-2, sst-5, and mrpc. \n\n"]}
{"question_id": "91d4fd5796c13005fe306bcd895caaed7fa77030", "predicted_answer": " sentiment classification, news classification, and three others.", "predicted_evidence": ["in the abstract, it is stated that the proposed model is trained on five different text classification tasks, including sentiment classification and news classification.\n\n"]}
{"question_id": "27d7a30e42921e77cfffafac5cb0d16ce5a7df99", "predicted_answer": " recurrent", "predicted_evidence": ["recurrent neural architecture is explored. \n\n"]}
{"question_id": "7561bd3b8ba7829b3a01ff07f9f3e93a7b8869cc", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "a3ba21341f0cb79d068d24de33b23c36fa646752", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "5bfbc9ca7fd41be9627f6ef587bb7e21c7983be0", "predicted_answer": " 119,292 documents", "predicted_evidence": ["the gamewikisum dataset contains 119,292 documents.\n\n"]}
{"question_id": "5181527e6a61a9a192db5f8064e56ec263c42661", "predicted_answer": " unanswerable.", "predicted_evidence": ["the document doesn't provide clear information on the language of the system. \n\n"]}
{"question_id": "334aa5540c207768931a0fe78aa4981a895ba37c", "predicted_answer": " f1-score and mrr for entity and answer extraction. sus and a post-task questionnaire for user satisfaction.", "predicted_evidence": ["the metrics used for evaluation are the f1-score and mean reciprocal rank (mrr) for entity and answer extraction. additionally, user satisfaction is measured using the system usability scale (sus) and a post-task questionnaire.\n\n"]}
{"question_id": "fea9b4d136156f23a88e5c7841874a467f2ba86d", "predicted_answer": " yes", "predicted_evidence": ["the authors experiment with a small, parallel corpus of only 2000 sentences and show that the decoder has a much higher accuracy than the encoder on this task.\n  \n"]}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "predicted_answer": " mutual information.\n\nexplanation: they measure the conditional information strength between the source and target words by using mutual information.", "predicted_evidence": [""]}
{"question_id": "7ef7a5867060f91eac8ad857c186e51b767c734b", "predicted_answer": " they generate input noise by masking tokens randomly. \n\nexplanation: in section 4.2, the authors explain that they generate input noise by masking tokens randomly, as opposed to adding impulsive noise. they also mention that they experimented with adding noise, but found that it degraded the translation quality too much.", "predicted_evidence": [""]}
{"question_id": "0b10cfa61595b21bf3ff13b4df0fe1c17bbbf4e9", "predicted_answer": " the joint training is performed by incorporating two individual tasks, i.e., entity recognition and relation classification, into one multi-task learning objective.", "predicted_evidence": [""]}
{"question_id": "67104a5111bf8ea626532581f20b33b851b5abc1", "predicted_answer": " 108m", "predicted_evidence": ["the model architecture and setup details mention that their model has 108m parameters.\n\n"]}
{"question_id": "1d40d177c5e410cef1142ec9a5fab9204db22ae1", "predicted_answer": " unanswerable", "predicted_evidence": ["previous state-of-the-art models are not mentioned in the document. \n\n"]}
{"question_id": "344238de7208902f7b3a46819cc6d83cc37448a0", "predicted_answer": " yes", "predicted_evidence": ["yes, the survey identified several features commonly found to be predictive of abusive content on online platforms, including profanity, hate speech, and personal attacks. \n\n"]}
{"question_id": "56bbca3fe24c2e9384cc57f55f35f7f5ad5c5716", "predicted_answer": " no", "predicted_evidence": [""]}
{"question_id": "4c40fa01f626def0b69d1cb7bf9181b574ff6382", "predicted_answer": " 13 different datasets were used.", "predicted_evidence": ["we conduct a detailed review of 63 recent works that report results on 13 different datasets. these datasets differ in their nature, size, and type of abusive content, and are used for a range of tasks including but not limited to: detecting abusive accounts, identifying abusive language, sentiment analysis, topic modeling, and predicting aggression and cyberbullying. \n\n"]}
{"question_id": "71b29ab3ddcdd11dcc63b0bb55e75914c07a2217", "predicted_answer": " broadly", "predicted_evidence": ["abuse is defined broadly as any negative behavior that targets an individual or group, including harassment, hate speech, and cyberbullying. \n\n"]}
{"question_id": "22225ba18a6efe74b1315cc08405011d5431498e", "predicted_answer": " yes", "predicted_evidence": ["yes, they used external financial knowledge in their approach by leveraging relevant financial indicators and forecasting data. \n\n"]}
{"question_id": "9c529bd3f7565b2178a79aae01c98c90f9d372ad", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "cf82251a6a5a77e29627560eb7c05c3eddc20825", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "b1fe6a39b474933038b44b6d45e5ca32af7c3e36", "predicted_answer": " english, french and german. \n\nexplanation: the paper conducted translation experiments on three different language pairs: english-german, english-french and german-french.", "predicted_evidence": [""]}
{"question_id": "919681faa9731057b3fae5052b7da598abd3e04b", "predicted_answer": " word error rate (wer) and gender bias metric (gbm)", "predicted_evidence": ["they used word error rate (wer) and gender bias metric (gbm), a metric that measures the gender bias in the translation of gender-specific pronouns.\n\n"]}
{"question_id": "2749fb1725a2c4bdba5848e2fc424a43e7c4be51", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "7239c02a0dcc0c3c9d9cddb5e895bcf9cfcefee6", "predicted_answer": " bookcorpus and english wikipedia", "predicted_evidence": ["the authors use two different datasets in their experiments: the bookcorpus and the english wikipedia.\n\n"]}
{"question_id": "9dcc10a4a325d4c9cb3bb8134831ee470be47e93", "predicted_answer": " sentiment analysis, question classification, paraphrase detection, and textual entailment.", "predicted_evidence": ["they evaluate supervised systems on tasks such as sentiment analysis, question classification, paraphrase detection, and textual entailment.\n\n"]}
{"question_id": "31236a876277c6e1c80891a3293c105a1b1be008", "predicted_answer": " yes", "predicted_evidence": ["they evaluate domain portability by comparing the performance of their model on different datasets from different domains.\n\n"]}
{"question_id": "19ebfba9aa5a9596b09a0cfb084ff8ebf24a3b91", "predicted_answer": " predicting order of sentences and reconstructing a document.", "predicted_evidence": ["our model is trained by framing unsupervised learning as a sequence to sequence language model task, which forces the model to learn to predict the surrounding sentences given a sentence or vice versa. additionally, we introduced two unsupervised learning objectives to encourage our model to learn sentence representations that contain global semantic information. these objectives are: 1) predicting the order of two sentences (section 3.2.1), and 2) reconstructing a document given a corrupted version (section 3.2.2).\n\n"]}
{"question_id": "2288f567d2f5cfbfc5097d8eddf9abd238ffbe25", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "caebea05935cae1f5d88749a2fc748e62976eab7", "predicted_answer": " yes", "predicted_evidence": ["yes, they compare their maximum entropy model with other models such as svm, decision tree, and random forest. \n\n"]}
{"question_id": "e381f1811774806be109f9b05896a2a3c5e1ef43", "predicted_answer": " pipeline and joint models", "predicted_evidence": ["the authors compare their model to both pipeline and joint models. \n\n"]}
{"question_id": "9eec16e560f9ccafd7ba6f1e0db742b330b42ba9", "predicted_answer": " rst discourse treebank", "predicted_evidence": ["the authors trained and evaluated on the rst discourse treebank. \n\n"]}
{"question_id": "ec70c7c560e08cff2820bad93f5216bc0a469f5a", "predicted_answer": " different settings to study domain shift in extractive summarization.", "predicted_evidence": ["table 1 describes different experiments we performed to study domain shift in extractive summarization.  \n\n"]}
{"question_id": "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "predicted_answer": " news and biomedical", "predicted_evidence": ["they explored two domains: news and biomedical documents.\n\n"]}
{"question_id": "0b1cc6c0de286eb724b1fd18dbc93e67ab89a236", "predicted_answer": "  multi-news dataset \n\nexplanation: the multi-news dataset is repurposed by filtering it out into different domains for evaluation purposes in this paper.", "predicted_evidence": [""]}
{"question_id": "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "predicted_answer": " unanswerable. \n\nexplanation: the document provided does not mention any specific four learning strategies investigated.", "predicted_evidence": [""]}
{"question_id": "654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "predicted_answer": " unanswerable (not enough information provided in the document)", "predicted_evidence": [""]}
{"question_id": "5a7d1ae6796e09299522ebda7bfcfad312d6d128", "predicted_answer": " both language-independent models and models trained specifically for finnish", "predicted_evidence": ["they explored both language-independent models such as multilingual bert and models trained specifically for finnish. \n\n"]}
{"question_id": "bd191d95806cee4cf80295e9ce1cd227aba100ab", "predicted_answer": " wikipedia and web-crawl data.", "predicted_evidence": ["the new finnish model was trained on finnish wikipedia and finnish web-crawl data.\n\n"]}
{"question_id": "a9cae57f494deb0245b40217d699e9a22db0ea6e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "0a736e0e3305a50d771dfc059c7d94b8bd27032e", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "283d358606341c399e369f2ba7952cd955326f73", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "predicted_answer": " unanswerable.", "predicted_evidence": ["the paper does not provide information on eight predefined categories.\n\n"]}
{"question_id": "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c", "predicted_answer": " yes", "predicted_evidence": [""]}
{"question_id": "bba70f3cf4ca1e0bb8c4821e3339c655cdf515d6", "predicted_answer": " yes\n\nexplanation: according to the paper, the proposed model outperforms the baseline in most cases. however, it is mentioned that the baseline outperforms the proposed method in predicting the ratings of ted talks with a high number of views.", "predicted_evidence": [""]}
{"question_id": "c5f9894397b1a0bf6479f5fd9ee7ef3e38cfd607", "predicted_answer": " unanswerable.", "predicted_evidence": ["no evidence.\n\n"]}
{"question_id": "9f8c0e02a7a8e9ee69f4c1757817cde85c7944bd", "predicted_answer": " causality.", "predicted_evidence": ["to capture the causal relationship between the variables. \n\n"]}
{"question_id": "6cbbedb34da50286f44a0f3f6312346e876e2be5", "predicted_answer": " unanswerable", "predicted_evidence": ["we do not have enough information in the document to answer this question.\n\n"]}
{"question_id": "173060673cb15910cc310058bbb9750614abda52", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "98c8ed9019e43839ffb53a714bc37fbb1c28fe2c", "predicted_answer": " insufficient information. \n\nexplanation: there is no information provided in the document about how the speakers' reputations bias the dataset.", "predicted_evidence": [""]}
{"question_id": "50c441a9cc7345a0fa408d1ce2e13f194c1e82a8", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper suggests various approaches, but does not specifically mention a state-of-the-art approach.\n\n"]}
{"question_id": "2895a3fc63f6f403445c11043460584e949fb16c", "predicted_answer": " number of bits.", "predicted_evidence": ["the description length is defined as the number of bits required to represent the verb given the state and arguments. \n\n"]}
{"question_id": "1e7e3f0f760cd628f698b73d82c0f946707855ca", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence \n\n"]}
{"question_id": "deed225dfa94120fafcc522d4bfd9ea57085ef8d", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "predicted_answer": " no", "predicted_evidence": [""]}
{"question_id": "242c626e89bca648b65af135caaa7ceae74e9720", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "bba677d1a1fe38a41f61274648b386bdb44f1851", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "b6c2a391c4a94eaa768150f151040bb67872c0bf", "predicted_answer": " unanswerable", "predicted_evidence": ["no evidence\n\n"]}
{"question_id": "06d5de706348dbe8c29bfacb68ce65a2c55d0391", "predicted_answer": " unanswerable", "predicted_evidence": [""]}
{"question_id": "6014c2219d29bae17279625716e7c2a1f8a2bd05", "predicted_answer": " unanswerable", "predicted_evidence": ["the paper does not mention any \"old method\". \n\n"]}
{"question_id": "9be9354eeb2bb1827eeb1e23a20cfdca59fb349a", "predicted_answer": " the system recommends features based on the input text and the task type, using pre-defined sets of text features for each task type, and returns the corresponding feature values. \n\nexplanation: autonlp uses machine learning to recommend the most effective set of features for a particular text analytics task based on an input text. it leverages a pre-defined set of features for each task type such as sentiment analysis, document classification, named entity recognition, and more, then returns corresponding feature values.", "predicted_evidence": [""]}
